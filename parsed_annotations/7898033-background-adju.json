[{"paper_id": "7898033", "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "background_label": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach.", "abstract": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach."}, {"paper_id": "2468783", "adju_relevance": 3, "title": "Similarity of Semantic Relations", "background_label": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus.", "method_label": "This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs.", "result_label": "LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.", "abstract": "There are at least two kinds of similarity. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM."}, {"paper_id": "5052538", "adju_relevance": 3, "title": "Learning Analogies and Semantic Relations", "background_label": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal analogy has the form A:B::C:D, meaning\"A is to B as C is to D\"; for example, mason:stone::carpenter:wood.", "method_label": "SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct).", "abstract": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal analogy has the form A:B::C:D, meaning\"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct)."}, {"paper_id": "9322367", "adju_relevance": 3, "title": "Corpus-based Learning of Analogies and Semantic Relations", "background_label": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning\"A is to B as C is to D\"; for example, mason:stone::carpenter:wood.", "method_label": "SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly).", "abstract": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning\"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly)."}, {"paper_id": "17792779", "adju_relevance": 2, "title": "Taking Antonymy Mask off in Vector Space", "background_label": "AbstractAutomatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. However, current unsupervised approaches to antonymy detection are still not fully effective because they cannot discriminate antonyms from synonyms.", "method_label": "In this paper, we introduce APAnt, a new AveragePrecision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). APAnt makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words.", "result_label": "Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis.", "abstract": "AbstractAutomatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. AbstractAutomatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. However, current unsupervised approaches to antonymy detection are still not fully effective because they cannot discriminate antonyms from synonyms. In this paper, we introduce APAnt, a new AveragePrecision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). In this paper, we introduce APAnt, a new AveragePrecision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). APAnt makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis."}, {"paper_id": "8494712", "adju_relevance": 2, "title": "Word Embedding-based Antonym Detection using Thesauri and Distributional Information", "background_label": "Word embeddings have shown to capture synonyms and analogies. Such word embeddings, however, cannot capture antonyms since they depend on the distributional hypothesis.", "method_label": "Our approach utilizes supervised synonym and antonym information from thesauri, as well as distributional information from large-scale unlabelled text data.", "result_label": "The evaluation results on the GRE antonym question task show that our model outperforms the state-of-the-art systems and it can answer the antonym questions in the F-score of 89%.", "abstract": " Word embeddings have shown to capture synonyms and analogies. Word embeddings have shown to capture synonyms and analogies. Such word embeddings, however, cannot capture antonyms since they depend on the distributional hypothesis. Our approach utilizes supervised synonym and antonym information from thesauri, as well as distributional information from large-scale unlabelled text data. The evaluation results on the GRE antonym question task show that our model outperforms the state-of-the-art systems and it can answer the antonym questions in the F-score of 89%."}, {"paper_id": "29074", "adju_relevance": 2, "title": "Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems", "background_label": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions.", "abstract": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions."}, {"paper_id": "17342054", "adju_relevance": 1, "title": "A VSM-based Statistical Model for the Semantic Relation Interpretation of Noun-Modifier Pairs", "background_label": "AbstractThe paper addresses the task of automatic interpretation of semantic relation in noun compounds. The problem has been attempted with both Ontology-based and Statistical approaches, but both approaches having their own limitations.", "method_label": "We present a novel VSMbased statistical model which represents each relation with a weighted vector of prepositional and verbal paraphrases. The model ranks the paraphrases on their relevance and assigns higher weights to more relevant paraphrases.", "result_label": "The performance of the model is compared with the Ontology model and the results are quite encouraging. We finally propose a Hybrid of the two models which compares on par with the best performing systems on Nastase and Szpakowicz (2003) dataset.", "abstract": "AbstractThe paper addresses the task of automatic interpretation of semantic relation in noun compounds. AbstractThe paper addresses the task of automatic interpretation of semantic relation in noun compounds. The problem has been attempted with both Ontology-based and Statistical approaches, but both approaches having their own limitations. We present a novel VSMbased statistical model which represents each relation with a weighted vector of prepositional and verbal paraphrases. We present a novel VSMbased statistical model which represents each relation with a weighted vector of prepositional and verbal paraphrases. The model ranks the paraphrases on their relevance and assigns higher weights to more relevant paraphrases. The performance of the model is compared with the Ontology model and the results are quite encouraging. The performance of the model is compared with the Ontology model and the results are quite encouraging. We finally propose a Hybrid of the two models which compares on par with the best performing systems on Nastase and Szpakowicz (2003) dataset."}, {"paper_id": "8073851", "adju_relevance": 1, "title": "Using the Wiktionary Graph Structure for Synonym Detection", "background_label": "This paper presents our work on using the graph structure of Wiktionary for synonym detection.", "method_label": "We implement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be \"close\" to a given vertex.", "result_label": "Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures.", "abstract": "This paper presents our work on using the graph structure of Wiktionary for synonym detection. We implement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be \"close\" to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures."}, {"paper_id": "1796933", "adju_relevance": 1, "title": "Which Noun Phrases Denote Which Concepts?", "background_label": "AbstractResolving polysemy and synonymy is required for high-quality information extraction.", "abstract": "AbstractResolving polysemy and synonymy is required for high-quality information extraction."}, {"paper_id": "13245557", "adju_relevance": 1, "title": "Exploring relation types for literature-based discovery", "background_label": "Many LBD approaches use simple techniques to identify semantically weak relations between concepts, for example, document co-occurrence. Experiments were carried out comparing a number of techniques for identifying relations.", "method_label": "These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. Such approaches generate fewer hypotheses, but may miss hidden knowledge. The authors investigate this trade-off in detail, comparing techniques for identifying related concepts to discover which are most suitable for LBD. MATERIALS AND METHODS A generic LBD system that can utilize a range of relation types was developed. Two approaches were used for evaluation: replication of existing discoveries and the \"time slicing\" approach. (1) RESULTS: Previous LBD discoveries could be replicated using relations based either on document co-occurrence or linguistic analysis.", "result_label": "Using relations based on linguistic analysis generated many fewer hypotheses, but a significantly greater proportion of them were candidates for hidden knowledge. DISCUSSION AND CONCLUSION The use of linguistic analysis-based relations improves accuracy of LBD without overly damaging coverage. LBD systems often generate huge numbers of hypotheses, which are infeasible to manually review. Improving their accuracy has the potential to make these systems significantly more usable.", "abstract": " Many LBD approaches use simple techniques to identify semantically weak relations between concepts, for example, document co-occurrence. These generate huge numbers of hypotheses, difficult for humans to assess. These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. Such approaches generate fewer hypotheses, but may miss hidden knowledge. These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. Such approaches generate fewer hypotheses, but may miss hidden knowledge. The authors investigate this trade-off in detail, comparing techniques for identifying related concepts to discover which are most suitable for LBD. These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. Such approaches generate fewer hypotheses, but may miss hidden knowledge. The authors investigate this trade-off in detail, comparing techniques for identifying related concepts to discover which are most suitable for LBD. MATERIALS AND METHODS A generic LBD system that can utilize a range of relation types was developed. Many LBD approaches use simple techniques to identify semantically weak relations between concepts, for example, document co-occurrence. Experiments were carried out comparing a number of techniques for identifying relations. These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. Such approaches generate fewer hypotheses, but may miss hidden knowledge. The authors investigate this trade-off in detail, comparing techniques for identifying related concepts to discover which are most suitable for LBD. MATERIALS AND METHODS A generic LBD system that can utilize a range of relation types was developed. Two approaches were used for evaluation: replication of existing discoveries and the \"time slicing\" approach. These generate huge numbers of hypotheses, difficult for humans to assess. More complex techniques rely on linguistic analysis, for example, shallow parsing, to identify semantically stronger relations. Such approaches generate fewer hypotheses, but may miss hidden knowledge. The authors investigate this trade-off in detail, comparing techniques for identifying related concepts to discover which are most suitable for LBD. MATERIALS AND METHODS A generic LBD system that can utilize a range of relation types was developed. Two approaches were used for evaluation: replication of existing discoveries and the \"time slicing\" approach. (1) RESULTS: Previous LBD discoveries could be replicated using relations based either on document co-occurrence or linguistic analysis. Using relations based on linguistic analysis generated many fewer hypotheses, but a significantly greater proportion of them were candidates for hidden knowledge. Using relations based on linguistic analysis generated many fewer hypotheses, but a significantly greater proportion of them were candidates for hidden knowledge. DISCUSSION AND CONCLUSION The use of linguistic analysis-based relations improves accuracy of LBD without overly damaging coverage. Using relations based on linguistic analysis generated many fewer hypotheses, but a significantly greater proportion of them were candidates for hidden knowledge. DISCUSSION AND CONCLUSION The use of linguistic analysis-based relations improves accuracy of LBD without overly damaging coverage. LBD systems often generate huge numbers of hypotheses, which are infeasible to manually review. Using relations based on linguistic analysis generated many fewer hypotheses, but a significantly greater proportion of them were candidates for hidden knowledge. DISCUSSION AND CONCLUSION The use of linguistic analysis-based relations improves accuracy of LBD without overly damaging coverage. LBD systems often generate huge numbers of hypotheses, which are infeasible to manually review. Improving their accuracy has the potential to make these systems significantly more usable."}, {"paper_id": "6708547", "adju_relevance": 1, "title": "Solving Analogies on Words: An Algorithm", "background_label": "To introduce the algorithm presented in this paper, we take a path that is inverse to the historical development of the idea of analogy (see (Hoffman 95)). This is necessary, because a certain incomprehension is faced when speaking about linguistic analogy, i.e., it is generally given a broader and more psychological definition. Also, with our proposal being computational, it is impossible to ignore works about analogy in computer science, which has come to mean artificial intelligence.", "abstract": "To introduce the algorithm presented in this paper, we take a path that is inverse to the historical development of the idea of analogy (see (Hoffman 95)). To introduce the algorithm presented in this paper, we take a path that is inverse to the historical development of the idea of analogy (see (Hoffman 95)). This is necessary, because a certain incomprehension is faced when speaking about linguistic analogy, i.e., it is generally given a broader and more psychological definition. To introduce the algorithm presented in this paper, we take a path that is inverse to the historical development of the idea of analogy (see (Hoffman 95)). This is necessary, because a certain incomprehension is faced when speaking about linguistic analogy, i.e., it is generally given a broader and more psychological definition. Also, with our proposal being computational, it is impossible to ignore works about analogy in computer science, which has come to mean artificial intelligence."}, {"paper_id": "12998616", "adju_relevance": 1, "title": "Near-Synonymy And Lexical Choice", "background_label": "We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation.", "abstract": "We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation."}, {"paper_id": "14184076", "adju_relevance": 1, "title": "Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network", "background_label": "Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations.", "method_label": "In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature.", "result_label": "The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods.", "abstract": "Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods."}, {"paper_id": "13179349", "adju_relevance": 1, "title": "Context, cortex, and associations: a connectionist developmental approach to verbal analogies", "background_label": "We present a PDP model of binary choice verbal analogy problems (A:B as C:[D1|D2], where D1 and D2 represent choice alternatives).", "method_label": "We train a recurrent neural network in item-relation-item triples and use this network to test performance on analogy questions. Without training on analogy problems per se, the model explains the developmental shift from associative to relational responding as an emergent consequence of learning upon the environment's statistics. Such learning allows gradual, item-specific acquisition of relational knowledge to overcome the influence of unbalanced association frequency, accounting for association effects of analogical reasoning seen in cognitive development.", "result_label": "The network also captures the overall degradation in performance after anterior temporal damage by deleting a fraction of learned connections, while capturing the return of associative dominance after frontal damage by treating frontal structures as necessary for maintaining activation of A and B while seeking a relation between C and D. While our theory is still far from being complete it provides a unified explanation of findings that need to be considered together in any integrated account of analogical reasoning.", "abstract": "We present a PDP model of binary choice verbal analogy problems (A:B as C:[D1|D2], where D1 and D2 represent choice alternatives). We train a recurrent neural network in item-relation-item triples and use this network to test performance on analogy questions. We train a recurrent neural network in item-relation-item triples and use this network to test performance on analogy questions. Without training on analogy problems per se, the model explains the developmental shift from associative to relational responding as an emergent consequence of learning upon the environment's statistics. We train a recurrent neural network in item-relation-item triples and use this network to test performance on analogy questions. Without training on analogy problems per se, the model explains the developmental shift from associative to relational responding as an emergent consequence of learning upon the environment's statistics. Such learning allows gradual, item-specific acquisition of relational knowledge to overcome the influence of unbalanced association frequency, accounting for association effects of analogical reasoning seen in cognitive development. The network also captures the overall degradation in performance after anterior temporal damage by deleting a fraction of learned connections, while capturing the return of associative dominance after frontal damage by treating frontal structures as necessary for maintaining activation of A and B while seeking a relation between C and D. While our theory is still far from being complete it provides a unified explanation of findings that need to be considered together in any integrated account of analogical reasoning."}, {"paper_id": "2010697", "adju_relevance": 1, "title": "Polarity Inducing Latent Semantic Analysis", "background_label": "AbstractExisting vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy.", "method_label": "We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one.We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus -a word sense along with its synonyms and antonyms -is treated as a \"document,\" and the resulting document collection is subjected to LSA.", "result_label": "The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property.We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure.", "abstract": "AbstractExisting vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one.We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one.We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus -a word sense along with its synonyms and antonyms -is treated as a \"document,\" and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property.We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property.We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property.We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure."}, {"paper_id": "1359050", "adju_relevance": 1, "title": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy", "abstract": ""}, {"paper_id": "10202222", "adju_relevance": 1, "title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "background_label": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks.", "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks."}, {"paper_id": "10544909", "adju_relevance": 1, "title": "Retrofitting Concept Vector Representations of Medical Concepts to Improve Estimates of Semantic Similarity and Relatedness", "background_label": "Estimation of semantic similarity and relatedness between biomedical concepts has utility for many informatics applications. Methods in the former category disregard taxonomic structure, while those in the latter fail to consider semantically relevant empirical information.", "method_label": "Automated methods fall into two categories: methods based on distributional statistics drawn from text corpora, and methods using the structure of existing knowledge resources. In this paper, we present a method that retrofits distributional context vector representations of biomedical concepts using structural information from the UMLS Metathesaurus, such that the similarity between vector representations of linked concepts is augmented. We evaluated it on the UMNSRS benchmark.", "result_label": "Our results demonstrate that retrofitting of concept vector representations leads to better correlation with human raters for both similarity and relatedness, surpassing the best results reported to date. They also demonstrate a clear improvement in performance on this reference standard for retrofitted vector representations, as compared to those without retrofitting.", "abstract": "Estimation of semantic similarity and relatedness between biomedical concepts has utility for many informatics applications. Automated methods fall into two categories: methods based on distributional statistics drawn from text corpora, and methods using the structure of existing knowledge resources. Estimation of semantic similarity and relatedness between biomedical concepts has utility for many informatics applications. Methods in the former category disregard taxonomic structure, while those in the latter fail to consider semantically relevant empirical information. Automated methods fall into two categories: methods based on distributional statistics drawn from text corpora, and methods using the structure of existing knowledge resources. In this paper, we present a method that retrofits distributional context vector representations of biomedical concepts using structural information from the UMLS Metathesaurus, such that the similarity between vector representations of linked concepts is augmented. Automated methods fall into two categories: methods based on distributional statistics drawn from text corpora, and methods using the structure of existing knowledge resources. In this paper, we present a method that retrofits distributional context vector representations of biomedical concepts using structural information from the UMLS Metathesaurus, such that the similarity between vector representations of linked concepts is augmented. We evaluated it on the UMNSRS benchmark. Our results demonstrate that retrofitting of concept vector representations leads to better correlation with human raters for both similarity and relatedness, surpassing the best results reported to date. Our results demonstrate that retrofitting of concept vector representations leads to better correlation with human raters for both similarity and relatedness, surpassing the best results reported to date. They also demonstrate a clear improvement in performance on this reference standard for retrofitted vector representations, as compared to those without retrofitting."}, {"paper_id": "14624577", "adju_relevance": 1, "title": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals", "background_label": "The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text.", "abstract": "The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text."}, {"paper_id": "682154", "adju_relevance": 1, "title": "Solving Relational Similarity Problems Using the Web as a Corpus", "background_label": "AbstractWe present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. The approach leverages the vast size of the Web in order to build lexically-specific features.", "abstract": "AbstractWe present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. AbstractWe present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. The approach leverages the vast size of the Web in order to build lexically-specific features."}, {"paper_id": "8570237", "adju_relevance": 1, "title": "Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy", "background_label": "AbstractWe are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).", "abstract": "AbstractWe are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems)."}, {"paper_id": "201103753", "adju_relevance": 1, "title": "CA-EHN: Commonsense Word Analogy from E-HowNet", "background_label": "Word analogy tasks have tended to be handcrafted, involving permutations of hundreds of words with dozens of relations, mostly morphological relations and named entities.", "abstract": "Word analogy tasks have tended to be handcrafted, involving permutations of hundreds of words with dozens of relations, mostly morphological relations and named entities."}, {"paper_id": "14680675", "adju_relevance": 1, "title": "Semantic Taxonomy Induction From Heterogenous Evidence", "background_label": "We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns.", "method_label": "By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers.", "result_label": "Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.", "abstract": "We propose a novel algorithm for inducing semantic taxonomies. We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs."}, {"paper_id": "17017087", "adju_relevance": 1, "title": "Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity", "background_label": "AbstractSemantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type.", "method_label": "We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data.", "result_label": "This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening.", "abstract": "AbstractSemantic similarity is an essential component of many Natural Language Processing applications. AbstractSemantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening."}, {"paper_id": "16186615", "adju_relevance": 1, "title": "Uncovering Distributional Differences between Synonyms and Antonyms in a Word Space Model", "background_label": "AbstractFor many NLP applications such as Information Extraction and Sentiment Detection, it is of vital importance to distinguish between synonyms and antonyms.", "method_label": "While the general assumption is that distributional models are not suitable for this task, we demonstrate that using suitable features, differences in the contexts of synonymous and antonymous German adjective pairs can be identified with a simple word space model.", "result_label": "Experimenting with two context settings (a simple windowbased model and a 'co-disambiguation model' to approximate adjective sense disambiguation), our best model significantly outperforms the 50% baseline and achieves 70.6% accuracy in a synonym/antonym classification task.", "abstract": "AbstractFor many NLP applications such as Information Extraction and Sentiment Detection, it is of vital importance to distinguish between synonyms and antonyms. While the general assumption is that distributional models are not suitable for this task, we demonstrate that using suitable features, differences in the contexts of synonymous and antonymous German adjective pairs can be identified with a simple word space model. Experimenting with two context settings (a simple windowbased model and a 'co-disambiguation model' to approximate adjective sense disambiguation), our best model significantly outperforms the 50% baseline and achieves 70.6% accuracy in a synonym/antonym classification task."}, {"paper_id": "9828393", "adju_relevance": 1, "title": "The role of polarity in antonym and synonym conceptual knowledge: Evidence from stroke aphasia and multidimensional ratings of abstract words", "background_label": "This study describes an investigation of different types of semantic relationship among abstract words: antonyms (e.g. good-bad), synonyms (e.g.", "abstract": "This study describes an investigation of different types of semantic relationship among abstract words: antonyms (e.g. This study describes an investigation of different types of semantic relationship among abstract words: antonyms (e.g. good-bad), synonyms (e.g."}, {"paper_id": "3201604", "adju_relevance": 1, "title": "Searching in metric spaces", "background_label": "The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. This includes a quantitative definition of the elusive concept of \"intrinsic dimensionality.\" We also present a unified view of all the known proposals to organize metric spaces, so as to be able to understand them under a common framework. Most approaches turn out to be variations on a few different concepts.", "method_label": "Because of this, the same ideas have been reconceived several times, and very different presentations have been given for the same approaches. We organize those works in a taxonomy that allows us to devise new algorithms from combinations of concepts not noticed before because of the lack of communication between different communities.", "result_label": "We present some basic results that explain the intrinsic difficulty of the search problem. We present experiments validating our results and comparing the existing approaches. We finish with recommendations for practitioners and open questions for future development.", "abstract": "The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. Because of this, the same ideas have been reconceived several times, and very different presentations have been given for the same approaches. We present some basic results that explain the intrinsic difficulty of the search problem. The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. This includes a quantitative definition of the elusive concept of \"intrinsic dimensionality.\" The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. This includes a quantitative definition of the elusive concept of \"intrinsic dimensionality.\" We also present a unified view of all the known proposals to organize metric spaces, so as to be able to understand them under a common framework. The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. This includes a quantitative definition of the elusive concept of \"intrinsic dimensionality.\" We also present a unified view of all the known proposals to organize metric spaces, so as to be able to understand them under a common framework. Most approaches turn out to be variations on a few different concepts. Because of this, the same ideas have been reconceived several times, and very different presentations have been given for the same approaches. We organize those works in a taxonomy that allows us to devise new algorithms from combinations of concepts not noticed before because of the lack of communication between different communities. We present some basic results that explain the intrinsic difficulty of the search problem. We present experiments validating our results and comparing the existing approaches. We present some basic results that explain the intrinsic difficulty of the search problem. We present experiments validating our results and comparing the existing approaches. We finish with recommendations for practitioners and open questions for future development."}, {"paper_id": "15863641", "adju_relevance": 1, "title": "Extracting Synonyms from Dictionary Definitions", "background_label": "AbstractWe investigate the problem of extracting synonyms from dictionary definitions.", "abstract": "AbstractWe investigate the problem of extracting synonyms from dictionary definitions."}, {"paper_id": "7289480", "adju_relevance": 1, "title": "Optimizing Synonym Extraction Using Monolingual And Bilingual Resources", "background_label": "Automatically acquiring synonymous words (synonyms) from corpora is a challenging task. For this task, methods that use only one kind of resources are inadequate because of low precision or low recall.", "method_label": "To improve the performance of synonym extraction, we propose a method to extract synonyms with multiple resources including a monolingual dictionary, a bilingual corpus, and a large monolingual corpus. This approach uses an ensemble to combine the synonyms extracted by individual extractors which use the three resources.", "result_label": "Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms.", "abstract": "Automatically acquiring synonymous words (synonyms) from corpora is a challenging task. Automatically acquiring synonymous words (synonyms) from corpora is a challenging task. For this task, methods that use only one kind of resources are inadequate because of low precision or low recall. To improve the performance of synonym extraction, we propose a method to extract synonyms with multiple resources including a monolingual dictionary, a bilingual corpus, and a large monolingual corpus. To improve the performance of synonym extraction, we propose a method to extract synonyms with multiple resources including a monolingual dictionary, a bilingual corpus, and a large monolingual corpus. This approach uses an ensemble to combine the synonyms extracted by individual extractors which use the three resources. Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms."}, {"paper_id": "5509836", "adju_relevance": 1, "title": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL", "background_label": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine.", "method_label": "The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions.", "result_label": "The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).", "abstract": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing)."}, {"paper_id": "14673969", "adju_relevance": 0, "title": "Grouping Synonyms by Definitions", "method_label": "We present a method for grouping the synonyms of a lemma according to its dictionary senses. The senses are defined by a large machine readable dictionary for French, the TLFi (Tr\\'esor de la langue fran\\c{c}aise informatis\\'e) and the synonyms are given by 5 synonym dictionaries (also for French). To evaluate the proposed method, we manually constructed a gold standard where for each (word, definition) pair and given the set of synonyms defined for that word by the 5 synonym dictionaries, 4 lexicographers specified the set of synonyms they judge adequate.", "result_label": "While inter-annotator agreement ranges on that task from 67% to at best 88% depending on the annotator pair and on the synonym dictionary being considered, the automatic procedure we propose scores a precision of 67% and a recall of 71%. The proposed method is compared with related work namely, word sense disambiguation, synonym lexicon acquisition and WordNet construction.", "abstract": "We present a method for grouping the synonyms of a lemma according to its dictionary senses. We present a method for grouping the synonyms of a lemma according to its dictionary senses. The senses are defined by a large machine readable dictionary for French, the TLFi (Tr\\'esor de la langue fran\\c{c}aise informatis\\'e) and the synonyms are given by 5 synonym dictionaries (also for French). We present a method for grouping the synonyms of a lemma according to its dictionary senses. The senses are defined by a large machine readable dictionary for French, the TLFi (Tr\\'esor de la langue fran\\c{c}aise informatis\\'e) and the synonyms are given by 5 synonym dictionaries (also for French). To evaluate the proposed method, we manually constructed a gold standard where for each (word, definition) pair and given the set of synonyms defined for that word by the 5 synonym dictionaries, 4 lexicographers specified the set of synonyms they judge adequate. While inter-annotator agreement ranges on that task from 67% to at best 88% depending on the annotator pair and on the synonym dictionary being considered, the automatic procedure we propose scores a precision of 67% and a recall of 71%. While inter-annotator agreement ranges on that task from 67% to at best 88% depending on the annotator pair and on the synonym dictionary being considered, the automatic procedure we propose scores a precision of 67% and a recall of 71%. The proposed method is compared with related work namely, word sense disambiguation, synonym lexicon acquisition and WordNet construction."}, {"paper_id": "195316636", "adju_relevance": 0, "title": "A Comparative Survey of Recent Natural Language Interfaces for Databases", "background_label": "Over the last few years natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These systems use very different approaches as described in recent survey papers. However, these systems have not been systematically compared against a set of benchmark questions in order to rigorously evaluate their functionalities and expressive power.", "abstract": "Over the last few years natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. Over the last few years natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These systems use very different approaches as described in recent survey papers. Over the last few years natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These systems use very different approaches as described in recent survey papers. However, these systems have not been systematically compared against a set of benchmark questions in order to rigorously evaluate their functionalities and expressive power."}, {"paper_id": "18050299", "adju_relevance": 0, "title": "An Analysis of Visual Question Answering Algorithms", "background_label": "In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods.", "abstract": "In visual question answering (VQA), an algorithm must answer text-based questions about images. In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods."}, {"paper_id": "8402900", "adju_relevance": 0, "title": "Language-naive chimpanzees (Pan troglodytes) judge relations between relations in a conceptual matching-to-sample task.", "background_label": "Three chimpanzees with a history of conditional and numeric token training spontaneously matched relations between relations under conditions of nondifferential reinforcement. Heretofore, this conceptual ability was demonstrated only in language-trained chimpanzees. The performance levels of the language-naive animals in this study, however, were equivalent to those of a 4th animal--Sarah--whose history included language training and analogical problem solving. There was no evidence that associative factors mediated successful performance in any of the animals.", "result_label": "Prior claims of a profound disparity between language-trained and language-naive chimpanzees apparently can be attributed to prior experience with arbitrary tokens consistently associated with abstract relations and not language per se.", "abstract": "Three chimpanzees with a history of conditional and numeric token training spontaneously matched relations between relations under conditions of nondifferential reinforcement. Three chimpanzees with a history of conditional and numeric token training spontaneously matched relations between relations under conditions of nondifferential reinforcement. Heretofore, this conceptual ability was demonstrated only in language-trained chimpanzees. Three chimpanzees with a history of conditional and numeric token training spontaneously matched relations between relations under conditions of nondifferential reinforcement. Heretofore, this conceptual ability was demonstrated only in language-trained chimpanzees. The performance levels of the language-naive animals in this study, however, were equivalent to those of a 4th animal--Sarah--whose history included language training and analogical problem solving. Three chimpanzees with a history of conditional and numeric token training spontaneously matched relations between relations under conditions of nondifferential reinforcement. Heretofore, this conceptual ability was demonstrated only in language-trained chimpanzees. The performance levels of the language-naive animals in this study, however, were equivalent to those of a 4th animal--Sarah--whose history included language training and analogical problem solving. There was no evidence that associative factors mediated successful performance in any of the animals. Prior claims of a profound disparity between language-trained and language-naive chimpanzees apparently can be attributed to prior experience with arbitrary tokens consistently associated with abstract relations and not language per se."}, {"paper_id": "21528614", "adju_relevance": 0, "title": "The supermatrix approach to systematics.", "background_label": "Recent reviews of the construction of large phylogenies have focused on supertree methods that involve separate analyses of data sets and subsequent integration of the resulting trees.", "method_label": "Here, we consider the alternative method of analyzing all character data simultaneously. Such 'supermatrix' analyses use information from each character directly and enable straightforward incorporation of diverse kinds of data, including characters from fossils. The approach has been extended by the development of new methods, including model-based techniques for analyzing heterogeneous data and hierarchical methods for constructing extremely large trees. Recent work also suggests that the problem of missing data in supermatrix analyses has been overstated.", "result_label": "Although the supermatrix approach is not suited for all cases, we suggest that its inherent strengths will ensure that it will continue to have a central role in inferring large phylogenetic trees from diverse data.", "abstract": "Recent reviews of the construction of large phylogenies have focused on supertree methods that involve separate analyses of data sets and subsequent integration of the resulting trees. Here, we consider the alternative method of analyzing all character data simultaneously. Here, we consider the alternative method of analyzing all character data simultaneously. Such 'supermatrix' analyses use information from each character directly and enable straightforward incorporation of diverse kinds of data, including characters from fossils. Here, we consider the alternative method of analyzing all character data simultaneously. Such 'supermatrix' analyses use information from each character directly and enable straightforward incorporation of diverse kinds of data, including characters from fossils. The approach has been extended by the development of new methods, including model-based techniques for analyzing heterogeneous data and hierarchical methods for constructing extremely large trees. Here, we consider the alternative method of analyzing all character data simultaneously. Such 'supermatrix' analyses use information from each character directly and enable straightforward incorporation of diverse kinds of data, including characters from fossils. The approach has been extended by the development of new methods, including model-based techniques for analyzing heterogeneous data and hierarchical methods for constructing extremely large trees. Recent work also suggests that the problem of missing data in supermatrix analyses has been overstated. Although the supermatrix approach is not suited for all cases, we suggest that its inherent strengths will ensure that it will continue to have a central role in inferring large phylogenetic trees from diverse data."}, {"paper_id": "6468765", "adju_relevance": 0, "title": "Semi-supervised Question Retrieval with Gated Convolutions", "background_label": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented.", "method_label": "We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations.", "result_label": "Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).", "abstract": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs)."}, {"paper_id": "144086530", "adju_relevance": 0, "title": "Lattice analysis and the representation of handicap associations", "method_label": "The lattices can be taken as a conceptual as well as an implicational model of multiple handicaps because of their capacity for formalizing the definitions of disorders in terms of extensions or intensions.", "result_label": "Certain substructures of the lattice, weighted by the frequencies of the subject groups, can display the assessed associations between handicaps, thus addressing quite directly the research questions of the health services.", "abstract": " The lattices can be taken as a conceptual as well as an implicational model of multiple handicaps because of their capacity for formalizing the definitions of disorders in terms of extensions or intensions. Certain substructures of the lattice, weighted by the frequencies of the subject groups, can display the assessed associations between handicaps, thus addressing quite directly the research questions of the health services."}, {"paper_id": "3968254", "adju_relevance": 0, "title": "Stacking with Auxiliary Features for Visual Question Answering", "background_label": "AbstractVisual Question Answering (VQA) is a wellknown and challenging task that requires systems to jointly reason about natural language and vision. Deep learning models in various forms have been the standard for solving VQA. However, some of these VQA models are better at certain types of image-question pairs than other models.", "method_label": "Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art.", "result_label": "Our work also highlights the advantages of explainable AI models.", "abstract": "AbstractVisual Question Answering (VQA) is a wellknown and challenging task that requires systems to jointly reason about natural language and vision. AbstractVisual Question Answering (VQA) is a wellknown and challenging task that requires systems to jointly reason about natural language and vision. Deep learning models in various forms have been the standard for solving VQA. AbstractVisual Question Answering (VQA) is a wellknown and challenging task that requires systems to jointly reason about natural language and vision. Deep learning models in various forms have been the standard for solving VQA. However, some of these VQA models are better at certain types of image-question pairs than other models. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous.Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art. Our work also highlights the advantages of explainable AI models."}, {"paper_id": "67033226", "adju_relevance": 0, "title": "Purest Ever Example-Based Machine Translation: detailed presentation and assessment", "background_label": "We have designed, implemented and assessed an EBMT system that can be dubbed the 'purest ever built': it strictly does not make any use of variables, templates or patterns, does not have any explicit transfer component, and does not require any preprocessing or training of the aligned examples.", "method_label": "It only uses a specic operation, proportional analogy, that implicitly neutralises divergences between lan- guages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. Exactly the same genuine implementation of such a core engine was evaluated on dieren t tasks and language pairs. To begin with, we compared our system on two tasks of a previous MT evaluation campaign to rank it among other current state-of-the-art systems. Then, we illustrated the 'universality' of our system by participating in a recent MT evaluation campaign, with exactly the same core engine, for a wide variety of language pairs.", "result_label": "Finally, we studied the inuence of extra data like dictionaries and paraphrases on the system performance.", "abstract": "We have designed, implemented and assessed an EBMT system that can be dubbed the 'purest ever built': it strictly does not make any use of variables, templates or patterns, does not have any explicit transfer component, and does not require any preprocessing or training of the aligned examples. It only uses a specic operation, proportional analogy, that implicitly neutralises divergences between lan- guages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. It only uses a specic operation, proportional analogy, that implicitly neutralises divergences between lan- guages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. Exactly the same genuine implementation of such a core engine was evaluated on dieren t tasks and language pairs. It only uses a specic operation, proportional analogy, that implicitly neutralises divergences between lan- guages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. Exactly the same genuine implementation of such a core engine was evaluated on dieren t tasks and language pairs. To begin with, we compared our system on two tasks of a previous MT evaluation campaign to rank it among other current state-of-the-art systems. It only uses a specic operation, proportional analogy, that implicitly neutralises divergences between lan- guages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. Exactly the same genuine implementation of such a core engine was evaluated on dieren t tasks and language pairs. To begin with, we compared our system on two tasks of a previous MT evaluation campaign to rank it among other current state-of-the-art systems. Then, we illustrated the 'universality' of our system by participating in a recent MT evaluation campaign, with exactly the same core engine, for a wide variety of language pairs. Finally, we studied the inuence of extra data like dictionaries and paraphrases on the system performance."}, {"paper_id": "291860", "adju_relevance": 0, "title": "The TSIMMIS Approach to Mediation: Data Models and Languages", "background_label": "TSIMMIS\u2014The Stanford-IBM Manager of Multiple InformationSources\u2014is a system for integrating information. It offers a datamodel and a common query language that are designed to support thecombining of information from many different sources. It also offerstools for generating automatically the components that are needed tobuild systems for integrating information.", "abstract": "TSIMMIS\u2014The Stanford-IBM Manager of Multiple InformationSources\u2014is a system for integrating information. TSIMMIS\u2014The Stanford-IBM Manager of Multiple InformationSources\u2014is a system for integrating information. It offers a datamodel and a common query language that are designed to support thecombining of information from many different sources. TSIMMIS\u2014The Stanford-IBM Manager of Multiple InformationSources\u2014is a system for integrating information. It offers a datamodel and a common query language that are designed to support thecombining of information from many different sources. It also offerstools for generating automatically the components that are needed tobuild systems for integrating information."}, {"paper_id": "12137139", "adju_relevance": 0, "title": "An empirical study of gene synonym query expansion in biomedical information retrieval", "background_label": "Due to the heavy use of gene synonyms in biomedical text, people have tried many query expansion techniques using synonyms in order to improve performance in biomedical information retrieval. However, mixed results have been reported. The main challenge is that it is not trivial to assign appropriate weights to the added gene synonyms in the expanded query; under-weighting of synonyms would not bring much benefit, while overweighting some unreliable synonyms can hurt performance significantly. So far, there has been no systematic evaluation of various synonym query expansion strategies for biomedical text.", "method_label": "In this work, we propose two different strategies to extend a standard language modeling approach for gene synonym query expansion and conduct a systematic evaluation of these methods on all the available TREC biomedical text collections for ad hoc document retrieval.", "result_label": "Our experiment results show that synonym expansion can significantly improve the retrieval accuracy. However, different query types require different synonym expansion methods, and appropriate weighting of gene names and synonym terms is critical for improving performance.", "abstract": "Due to the heavy use of gene synonyms in biomedical text, people have tried many query expansion techniques using synonyms in order to improve performance in biomedical information retrieval. Due to the heavy use of gene synonyms in biomedical text, people have tried many query expansion techniques using synonyms in order to improve performance in biomedical information retrieval. However, mixed results have been reported. Due to the heavy use of gene synonyms in biomedical text, people have tried many query expansion techniques using synonyms in order to improve performance in biomedical information retrieval. However, mixed results have been reported. The main challenge is that it is not trivial to assign appropriate weights to the added gene synonyms in the expanded query; under-weighting of synonyms would not bring much benefit, while overweighting some unreliable synonyms can hurt performance significantly. Due to the heavy use of gene synonyms in biomedical text, people have tried many query expansion techniques using synonyms in order to improve performance in biomedical information retrieval. However, mixed results have been reported. The main challenge is that it is not trivial to assign appropriate weights to the added gene synonyms in the expanded query; under-weighting of synonyms would not bring much benefit, while overweighting some unreliable synonyms can hurt performance significantly. So far, there has been no systematic evaluation of various synonym query expansion strategies for biomedical text. In this work, we propose two different strategies to extend a standard language modeling approach for gene synonym query expansion and conduct a systematic evaluation of these methods on all the available TREC biomedical text collections for ad hoc document retrieval. Our experiment results show that synonym expansion can significantly improve the retrieval accuracy. Our experiment results show that synonym expansion can significantly improve the retrieval accuracy. However, different query types require different synonym expansion methods, and appropriate weighting of gene names and synonym terms is critical for improving performance."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "17525632", "adju_relevance": 0, "title": "Learning Distributed Representations of Data in Community Question Answering for Question Retrieval", "background_label": "We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. In our method, we simultaneously learn vectors of words and vectors of question categories by optimizing an objective function naturally derived from the framework. We conduct experiments on large scale data from Yahoo!", "method_label": "To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. In question retrieval, we incorporate learnt representations into traditional language models in an effective and efficient way. Answers and Baidu Knows, and compared our method with state-of-the-art methods on two public data sets.", "result_label": "We propose a principled and efficient approach to learning representations of data in CQA. Experimental results show that our method can significantly improve on baseline methods for retrieval relevance. On 1 million training data, our method takes less than 50 minutes to learn a model on a single multicore machine, while the translation based language model needs more than 2 days to learn a translation table on the same machine.", "abstract": "We study the problem of question retrieval in community question answering (CQA). We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. We propose a principled and efficient approach to learning representations of data in CQA. We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. In our method, we simultaneously learn vectors of words and vectors of question categories by optimizing an objective function naturally derived from the framework. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. In question retrieval, we incorporate learnt representations into traditional language models in an effective and efficient way. We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. In our method, we simultaneously learn vectors of words and vectors of question categories by optimizing an objective function naturally derived from the framework. We conduct experiments on large scale data from Yahoo! To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. In question retrieval, we incorporate learnt representations into traditional language models in an effective and efficient way. Answers and Baidu Knows, and compared our method with state-of-the-art methods on two public data sets. We propose a principled and efficient approach to learning representations of data in CQA. Experimental results show that our method can significantly improve on baseline methods for retrieval relevance. We propose a principled and efficient approach to learning representations of data in CQA. Experimental results show that our method can significantly improve on baseline methods for retrieval relevance. On 1 million training data, our method takes less than 50 minutes to learn a model on a single multicore machine, while the translation based language model needs more than 2 days to learn a translation table on the same machine."}, {"paper_id": "675997", "adju_relevance": 0, "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding", "background_label": "Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words.", "method_label": "To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each type of questions, we propose a specific solver based on the obtained distributed word representations and relation representations.", "result_label": "Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. The results indicate that with appropriate uses of the deep learning technologies we might be a further step closer to the human intelligence.", "abstract": "Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework consisting of three components. To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each type of questions, we propose a specific solver based on the obtained distributed word representations and relation representations. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. The results indicate that with appropriate uses of the deep learning technologies we might be a further step closer to the human intelligence."}, {"paper_id": "16142140", "adju_relevance": 0, "title": "The optimality of attaching unlinked labels to unlinked meanings", "background_label": "Vocabulary learning by children can be characterized by many biases. When encountering a new word, children as well as adults, are biased towards assuming that it means something totally different from the words that they already know.", "abstract": "Vocabulary learning by children can be characterized by many biases. Vocabulary learning by children can be characterized by many biases. When encountering a new word, children as well as adults, are biased towards assuming that it means something totally different from the words that they already know."}, {"paper_id": "7121547", "adju_relevance": 0, "title": "Computational Lexicons: the Neat Examples and the Odd Exemplars", "background_label": "When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with.", "abstract": "When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with."}, {"paper_id": "64259583", "adju_relevance": 0, "title": "Data Mining Practical Machine Learning Tools And Techniques With Java Implementations", "background_label": "Thank you for reading data mining practical machine learning tools and techniques with java implementations. As you may know, people have look hundreds times for their favorite novels like this data mining practical machine learning tools and techniques with java implementations, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their laptop.", "abstract": "Thank you for reading data mining practical machine learning tools and techniques with java implementations. Thank you for reading data mining practical machine learning tools and techniques with java implementations. As you may know, people have look hundreds times for their favorite novels like this data mining practical machine learning tools and techniques with java implementations, but end up in infectious downloads. Thank you for reading data mining practical machine learning tools and techniques with java implementations. As you may know, people have look hundreds times for their favorite novels like this data mining practical machine learning tools and techniques with java implementations, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their laptop."}, {"paper_id": "14692997", "adju_relevance": 0, "title": "PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names", "background_label": "AbstractThis paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as \"topic models\" to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees.", "method_label": "The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.", "abstract": "AbstractThis paper establishes a connection between two apparently very different kinds of probabilistic models. AbstractThis paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as \"topic models\" to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names."}, {"paper_id": "7077951", "adju_relevance": 0, "title": "Clustering by Committee", "background_label": "Text contains a wealth of knowledge about who we are, what we know, how we think, and how we communicate. We are just beginning to tap into the information that is available in the tales we read to our children, the narratives that capture our thoughts, and the stories that shape our world.", "abstract": "Text contains a wealth of knowledge about who we are, what we know, how we think, and how we communicate. Text contains a wealth of knowledge about who we are, what we know, how we think, and how we communicate. We are just beginning to tap into the information that is available in the tales we read to our children, the narratives that capture our thoughts, and the stories that shape our world."}, {"paper_id": "64214454", "adju_relevance": 0, "title": "Semantic Relations And The Lexicon Antonymy Synonymy And Other Paradigms", "background_label": "Thank you very much for downloading semantic relations and the lexicon antonymy synonymy and other paradigms. As you may know, people have search numerous times for their favorite novels like this semantic relations and the lexicon antonymy synonymy and other paradigms, but end up in harmful downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their desktop computer.", "abstract": "Thank you very much for downloading semantic relations and the lexicon antonymy synonymy and other paradigms. Thank you very much for downloading semantic relations and the lexicon antonymy synonymy and other paradigms. As you may know, people have search numerous times for their favorite novels like this semantic relations and the lexicon antonymy synonymy and other paradigms, but end up in harmful downloads. Thank you very much for downloading semantic relations and the lexicon antonymy synonymy and other paradigms. As you may know, people have search numerous times for their favorite novels like this semantic relations and the lexicon antonymy synonymy and other paradigms, but end up in harmful downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their desktop computer."}, {"paper_id": "6116435", "adju_relevance": 0, "title": "A statistical approach to snakes for bimodal and trimodal imagery", "background_label": "We describe a new region based approach to active contours for segmenting images composed of two or three types of regions characterizable by a given statistic.", "abstract": "We describe a new region based approach to active contours for segmenting images composed of two or three types of regions characterizable by a given statistic."}, {"paper_id": "3582999", "adju_relevance": 0, "title": "One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data", "background_label": "Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models.", "method_label": "This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively.", "result_label": "Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data.", "abstract": "Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models. This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data."}, {"paper_id": "6763915", "adju_relevance": 0, "title": "Acquiring Collocations For Lexical Choice Between Near-Synonyms", "background_label": "We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. This type of knowledge is useful in the process of lexical choice between near-synonyms.", "method_label": "We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web).", "result_label": "We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry.", "abstract": "We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. This type of knowledge is useful in the process of lexical choice between near-synonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry."}, {"paper_id": "85558243", "adju_relevance": 0, "title": "Worldsheet approach to heterotic instantons and solitons", "background_label": "Abstract Instantons and soliton solutions of heterotic string theory are investigated with an emphasis on the worldsheet point of view. The solitons have the structure of a fivebrane in ten dimensions, the instantons are simply related to the solitons by Wick rotation.", "method_label": "Some new fivebrane solutions are presented with a four-dimensional cross-section of the fivebrane given by an infinitely long, semi-wormhole at the core stabilized by axion charge and containing non-abelian gauge excitations. The generic such solution is shown to correspond to a sigma model with (4, 0) worldsheet supersymmetry. At special symmetric points in the moduli space of multi-instanton or multi-soliton solutions, the spin connection with torsion becomes identical to the Yang-Mills gauge connection. It is further shown that the wormhole throat is then described by an exactly soluble conformal field theory which is essentially a Wess-Zumino-Witten model whose level is related to the axion charge.", "result_label": "This results in (4, 4) worldsheet supersymmetry, and we argue that the resulting sigma models with torsion are finite and conformally invariant with no \u03b1\u2032 corrections. Because of the simplicity and exactness of these symmetric solutions, they should provide a useful starting point for the analysis of such issues as integration of collective coordinates in string theory, duality between strings and fivebranes and the quantization of fundamental fivebranes.", "abstract": "Abstract Instantons and soliton solutions of heterotic string theory are investigated with an emphasis on the worldsheet point of view. Abstract Instantons and soliton solutions of heterotic string theory are investigated with an emphasis on the worldsheet point of view. The solitons have the structure of a fivebrane in ten dimensions, the instantons are simply related to the solitons by Wick rotation. Some new fivebrane solutions are presented with a four-dimensional cross-section of the fivebrane given by an infinitely long, semi-wormhole at the core stabilized by axion charge and containing non-abelian gauge excitations. Some new fivebrane solutions are presented with a four-dimensional cross-section of the fivebrane given by an infinitely long, semi-wormhole at the core stabilized by axion charge and containing non-abelian gauge excitations. The generic such solution is shown to correspond to a sigma model with (4, 0) worldsheet supersymmetry. Some new fivebrane solutions are presented with a four-dimensional cross-section of the fivebrane given by an infinitely long, semi-wormhole at the core stabilized by axion charge and containing non-abelian gauge excitations. The generic such solution is shown to correspond to a sigma model with (4, 0) worldsheet supersymmetry. At special symmetric points in the moduli space of multi-instanton or multi-soliton solutions, the spin connection with torsion becomes identical to the Yang-Mills gauge connection. This results in (4, 4) worldsheet supersymmetry, and we argue that the resulting sigma models with torsion are finite and conformally invariant with no \u03b1\u2032 corrections. Some new fivebrane solutions are presented with a four-dimensional cross-section of the fivebrane given by an infinitely long, semi-wormhole at the core stabilized by axion charge and containing non-abelian gauge excitations. The generic such solution is shown to correspond to a sigma model with (4, 0) worldsheet supersymmetry. At special symmetric points in the moduli space of multi-instanton or multi-soliton solutions, the spin connection with torsion becomes identical to the Yang-Mills gauge connection. It is further shown that the wormhole throat is then described by an exactly soluble conformal field theory which is essentially a Wess-Zumino-Witten model whose level is related to the axion charge. This results in (4, 4) worldsheet supersymmetry, and we argue that the resulting sigma models with torsion are finite and conformally invariant with no \u03b1\u2032 corrections. Because of the simplicity and exactness of these symmetric solutions, they should provide a useful starting point for the analysis of such issues as integration of collective coordinates in string theory, duality between strings and fivebranes and the quantization of fundamental fivebranes."}, {"paper_id": "16406612", "adju_relevance": 0, "title": "Associated type synonyms", "background_label": "Haskell programmers often use a multi-parameter type class in which one or more type parameters are functionally dependent on the first. Although such functional dependencies have proved quite popular in practice, they express the programmer's intent somewhat indirectly.", "abstract": "Haskell programmers often use a multi-parameter type class in which one or more type parameters are functionally dependent on the first. Haskell programmers often use a multi-parameter type class in which one or more type parameters are functionally dependent on the first. Although such functional dependencies have proved quite popular in practice, they express the programmer's intent somewhat indirectly."}, {"paper_id": "8736393", "adju_relevance": 0, "title": "The Computation Of Word Associations: Comparing Syntagmatic And Paradigmatic Approaches", "background_label": "It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora. According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning.", "method_label": "The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data.", "result_label": "It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects.", "abstract": "It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora. It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora. According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data. It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects."}, {"paper_id": "10865063", "adju_relevance": 0, "title": "Computational Approaches to Sentence Completion", "background_label": "AbstractThis paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena.", "method_label": "We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels.", "result_label": "We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.", "abstract": "AbstractThis paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. AbstractThis paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research."}, {"paper_id": "143594444", "adju_relevance": 0, "title": "Antonyms in children's and child-directed speech", "background_label": "This article presents two studies based on a corpus of American English speech by and to five children from 2 to 5 years old. The first study investigates frequency of antonym co-occurrence in speakers' turns.", "method_label": "The second examines the discourse-functional properties of those co-occurrences, with comparison to adult-directed adult English.", "result_label": "We find: (1) children know/use antonyms at earlier ages than experimental studies have shown; (2) children use antonyms for mostly the same discursive purposes as adults do; (3) children can be categorized as being either `heavy' or `light' antonym users, and `heaviness' of antonym use seems to correlate to other aspects of antonym behaviour.", "abstract": "This article presents two studies based on a corpus of American English speech by and to five children from 2 to 5 years old. This article presents two studies based on a corpus of American English speech by and to five children from 2 to 5 years old. The first study investigates frequency of antonym co-occurrence in speakers' turns. The second examines the discourse-functional properties of those co-occurrences, with comparison to adult-directed adult English. We find: (1) children know/use antonyms at earlier ages than experimental studies have shown; (2) children use antonyms for mostly the same discursive purposes as adults do; (3) children can be categorized as being either `heavy' or `light' antonym users, and `heaviness' of antonym use seems to correlate to other aspects of antonym behaviour."}, {"paper_id": "5995501", "adju_relevance": 0, "title": "Context sensitive synonym discovery for web search queries", "background_label": "We propose a simple yet effective approach to context sensitive synonym discovery for Web search queries based on co-click analysis; i.e., analyzing queries leading to clicking same documents.", "method_label": "In addition to deriving word based synonyms, we also derive concept based synonyms with the help of query segmentation.", "result_label": "Evaluation results show that this approach dramatically outperforms the thesaurus based synonym replacement method in keeping search intent, from accuracy of 40% to above 80%.", "abstract": "We propose a simple yet effective approach to context sensitive synonym discovery for Web search queries based on co-click analysis; i.e., analyzing queries leading to clicking same documents. In addition to deriving word based synonyms, we also derive concept based synonyms with the help of query segmentation. Evaluation results show that this approach dramatically outperforms the thesaurus based synonym replacement method in keeping search intent, from accuracy of 40% to above 80%."}, {"paper_id": "14983093", "adju_relevance": 0, "title": "A consolidated approach to the axiomatization of outranking relations: a survey and new results", "background_label": "Outranking relations such as produced by the Electre I or II or the Tactic methods are based on a concordance and non-discordance principle that leads to declaring that an alternative is \u201csuperior\u201d to another, if the coalition of attributes supporting this proposition is \u201csufficiently important\u201d (concordance condition) and if there is no attribute that \u201cstrongly rejects\u201d it (non-discordance condition). Such a way of comparing alternatives is rather natural and does not require a detailed analysis of tradeoffs between the various attributes. However, it is well known that it may produce binary relations that do not possess any remarkable property of transitivity or completeness. The axiomatic foundations of outranking relations have recently received attention. In this paper we briefly review the various kinds of axiomatizations of outranking relations proposed so far in the literature.", "method_label": "Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. Then we analyze the relationships between reflexive and asymmetric outranking relations in a conjoint measurement framework, consolidating our previous work. Co-duality plays an essential r\u00f4le in our analysis. It allows us to understand the correspondence between the previous characterizations. Making a step further, we provide a common axiomatic characterization for both types of relations. Applying the co-duality operator to concordance\u2013discordance relations also yields a new and interesting type of preference relation that we call concordance relation with bonus.", "result_label": "A different characterization has been provided for strict (asymmetric) preference relations such as produced by Tactic. The axiomatic characterization of such relations results directly from co-duality arguments.", "abstract": "Outranking relations such as produced by the Electre I or II or the Tactic methods are based on a concordance and non-discordance principle that leads to declaring that an alternative is \u201csuperior\u201d to another, if the coalition of attributes supporting this proposition is \u201csufficiently important\u201d (concordance condition) and if there is no attribute that \u201cstrongly rejects\u201d it (non-discordance condition). Outranking relations such as produced by the Electre I or II or the Tactic methods are based on a concordance and non-discordance principle that leads to declaring that an alternative is \u201csuperior\u201d to another, if the coalition of attributes supporting this proposition is \u201csufficiently important\u201d (concordance condition) and if there is no attribute that \u201cstrongly rejects\u201d it (non-discordance condition). Such a way of comparing alternatives is rather natural and does not require a detailed analysis of tradeoffs between the various attributes. Outranking relations such as produced by the Electre I or II or the Tactic methods are based on a concordance and non-discordance principle that leads to declaring that an alternative is \u201csuperior\u201d to another, if the coalition of attributes supporting this proposition is \u201csufficiently important\u201d (concordance condition) and if there is no attribute that \u201cstrongly rejects\u201d it (non-discordance condition). Such a way of comparing alternatives is rather natural and does not require a detailed analysis of tradeoffs between the various attributes. However, it is well known that it may produce binary relations that do not possess any remarkable property of transitivity or completeness. Outranking relations such as produced by the Electre I or II or the Tactic methods are based on a concordance and non-discordance principle that leads to declaring that an alternative is \u201csuperior\u201d to another, if the coalition of attributes supporting this proposition is \u201csufficiently important\u201d (concordance condition) and if there is no attribute that \u201cstrongly rejects\u201d it (non-discordance condition). Such a way of comparing alternatives is rather natural and does not require a detailed analysis of tradeoffs between the various attributes. However, it is well known that it may produce binary relations that do not possess any remarkable property of transitivity or completeness. The axiomatic foundations of outranking relations have recently received attention. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. A different characterization has been provided for strict (asymmetric) preference relations such as produced by Tactic. Outranking relations such as produced by the Electre I or II or the Tactic methods are based on a concordance and non-discordance principle that leads to declaring that an alternative is \u201csuperior\u201d to another, if the coalition of attributes supporting this proposition is \u201csufficiently important\u201d (concordance condition) and if there is no attribute that \u201cstrongly rejects\u201d it (non-discordance condition). Such a way of comparing alternatives is rather natural and does not require a detailed analysis of tradeoffs between the various attributes. However, it is well known that it may produce binary relations that do not possess any remarkable property of transitivity or completeness. The axiomatic foundations of outranking relations have recently received attention. In this paper we briefly review the various kinds of axiomatizations of outranking relations proposed so far in the literature. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. Then we analyze the relationships between reflexive and asymmetric outranking relations in a conjoint measurement framework, consolidating our previous work. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. Then we analyze the relationships between reflexive and asymmetric outranking relations in a conjoint measurement framework, consolidating our previous work. Co-duality plays an essential r\u00f4le in our analysis. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. Then we analyze the relationships between reflexive and asymmetric outranking relations in a conjoint measurement framework, consolidating our previous work. Co-duality plays an essential r\u00f4le in our analysis. It allows us to understand the correspondence between the previous characterizations. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. Then we analyze the relationships between reflexive and asymmetric outranking relations in a conjoint measurement framework, consolidating our previous work. Co-duality plays an essential r\u00f4le in our analysis. It allows us to understand the correspondence between the previous characterizations. Making a step further, we provide a common axiomatic characterization for both types of relations. Within a conjoint measurement framework, characterizations of reflexive concordance\u2013discordance relations have been obtained. These relations encompass those generated by the Electre I and II methods, which are non-strict (reflexive) relations. Then we analyze the relationships between reflexive and asymmetric outranking relations in a conjoint measurement framework, consolidating our previous work. Co-duality plays an essential r\u00f4le in our analysis. It allows us to understand the correspondence between the previous characterizations. Making a step further, we provide a common axiomatic characterization for both types of relations. Applying the co-duality operator to concordance\u2013discordance relations also yields a new and interesting type of preference relation that we call concordance relation with bonus. A different characterization has been provided for strict (asymmetric) preference relations such as produced by Tactic. The axiomatic characterization of such relations results directly from co-duality arguments."}, {"paper_id": "6946120", "adju_relevance": 0, "title": "Labeled Graph Kernel for Behavior Analysis", "background_label": "Automatic behavior analysis from video is a major topic in many areas of research, including computer vision, multimedia, robotics, biology, cognitive science, social psychology, psychiatry, and linguistics. Two major problems are of interest when analyzing behavior.", "abstract": "Automatic behavior analysis from video is a major topic in many areas of research, including computer vision, multimedia, robotics, biology, cognitive science, social psychology, psychiatry, and linguistics. Automatic behavior analysis from video is a major topic in many areas of research, including computer vision, multimedia, robotics, biology, cognitive science, social psychology, psychiatry, and linguistics. Two major problems are of interest when analyzing behavior."}, {"paper_id": "11741230", "adju_relevance": 0, "title": "Understanding user intent in community question answering", "background_label": "Community Question Answering (CQA) services, such as Yahoo! Answers, are specifically designed to address the innate limitation of Web search engines by helping users obtain information from a community. Understanding the user intent of questions would enable a CQA system identify similar questions, find relevant answers, and recommend potential answerers more effectively and efficiently.", "abstract": "Community Question Answering (CQA) services, such as Yahoo! Community Question Answering (CQA) services, such as Yahoo! Answers, are specifically designed to address the innate limitation of Web search engines by helping users obtain information from a community. Community Question Answering (CQA) services, such as Yahoo! Answers, are specifically designed to address the innate limitation of Web search engines by helping users obtain information from a community. Understanding the user intent of questions would enable a CQA system identify similar questions, find relevant answers, and recommend potential answerers more effectively and efficiently."}, {"paper_id": "28756012", "adju_relevance": 0, "title": "Functional load and the lexicon: Evidence that syntactic category and frequency relationships in minimal lemma pairs predict the loss of phoneme contrasts in language change.", "background_label": "All languages use individually meaningless, contrastive categories in combination to create distinct words. Despite their central role in communication, these \"phoneme\" contrasts can be lost over the course of language change. The century-old functional load hypothesis proposes that loss of a phoneme contrast will be inhibited in relation to the work that it does in distinguishing words.", "method_label": "In a previous work we showed for the first time that a simple measure of functional load does significantly predict patterns of contrast loss within a diverse set of languages: the more minimal word pairs that a phoneme contrast distinguishes, the less likely those phonemes are to have merged over the course of language change. Here, we examine several lexical properties that are predicted to influence the uncertainty between word pairs in usage.", "result_label": "We present evidence that (a) the lemma rather than surface-form count of minimal pairs is more predictive of merger; (b) the count of minimal lemma pairs that share a syntactic category is a stronger predictor of merger than the count of those with divergent syntactic categories, and (c) that the count of minimal lemma pairs with members of similar frequency is a stronger predictor of merger than that of those with more divergent frequencies. These findings support the broad hypothesis that properties of individual utterances influence long-term language change, and are consistent with findings suggesting that phonetic cues are modulated in response to lexical uncertainty within utterances.", "abstract": "All languages use individually meaningless, contrastive categories in combination to create distinct words. All languages use individually meaningless, contrastive categories in combination to create distinct words. Despite their central role in communication, these \"phoneme\" contrasts can be lost over the course of language change. All languages use individually meaningless, contrastive categories in combination to create distinct words. Despite their central role in communication, these \"phoneme\" contrasts can be lost over the course of language change. The century-old functional load hypothesis proposes that loss of a phoneme contrast will be inhibited in relation to the work that it does in distinguishing words. In a previous work we showed for the first time that a simple measure of functional load does significantly predict patterns of contrast loss within a diverse set of languages: the more minimal word pairs that a phoneme contrast distinguishes, the less likely those phonemes are to have merged over the course of language change. In a previous work we showed for the first time that a simple measure of functional load does significantly predict patterns of contrast loss within a diverse set of languages: the more minimal word pairs that a phoneme contrast distinguishes, the less likely those phonemes are to have merged over the course of language change. Here, we examine several lexical properties that are predicted to influence the uncertainty between word pairs in usage. We present evidence that (a) the lemma rather than surface-form count of minimal pairs is more predictive of merger; (b) the count of minimal lemma pairs that share a syntactic category is a stronger predictor of merger than the count of those with divergent syntactic categories, and (c) that the count of minimal lemma pairs with members of similar frequency is a stronger predictor of merger than that of those with more divergent frequencies. We present evidence that (a) the lemma rather than surface-form count of minimal pairs is more predictive of merger; (b) the count of minimal lemma pairs that share a syntactic category is a stronger predictor of merger than the count of those with divergent syntactic categories, and (c) that the count of minimal lemma pairs with members of similar frequency is a stronger predictor of merger than that of those with more divergent frequencies. These findings support the broad hypothesis that properties of individual utterances influence long-term language change, and are consistent with findings suggesting that phonetic cues are modulated in response to lexical uncertainty within utterances."}, {"paper_id": "152218662", "adju_relevance": 0, "title": "ABX-Discriminability Measures and Applications", "background_label": "This thesis constitutes an indirect contribution to the problem of modeling phonetic category acquisition in infancy. Some specific computational models of phonetic category acquisition have been proposed, but they were never tested extensively nor compared quantitatively to see whether they were really able to account for a sizable portion of the available empirical observations.", "abstract": "This thesis constitutes an indirect contribution to the problem of modeling phonetic category acquisition in infancy. This thesis constitutes an indirect contribution to the problem of modeling phonetic category acquisition in infancy. Some specific computational models of phonetic category acquisition have been proposed, but they were never tested extensively nor compared quantitatively to see whether they were really able to account for a sizable portion of the available empirical observations."}, {"paper_id": "9518835", "adju_relevance": 0, "title": "An Evolutionary Algorithm to Learn SPARQL Queries for Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia", "background_label": "Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query.", "abstract": "Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query."}, {"paper_id": "17371059", "adju_relevance": 0, "title": "Representational similarity analysis reveals commonalities and differences in the semantic processing of words and objects.", "background_label": "Understanding the meanings of words and objects requires the activation of underlying conceptual representations. Semantic representations are often assumed to be coded such that meaning is evoked regardless of the input modality. However, the extent to which meaning is coded in modality-independent or amodal systems remains controversial.", "abstract": "Understanding the meanings of words and objects requires the activation of underlying conceptual representations. Understanding the meanings of words and objects requires the activation of underlying conceptual representations. Semantic representations are often assumed to be coded such that meaning is evoked regardless of the input modality. Understanding the meanings of words and objects requires the activation of underlying conceptual representations. Semantic representations are often assumed to be coded such that meaning is evoked regardless of the input modality. However, the extent to which meaning is coded in modality-independent or amodal systems remains controversial."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "47328136", "adju_relevance": 0, "title": "Bagging predictors", "background_label": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class.", "method_label": "The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets.", "result_label": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.", "abstract": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."}, {"paper_id": "142730854", "adju_relevance": 0, "title": "Pragmatic markers and sociolinguistic variation : a relevance-theoretic approach to the language of adolescents", "background_label": "This book combines theoretical work in linguistic pragmatics and sociolinguistics with empirical work based on a corpus of London adolescent conversation.", "abstract": "This book combines theoretical work in linguistic pragmatics and sociolinguistics with empirical work based on a corpus of London adolescent conversation."}, {"paper_id": "143253950", "adju_relevance": 0, "title": "Cognition and Thought: An Information-Processing Approach.", "background_label": "This book covers the very interesting subject of computer simulation of such human functions as cognition and thought. Its stated purpose is \". .", "abstract": "This book covers the very interesting subject of computer simulation of such human functions as cognition and thought. This book covers the very interesting subject of computer simulation of such human functions as cognition and thought. Its stated purpose is \". This book covers the very interesting subject of computer simulation of such human functions as cognition and thought. Its stated purpose is \". ."}, {"paper_id": "155144146", "adju_relevance": 0, "title": "Learning to Spot and Refactor Inconsistent Method Names", "background_label": "To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency.", "method_label": "We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact.", "result_label": "Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.", "abstract": "To ensure code readability and facilitate software maintenance, program methods must be named properly. To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild."}, {"paper_id": "15230614", "adju_relevance": 0, "title": "Structured Analogies for Forecasting", "background_label": "When people forecast, they often use analogies but in an unstructured manner.", "method_label": "We propose a structured judgmental procedure that involves asking experts to list as many analogies as they can, rate how similar the analogies are to the target situation, and match the outcomes of the analogies with possible outcomes of the target. An administrator would then derive a forecast from the experts' information. We compared structured analogies with unaided judgments for predicting the decisions made in eight conflict situations.", "result_label": "These were difficult forecasting problems; the 32% accuracy of the unaided experts was only slightly better than chance. In contrast, 46% of structured analogies forecasts were accurate. Among experts who were independently able to think of two or more analogies and who had direct experience with their closest analogy, 60% of forecasts were accurate. Collaboration did not improve accuracy.", "abstract": "When people forecast, they often use analogies but in an unstructured manner. We propose a structured judgmental procedure that involves asking experts to list as many analogies as they can, rate how similar the analogies are to the target situation, and match the outcomes of the analogies with possible outcomes of the target. We propose a structured judgmental procedure that involves asking experts to list as many analogies as they can, rate how similar the analogies are to the target situation, and match the outcomes of the analogies with possible outcomes of the target. An administrator would then derive a forecast from the experts' information. We propose a structured judgmental procedure that involves asking experts to list as many analogies as they can, rate how similar the analogies are to the target situation, and match the outcomes of the analogies with possible outcomes of the target. An administrator would then derive a forecast from the experts' information. We compared structured analogies with unaided judgments for predicting the decisions made in eight conflict situations. These were difficult forecasting problems; the 32% accuracy of the unaided experts was only slightly better than chance. These were difficult forecasting problems; the 32% accuracy of the unaided experts was only slightly better than chance. In contrast, 46% of structured analogies forecasts were accurate. These were difficult forecasting problems; the 32% accuracy of the unaided experts was only slightly better than chance. In contrast, 46% of structured analogies forecasts were accurate. Among experts who were independently able to think of two or more analogies and who had direct experience with their closest analogy, 60% of forecasts were accurate. These were difficult forecasting problems; the 32% accuracy of the unaided experts was only slightly better than chance. In contrast, 46% of structured analogies forecasts were accurate. Among experts who were independently able to think of two or more analogies and who had direct experience with their closest analogy, 60% of forecasts were accurate. Collaboration did not improve accuracy."}, {"paper_id": "14772601", "adju_relevance": 0, "title": "Sweetening WORDNET with DOLCE", "background_label": "Despite its original intended use, which was very different, WORDNET is used more and more today as an ontology, where the hyponym relation between word senses is interpreted as a subsumption relation between concepts.", "abstract": "Despite its original intended use, which was very different, WORDNET is used more and more today as an ontology, where the hyponym relation between word senses is interpreted as a subsumption relation between concepts."}, {"paper_id": "5481961", "adju_relevance": 0, "title": "IX. Word-Word Associations in Document Retrieval Systems", "background_label": "The SMART automatic document retrieval system is used to study association procedures for automatic content analysis. The effect of word frequency and other parameters on the association process is investigated through examination of related pairs and through retrieval experiments. Associated pairs of words usually reflect localized word meanings, and true synonyms cannot readily be found from first or second order relationships in our document collections. There is little overlap between word relationships found through associations and those used in thesaurus construction, and the effects of word associations and a thesaurus in retrieval are independent.", "method_label": "The use of associations in retrieval experiments improves not only recall, by permitting new matches between requests and documents, but also precision, by reinforcing existing matches.", "result_label": "In our experiments, the precision effect is responsible for most of the improvement possible with associations. A properly constructed thesaurus, however, offers better performance than statistical association methods.", "abstract": "The SMART automatic document retrieval system is used to study association procedures for automatic content analysis. The SMART automatic document retrieval system is used to study association procedures for automatic content analysis. The effect of word frequency and other parameters on the association process is investigated through examination of related pairs and through retrieval experiments. The SMART automatic document retrieval system is used to study association procedures for automatic content analysis. The effect of word frequency and other parameters on the association process is investigated through examination of related pairs and through retrieval experiments. Associated pairs of words usually reflect localized word meanings, and true synonyms cannot readily be found from first or second order relationships in our document collections. The SMART automatic document retrieval system is used to study association procedures for automatic content analysis. The effect of word frequency and other parameters on the association process is investigated through examination of related pairs and through retrieval experiments. Associated pairs of words usually reflect localized word meanings, and true synonyms cannot readily be found from first or second order relationships in our document collections. There is little overlap between word relationships found through associations and those used in thesaurus construction, and the effects of word associations and a thesaurus in retrieval are independent. The use of associations in retrieval experiments improves not only recall, by permitting new matches between requests and documents, but also precision, by reinforcing existing matches. In our experiments, the precision effect is responsible for most of the improvement possible with associations. In our experiments, the precision effect is responsible for most of the improvement possible with associations. A properly constructed thesaurus, however, offers better performance than statistical association methods."}, {"paper_id": "1144461", "adju_relevance": 0, "title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "background_label": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research.", "method_label": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched.", "abstract": "How do people know as much as they do with as little information as they get? How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "9831789", "adju_relevance": 0, "title": "Extracting relevant questions to an RDF dataset using formal concept analysis", "background_label": "With the rise of linked data, more and more semantically described information is being published online according to the principles and technologies of the Semantic Web (especially, RDF and SPARQL). The use of such standard technologies means that this data should be exploitable, integrable and reusable straight away. However, once a potentially interesting dataset has been discovered, significant efforts are currently required in order to understand its schema, its content, the way to query it and what it can answer.", "method_label": "In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. We use formal concept analysis to build a hierarchy of meaningful sets of entities from a dataset. These sets of entities represent answers, which common characteristics represent the clauses of the corresponding questions. This hierarchy can then be used as a querying interface, proposing questions of varying levels of granularity and specificity to the user. A major issue is however that thousands of questions can be included in this hierarchy. Based on an empirical analysis and using metrics inspired both from formal concept analysis and from ontology summarization, we devise an approach for identifying relevant questions to act as a starting point to the navigation in the question hierarchy.", "abstract": "With the rise of linked data, more and more semantically described information is being published online according to the principles and technologies of the Semantic Web (especially, RDF and SPARQL). With the rise of linked data, more and more semantically described information is being published online according to the principles and technologies of the Semantic Web (especially, RDF and SPARQL). The use of such standard technologies means that this data should be exploitable, integrable and reusable straight away. With the rise of linked data, more and more semantically described information is being published online according to the principles and technologies of the Semantic Web (especially, RDF and SPARQL). The use of such standard technologies means that this data should be exploitable, integrable and reusable straight away. However, once a potentially interesting dataset has been discovered, significant efforts are currently required in order to understand its schema, its content, the way to query it and what it can answer. In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. We use formal concept analysis to build a hierarchy of meaningful sets of entities from a dataset. In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. We use formal concept analysis to build a hierarchy of meaningful sets of entities from a dataset. These sets of entities represent answers, which common characteristics represent the clauses of the corresponding questions. In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. We use formal concept analysis to build a hierarchy of meaningful sets of entities from a dataset. These sets of entities represent answers, which common characteristics represent the clauses of the corresponding questions. This hierarchy can then be used as a querying interface, proposing questions of varying levels of granularity and specificity to the user. In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. We use formal concept analysis to build a hierarchy of meaningful sets of entities from a dataset. These sets of entities represent answers, which common characteristics represent the clauses of the corresponding questions. This hierarchy can then be used as a querying interface, proposing questions of varying levels of granularity and specificity to the user. A major issue is however that thousands of questions can be included in this hierarchy. In this paper, we propose a method and a tool to automatically discover questions that can be answered by an RDF dataset. We use formal concept analysis to build a hierarchy of meaningful sets of entities from a dataset. These sets of entities represent answers, which common characteristics represent the clauses of the corresponding questions. This hierarchy can then be used as a querying interface, proposing questions of varying levels of granularity and specificity to the user. A major issue is however that thousands of questions can be included in this hierarchy. Based on an empirical analysis and using metrics inspired both from formal concept analysis and from ontology summarization, we devise an approach for identifying relevant questions to act as a starting point to the navigation in the question hierarchy."}, {"paper_id": "5121862", "adju_relevance": 0, "title": "NEAL: A Neurally Enhanced Approach to Linking Citation and Reference", "method_label": "As a way to tackle Task 1A in CL-SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q/A domain [2, 7] .", "result_label": "We discuss an experiment using a development data, results thereof, and some remaining issues.", "abstract": " As a way to tackle Task 1A in CL-SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q/A domain [2, 7] . We discuss an experiment using a development data, results thereof, and some remaining issues."}, {"paper_id": "15705646", "adju_relevance": 0, "title": "Corpus-based discovery of semantic intensity scales", "background_label": "Gradable terms such as brief, lengthy and extended illustrate varying degrees of a scale and can therefore participate in comparative constructs. Knowing the set of words that can be compared on the same scale and the associated ordering between them (brief < lengthy < extended) is very useful for a variety of lexical semantic tasks. Current techniques to derive such an ordering rely on WordNet to determine which words belong on the same scale and are limited to adjectives.", "method_label": "Here we describe an extension to recent work: we investigate a fully automated pipeline to extract gradable terms from a corpus, group them into clusters reflecting the same scale and establish an ordering among them. This methodology reduces the amount of required handcrafted knowledge, and can infer gradability of words independent of their part of speech. Our approach infers an ordering for adjectives with comparable performance to previous work, but also for adverbs with an accuracy of 71%.", "result_label": "We find that the technique is useful for inferring such rankings among words across different domains, and present an example using biomedical text.", "abstract": "Gradable terms such as brief, lengthy and extended illustrate varying degrees of a scale and can therefore participate in comparative constructs. Gradable terms such as brief, lengthy and extended illustrate varying degrees of a scale and can therefore participate in comparative constructs. Knowing the set of words that can be compared on the same scale and the associated ordering between them (brief < lengthy < extended) is very useful for a variety of lexical semantic tasks. Gradable terms such as brief, lengthy and extended illustrate varying degrees of a scale and can therefore participate in comparative constructs. Knowing the set of words that can be compared on the same scale and the associated ordering between them (brief < lengthy < extended) is very useful for a variety of lexical semantic tasks. Current techniques to derive such an ordering rely on WordNet to determine which words belong on the same scale and are limited to adjectives. Here we describe an extension to recent work: we investigate a fully automated pipeline to extract gradable terms from a corpus, group them into clusters reflecting the same scale and establish an ordering among them. Here we describe an extension to recent work: we investigate a fully automated pipeline to extract gradable terms from a corpus, group them into clusters reflecting the same scale and establish an ordering among them. This methodology reduces the amount of required handcrafted knowledge, and can infer gradability of words independent of their part of speech. Here we describe an extension to recent work: we investigate a fully automated pipeline to extract gradable terms from a corpus, group them into clusters reflecting the same scale and establish an ordering among them. This methodology reduces the amount of required handcrafted knowledge, and can infer gradability of words independent of their part of speech. Our approach infers an ordering for adjectives with comparable performance to previous work, but also for adverbs with an accuracy of 71%. We find that the technique is useful for inferring such rankings among words across different domains, and present an example using biomedical text."}, {"paper_id": "129179366", "adju_relevance": 0, "title": "Bayesian Approach to Sapwood Estimates and Felling Dates in Dendrochronology", "background_label": "An improved method of generating sapwood estimates for oak is developed. This suggests a revision of the 95% confidence range from 10\u201340 to 9\u201336 rings for trees from southern England.", "method_label": "Current methods for estimating felling dates on timbers with incomplete sapwood do not generate true 95% confidence limits, and a Bayesian method for deriving such limits is presented. For timbers with no sapwood, the addition of 12 years to the date of the final ring is shown to give a 95% confidence limit on the terminus post quem for felling.", "result_label": "The further application of these methods is illustrated by calculation of the common felling date for timbers from the Great Kitchen at Windsor Castle.", "abstract": "An improved method of generating sapwood estimates for oak is developed. An improved method of generating sapwood estimates for oak is developed. This suggests a revision of the 95% confidence range from 10\u201340 to 9\u201336 rings for trees from southern England. Current methods for estimating felling dates on timbers with incomplete sapwood do not generate true 95% confidence limits, and a Bayesian method for deriving such limits is presented. Current methods for estimating felling dates on timbers with incomplete sapwood do not generate true 95% confidence limits, and a Bayesian method for deriving such limits is presented. For timbers with no sapwood, the addition of 12 years to the date of the final ring is shown to give a 95% confidence limit on the terminus post quem for felling. The further application of these methods is illustrated by calculation of the common felling date for timbers from the Great Kitchen at Windsor Castle."}, {"paper_id": "41372474", "adju_relevance": 0, "title": "ClearTK-TimeML: A minimalist approach to TempEval 2013", "background_label": "AbstractThe ClearTK-TimeML submission to TempEval 2013 competed in all English tasks: identifying events, identifying times, and identifying temporal relations.", "method_label": "The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types.", "result_label": "ClearTKTimeML ranked 1 st for temporal relation F1, time extent strict F1 and event tense accuracy.", "abstract": "AbstractThe ClearTK-TimeML submission to TempEval 2013 competed in all English tasks: identifying events, identifying times, and identifying temporal relations. The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types. ClearTKTimeML ranked 1 st for temporal relation F1, time extent strict F1 and event tense accuracy."}, {"paper_id": "14510205", "adju_relevance": 0, "title": "Question Classification with Support Vector Machines and Error Correcting Codes", "background_label": "In this paper we consider a machine learning technique for question classification.", "abstract": "In this paper we consider a machine learning technique for question classification."}, {"paper_id": "5649696", "adju_relevance": 0, "title": "Objects, Associations and Subsystems: A Hierarchical Approach to Encapsulation", "background_label": "We describe a compositional approach to the formal interpretation of type view diagrams and statecharts. We define theories for object instances and classes, and theories for associations between them.", "method_label": "These theories are combined with categorical constructions to yield a formalisation of the entire system. We observe that some notations require the identification of theories intermediate between the theories of the constituent classes and associations and that of the entire system.", "result_label": "This leads us to propose a notion of subsystem which generalises the concept of object and yields an approach to system specification employing object-like encapsulation in a nested hierarchy of components.", "abstract": "We describe a compositional approach to the formal interpretation of type view diagrams and statecharts. We describe a compositional approach to the formal interpretation of type view diagrams and statecharts. We define theories for object instances and classes, and theories for associations between them. These theories are combined with categorical constructions to yield a formalisation of the entire system. These theories are combined with categorical constructions to yield a formalisation of the entire system. We observe that some notations require the identification of theories intermediate between the theories of the constituent classes and associations and that of the entire system. This leads us to propose a notion of subsystem which generalises the concept of object and yields an approach to system specification employing object-like encapsulation in a nested hierarchy of components."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "49585076", "adju_relevance": 0, "title": "Overcoming low-utility facets for complex answer retrieval", "background_label": "Many questions cannot be answered simply; their answers must include numerous nuanced details and additional context. Complex Answer Retrieval (CAR) is the retrieval of answers to such questions. In their simplest form, these questions are constructed from a topic entity (e.g., `cheese') and a facet (e.g., `health effects').", "method_label": "While topic matching has been thoroughly explored, we observe that some facets use general language that is unlikely to appear verbatim in answers. We call these low-utility facets.", "abstract": "Many questions cannot be answered simply; their answers must include numerous nuanced details and additional context. Many questions cannot be answered simply; their answers must include numerous nuanced details and additional context. Complex Answer Retrieval (CAR) is the retrieval of answers to such questions. Many questions cannot be answered simply; their answers must include numerous nuanced details and additional context. Complex Answer Retrieval (CAR) is the retrieval of answers to such questions. In their simplest form, these questions are constructed from a topic entity (e.g., `cheese') and a facet (e.g., `health effects'). While topic matching has been thoroughly explored, we observe that some facets use general language that is unlikely to appear verbatim in answers. While topic matching has been thoroughly explored, we observe that some facets use general language that is unlikely to appear verbatim in answers. We call these low-utility facets."}, {"paper_id": "512194", "adju_relevance": 0, "title": "A Formal Approach to Null Values in Database Relations", "background_label": "We study the problem of null values. By this we mean that an attribute is applicable but its value at present is unknown and also that an attribute is applicable but its value is arbitrary. We adopt the view that tuples denote statements of predicate logic about database relations.", "method_label": "Then, a null value of the first kind, respectively second kind, corresponds to an existentially quantified variable, respectively universally quantified variable. For instance if r is a database relation without null values and X is a range declaration for r then the tuple (a, \u2200,b, \u2203) \u2208 R is intended to mean \u201cthere exists an x \u2208 X such that for all y \u2208 X: (a,y,b,x) \u2208 r\u201d. We extend basic operations of the well-known relational algebra to relations with null values. Using formal notions of correctness and completeness (adapted from predicate logic) we show that our extensions are meaningful and natural. Furthermore we reexamine the generalized join within our framework.", "result_label": "Finally we investigate the algebraic structure of the class of relations with null values under a partial ordering which can be interpreted as a kind of logical implication.", "abstract": "We study the problem of null values. We study the problem of null values. By this we mean that an attribute is applicable but its value at present is unknown and also that an attribute is applicable but its value is arbitrary. We study the problem of null values. By this we mean that an attribute is applicable but its value at present is unknown and also that an attribute is applicable but its value is arbitrary. We adopt the view that tuples denote statements of predicate logic about database relations. Then, a null value of the first kind, respectively second kind, corresponds to an existentially quantified variable, respectively universally quantified variable. Then, a null value of the first kind, respectively second kind, corresponds to an existentially quantified variable, respectively universally quantified variable. For instance if r is a database relation without null values and X is a range declaration for r then the tuple (a, \u2200,b, \u2203) \u2208 R is intended to mean \u201cthere exists an x \u2208 X such that for all y \u2208 X: (a,y,b,x) \u2208 r\u201d. Then, a null value of the first kind, respectively second kind, corresponds to an existentially quantified variable, respectively universally quantified variable. For instance if r is a database relation without null values and X is a range declaration for r then the tuple (a, \u2200,b, \u2203) \u2208 R is intended to mean \u201cthere exists an x \u2208 X such that for all y \u2208 X: (a,y,b,x) \u2208 r\u201d. We extend basic operations of the well-known relational algebra to relations with null values. Then, a null value of the first kind, respectively second kind, corresponds to an existentially quantified variable, respectively universally quantified variable. For instance if r is a database relation without null values and X is a range declaration for r then the tuple (a, \u2200,b, \u2203) \u2208 R is intended to mean \u201cthere exists an x \u2208 X such that for all y \u2208 X: (a,y,b,x) \u2208 r\u201d. We extend basic operations of the well-known relational algebra to relations with null values. Using formal notions of correctness and completeness (adapted from predicate logic) we show that our extensions are meaningful and natural. Then, a null value of the first kind, respectively second kind, corresponds to an existentially quantified variable, respectively universally quantified variable. For instance if r is a database relation without null values and X is a range declaration for r then the tuple (a, \u2200,b, \u2203) \u2208 R is intended to mean \u201cthere exists an x \u2208 X such that for all y \u2208 X: (a,y,b,x) \u2208 r\u201d. We extend basic operations of the well-known relational algebra to relations with null values. Using formal notions of correctness and completeness (adapted from predicate logic) we show that our extensions are meaningful and natural. Furthermore we reexamine the generalized join within our framework. Finally we investigate the algebraic structure of the class of relations with null values under a partial ordering which can be interpreted as a kind of logical implication."}, {"paper_id": "53748665", "adju_relevance": 0, "title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships", "background_label": "Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges.", "abstract": "Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges."}, {"paper_id": "8984950", "adju_relevance": 0, "title": "The proactive brain : using analogies and associations to generate predictions", "background_label": "Rather than passively 'waiting' to be activated by sensations, it is proposed that the human brain is continuously busy generating predictions that approximate the relevant future.", "method_label": "Building on previous work, this proposal posits that rudimentary information is extracted rapidly from the input to derive analogies linking that input with representations in memory. The linked stored representations then activate the associations that are relevant in the specific context, which provides focused predictions. These predictions facilitate perception and cognition by pre-sensitizing relevant representations. Predictions regarding complex information, such as those required in social interactions, integrate multiple analogies.", "result_label": "This cognitive neuroscience framework can help explain a variety of phenomena, ranging from recognition to first impressions, and from the brain's 'default mode' to a host of mental disorders.", "abstract": "Rather than passively 'waiting' to be activated by sensations, it is proposed that the human brain is continuously busy generating predictions that approximate the relevant future. Building on previous work, this proposal posits that rudimentary information is extracted rapidly from the input to derive analogies linking that input with representations in memory. Building on previous work, this proposal posits that rudimentary information is extracted rapidly from the input to derive analogies linking that input with representations in memory. The linked stored representations then activate the associations that are relevant in the specific context, which provides focused predictions. Building on previous work, this proposal posits that rudimentary information is extracted rapidly from the input to derive analogies linking that input with representations in memory. The linked stored representations then activate the associations that are relevant in the specific context, which provides focused predictions. These predictions facilitate perception and cognition by pre-sensitizing relevant representations. Building on previous work, this proposal posits that rudimentary information is extracted rapidly from the input to derive analogies linking that input with representations in memory. The linked stored representations then activate the associations that are relevant in the specific context, which provides focused predictions. These predictions facilitate perception and cognition by pre-sensitizing relevant representations. Predictions regarding complex information, such as those required in social interactions, integrate multiple analogies. This cognitive neuroscience framework can help explain a variety of phenomena, ranging from recognition to first impressions, and from the brain's 'default mode' to a host of mental disorders."}, {"paper_id": "17086579", "adju_relevance": 0, "title": "Method for refining automatically-discovered lexical relations: Combining weak techniques for stronger results", "background_label": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suffice for a particular task; for this reason we are interested in finding ways to combine various techniques and improve their results.", "method_label": "Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we applied a termsimilarity determination technique to the problem of choosing where, in an existing lexical hierarchy, to install a lexical relation.", "result_label": "The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future.", "abstract": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suffice for a particular task; for this reason we are interested in finding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we applied a termsimilarity determination technique to the problem of choosing where, in an existing lexical hierarchy, to install a lexical relation. The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future."}, {"paper_id": "14567261", "adju_relevance": 0, "title": "Revising The Wordnet Domains Hierarchy: Semantics, Coverage And Balancing", "background_label": "The continuous expansion of the multilingual information society has led in recent years to a pressing demand for multilingual linguistic resources suitable to be used for different applications. In this paper we present the WordNet Domains Hierarchy (WDH), a language-independent resource composed of 164, hierarchically organized, domain labels (e.g.", "abstract": "The continuous expansion of the multilingual information society has led in recent years to a pressing demand for multilingual linguistic resources suitable to be used for different applications. The continuous expansion of the multilingual information society has led in recent years to a pressing demand for multilingual linguistic resources suitable to be used for different applications. In this paper we present the WordNet Domains Hierarchy (WDH), a language-independent resource composed of 164, hierarchically organized, domain labels (e.g."}, {"paper_id": "21461440", "adju_relevance": 0, "title": "Semantic and associative priming in the cerebral hemispheres: some words do, some words don't ... sometimes, some places.", "background_label": "This study investigated spreading activation for words presented to the left and right hemispheres using an automatic semantic priming paradigm. Three types of semantic relations were used: similar-only (Deer-Pony), associated-only (Bee-Honey), and similar + associated (Doctor-Nurse). Priming of lexical decisions was symmetrical over visual fields for all semantic relations when prime words were centrally presented.", "method_label": "However, when primes and targets were lateralized to the same visual field, similar-only priming was greater in the LVF than in the RVF, no priming was obtained for associated-only words, and priming was equivalent over visual fields for similar + associated words.", "result_label": "Similar results were found using a naming task. These findings suggest that it is important to lateralize both prime and target information to assess hemisphere-specific spreading activation processes. Further, while spreading activation occurs in either hemisphere for the most highly related words (those related by category membership and association), our findings suggest that automatic access to semantic category relatedness occurs primarily in the right cerebral hemisphere. These results imply a unique role for the right hemisphere in the processing of word meanings. We relate our results to our previous proposal (Burgess & Simpson, 1988a; Chiarello, 1988c) that there is rapid selection of one meaning and suppression of other candidates in the left hemisphere, while activation spreads more diffusely in the right hemisphere. We also outline a new proposal that activation spreads in a different manner for associated words than for words related by semantic similarity.", "abstract": "This study investigated spreading activation for words presented to the left and right hemispheres using an automatic semantic priming paradigm. This study investigated spreading activation for words presented to the left and right hemispheres using an automatic semantic priming paradigm. Three types of semantic relations were used: similar-only (Deer-Pony), associated-only (Bee-Honey), and similar + associated (Doctor-Nurse). This study investigated spreading activation for words presented to the left and right hemispheres using an automatic semantic priming paradigm. Three types of semantic relations were used: similar-only (Deer-Pony), associated-only (Bee-Honey), and similar + associated (Doctor-Nurse). Priming of lexical decisions was symmetrical over visual fields for all semantic relations when prime words were centrally presented. However, when primes and targets were lateralized to the same visual field, similar-only priming was greater in the LVF than in the RVF, no priming was obtained for associated-only words, and priming was equivalent over visual fields for similar + associated words. Similar results were found using a naming task. Similar results were found using a naming task. These findings suggest that it is important to lateralize both prime and target information to assess hemisphere-specific spreading activation processes. Similar results were found using a naming task. These findings suggest that it is important to lateralize both prime and target information to assess hemisphere-specific spreading activation processes. Further, while spreading activation occurs in either hemisphere for the most highly related words (those related by category membership and association), our findings suggest that automatic access to semantic category relatedness occurs primarily in the right cerebral hemisphere. Similar results were found using a naming task. These findings suggest that it is important to lateralize both prime and target information to assess hemisphere-specific spreading activation processes. Further, while spreading activation occurs in either hemisphere for the most highly related words (those related by category membership and association), our findings suggest that automatic access to semantic category relatedness occurs primarily in the right cerebral hemisphere. These results imply a unique role for the right hemisphere in the processing of word meanings. Similar results were found using a naming task. These findings suggest that it is important to lateralize both prime and target information to assess hemisphere-specific spreading activation processes. Further, while spreading activation occurs in either hemisphere for the most highly related words (those related by category membership and association), our findings suggest that automatic access to semantic category relatedness occurs primarily in the right cerebral hemisphere. These results imply a unique role for the right hemisphere in the processing of word meanings. We relate our results to our previous proposal (Burgess & Simpson, 1988a; Chiarello, 1988c) that there is rapid selection of one meaning and suppression of other candidates in the left hemisphere, while activation spreads more diffusely in the right hemisphere. Similar results were found using a naming task. These findings suggest that it is important to lateralize both prime and target information to assess hemisphere-specific spreading activation processes. Further, while spreading activation occurs in either hemisphere for the most highly related words (those related by category membership and association), our findings suggest that automatic access to semantic category relatedness occurs primarily in the right cerebral hemisphere. These results imply a unique role for the right hemisphere in the processing of word meanings. We relate our results to our previous proposal (Burgess & Simpson, 1988a; Chiarello, 1988c) that there is rapid selection of one meaning and suppression of other candidates in the left hemisphere, while activation spreads more diffusely in the right hemisphere. We also outline a new proposal that activation spreads in a different manner for associated words than for words related by semantic similarity."}, {"paper_id": "6430524", "adju_relevance": 0, "title": "Ontology Driven Disease Incidence Detection on Twitter", "background_label": "In this work we address the issue of generic automated disease incidence monitoring on twitter.", "method_label": "We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We show that our ontological approach achieves good performance on this task using a variety of Natural Language Processing Techniques.", "result_label": "We also show that word vectors can be learned directly from our concepts to achieve even better results.", "abstract": "In this work we address the issue of generic automated disease incidence monitoring on twitter. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We show that our ontological approach achieves good performance on this task using a variety of Natural Language Processing Techniques. We also show that word vectors can be learned directly from our concepts to achieve even better results."}, {"paper_id": "7622169", "adju_relevance": 0, "title": "Analysis of Statistical Question Classification for Fact-Based Questions", "background_label": "Question classification systems play an important role in question answering systems and can be used in a wide range of other domains.", "abstract": "Question classification systems play an important role in question answering systems and can be used in a wide range of other domains."}, {"paper_id": "12982947", "adju_relevance": 0, "title": "Support Vector Learning for Semantic Argument Classification", "background_label": "The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. It could play a key role in NLP tasks like Information Extraction, Question Answering and Summarization. Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers. We show performance improvements through a number of new features designed to improve generalization to unseen data, such as automatic clustering of verbs. We also report on various analytic studies examining which features are most important, comparing our classifier to other machine learning algorithms in the literature, and testing its generalization to new test set from different genre.", "method_label": "This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. We propose a machine learning algorithm for semantic role parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. On the task of assigning semantic labels to the PropBank (Kingsbury, Palmer, & Marcus, 2002) corpus, our final system has a precision of 84% and a recall of 75%, which are the best results currently reported for this task. Finally, we explore a completely different architecture which does not requires a deep syntactic parse.", "result_label": "We reformulate the task as a combined chunking and classification problem, thus allowing our algorithm to be applied to new languages or genres of text for which statistical syntactic parsers may not be available.", "abstract": "The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. It could play a key role in NLP tasks like Information Extraction, Question Answering and Summarization. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. We propose a machine learning algorithm for semantic role parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. We propose a machine learning algorithm for semantic role parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. It could play a key role in NLP tasks like Information Extraction, Question Answering and Summarization. Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers. The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. It could play a key role in NLP tasks like Information Extraction, Question Answering and Summarization. Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers. We show performance improvements through a number of new features designed to improve generalization to unseen data, such as automatic clustering of verbs. The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing\u2014the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. It could play a key role in NLP tasks like Information Extraction, Question Answering and Summarization. Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers. We show performance improvements through a number of new features designed to improve generalization to unseen data, such as automatic clustering of verbs. We also report on various analytic studies examining which features are most important, comparing our classifier to other machine learning algorithms in the literature, and testing its generalization to new test set from different genre. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. We propose a machine learning algorithm for semantic role parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. On the task of assigning semantic labels to the PropBank (Kingsbury, Palmer, & Marcus, 2002) corpus, our final system has a precision of 84% and a recall of 75%, which are the best results currently reported for this task. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. We propose a machine learning algorithm for semantic role parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. On the task of assigning semantic labels to the PropBank (Kingsbury, Palmer, & Marcus, 2002) corpus, our final system has a precision of 84% and a recall of 75%, which are the best results currently reported for this task. Finally, we explore a completely different architecture which does not requires a deep syntactic parse. We reformulate the task as a combined chunking and classification problem, thus allowing our algorithm to be applied to new languages or genres of text for which statistical syntactic parsers may not be available."}, {"paper_id": "14099741", "adju_relevance": 0, "title": "Visualizing and Understanding Neural Models in NLP", "background_label": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases.", "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases."}, {"paper_id": "15121152", "adju_relevance": 0, "title": "Learning to suggest questions in social media", "background_label": "Social media systems with Q&A functionalities have accumulated large archives of questions and answers. Two representative types are online forums and community-based Q&A services. To enable users to explore the large number of questions and answers in social media systems effectively, it is essential to suggest interesting items to an active user.", "abstract": "Social media systems with Q&A functionalities have accumulated large archives of questions and answers. Social media systems with Q&A functionalities have accumulated large archives of questions and answers. Two representative types are online forums and community-based Q&A services. Social media systems with Q&A functionalities have accumulated large archives of questions and answers. Two representative types are online forums and community-based Q&A services. To enable users to explore the large number of questions and answers in social media systems effectively, it is essential to suggest interesting items to an active user."}, {"paper_id": "14496668", "adju_relevance": 0, "title": "D.P.: From variadic functions to variadic relations: A miniKanren perspective", "background_label": "We present an implementation of miniKanren, an embedding of logic programming in R 5 RS Scheme that comprises three logic operators.", "method_label": "We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We call such a relation pseudo-variadic. Combining Scheme\u2019s var-args facility with pseudo-variadic helper relations leads to variadic relations, which take a variable number of arguments. We focus on pseudo-variadic relations, which we demonstrate are more flexible than their variadic equivalents. We show how to define plus o in terms of plus o using foldr o and foldl o , higher-order relational abstractions derived from Haskell\u2019s foldr and foldl functions. We define many other pseudo-variadic relations using foldr o and foldl o , consider the limitations of these abstractions, and explore their eect on the divergence behavior of the relations they define.", "result_label": "These higherorder abstractions demonstrate the benefit of embedding relational operators in a functional language. We also consider double-pseudo-variadic relations, a generalization of pseudo-variadic relations that take as their first argument a list of lists or a logical variable representing a list of lists.", "abstract": "We present an implementation of miniKanren, an embedding of logic programming in R 5 RS Scheme that comprises three logic operators. We describe these operators, and use them to define plus o , a relation that adds two numbers. We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We call such a relation pseudo-variadic. We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We call such a relation pseudo-variadic. Combining Scheme\u2019s var-args facility with pseudo-variadic helper relations leads to variadic relations, which take a variable number of arguments. We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We call such a relation pseudo-variadic. Combining Scheme\u2019s var-args facility with pseudo-variadic helper relations leads to variadic relations, which take a variable number of arguments. We focus on pseudo-variadic relations, which we demonstrate are more flexible than their variadic equivalents. We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We call such a relation pseudo-variadic. Combining Scheme\u2019s var-args facility with pseudo-variadic helper relations leads to variadic relations, which take a variable number of arguments. We focus on pseudo-variadic relations, which we demonstrate are more flexible than their variadic equivalents. We show how to define plus o in terms of plus o using foldr o and foldl o , higher-order relational abstractions derived from Haskell\u2019s foldr and foldl functions. These higherorder abstractions demonstrate the benefit of embedding relational operators in a functional language. We describe these operators, and use them to define plus o , a relation that adds two numbers. We then define plus o , which adds zero or more numbers; plus o takes exactly two arguments, the first of which is a list of numbers to be added or a logical variable representing such a list. We call such a relation pseudo-variadic. Combining Scheme\u2019s var-args facility with pseudo-variadic helper relations leads to variadic relations, which take a variable number of arguments. We focus on pseudo-variadic relations, which we demonstrate are more flexible than their variadic equivalents. We show how to define plus o in terms of plus o using foldr o and foldl o , higher-order relational abstractions derived from Haskell\u2019s foldr and foldl functions. We define many other pseudo-variadic relations using foldr o and foldl o , consider the limitations of these abstractions, and explore their eect on the divergence behavior of the relations they define. These higherorder abstractions demonstrate the benefit of embedding relational operators in a functional language. We also consider double-pseudo-variadic relations, a generalization of pseudo-variadic relations that take as their first argument a list of lists or a logical variable representing a list of lists."}]