[{"paper_id": "10695055", "title": "Naturalizing a Programming Language via Interactive Learning", "background_label": "However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language.", "method_label": "To bridge this gap, we start with a core programming language and allow users to\"naturalize\"the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures.", "result_label": "Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9\\% of the last 10K utterances.", "abstract": " However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to\"naturalize\"the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. To bridge this gap, we start with a core programming language and allow users to\"naturalize\"the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9\\% of the last 10K utterances."}, {"paper_id": "13004424", "adju_relevance": 3, "title": "NLyze: interactive programming by natural language for spreadsheet data analysis and manipulation", "background_label": "Millions of computer end users need to perform tasks over tabular spreadsheet data, yet lack the programming knowledge to do such tasks automatically.", "abstract": "Millions of computer end users need to perform tasks over tabular spreadsheet data, yet lack the programming knowledge to do such tasks automatically."}, {"paper_id": "2527577", "adju_relevance": 2, "title": "Programming in natural language: \u201cNLC\u201d as a prototype", "background_label": "The state of the art in computational linguistics has progressed to the point where it is now possible to process simple programs written in natural language.", "abstract": "The state of the art in computational linguistics has progressed to the point where it is now possible to process simple programs written in natural language."}, {"paper_id": "18698162", "adju_relevance": 2, "title": "Provably correct reactive control from natural language", "background_label": "This paper presents an integrated system for generating, troubleshooting, and executing correct-by-construction controllers for autonomous robots using natural language input, allowing non-expert users to command robots to perform high-level tasks.", "method_label": "This system unites the power of formal methods with the accessibility of natural language, providing controllers for implementable high-level task specifications, easy-to-understand feedback on those that cannot be achieved, and natural language explanation of the reason for the robot\u2019s actions during execution. The natural language system uses domain-general components that can easily be adapted to cover the vocabulary of new applications. Generation of a linear temporal logic specification from the user\u2019s natural language input uses a novel data structure that allows for subsequent mapping of logical propositions back to natural language, enabling natural language feedback about problems with the specification that are only identifiable in the logical form.", "result_label": "We demonstrate the robustness of the natural language understanding system through a user study where participants interacted with a simulated robot in a search and rescue scenario. Automated analysis and user feedback on unimplementable specifications is demonstrated using an example involving a robot assistant in a hospital.", "abstract": "This paper presents an integrated system for generating, troubleshooting, and executing correct-by-construction controllers for autonomous robots using natural language input, allowing non-expert users to command robots to perform high-level tasks. This system unites the power of formal methods with the accessibility of natural language, providing controllers for implementable high-level task specifications, easy-to-understand feedback on those that cannot be achieved, and natural language explanation of the reason for the robot\u2019s actions during execution. This system unites the power of formal methods with the accessibility of natural language, providing controllers for implementable high-level task specifications, easy-to-understand feedback on those that cannot be achieved, and natural language explanation of the reason for the robot\u2019s actions during execution. The natural language system uses domain-general components that can easily be adapted to cover the vocabulary of new applications. This system unites the power of formal methods with the accessibility of natural language, providing controllers for implementable high-level task specifications, easy-to-understand feedback on those that cannot be achieved, and natural language explanation of the reason for the robot\u2019s actions during execution. The natural language system uses domain-general components that can easily be adapted to cover the vocabulary of new applications. Generation of a linear temporal logic specification from the user\u2019s natural language input uses a novel data structure that allows for subsequent mapping of logical propositions back to natural language, enabling natural language feedback about problems with the specification that are only identifiable in the logical form. We demonstrate the robustness of the natural language understanding system through a user study where participants interacted with a simulated robot in a search and rescue scenario. We demonstrate the robustness of the natural language understanding system through a user study where participants interacted with a simulated robot in a search and rescue scenario. Automated analysis and user feedback on unimplementable specifications is demonstrated using an example involving a robot assistant in a hospital."}, {"paper_id": "62564371", "adju_relevance": 2, "title": "When FrameNet meets a Controlled Natural Language", "background_label": "There are two approaches to the natural language processing \u2013 one is going in width to cover at shallow level (parsing, syntax) the rich linguistic variety found in the natural language, while another is going in depth (semantics, discourse structure) for a monosemous subset of natural language referred to as a controlled natural language (CNL). Today we are nowhere near to bridging the gap between the two approaches.", "abstract": "There are two approaches to the natural language processing \u2013 one is going in width to cover at shallow level (parsing, syntax) the rich linguistic variety found in the natural language, while another is going in depth (semantics, discourse structure) for a monosemous subset of natural language referred to as a controlled natural language (CNL). There are two approaches to the natural language processing \u2013 one is going in width to cover at shallow level (parsing, syntax) the rich linguistic variety found in the natural language, while another is going in depth (semantics, discourse structure) for a monosemous subset of natural language referred to as a controlled natural language (CNL). Today we are nowhere near to bridging the gap between the two approaches."}, {"paper_id": "9864100", "adju_relevance": 2, "title": "Learning a Natural Language Interface with Neural Programmer", "background_label": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database.", "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database."}, {"paper_id": "6630126", "adju_relevance": 2, "title": "Programming With Unrestricted Natural Language", "background_label": "AbstractWe argue it is better to program in a natural language such as English, instead of a programming language like Java. A natural language interface for programming should result in greater readability, as well as making possible a more intuitive way of writing code.", "method_label": "In contrast to previous controlled language systems, we allow unrestricted syntax, using wide-coverage syntactic and semantic methods to extract information from the user's instructions.We also look at how people actually give programming instructions in English, collecting and annotating a corpus of such statements. We identify differences between sentences in this corpus and in typical newspaper text, and the effect they have on how we process the natural language input.", "result_label": "Finally, we demonstrate a prototype system, that is capable of translating some English instructions into executable code.", "abstract": "AbstractWe argue it is better to program in a natural language such as English, instead of a programming language like Java. AbstractWe argue it is better to program in a natural language such as English, instead of a programming language like Java. A natural language interface for programming should result in greater readability, as well as making possible a more intuitive way of writing code. In contrast to previous controlled language systems, we allow unrestricted syntax, using wide-coverage syntactic and semantic methods to extract information from the user's instructions.We also look at how people actually give programming instructions in English, collecting and annotating a corpus of such statements. In contrast to previous controlled language systems, we allow unrestricted syntax, using wide-coverage syntactic and semantic methods to extract information from the user's instructions.We also look at how people actually give programming instructions in English, collecting and annotating a corpus of such statements. We identify differences between sentences in this corpus and in typical newspaper text, and the effect they have on how we process the natural language input. Finally, we demonstrate a prototype system, that is capable of translating some English instructions into executable code."}, {"paper_id": "7697328", "adju_relevance": 2, "title": "Semantic Acquisition In TELI: A Transportable, User-Customized Natural Language Processor", "background_label": "We discuss ways of allowing the users of a natural language processor to define, examine, and modify the definitions of any domain-specific words or phrases known to the system. An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface (TELI). which answers English questions about tabular (first normal-form) data files and runs on a Symbolics Lisp Machine.", "method_label": "However, our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to. In addition to its obvious practical value.", "result_label": "this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms.", "abstract": "We discuss ways of allowing the users of a natural language processor to define, examine, and modify the definitions of any domain-specific words or phrases known to the system. We discuss ways of allowing the users of a natural language processor to define, examine, and modify the definitions of any domain-specific words or phrases known to the system. An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface (TELI). We discuss ways of allowing the users of a natural language processor to define, examine, and modify the definitions of any domain-specific words or phrases known to the system. An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface (TELI). which answers English questions about tabular (first normal-form) data files and runs on a Symbolics Lisp Machine. However, our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to. However, our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to. In addition to its obvious practical value. this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms."}, {"paper_id": "2373023", "adju_relevance": 2, "title": "A natural language interface plug-in for cooperative query answering in biological databases", "background_label": "BACKGROUND One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. It may be argued that from a biologist's standpoint, queries are not always best posed using a structured language. By this we mean that approximate and flexible responses to natural language like queries are well suited for this domain. This is partly due to biologists' tendency to seek simpler interfaces and partly due to the fact that questions in biology involve high level concepts that are open to interpretations computed using sophisticated tools. In such highly interpretive environments, rigidly structured databases do not always perform well.", "abstract": "BACKGROUND One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. BACKGROUND One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. It may be argued that from a biologist's standpoint, queries are not always best posed using a structured language. BACKGROUND One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. It may be argued that from a biologist's standpoint, queries are not always best posed using a structured language. By this we mean that approximate and flexible responses to natural language like queries are well suited for this domain. BACKGROUND One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. It may be argued that from a biologist's standpoint, queries are not always best posed using a structured language. By this we mean that approximate and flexible responses to natural language like queries are well suited for this domain. This is partly due to biologists' tendency to seek simpler interfaces and partly due to the fact that questions in biology involve high level concepts that are open to interpretations computed using sophisticated tools. BACKGROUND One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. It may be argued that from a biologist's standpoint, queries are not always best posed using a structured language. By this we mean that approximate and flexible responses to natural language like queries are well suited for this domain. This is partly due to biologists' tendency to seek simpler interfaces and partly due to the fact that questions in biology involve high level concepts that are open to interpretations computed using sophisticated tools. In such highly interpretive environments, rigidly structured databases do not always perform well."}, {"paper_id": "62573057", "adju_relevance": 2, "title": "Controlled English to facilitate human/machine analytical processing", "background_label": "Controlled English is a human-readable information representation format that is implemented using a restricted subset of the English language, but which is unambiguous and directly accessible by simple machine processes. We have been researching the capabilities of CE in a number of contexts, and exploring the degree to which a flexible and more human-friendly information representation format could aid the intelligence analyst in a multi-agent collaborative operational environment; especially in cases where the agents are a mixture of other human users and machine processes aimed at assisting the human users. CE itself is built upon a formal logic basis, but allows users to easily specify models for a domain of interest in a human-friendly language.", "method_label": "In our research we have been developing an experimental component known as the \u201cCE Store\u201d in which CE information can be quickly and flexibly processed and shared between human and machine agents. The CE Store environment contains a number of specialized machine agents for common processing tasks and also supports execution of logical inference rules that can be defined in the same CE language. This paper outlines the basic architecture of this approach, discusses some of the example machine agents that have been developed, and provides some typical examples of the CE language and the way in which it has been used to support complex analytical tasks on synthetic data sources.", "result_label": "We highlight the fusion of human and machine processing supported through the use of the CE language and CE Store environment, and show this environment with examples of highly dynamic extensions to the model(s) and integration between different user-defined models in a collaborative setting.", "abstract": "Controlled English is a human-readable information representation format that is implemented using a restricted subset of the English language, but which is unambiguous and directly accessible by simple machine processes. Controlled English is a human-readable information representation format that is implemented using a restricted subset of the English language, but which is unambiguous and directly accessible by simple machine processes. We have been researching the capabilities of CE in a number of contexts, and exploring the degree to which a flexible and more human-friendly information representation format could aid the intelligence analyst in a multi-agent collaborative operational environment; especially in cases where the agents are a mixture of other human users and machine processes aimed at assisting the human users. Controlled English is a human-readable information representation format that is implemented using a restricted subset of the English language, but which is unambiguous and directly accessible by simple machine processes. We have been researching the capabilities of CE in a number of contexts, and exploring the degree to which a flexible and more human-friendly information representation format could aid the intelligence analyst in a multi-agent collaborative operational environment; especially in cases where the agents are a mixture of other human users and machine processes aimed at assisting the human users. CE itself is built upon a formal logic basis, but allows users to easily specify models for a domain of interest in a human-friendly language. In our research we have been developing an experimental component known as the \u201cCE Store\u201d in which CE information can be quickly and flexibly processed and shared between human and machine agents. In our research we have been developing an experimental component known as the \u201cCE Store\u201d in which CE information can be quickly and flexibly processed and shared between human and machine agents. The CE Store environment contains a number of specialized machine agents for common processing tasks and also supports execution of logical inference rules that can be defined in the same CE language. In our research we have been developing an experimental component known as the \u201cCE Store\u201d in which CE information can be quickly and flexibly processed and shared between human and machine agents. The CE Store environment contains a number of specialized machine agents for common processing tasks and also supports execution of logical inference rules that can be defined in the same CE language. This paper outlines the basic architecture of this approach, discusses some of the example machine agents that have been developed, and provides some typical examples of the CE language and the way in which it has been used to support complex analytical tasks on synthetic data sources. We highlight the fusion of human and machine processing supported through the use of the CE language and CE Store environment, and show this environment with examples of highly dynamic extensions to the model(s) and integration between different user-defined models in a collaborative setting."}, {"paper_id": "61651749", "adju_relevance": 2, "title": "Improving Spoken Programming Through Language Design and the Incorporation of Dynamic Context", "background_label": "Spoken programming refers to the idea that human speech can be used as input (via dictation) to create the text of a program. Most previous research on spoken programming has focused on enabling diction of text in existing programming languages, either by creating a spoken syntax for the existing language or by attempting to extract the meaning from general English and using it to produce a program.", "method_label": "We take an alternate approach of modifying the programming language syntax to support speech: We have created a new programming language that uses English words and phrases for its syntax.", "result_label": "This provides several benefits to the spoken programming process, including easier reuse of existing English-based speech tools and a more direct mapping from user speech onto the visible program text.", "abstract": "Spoken programming refers to the idea that human speech can be used as input (via dictation) to create the text of a program. Spoken programming refers to the idea that human speech can be used as input (via dictation) to create the text of a program. Most previous research on spoken programming has focused on enabling diction of text in existing programming languages, either by creating a spoken syntax for the existing language or by attempting to extract the meaning from general English and using it to produce a program. We take an alternate approach of modifying the programming language syntax to support speech: We have created a new programming language that uses English words and phrases for its syntax. This provides several benefits to the spoken programming process, including easier reuse of existing English-based speech tools and a more direct mapping from user speech onto the visible program text."}, {"paper_id": "199627711", "adju_relevance": 2, "title": "Taking shortcuts with OWL using safe macros", "background_label": "Accurate representation of complex domains such as biology demands powerful and expressive ontology languages such as OWL. However, the complex nested class expressions required for modeling can be a hindrance to ontology authoring and adoption. These class expressions can appear opaque to domain experts, and even users proficient in OWL can benefit from some kind of syntactic sugar or \u201cshort-cut\u201d strategy, especially when authoring large ontologies.", "method_label": "One solution is to have domain experts fill in simple templates (for example, in Excel) and translate the results into more complex axioms, but this has the disadvantage of being disconnected from full ontology authoring and reasoning environment. We present here a method of specifying shortcut properties directly in OWL. These shortcut properties can be used in similar ways as object properties within the OWL environment, with the resulting simple axioms translated automatically to more complex axioms via macro expansion. We describe some example scenarios where this is of use in authoring existing bio-ontologies.", "result_label": "One of the main implications of this work is a way to simplify the translation between OBO format and OWL, and the use of RDF triple-stores with complex OWL ontologies.", "abstract": "Accurate representation of complex domains such as biology demands powerful and expressive ontology languages such as OWL. Accurate representation of complex domains such as biology demands powerful and expressive ontology languages such as OWL. However, the complex nested class expressions required for modeling can be a hindrance to ontology authoring and adoption. Accurate representation of complex domains such as biology demands powerful and expressive ontology languages such as OWL. However, the complex nested class expressions required for modeling can be a hindrance to ontology authoring and adoption. These class expressions can appear opaque to domain experts, and even users proficient in OWL can benefit from some kind of syntactic sugar or \u201cshort-cut\u201d strategy, especially when authoring large ontologies. One solution is to have domain experts fill in simple templates (for example, in Excel) and translate the results into more complex axioms, but this has the disadvantage of being disconnected from full ontology authoring and reasoning environment. One solution is to have domain experts fill in simple templates (for example, in Excel) and translate the results into more complex axioms, but this has the disadvantage of being disconnected from full ontology authoring and reasoning environment. We present here a method of specifying shortcut properties directly in OWL. One solution is to have domain experts fill in simple templates (for example, in Excel) and translate the results into more complex axioms, but this has the disadvantage of being disconnected from full ontology authoring and reasoning environment. We present here a method of specifying shortcut properties directly in OWL. These shortcut properties can be used in similar ways as object properties within the OWL environment, with the resulting simple axioms translated automatically to more complex axioms via macro expansion. One solution is to have domain experts fill in simple templates (for example, in Excel) and translate the results into more complex axioms, but this has the disadvantage of being disconnected from full ontology authoring and reasoning environment. We present here a method of specifying shortcut properties directly in OWL. These shortcut properties can be used in similar ways as object properties within the OWL environment, with the resulting simple axioms translated automatically to more complex axioms via macro expansion. We describe some example scenarios where this is of use in authoring existing bio-ontologies. One of the main implications of this work is a way to simplify the translation between OBO format and OWL, and the use of RDF triple-stores with complex OWL ontologies."}, {"paper_id": "13234077", "adju_relevance": 2, "title": "Program Synthesis using Natural Language", "background_label": "Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks.", "abstract": "Interacting with computers is a ubiquitous activity for millions of people. Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks."}, {"paper_id": "14534772", "adju_relevance": 2, "title": "NLify: lightweight spoken natural language interfaces via exhaustive paraphrasing", "abstract": ""}, {"paper_id": "3514435", "adju_relevance": 2, "title": "NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System", "background_label": "We present new data and semantic parsing methods for the problem of mapping English sentences to Bash commands (NL2Bash).", "abstract": "We present new data and semantic parsing methods for the problem of mapping English sentences to Bash commands (NL2Bash)."}, {"paper_id": "12225249", "adju_relevance": 2, "title": "Constructing an Interactive Natural Language Interface for Relational Databases", "background_label": "ABSTRACTNatural language has been the holy grail of query interface designers, but has generally been considered too hard to work with, except in limited specific circumstances.", "abstract": "ABSTRACTNatural language has been the holy grail of query interface designers, but has generally been considered too hard to work with, except in limited specific circumstances."}, {"paper_id": "12463490", "adju_relevance": 2, "title": "Towards portable natural language interfaces to knowledge bases - The case of the ORAKEL system", "background_label": "AbstractThe customization of a natural language interface to a certain application, domain or knowledge base still represents a major effort for end users given the current state-of-the-art.", "abstract": "AbstractThe customization of a natural language interface to a certain application, domain or knowledge base still represents a major effort for end users given the current state-of-the-art."}, {"paper_id": "7190108", "adju_relevance": 2, "title": "NaLIX: an interactive natural language interface for querying XML", "background_label": "Database query languages can be intimidating to the non-expert, leading to the immense recent popularity for keyword based search in spite of its significant limitations. The holy grail has been the development of a natural language query interface.", "abstract": "Database query languages can be intimidating to the non-expert, leading to the immense recent popularity for keyword based search in spite of its significant limitations. Database query languages can be intimidating to the non-expert, leading to the immense recent popularity for keyword based search in spite of its significant limitations. The holy grail has been the development of a natural language query interface."}, {"paper_id": "479235", "adju_relevance": 1, "title": "A Multilingual Natural-Language Interface To Regular Expressions", "background_label": "This report explains a natural-language interface to the formalism of XFST (Xerox Finite State Tool), which is a rich language used for specifying finite state automata and transducers. By using the interface, it is possible to give input to XFST in English and French, as well as to translate formal XFST code into these languages. It is also possible to edit XFST source files and their natural-language equivalents interactively, in parallel.", "method_label": "The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The relations between the different components are defined by compositional interpretation and generation functions, and by corresponding combinatory parsers. This design has been inspired by the logical grammar of Montague. The grammar-driven design makes it easy to extend and to modify the interface, and also to link it with other functionalities such as compiling and semantic reasoning. It is also easy to add new languages to the interface. Both the grammatical theory and the interface facilities based on it have been implemented in the functional programming language Haskell, which supports a declarative and modular style of programming.", "result_label": "Some of the modules developed for the interface have other uses as well: there is a type system of regular expressions, preventing some compiler errors, a denotational semantics in terms of lazy lists, and an extension of the XFST script language by definitions of functions.", "abstract": "This report explains a natural-language interface to the formalism of XFST (Xerox Finite State Tool), which is a rich language used for specifying finite state automata and transducers. This report explains a natural-language interface to the formalism of XFST (Xerox Finite State Tool), which is a rich language used for specifying finite state automata and transducers. By using the interface, it is possible to give input to XFST in English and French, as well as to translate formal XFST code into these languages. This report explains a natural-language interface to the formalism of XFST (Xerox Finite State Tool), which is a rich language used for specifying finite state automata and transducers. By using the interface, it is possible to give input to XFST in English and French, as well as to translate formal XFST code into these languages. It is also possible to edit XFST source files and their natural-language equivalents interactively, in parallel. The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The relations between the different components are defined by compositional interpretation and generation functions, and by corresponding combinatory parsers. The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The relations between the different components are defined by compositional interpretation and generation functions, and by corresponding combinatory parsers. This design has been inspired by the logical grammar of Montague. The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The relations between the different components are defined by compositional interpretation and generation functions, and by corresponding combinatory parsers. This design has been inspired by the logical grammar of Montague. The grammar-driven design makes it easy to extend and to modify the interface, and also to link it with other functionalities such as compiling and semantic reasoning. The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The relations between the different components are defined by compositional interpretation and generation functions, and by corresponding combinatory parsers. This design has been inspired by the logical grammar of Montague. The grammar-driven design makes it easy to extend and to modify the interface, and also to link it with other functionalities such as compiling and semantic reasoning. It is also easy to add new languages to the interface. The interface is based on an abstract syntax of the regular expression language and of a corresponding fragment of natural language. The relations between the different components are defined by compositional interpretation and generation functions, and by corresponding combinatory parsers. This design has been inspired by the logical grammar of Montague. The grammar-driven design makes it easy to extend and to modify the interface, and also to link it with other functionalities such as compiling and semantic reasoning. It is also easy to add new languages to the interface. Both the grammatical theory and the interface facilities based on it have been implemented in the functional programming language Haskell, which supports a declarative and modular style of programming. Some of the modules developed for the interface have other uses as well: there is a type system of regular expressions, preventing some compiler errors, a denotational semantics in terms of lazy lists, and an extension of the XFST script language by definitions of functions."}, {"paper_id": "105477", "adju_relevance": 1, "title": "A Semantics-based Communication System for Dysphasic Subjects", "background_label": "Dysphasic subjects do not have complete linguistic abilities and only produce a weakly structured, topicalized language. They are offered artificial symbolic languages to help them communicate in a way more adapted to their linguistic abilities.", "method_label": "After a structural analysis of a corpus of utterances from children with cerebral palsy, we define a semantic lexicon for such a symbolic language. We use it as the basis of a semantic analysis process able to retrieve an interpretation of the utterances.", "result_label": "This semantic analyser is currently used in an application designed to convert iconic languages into natural language; it might find other uses in the field of language rehabilitation.", "abstract": "Dysphasic subjects do not have complete linguistic abilities and only produce a weakly structured, topicalized language. Dysphasic subjects do not have complete linguistic abilities and only produce a weakly structured, topicalized language. They are offered artificial symbolic languages to help them communicate in a way more adapted to their linguistic abilities. After a structural analysis of a corpus of utterances from children with cerebral palsy, we define a semantic lexicon for such a symbolic language. After a structural analysis of a corpus of utterances from children with cerebral palsy, we define a semantic lexicon for such a symbolic language. We use it as the basis of a semantic analysis process able to retrieve an interpretation of the utterances. This semantic analyser is currently used in an application designed to convert iconic languages into natural language; it might find other uses in the field of language rehabilitation."}, {"paper_id": "2604338", "adju_relevance": 1, "title": "Usable natural language interfaces through menu-based natural language understanding", "background_label": "Conventional natural language interfaces suffer from several ease-of-use problems. They require a user to type and to formulate questions in a way that the system can understand. They have high failure rates which often frustrate users, and users often do not use features of the systems because they are unaware of them or don't trust them. In addition, conventional natural language systems are expensive to build and require large amounts of storage to use.", "abstract": "Conventional natural language interfaces suffer from several ease-of-use problems. Conventional natural language interfaces suffer from several ease-of-use problems. They require a user to type and to formulate questions in a way that the system can understand. Conventional natural language interfaces suffer from several ease-of-use problems. They require a user to type and to formulate questions in a way that the system can understand. They have high failure rates which often frustrate users, and users often do not use features of the systems because they are unaware of them or don't trust them. Conventional natural language interfaces suffer from several ease-of-use problems. They require a user to type and to formulate questions in a way that the system can understand. They have high failure rates which often frustrate users, and users often do not use features of the systems because they are unaware of them or don't trust them. In addition, conventional natural language systems are expensive to build and require large amounts of storage to use."}, {"paper_id": "26580610", "adju_relevance": 1, "title": "Natural Language Data Management and Interfaces: Recent Development and Open Challenges", "background_label": "The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated to publishing and the progress on the digitization of printed texts. This growth combined with the proliferation of natural language systems for search and retrieving information provides tremendous opportunities for studying some of the areas where database systems and natural language processing systems overlap.", "method_label": "This tutorial explores two more relevant areas of overlap to the database community: (1) managing natural language text data in a relational database, and (2) developing natural language interfaces to databases. The tutorial presents state-of-the-art methods, related systems, research opportunities and challenges covering both areas.", "abstract": "The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated to publishing and the progress on the digitization of printed texts. The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated to publishing and the progress on the digitization of printed texts. This growth combined with the proliferation of natural language systems for search and retrieving information provides tremendous opportunities for studying some of the areas where database systems and natural language processing systems overlap. This tutorial explores two more relevant areas of overlap to the database community: (1) managing natural language text data in a relational database, and (2) developing natural language interfaces to databases. This tutorial explores two more relevant areas of overlap to the database community: (1) managing natural language text data in a relational database, and (2) developing natural language interfaces to databases. The tutorial presents state-of-the-art methods, related systems, research opportunities and challenges covering both areas."}, {"paper_id": "6216733", "adju_relevance": 1, "title": "Using Semantic Unification to Generate Regular Expressions from Natural Language", "background_label": "AbstractWe consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language.", "result_label": "We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy.", "abstract": "AbstractWe consider the problem of translating natural language text queries into regular expressions which represent their meaning. AbstractWe consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. AbstractWe consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy."}, {"paper_id": "6228816", "adju_relevance": 1, "title": "Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification", "method_label": "Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.", "result_label": "Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.", "abstract": " Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations."}, {"paper_id": "2916543", "adju_relevance": 1, "title": "Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes", "background_label": "Using natural language to write programs is a touchstone problem for computational linguistics.", "abstract": "Using natural language to write programs is a touchstone problem for computational linguistics."}, {"paper_id": "29045129", "adju_relevance": 1, "title": "XQBE: A Graphical Environment to Query XML Data", "background_label": "XQuery, the standard query language for XML, is increasingly popular among computer scientists with SQL background, since XQuery and SQL require comparable skills. However, these experts are limited in number, and the availability of easier XQuery \u201cdialects\u201d could be extremely valuable.", "abstract": "XQuery, the standard query language for XML, is increasingly popular among computer scientists with SQL background, since XQuery and SQL require comparable skills. XQuery, the standard query language for XML, is increasingly popular among computer scientists with SQL background, since XQuery and SQL require comparable skills. However, these experts are limited in number, and the availability of easier XQuery \u201cdialects\u201d could be extremely valuable."}, {"paper_id": "9955699", "adju_relevance": 1, "title": "Macho: Writing Programs with Natural Language and Examples", "background_label": "Current natural language programming systems avoid the difficulties of dealing with abstract and ambiguous concepts by restricting the input words to those comparable to a normal high-level programming language. Our system, Macho can write programs from significantly more abstract language by asking the programmer to provide a unit test: one or more examples of correct input and output.", "result_label": "This may seem unnecessarily complicated, but we show that natural language and examples have a surprising synergy both in constraining the ambiguity of the specification and in generating correct solutions.", "abstract": "Current natural language programming systems avoid the difficulties of dealing with abstract and ambiguous concepts by restricting the input words to those comparable to a normal high-level programming language. Current natural language programming systems avoid the difficulties of dealing with abstract and ambiguous concepts by restricting the input words to those comparable to a normal high-level programming language. Our system, Macho can write programs from significantly more abstract language by asking the programmer to provide a unit test: one or more examples of correct input and output. This may seem unnecessarily complicated, but we show that natural language and examples have a surprising synergy both in constraining the ambiguity of the specification and in generating correct solutions."}, {"paper_id": "2975249", "adju_relevance": 1, "title": "Frame semantics in text-to-scene generation", "background_label": "3D graphics scenes are difficult to create, requiring users to learn and utilize a series of complex menus, dialog boxes, and often tedious direct manipulation techniques. By giving up some amount of control afforded by such interfaces we have found that users can use natural language to quickly and easily create a wide variety of 3D scenes. Natural language offers an interface that is intuitive and immediately accessible by anyone, without requiring any special skill or training.", "method_label": "The WordsEye system (http://www.wordseye.com) has been used by several thousand users on the web to create over 10,000 scenes. The system relies on a large database of 3D models and poses to depict entities and actions.", "result_label": "We describe how the current version of the system incorporates the type of lexical and real-world knowledge needed to depict scenes from language.", "abstract": " 3D graphics scenes are difficult to create, requiring users to learn and utilize a series of complex menus, dialog boxes, and often tedious direct manipulation techniques. 3D graphics scenes are difficult to create, requiring users to learn and utilize a series of complex menus, dialog boxes, and often tedious direct manipulation techniques. By giving up some amount of control afforded by such interfaces we have found that users can use natural language to quickly and easily create a wide variety of 3D scenes. 3D graphics scenes are difficult to create, requiring users to learn and utilize a series of complex menus, dialog boxes, and often tedious direct manipulation techniques. By giving up some amount of control afforded by such interfaces we have found that users can use natural language to quickly and easily create a wide variety of 3D scenes. Natural language offers an interface that is intuitive and immediately accessible by anyone, without requiring any special skill or training. The WordsEye system (http://www.wordseye.com) has been used by several thousand users on the web to create over 10,000 scenes. The WordsEye system (http://www.wordseye.com) has been used by several thousand users on the web to create over 10,000 scenes. The system relies on a large database of 3D models and poses to depict entities and actions. We describe how the current version of the system incorporates the type of lexical and real-world knowledge needed to depict scenes from language."}, {"paper_id": "14790149", "adju_relevance": 1, "title": "NLP (Natural Language Processing) for NLP (Natural Language Programming)", "background_label": "Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will (hopefully) be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted. We would like to challenge this assumption.", "method_label": "We believe that modem Natural Language Processing techniques can make possible the use of natural language to (at least partially) express programming ideas, thus drastically increasing the accessibility of programming to non-expert users.", "abstract": "Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will (hopefully) be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will (hopefully) be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will (hopefully) be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted. We would like to challenge this assumption. We believe that modem Natural Language Processing techniques can make possible the use of natural language to (at least partially) express programming ideas, thus drastically increasing the accessibility of programming to non-expert users."}, {"paper_id": "16897092", "adju_relevance": 1, "title": "NaturalJava: a natural language interface for programming in Java", "background_label": "NaturalJava is a prototype for an intelligent natural-language-based user interface for creating, modifying, and examining Java programs. The interface exploits three subsystems.", "method_label": "The Sundance natural language processing system accepts English sentences as input and uses information extraction techniques to generate case frames representing program construction and editing directives. A knowledge-based case frame interpreter, PRISM, uses a decision tree to infer program modification operations from the case frames. A Java abstract syntax tree manager, TreeFace, provides the interface that PRISM uses to build and navigate the tree representation of an evolving Java program.", "result_label": "In this paper, we describe the technical details of each component, explain the capabilities of the user interface, and present examples of NaturalJava in use.", "abstract": "NaturalJava is a prototype for an intelligent natural-language-based user interface for creating, modifying, and examining Java programs. NaturalJava is a prototype for an intelligent natural-language-based user interface for creating, modifying, and examining Java programs. The interface exploits three subsystems. The Sundance natural language processing system accepts English sentences as input and uses information extraction techniques to generate case frames representing program construction and editing directives. The Sundance natural language processing system accepts English sentences as input and uses information extraction techniques to generate case frames representing program construction and editing directives. A knowledge-based case frame interpreter, PRISM, uses a decision tree to infer program modification operations from the case frames. The Sundance natural language processing system accepts English sentences as input and uses information extraction techniques to generate case frames representing program construction and editing directives. A knowledge-based case frame interpreter, PRISM, uses a decision tree to infer program modification operations from the case frames. A Java abstract syntax tree manager, TreeFace, provides the interface that PRISM uses to build and navigate the tree representation of an evolving Java program. In this paper, we describe the technical details of each component, explain the capabilities of the user interface, and present examples of NaturalJava in use."}, {"paper_id": "195705682", "adju_relevance": 1, "title": "The Domain-Specific Language Monaco and its Visual Interactive Programming Environment", "background_label": "Monaco is a domain-specific language for machine automation programming. It has been developed with the objective to empower domain experts with limited programming capabilities. Its main language features are an imperative notation for reactive systems, concepts for describing asynchronous event handling in a concise way, and a state-of-the-art component approach. Monaco is a programming language with a Pascal-like syntax, but also comes with a visual programming environment.", "result_label": "In this paper we review the language Monaco, show the visual representation scheme, report on the programming environment and compare our visual notation to Statecharts.", "abstract": "Monaco is a domain-specific language for machine automation programming. Monaco is a domain-specific language for machine automation programming. It has been developed with the objective to empower domain experts with limited programming capabilities. Monaco is a domain-specific language for machine automation programming. It has been developed with the objective to empower domain experts with limited programming capabilities. Its main language features are an imperative notation for reactive systems, concepts for describing asynchronous event handling in a concise way, and a state-of-the-art component approach. Monaco is a domain-specific language for machine automation programming. It has been developed with the objective to empower domain experts with limited programming capabilities. Its main language features are an imperative notation for reactive systems, concepts for describing asynchronous event handling in a concise way, and a state-of-the-art component approach. Monaco is a programming language with a Pascal-like syntax, but also comes with a visual programming environment. In this paper we review the language Monaco, show the visual representation scheme, report on the programming environment and compare our visual notation to Statecharts."}, {"paper_id": "195069123", "adju_relevance": 1, "title": "Declarative Learning-Based Programming as an Interface to AI Systems", "background_label": "Data-driven approaches are becoming more common as problem-solving techniques in many areas of research and industry. In most cases, machine learning models are the key component of these solutions, but a solution involves multiple such models, along with significant levels of reasoning with the models' output and input.", "abstract": "Data-driven approaches are becoming more common as problem-solving techniques in many areas of research and industry. Data-driven approaches are becoming more common as problem-solving techniques in many areas of research and industry. In most cases, machine learning models are the key component of these solutions, but a solution involves multiple such models, along with significant levels of reasoning with the models' output and input."}, {"paper_id": "12867072", "adju_relevance": 1, "title": "Task Oriented Programming with Purely Compositional Interactive Scalable Vector Graphics", "background_label": "iTasks enables the rapid creation of multi-user web-applications by automatically generating form-based graphical user interfaces (GUIs) for any first-order type. In some situations, however, form-based GUIs are not sufficient or do not even make sense.", "abstract": "iTasks enables the rapid creation of multi-user web-applications by automatically generating form-based graphical user interfaces (GUIs) for any first-order type. iTasks enables the rapid creation of multi-user web-applications by automatically generating form-based graphical user interfaces (GUIs) for any first-order type. In some situations, however, form-based GUIs are not sufficient or do not even make sense."}, {"paper_id": "38805447", "adju_relevance": 1, "title": "Natural language processing future", "background_label": "Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Most of these are very useful in everyday life for example a machine that takes instructions by voice. There are lots of research groups working on this topic to develop more practical are useful systems. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will hopefully be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted.", "abstract": "Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Most of these are very useful in everyday life for example a machine that takes instructions by voice. Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Most of these are very useful in everyday life for example a machine that takes instructions by voice. There are lots of research groups working on this topic to develop more practical are useful systems. Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Most of these are very useful in everyday life for example a machine that takes instructions by voice. There are lots of research groups working on this topic to develop more practical are useful systems. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will hopefully be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Most of these are very useful in everyday life for example a machine that takes instructions by voice. There are lots of research groups working on this topic to develop more practical are useful systems. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will hopefully be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted."}, {"paper_id": "30261317", "adju_relevance": 1, "title": "Programming Bots by Synthesizing Natural Language Expressions into API Invocations", "background_label": "At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code -- all made possible by APIs. Yet, developers today are not as empowered to program bots in much the same way.", "method_label": "To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. Our solution is two faceted: Firstly, we construct an API knowledge graph to encode and evolve APIs; secondly, leveraging the above we apply techniques in NLP, ML and Entity Recognition to perform the required synthesis from natural language user expressions into API calls.", "abstract": "At present, bots are still in their preliminary stages of development. At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code -- all made possible by APIs. At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code -- all made possible by APIs. Yet, developers today are not as empowered to program bots in much the same way. To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. Our solution is two faceted: Firstly, we construct an API knowledge graph to encode and evolve APIs; secondly, leveraging the above we apply techniques in NLP, ML and Entity Recognition to perform the required synthesis from natural language user expressions into API calls."}, {"paper_id": "1836129", "adju_relevance": 1, "title": "Realization of natural language interfaces using lazy functional programming", "background_label": "The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. Much of the research related to the design and implementation of natural language interfaces has involved the use of high-level declarative programming languages. This is to be expected as the task is extremely difficult, involving syntactic and semantic analysis of potentially ambiguous input. The use of LISP and Prolog in this area is well documented.", "result_label": "However, research involving the relatively new lazy functional programming paradigm is less well known. This paper provides a comprehensive survey of that research.", "abstract": "The construction of natural language interfaces to computers continues to be a major challenge. The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. Much of the research related to the design and implementation of natural language interfaces has involved the use of high-level declarative programming languages. The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. Much of the research related to the design and implementation of natural language interfaces has involved the use of high-level declarative programming languages. This is to be expected as the task is extremely difficult, involving syntactic and semantic analysis of potentially ambiguous input. The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. Much of the research related to the design and implementation of natural language interfaces has involved the use of high-level declarative programming languages. This is to be expected as the task is extremely difficult, involving syntactic and semantic analysis of potentially ambiguous input. The use of LISP and Prolog in this area is well documented. However, research involving the relatively new lazy functional programming paradigm is less well known. However, research involving the relatively new lazy functional programming paradigm is less well known. This paper provides a comprehensive survey of that research."}, {"paper_id": "11294519", "adju_relevance": 1, "title": "Eviza: A Natural Language Interface for Visual Analysis", "background_label": "Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective.", "method_label": "Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. The result of an interaction is a change to the view (e.g., filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults.", "result_label": "There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis.", "abstract": "Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. The result of an interaction is a change to the view (e.g., filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis."}, {"paper_id": "625316", "adju_relevance": 1, "title": "A New Skill Based Robot Programming Language Using UML/P Statecharts", "background_label": "This paper introduces the new robot programming language LightRocks (Light Weight Robot Coding for Skills), a domain specific language (DSL) for robot programming. The language offers three different level of abstraction for robot programming. On lowest level skills are coded by domain experts. On a more abstract level these skills are supposed to be combined by shop floor workers or technicians to define tasks.", "method_label": "The language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers. A Statechart like model is used to describe the different levels of detail. For this we apply the UML/P and the language workbench MontiCore. To this end we are able to generate code while hiding controller specific implementation details.", "result_label": "In addition the development in LightRocks is supported by a generic graphical editor implemented as an Eclipse plugin.", "abstract": "This paper introduces the new robot programming language LightRocks (Light Weight Robot Coding for Skills), a domain specific language (DSL) for robot programming. This paper introduces the new robot programming language LightRocks (Light Weight Robot Coding for Skills), a domain specific language (DSL) for robot programming. The language offers three different level of abstraction for robot programming. This paper introduces the new robot programming language LightRocks (Light Weight Robot Coding for Skills), a domain specific language (DSL) for robot programming. The language offers three different level of abstraction for robot programming. On lowest level skills are coded by domain experts. This paper introduces the new robot programming language LightRocks (Light Weight Robot Coding for Skills), a domain specific language (DSL) for robot programming. The language offers three different level of abstraction for robot programming. On lowest level skills are coded by domain experts. On a more abstract level these skills are supposed to be combined by shop floor workers or technicians to define tasks. The language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers. The language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers. A Statechart like model is used to describe the different levels of detail. The language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers. A Statechart like model is used to describe the different levels of detail. For this we apply the UML/P and the language workbench MontiCore. The language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers. A Statechart like model is used to describe the different levels of detail. For this we apply the UML/P and the language workbench MontiCore. To this end we are able to generate code while hiding controller specific implementation details. In addition the development in LightRocks is supported by a generic graphical editor implemented as an Eclipse plugin."}, {"paper_id": "776614", "adju_relevance": 1, "title": "NaLIX: A generic natural language search environment for XML data", "background_label": "We describe the construction of a generic natural language query interface to an XML database. Our interface can accept a large class of English sentences as a query, which can be quite complex and include aggregation, nesting, and value joins, among other things.", "method_label": "This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. Iterative search in the form of followup queries is also supported.", "result_label": "Our experimental assessment, through a user study, demonstrates that this type of natural language interface is good enough to be usable now, with no restrictions on the application domain.", "abstract": "We describe the construction of a generic natural language query interface to an XML database. We describe the construction of a generic natural language query interface to an XML database. Our interface can accept a large class of English sentences as a query, which can be quite complex and include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression. This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. Iterative search in the form of followup queries is also supported. Our experimental assessment, through a user study, demonstrates that this type of natural language interface is good enough to be usable now, with no restrictions on the application domain."}, {"paper_id": "7738268", "adju_relevance": 1, "title": "The PSG system: from formal language definitions to interactive programming environments", "background_label": "The PSG programming system generator developed at the Technical University of Darmstadt produces interactive, language-specific programming environments from formal language definitions. All language-dependent parts of the environment are generated from an entirely nonprocedural specification of the language's syntax, context conditions, and dynamic semantics.", "method_label": "The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The dynamic semantics of the language are defined in denotational style using a functional language based on the lambda calculus. Program fragments are compiled to terms of the functional language which are executed by an interpreter.", "result_label": "The PSG generator has been used to produce environments for Pascal, ALGOL 60, MODULA-2, and the formal language definition language itself.", "abstract": "The PSG programming system generator developed at the Technical University of Darmstadt produces interactive, language-specific programming environments from formal language definitions. The PSG programming system generator developed at the Technical University of Darmstadt produces interactive, language-specific programming environments from formal language definitions. All language-dependent parts of the environment are generated from an entirely nonprocedural specification of the language's syntax, context conditions, and dynamic semantics. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The dynamic semantics of the language are defined in denotational style using a functional language based on the lambda calculus. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The dynamic semantics of the language are defined in denotational style using a functional language based on the lambda calculus. Program fragments are compiled to terms of the functional language which are executed by an interpreter. The PSG generator has been used to produce environments for Pascal, ALGOL 60, MODULA-2, and the formal language definition language itself."}, {"paper_id": "449252", "adju_relevance": 1, "title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars", "method_label": "We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence.", "result_label": "We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.", "abstract": " We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains."}, {"paper_id": "8580484", "adju_relevance": 1, "title": "Ezhil: A Tamil Programming Language", "background_label": "Ezhil is a Tamil language based interpreted procedural programming language. Tamil keywords and grammar are chosen to make the native Tamil speaker write programs in the Ezhil system. Ezhil allows easy representation of computer program closer to the Tamil language logical constructs equivalent to the conditional, branch and loop statements in modern English based programming languages.", "method_label": "Ezhil is a compact programming language aimed towards Tamil speaking novice computer users.", "result_label": "Grammar for Ezhil and a few example programs are reported here, from the initial proof-of-concept implementation using the Python programming language1. To the best of our knowledge, Ezhil language is the first freely available Tamil programming language.", "abstract": "Ezhil is a Tamil language based interpreted procedural programming language. Ezhil is a Tamil language based interpreted procedural programming language. Tamil keywords and grammar are chosen to make the native Tamil speaker write programs in the Ezhil system. Ezhil is a Tamil language based interpreted procedural programming language. Tamil keywords and grammar are chosen to make the native Tamil speaker write programs in the Ezhil system. Ezhil allows easy representation of computer program closer to the Tamil language logical constructs equivalent to the conditional, branch and loop statements in modern English based programming languages. Ezhil is a compact programming language aimed towards Tamil speaking novice computer users. Grammar for Ezhil and a few example programs are reported here, from the initial proof-of-concept implementation using the Python programming language1. Grammar for Ezhil and a few example programs are reported here, from the initial proof-of-concept implementation using the Python programming language1. To the best of our knowledge, Ezhil language is the first freely available Tamil programming language."}, {"paper_id": "6774937", "adju_relevance": 1, "title": "DBMSs Should Talk Back Too", "background_label": "Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. The problem appears to be deceivingly simple, as there are no ambiguities or other complications in interpreting internal database elements, so initially a straightforward translation appears adequate. Reality teaches us quite the opposite, however, as the resulting text should be expressive, i.e., accurate in capturing the underlying queries or data, and effective, i.e., allowing fast and unique interpretation of them. Achieving both of these qualities is very difficult and raises several technical challenges that need to be addressed.", "method_label": "Likewise, information extraction has received considerable attention in the past ten years or so, identifying structured information in free text so that it may then be stored appropriately and queried. In this paper, we first expose the reader to several situations and applications that need translation into natural language, thereby, motivating the problem. We then outline, by example, the research problems that need to be solved, separately for data translations and query translations.", "result_label": "Validation of the records stored with a backward translation into text would again be very powerful.", "abstract": "Natural language user interfaces to database systems have been studied for several decades now. Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Likewise, information extraction has received considerable attention in the past ten years or so, identifying structured information in free text so that it may then be stored appropriately and queried. Validation of the records stored with a backward translation into text would again be very powerful. Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. The problem appears to be deceivingly simple, as there are no ambiguities or other complications in interpreting internal database elements, so initially a straightforward translation appears adequate. Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. The problem appears to be deceivingly simple, as there are no ambiguities or other complications in interpreting internal database elements, so initially a straightforward translation appears adequate. Reality teaches us quite the opposite, however, as the resulting text should be expressive, i.e., accurate in capturing the underlying queries or data, and effective, i.e., allowing fast and unique interpretation of them. Natural language user interfaces to database systems have been studied for several decades now. They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. The problem appears to be deceivingly simple, as there are no ambiguities or other complications in interpreting internal database elements, so initially a straightforward translation appears adequate. Reality teaches us quite the opposite, however, as the resulting text should be expressive, i.e., accurate in capturing the underlying queries or data, and effective, i.e., allowing fast and unique interpretation of them. Achieving both of these qualities is very difficult and raises several technical challenges that need to be addressed. Likewise, information extraction has received considerable attention in the past ten years or so, identifying structured information in free text so that it may then be stored appropriately and queried. In this paper, we first expose the reader to several situations and applications that need translation into natural language, thereby, motivating the problem. Likewise, information extraction has received considerable attention in the past ten years or so, identifying structured information in free text so that it may then be stored appropriately and queried. In this paper, we first expose the reader to several situations and applications that need translation into natural language, thereby, motivating the problem. We then outline, by example, the research problems that need to be solved, separately for data translations and query translations."}, {"paper_id": "8893652", "adju_relevance": 1, "title": "Menu-Based Natural Language Understanding", "background_label": "Menu-Based Natural Language Understanding is a new approach to building natural language interfaces. It retains the main goals of natural language systems: flexibility, expressive power, learnability and mnemonicity. However, it solves most of the problems inherent to conventional natural language systems. All queries are understood by the system, interface generation is much simpler, and less computing power is required.", "method_label": "Many interfaces have been built using the menu-based natural language technology.", "abstract": "Menu-Based Natural Language Understanding is a new approach to building natural language interfaces. Menu-Based Natural Language Understanding is a new approach to building natural language interfaces. It retains the main goals of natural language systems: flexibility, expressive power, learnability and mnemonicity. Menu-Based Natural Language Understanding is a new approach to building natural language interfaces. It retains the main goals of natural language systems: flexibility, expressive power, learnability and mnemonicity. However, it solves most of the problems inherent to conventional natural language systems. Menu-Based Natural Language Understanding is a new approach to building natural language interfaces. It retains the main goals of natural language systems: flexibility, expressive power, learnability and mnemonicity. However, it solves most of the problems inherent to conventional natural language systems. All queries are understood by the system, interface generation is much simpler, and less computing power is required. Many interfaces have been built using the menu-based natural language technology."}, {"paper_id": "62952792", "adju_relevance": 1, "title": "Natural Language Programming", "background_label": "A procedural semantics system is described for English imperative sentences in natural language programming. Issues related to the handling of dialog focus, noun group resolution, quantifier processing, and imperative verb execution are discussed.", "method_label": "Sequences of imperative sentences may be assembled to build natural language programs and techniques are given for processing such programs.", "result_label": "The final sections include a discussion of related research and a brief overview of the field.", "abstract": "A procedural semantics system is described for English imperative sentences in natural language programming. A procedural semantics system is described for English imperative sentences in natural language programming. Issues related to the handling of dialog focus, noun group resolution, quantifier processing, and imperative verb execution are discussed. Sequences of imperative sentences may be assembled to build natural language programs and techniques are given for processing such programs. The final sections include a discussion of related research and a brief overview of the field."}, {"paper_id": "15391397", "adju_relevance": 1, "title": "Developing a natural language interface to complex data", "background_label": "Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described.", "method_label": "The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed.", "result_label": "The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed.", "abstract": "Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed."}, {"paper_id": "18504610", "adju_relevance": 1, "title": "CommandSpace: modeling the relationships between tasks, descriptions and features", "background_label": "Users often describe what they want to accomplish with an application in a language that is very different from the application's domain language.", "method_label": "To address this gap between system and human language, we propose modeling an application's domain language by mining a large corpus of Web documents about the application using deep learning techniques. A high dimensional vector space representation can model the relationships between user tasks, system commands, and natural language descriptions and supports mapping operations, such as identifying likely system commands given natural language queries and identifying user tasks given a trace of user operations.", "result_label": "We demonstrate the feasibility of this approach with a system, CommandSpace, for the popular photo editing application Adobe Photoshop. We build and evaluate several applications enabled by our model showing the power and flexibility of this approach.", "abstract": "Users often describe what they want to accomplish with an application in a language that is very different from the application's domain language. To address this gap between system and human language, we propose modeling an application's domain language by mining a large corpus of Web documents about the application using deep learning techniques. To address this gap between system and human language, we propose modeling an application's domain language by mining a large corpus of Web documents about the application using deep learning techniques. A high dimensional vector space representation can model the relationships between user tasks, system commands, and natural language descriptions and supports mapping operations, such as identifying likely system commands given natural language queries and identifying user tasks given a trace of user operations. We demonstrate the feasibility of this approach with a system, CommandSpace, for the popular photo editing application Adobe Photoshop. We demonstrate the feasibility of this approach with a system, CommandSpace, for the popular photo editing application Adobe Photoshop. We build and evaluate several applications enabled by our model showing the power and flexibility of this approach."}, {"paper_id": "53783607", "adju_relevance": 1, "title": "From natural language to shell script: A case-based reasoning system for automatic UNIX programming", "background_label": "Abstract We present a case-based approach for natural language interface to a UNIX system with automatic programming ability.", "method_label": "Natural language commands are analyzed by case-based parsing and then transformed into UNIX shell-scripts by case-based planning with derivational analogy. Our system, a Dialogue Interface to uNIX (DINX) demonstrates that natural language parsing should be augmented with automatic programming to provide efficient and user friendly interfaces.", "result_label": "We suggest case-based reasoning as a unifying framework to combine natural language parsing with automatic programming in a synergistic way.", "abstract": "Abstract We present a case-based approach for natural language interface to a UNIX system with automatic programming ability. Natural language commands are analyzed by case-based parsing and then transformed into UNIX shell-scripts by case-based planning with derivational analogy. Natural language commands are analyzed by case-based parsing and then transformed into UNIX shell-scripts by case-based planning with derivational analogy. Our system, a Dialogue Interface to uNIX (DINX) demonstrates that natural language parsing should be augmented with automatic programming to provide efficient and user friendly interfaces. We suggest case-based reasoning as a unifying framework to combine natural language parsing with automatic programming in a synergistic way."}, {"paper_id": "52112656", "adju_relevance": 1, "title": "Mapping Natural Language Commands to Web Elements", "background_label": "The web provides a rich, open-domain environment with textual, structural, and spatial properties.", "abstract": "The web provides a rich, open-domain environment with textual, structural, and spatial properties."}, {"paper_id": "17071916", "adju_relevance": 1, "title": "Using graph transformation algorithms to generate natural language equivalents of icons expressing medical concepts", "background_label": "A graphical language addresses the need to communicate medical information in a synthetic way. Medical concepts are expressed by icons conveying fast visual information about patients' current state or about the known effects of drugs. In order to increase the visual language's acceptance and usability, a natural language generation interface is currently developed.", "method_label": "In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. The OWL concept may be considered as a star-shaped graph with a central node. The method transforms it into a graph representing the deep semantic structure of a natural language phrase.", "result_label": "This work may be of future use in other contexts where ontology concepts have to be mapped to half-formalized natural language expressions.", "abstract": "A graphical language addresses the need to communicate medical information in a synthetic way. A graphical language addresses the need to communicate medical information in a synthetic way. Medical concepts are expressed by icons conveying fast visual information about patients' current state or about the known effects of drugs. A graphical language addresses the need to communicate medical information in a synthetic way. Medical concepts are expressed by icons conveying fast visual information about patients' current state or about the known effects of drugs. In order to increase the visual language's acceptance and usability, a natural language generation interface is currently developed. In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. The OWL concept may be considered as a star-shaped graph with a central node. In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. The OWL concept may be considered as a star-shaped graph with a central node. The method transforms it into a graph representing the deep semantic structure of a natural language phrase. This work may be of future use in other contexts where ontology concepts have to be mapped to half-formalized natural language expressions."}, {"paper_id": "2623680", "adju_relevance": 1, "title": "Dyna: Extending Datalog For Modern AI \u22c6", "background_label": "Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive queries look up and aggregate relevant or potentially relevant values. If the results of these queries are memoized for reuse, the memos may need to be updated through change propagation. Many strategies could be used to actually solve those systems. Our examples motivate certain extensions to Datalog, which are connected to functional and object-oriented programming paradigms. Why a New Data-Oriented Language for AI?Modern AI systems are frustratingly big, making them time-consuming to engineer and difficult to modify.", "method_label": "We propose a declarative language, which generalizes Datalog, to support this work in a generic way. Through examples, we show that a broad spectrum of AI algorithms can be concisely captured by writing down systems of equations in our notation.", "abstract": " Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive queries look up and aggregate relevant or potentially relevant values. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive queries look up and aggregate relevant or potentially relevant values. If the results of these queries are memoized for reuse, the memos may need to be updated through change propagation. We propose a declarative language, which generalizes Datalog, to support this work in a generic way. We propose a declarative language, which generalizes Datalog, to support this work in a generic way. Through examples, we show that a broad spectrum of AI algorithms can be concisely captured by writing down systems of equations in our notation. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive queries look up and aggregate relevant or potentially relevant values. If the results of these queries are memoized for reuse, the memos may need to be updated through change propagation. Many strategies could be used to actually solve those systems. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive queries look up and aggregate relevant or potentially relevant values. If the results of these queries are memoized for reuse, the memos may need to be updated through change propagation. Many strategies could be used to actually solve those systems. Our examples motivate certain extensions to Datalog, which are connected to functional and object-oriented programming paradigms. Modern statistical AI systems are quite large and complex; this interferes with research, development, and education. We point out that most of the computation involves database-like queries and updates on complex views of the data. Specifically, recursive queries look up and aggregate relevant or potentially relevant values. If the results of these queries are memoized for reuse, the memos may need to be updated through change propagation. Many strategies could be used to actually solve those systems. Our examples motivate certain extensions to Datalog, which are connected to functional and object-oriented programming paradigms. Why a New Data-Oriented Language for AI?Modern AI systems are frustratingly big, making them time-consuming to engineer and difficult to modify."}, {"paper_id": "126147940", "adju_relevance": 1, "title": "Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands", "background_label": "To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort.", "abstract": "To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort."}, {"paper_id": "201140361", "adju_relevance": 1, "title": "Survey on Natural Language Database Interfaces", "background_label": "This is the era of information technology and most of our data or information is stored in the form of databases on our computers. To extract information from our database we need to use a structured language such as SQL (Structured Query Language). Thus only the person who has a good knowledge of the SQL language can interact with the database. Natural language can be used to retrieve the information from the database in an easier way. But computers cannot understand the natural language without any external help.", "method_label": "Thus Natural language database interfaces(NLDBI) were developed, that converts the query given by the user in natural language into a language that is understood by the computer's database management system i.e. Database Query Language(DBQL). In this paper, we present a literature review of various Natural Language Database interfaces which have been built over the time and the new techniques that were added every time to make the interface faster than before.", "result_label": "Then we present some future work that can be done in the field of Natural language Database interfaces to make them faster, reliable, robust and easier.", "abstract": "This is the era of information technology and most of our data or information is stored in the form of databases on our computers. This is the era of information technology and most of our data or information is stored in the form of databases on our computers. To extract information from our database we need to use a structured language such as SQL (Structured Query Language). This is the era of information technology and most of our data or information is stored in the form of databases on our computers. To extract information from our database we need to use a structured language such as SQL (Structured Query Language). Thus only the person who has a good knowledge of the SQL language can interact with the database. This is the era of information technology and most of our data or information is stored in the form of databases on our computers. To extract information from our database we need to use a structured language such as SQL (Structured Query Language). Thus only the person who has a good knowledge of the SQL language can interact with the database. Natural language can be used to retrieve the information from the database in an easier way. This is the era of information technology and most of our data or information is stored in the form of databases on our computers. To extract information from our database we need to use a structured language such as SQL (Structured Query Language). Thus only the person who has a good knowledge of the SQL language can interact with the database. Natural language can be used to retrieve the information from the database in an easier way. But computers cannot understand the natural language without any external help. Thus Natural language database interfaces(NLDBI) were developed, that converts the query given by the user in natural language into a language that is understood by the computer's database management system i.e. Thus Natural language database interfaces(NLDBI) were developed, that converts the query given by the user in natural language into a language that is understood by the computer's database management system i.e. Database Query Language(DBQL). Thus Natural language database interfaces(NLDBI) were developed, that converts the query given by the user in natural language into a language that is understood by the computer's database management system i.e. Database Query Language(DBQL). In this paper, we present a literature review of various Natural Language Database interfaces which have been built over the time and the new techniques that were added every time to make the interface faster than before. Then we present some future work that can be done in the field of Natural language Database interfaces to make them faster, reliable, robust and easier."}, {"paper_id": "269759", "adju_relevance": 1, "title": "AR2SPARQL: An Arabic Natural Language Interface for the Semantic Web", "background_label": "With the growing interest in supporting the Arabic language on the Semantic Web (SW), there is an emerging need to enable Arab users to query ontologies and RDF stores without being challenged with the formal logic of the SW. In the domain of English language, several efforts provided Natural Language (NL) interfaces to enable ordinary users to query ontologies using NL queries. However, none of these efforts were designed to support the Arabic language which has different morphological and semantic structures.", "abstract": "With the growing interest in supporting the Arabic language on the Semantic Web (SW), there is an emerging need to enable Arab users to query ontologies and RDF stores without being challenged with the formal logic of the SW. With the growing interest in supporting the Arabic language on the Semantic Web (SW), there is an emerging need to enable Arab users to query ontologies and RDF stores without being challenged with the formal logic of the SW. In the domain of English language, several efforts provided Natural Language (NL) interfaces to enable ordinary users to query ontologies using NL queries. With the growing interest in supporting the Arabic language on the Semantic Web (SW), there is an emerging need to enable Arab users to query ontologies and RDF stores without being challenged with the formal logic of the SW. In the domain of English language, several efforts provided Natural Language (NL) interfaces to enable ordinary users to query ontologies using NL queries. However, none of these efforts were designed to support the Arabic language which has different morphological and semantic structures."}, {"paper_id": "60955048", "adju_relevance": 1, "title": "LIFE - A Natural Language for Natural Language", "background_label": "Experimenting with formalisms for Natural Language Processing involves costly programming overhead in conventional computing idioms, even as \u201cadvanced\u201d as Lisp or Prolog. LIFE (Logic, Inheritance, Functions, and Equations) is a programming language which incorporates an elegant type system which supports a powerful facility for structured type inheritance. Also, LIFE reconciles styles from Functional Programming and Logic Programming by implicitly delegating control to an automatic suspension mechanism. Together, these features provide a convenient and versatile power of abstraction for very high-level expression of constrained data structures. Computational linguistics is a discipline where such abstractions are particularly useful.", "method_label": "This allows interleaving interpretation of relational and functional expressions which specify abstract structural dependencies on objects.", "result_label": "Therefore, obvious convenience is offered by LIFE for experimentation to the computational linguist, who becomes relieved from burdensome yet extrinsic programming complications. We presently attempt to show how LIFE may be a natural computer language for processing natural human languages.", "abstract": "Experimenting with formalisms for Natural Language Processing involves costly programming overhead in conventional computing idioms, even as \u201cadvanced\u201d as Lisp or Prolog. Experimenting with formalisms for Natural Language Processing involves costly programming overhead in conventional computing idioms, even as \u201cadvanced\u201d as Lisp or Prolog. LIFE (Logic, Inheritance, Functions, and Equations) is a programming language which incorporates an elegant type system which supports a powerful facility for structured type inheritance. Experimenting with formalisms for Natural Language Processing involves costly programming overhead in conventional computing idioms, even as \u201cadvanced\u201d as Lisp or Prolog. LIFE (Logic, Inheritance, Functions, and Equations) is a programming language which incorporates an elegant type system which supports a powerful facility for structured type inheritance. Also, LIFE reconciles styles from Functional Programming and Logic Programming by implicitly delegating control to an automatic suspension mechanism. This allows interleaving interpretation of relational and functional expressions which specify abstract structural dependencies on objects. Experimenting with formalisms for Natural Language Processing involves costly programming overhead in conventional computing idioms, even as \u201cadvanced\u201d as Lisp or Prolog. LIFE (Logic, Inheritance, Functions, and Equations) is a programming language which incorporates an elegant type system which supports a powerful facility for structured type inheritance. Also, LIFE reconciles styles from Functional Programming and Logic Programming by implicitly delegating control to an automatic suspension mechanism. Together, these features provide a convenient and versatile power of abstraction for very high-level expression of constrained data structures. Experimenting with formalisms for Natural Language Processing involves costly programming overhead in conventional computing idioms, even as \u201cadvanced\u201d as Lisp or Prolog. LIFE (Logic, Inheritance, Functions, and Equations) is a programming language which incorporates an elegant type system which supports a powerful facility for structured type inheritance. Also, LIFE reconciles styles from Functional Programming and Logic Programming by implicitly delegating control to an automatic suspension mechanism. Together, these features provide a convenient and versatile power of abstraction for very high-level expression of constrained data structures. Computational linguistics is a discipline where such abstractions are particularly useful. Therefore, obvious convenience is offered by LIFE for experimentation to the computational linguist, who becomes relieved from burdensome yet extrinsic programming complications. Therefore, obvious convenience is offered by LIFE for experimentation to the computational linguist, who becomes relieved from burdensome yet extrinsic programming complications. We presently attempt to show how LIFE may be a natural computer language for processing natural human languages."}, {"paper_id": "12736898", "adju_relevance": 1, "title": "mbeddr: an extensible C-based programming language and IDE for embedded systems", "background_label": "While the C programming language provides good support for writing efficient, low-level code, it is not adequate for defining higher-level abstractions relevant to embedded software. In this paper we present the mbeddr technology stack that supports extension of C with constructs adequate for embedded systems.", "method_label": "In mbeddr, efficient low-level programs can be written using the well-known concepts from C. Higher-level domain-specific abstractions can be seamlessly integrated into C by means of modular language extension regarding syntax, type system, semantics and IDE. In the paper we show how language extension can address the challenges of embedded software development and report on our experience in building these extensions. mbeddr is built on top of the JetBrains MPS language workbench.", "result_label": "We show that language workbenches deliver on the promise of significantly reducing the effort of language engineering and the construction of corresponding IDEs. Both MPS and mbeddr are open source software.", "abstract": "While the C programming language provides good support for writing efficient, low-level code, it is not adequate for defining higher-level abstractions relevant to embedded software. While the C programming language provides good support for writing efficient, low-level code, it is not adequate for defining higher-level abstractions relevant to embedded software. In this paper we present the mbeddr technology stack that supports extension of C with constructs adequate for embedded systems. In mbeddr, efficient low-level programs can be written using the well-known concepts from C. Higher-level domain-specific abstractions can be seamlessly integrated into C by means of modular language extension regarding syntax, type system, semantics and IDE. In mbeddr, efficient low-level programs can be written using the well-known concepts from C. Higher-level domain-specific abstractions can be seamlessly integrated into C by means of modular language extension regarding syntax, type system, semantics and IDE. In the paper we show how language extension can address the challenges of embedded software development and report on our experience in building these extensions. We show that language workbenches deliver on the promise of significantly reducing the effort of language engineering and the construction of corresponding IDEs. In mbeddr, efficient low-level programs can be written using the well-known concepts from C. Higher-level domain-specific abstractions can be seamlessly integrated into C by means of modular language extension regarding syntax, type system, semantics and IDE. In the paper we show how language extension can address the challenges of embedded software development and report on our experience in building these extensions. mbeddr is built on top of the JetBrains MPS language workbench. We show that language workbenches deliver on the promise of significantly reducing the effort of language engineering and the construction of corresponding IDEs. Both MPS and mbeddr are open source software."}, {"paper_id": "18681074", "adju_relevance": 1, "title": "Using natural language processing in order to create SQL queries", "background_label": "Using query language for dealing with databases is always a professional and complex problem. This complexity causes the userpsilas usage of data existing in database limits to use definite reports there are in some pre implemented softwares. However, you can create this opportunity that each none professional user transfers his questions and requirements to computer in natural language and derives his desired data by natural language processing.", "method_label": "In this paper we represent a method for building a ldquonatural languages interfaces to data basesrdquo (NLIDB) system. This system prepares an ldquoexpert systemrdquo implemented in prolog which it can identify synonymous words in any language. It first parses the input sentences, and then the natural language expressions are transformed to SQL language.", "abstract": "Using query language for dealing with databases is always a professional and complex problem. Using query language for dealing with databases is always a professional and complex problem. This complexity causes the userpsilas usage of data existing in database limits to use definite reports there are in some pre implemented softwares. Using query language for dealing with databases is always a professional and complex problem. This complexity causes the userpsilas usage of data existing in database limits to use definite reports there are in some pre implemented softwares. However, you can create this opportunity that each none professional user transfers his questions and requirements to computer in natural language and derives his desired data by natural language processing. In this paper we represent a method for building a ldquonatural languages interfaces to data basesrdquo (NLIDB) system. In this paper we represent a method for building a ldquonatural languages interfaces to data basesrdquo (NLIDB) system. This system prepares an ldquoexpert systemrdquo implemented in prolog which it can identify synonymous words in any language. In this paper we represent a method for building a ldquonatural languages interfaces to data basesrdquo (NLIDB) system. This system prepares an ldquoexpert systemrdquo implemented in prolog which it can identify synonymous words in any language. It first parses the input sentences, and then the natural language expressions are transformed to SQL language."}, {"paper_id": "3338644", "adju_relevance": 1, "title": "XQBE (XQuery By Example): A visual interface to the standard XML query language", "background_label": "The spreading of XML data in many contexts of modern computing infrastructures and systems causes a pressing need for adequate XML querying capabilities; to address this need, the W3C is proposing XQuery as the standard query language for XML, with a language paradigm and a syntactic flavor comparable to the SQL relational language.", "abstract": "The spreading of XML data in many contexts of modern computing infrastructures and systems causes a pressing need for adequate XML querying capabilities; to address this need, the W3C is proposing XQuery as the standard query language for XML, with a language paradigm and a syntactic flavor comparable to the SQL relational language."}, {"paper_id": "58822924", "adju_relevance": 1, "title": "Natural language learning by computer", "background_label": "Abstract : Learning a natural language is taken as an improvement in a system's ability to express situations in a natural language.", "abstract": "Abstract : Learning a natural language is taken as an improvement in a system's ability to express situations in a natural language."}, {"paper_id": "17953497", "adju_relevance": 1, "title": "Understanding Natural Language Queries over Relational Databases", "background_label": "Natural language has been the holy grail of query interface designers, but has generally been considered too hard to work with, except in limited specific circumstances.", "abstract": "Natural language has been the holy grail of query interface designers, but has generally been considered too hard to work with, except in limited specific circumstances."}, {"paper_id": "1109254", "adju_relevance": 1, "title": "Make It So : Continuous , Flexible Natural Language Interaction with an Autonomous Robot", "background_label": "While highly constrained language can be used for robot control, robots that can operate as fully autonomous subordinate agents communicating via rich language remain an open challenge.", "abstract": "While highly constrained language can be used for robot control, robots that can operate as fully autonomous subordinate agents communicating via rich language remain an open challenge."}, {"paper_id": "23441487", "adju_relevance": 1, "title": "Creating New Languages in Blockly: Two Case Studies in Media Computation and Robotics (Abstract Only)", "background_label": "Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. A relatively new breed of programming environments has emerged to address this need. Visual programming languages (VPLs) allow programming logic to be represented with diagrams that illustrate its execution flow. Popular VPLs (e.g., Scratch, Snap, Alice, App Inventor) exist as full-featured, stand-alone programming environments with diagrammatic representations of the program instructions. This representation removes the syntactical barrier to entry that may exist with textual languages.", "method_label": "Blockly, developed by Google, is a type of block language development kit that allows the rapid construction of new block-based visual programming languages to address a specific pedagogical or content focus.", "abstract": "Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. A relatively new breed of programming environments has emerged to address this need. Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. A relatively new breed of programming environments has emerged to address this need. Visual programming languages (VPLs) allow programming logic to be represented with diagrams that illustrate its execution flow. Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. A relatively new breed of programming environments has emerged to address this need. Visual programming languages (VPLs) allow programming logic to be represented with diagrams that illustrate its execution flow. Popular VPLs (e.g., Scratch, Snap, Alice, App Inventor) exist as full-featured, stand-alone programming environments with diagrammatic representations of the program instructions. Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. A relatively new breed of programming environments has emerged to address this need. Visual programming languages (VPLs) allow programming logic to be represented with diagrams that illustrate its execution flow. Popular VPLs (e.g., Scratch, Snap, Alice, App Inventor) exist as full-featured, stand-alone programming environments with diagrammatic representations of the program instructions. This representation removes the syntactical barrier to entry that may exist with textual languages. Blockly, developed by Google, is a type of block language development kit that allows the rapid construction of new block-based visual programming languages to address a specific pedagogical or content focus."}, {"paper_id": "7643679", "adju_relevance": 1, "title": "A Concise Query Language with Search and Transform Operations for Corpora with Multiple Levels of Annotation", "background_label": "The usefulness of annotated corpora is greatly increased if there is an associated tool that can allow various kinds of operations to be performed in a simple way. Different kinds of annotation frameworks and many query languages for them have been proposed, including some to deal with multiple layers of annotation.", "method_label": "We present here an easy to learn query language for a particular kind of annotation framework based on 'threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. Our language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks that otherwise require writing programs, can be performed with one or more queries.", "result_label": "We compare the language with some others and try to evaluate it.", "abstract": "The usefulness of annotated corpora is greatly increased if there is an associated tool that can allow various kinds of operations to be performed in a simple way. The usefulness of annotated corpora is greatly increased if there is an associated tool that can allow various kinds of operations to be performed in a simple way. Different kinds of annotation frameworks and many query languages for them have been proposed, including some to deal with multiple layers of annotation. We present here an easy to learn query language for a particular kind of annotation framework based on 'threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. We present here an easy to learn query language for a particular kind of annotation framework based on 'threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. We present here an easy to learn query language for a particular kind of annotation framework based on 'threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. Our language has a simple, intuitive and concise syntax and high expressive power. We present here an easy to learn query language for a particular kind of annotation framework based on 'threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. Our language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. We present here an easy to learn query language for a particular kind of annotation framework based on 'threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. Our language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks that otherwise require writing programs, can be performed with one or more queries. We compare the language with some others and try to evaluate it."}, {"paper_id": "196101565", "adju_relevance": 1, "title": "SQUALL: the expressiveness of SPARQL 1.1 made available as a controlled natural language 1", "background_label": "The Semantic Web (SW) is now made of billions of triples, which are available as Linked Open Data (LOD) or as RDF stores. The SPARQL query language provides a very expressive way to search and explore this wealth of semantic data. However, userfriendly interfaces are needed to bridge the gap between end-users and SW formalisms. Navigation-based interfaces and natural language interfaces require no or little training, but they cover a small fragment of SPARQL\u2019s expressivity.", "method_label": "We propose SQUALL, a query and update language that provides the full expressiveness of SPARQL 1.1 through a exible controlled natural language (e.g., solution modiers through superlatives, relational algebra through coordinations, lters through comparatives). A comprehensive and modular denition is given as a Montague grammar, and an evaluation of naturalness is done on the QALD challenge. SQUALL is conceived as a component of natural language interfaces, to be combined with lexicons, guided input, and contextual disambiguation.", "result_label": "It is available as a Web service that translates SQUALL sentences to SPARQL, and submits them to SPARQL endpoints (e.g., DBpedia), therefore ensuring SW compliance, and leveraging the eciency of SPARQL engines.", "abstract": "The Semantic Web (SW) is now made of billions of triples, which are available as Linked Open Data (LOD) or as RDF stores. The Semantic Web (SW) is now made of billions of triples, which are available as Linked Open Data (LOD) or as RDF stores. The SPARQL query language provides a very expressive way to search and explore this wealth of semantic data. The Semantic Web (SW) is now made of billions of triples, which are available as Linked Open Data (LOD) or as RDF stores. The SPARQL query language provides a very expressive way to search and explore this wealth of semantic data. However, userfriendly interfaces are needed to bridge the gap between end-users and SW formalisms. The Semantic Web (SW) is now made of billions of triples, which are available as Linked Open Data (LOD) or as RDF stores. The SPARQL query language provides a very expressive way to search and explore this wealth of semantic data. However, userfriendly interfaces are needed to bridge the gap between end-users and SW formalisms. Navigation-based interfaces and natural language interfaces require no or little training, but they cover a small fragment of SPARQL\u2019s expressivity. We propose SQUALL, a query and update language that provides the full expressiveness of SPARQL 1.1 through a exible controlled natural language (e.g., solution modiers through superlatives, relational algebra through coordinations, lters through comparatives). We propose SQUALL, a query and update language that provides the full expressiveness of SPARQL 1.1 through a exible controlled natural language (e.g., solution modiers through superlatives, relational algebra through coordinations, lters through comparatives). A comprehensive and modular denition is given as a Montague grammar, and an evaluation of naturalness is done on the QALD challenge. We propose SQUALL, a query and update language that provides the full expressiveness of SPARQL 1.1 through a exible controlled natural language (e.g., solution modiers through superlatives, relational algebra through coordinations, lters through comparatives). A comprehensive and modular denition is given as a Montague grammar, and an evaluation of naturalness is done on the QALD challenge. SQUALL is conceived as a component of natural language interfaces, to be combined with lexicons, guided input, and contextual disambiguation. It is available as a Web service that translates SQUALL sentences to SPARQL, and submits them to SPARQL endpoints (e.g., DBpedia), therefore ensuring SW compliance, and leveraging the eciency of SPARQL engines."}, {"paper_id": "2705742", "adju_relevance": 1, "title": "Learning Language Games through Interaction", "background_label": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks).", "method_label": "The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance.", "result_label": "Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.", "abstract": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players."}, {"paper_id": "340852", "adju_relevance": 1, "title": "Learning Dependency-Based Compositional Semantics", "background_label": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.", "abstract": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive."}, {"paper_id": "13712055", "adju_relevance": 1, "title": "Natural Language Interfaces with Fine-Grained User Interaction: A Case Study on Web APIs", "background_label": "The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands.", "abstract": "The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. The rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands."}, {"paper_id": "55371239", "adju_relevance": 1, "title": "Modeling robotic operations controlled by natural language", "background_label": "There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. Natural language based control attracts increasing attention due to its versatility and less requirements for users. Since natural language instructions from users cannot be understood by the robots directly, the linguistic input has to be processed into a formal representation which captures the task specification and removes the ambiguity inherent in natural language. For most of existing natural language controlled robotic system, they assume the given language instructions are already in correct orders. However, it is very likely for untrained users to give commands in a mixed order based on their direct observation and intuitive thinking. Simply following the order of the commands can lead to failures of tasks.", "method_label": "To provide a remedy for the problem, we propose a novel framework named dependency relation matrix (DRM) to model and organize the semantic information extracted from language input, in order to figure out an executable sequence of subtasks for later execution. In addition, the proposed approach projects abstract language input and detailed sensory information into the same space, and uses the difference between the goal specification and temporal status of the task under implementation to monitor the progress of task execution.", "result_label": "In this paper, we describe the DRM framework in detail, and illustrate the utility of this approach with experiment results.", "abstract": "There are multiple ways to control a robotic system. There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. Natural language based control attracts increasing attention due to its versatility and less requirements for users. There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. Natural language based control attracts increasing attention due to its versatility and less requirements for users. Since natural language instructions from users cannot be understood by the robots directly, the linguistic input has to be processed into a formal representation which captures the task specification and removes the ambiguity inherent in natural language. There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. Natural language based control attracts increasing attention due to its versatility and less requirements for users. Since natural language instructions from users cannot be understood by the robots directly, the linguistic input has to be processed into a formal representation which captures the task specification and removes the ambiguity inherent in natural language. For most of existing natural language controlled robotic system, they assume the given language instructions are already in correct orders. There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. Natural language based control attracts increasing attention due to its versatility and less requirements for users. Since natural language instructions from users cannot be understood by the robots directly, the linguistic input has to be processed into a formal representation which captures the task specification and removes the ambiguity inherent in natural language. For most of existing natural language controlled robotic system, they assume the given language instructions are already in correct orders. However, it is very likely for untrained users to give commands in a mixed order based on their direct observation and intuitive thinking. There are multiple ways to control a robotic system. Most of them require the users to have prior knowledge about robots or get trained before using them. Natural language based control attracts increasing attention due to its versatility and less requirements for users. Since natural language instructions from users cannot be understood by the robots directly, the linguistic input has to be processed into a formal representation which captures the task specification and removes the ambiguity inherent in natural language. For most of existing natural language controlled robotic system, they assume the given language instructions are already in correct orders. However, it is very likely for untrained users to give commands in a mixed order based on their direct observation and intuitive thinking. Simply following the order of the commands can lead to failures of tasks. To provide a remedy for the problem, we propose a novel framework named dependency relation matrix (DRM) to model and organize the semantic information extracted from language input, in order to figure out an executable sequence of subtasks for later execution. To provide a remedy for the problem, we propose a novel framework named dependency relation matrix (DRM) to model and organize the semantic information extracted from language input, in order to figure out an executable sequence of subtasks for later execution. In addition, the proposed approach projects abstract language input and detailed sensory information into the same space, and uses the difference between the goal specification and temporal status of the task under implementation to monitor the progress of task execution. In this paper, we describe the DRM framework in detail, and illustrate the utility of this approach with experiment results."}, {"paper_id": "202542455", "adju_relevance": 0, "title": "Countering Language Drift via Visual Grounding", "background_label": "Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language.", "method_label": "We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift.", "result_label": "We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.", "abstract": "Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning."}, {"paper_id": "202661163", "adju_relevance": 0, "title": "Natural Language Generation for Non-Expert Users", "abstract": ""}, {"paper_id": "10015034", "adju_relevance": 0, "title": "Factorie: Efficient probabilistic programming via imperative declarations of structure, inference and learning", "background_label": "Discriminatively trained undirected graphical models have garnered tremendous interest and empirical success in natural language processing, computer vision, bioinformatics and many other areas [16, 1, 11]. Some of these models use simple structure (e.g. linear-chains, grids, fully-connected affinity graphs), but there has been increasing interest in more complex relational structure\u2014 capturing more arbitrary dependencies among sets of variables, in repeated patterns. Reimplementing variant structures from scratch is difficult and error-prone, however, and thus there have been several efforts to provide a high-level language in which new undirected model structures can be specified.", "result_label": "These include SQL [17], first-order logic [13], and others such as Csoft [18].", "abstract": "Discriminatively trained undirected graphical models have garnered tremendous interest and empirical success in natural language processing, computer vision, bioinformatics and many other areas [16, 1, 11]. Discriminatively trained undirected graphical models have garnered tremendous interest and empirical success in natural language processing, computer vision, bioinformatics and many other areas [16, 1, 11]. Some of these models use simple structure (e.g. Discriminatively trained undirected graphical models have garnered tremendous interest and empirical success in natural language processing, computer vision, bioinformatics and many other areas [16, 1, 11]. Some of these models use simple structure (e.g. linear-chains, grids, fully-connected affinity graphs), but there has been increasing interest in more complex relational structure\u2014 capturing more arbitrary dependencies among sets of variables, in repeated patterns. Discriminatively trained undirected graphical models have garnered tremendous interest and empirical success in natural language processing, computer vision, bioinformatics and many other areas [16, 1, 11]. Some of these models use simple structure (e.g. linear-chains, grids, fully-connected affinity graphs), but there has been increasing interest in more complex relational structure\u2014 capturing more arbitrary dependencies among sets of variables, in repeated patterns. Reimplementing variant structures from scratch is difficult and error-prone, however, and thus there have been several efforts to provide a high-level language in which new undirected model structures can be specified. These include SQL [17], first-order logic [13], and others such as Csoft [18]."}, {"paper_id": "8106325", "adju_relevance": 0, "title": "Connecting language to the world", "background_label": "Language in the WorldHow does language relate to the non-linguistic world? If an agent is able to communicate linguistically and is also able to directly perceive and/or act on the world, how do perception, action, and language interact with and influence each other? Such questions are surely amongst the most important in Cognitive Science and Artificial Intelligence (AI). Language, after all, is a central aspect of the human mind -indeed it may be what distinguishes us from other species.There is sometimes a tendency in the academic world to study language in isolation, as a formal system with rules for well-constructed sentences; or to focus on how language relates to formal notations such as symbolic logic. But language did not evolve as an isolated system or as a way of communicating symbolic logic; it presumably evolved as a mechanism for exchanging information about the world, ultimately providing the medium for cultural transmission across generations.", "abstract": "Language in the WorldHow does language relate to the non-linguistic world? Language in the WorldHow does language relate to the non-linguistic world? If an agent is able to communicate linguistically and is also able to directly perceive and/or act on the world, how do perception, action, and language interact with and influence each other? Language in the WorldHow does language relate to the non-linguistic world? If an agent is able to communicate linguistically and is also able to directly perceive and/or act on the world, how do perception, action, and language interact with and influence each other? Such questions are surely amongst the most important in Cognitive Science and Artificial Intelligence (AI). Language in the WorldHow does language relate to the non-linguistic world? If an agent is able to communicate linguistically and is also able to directly perceive and/or act on the world, how do perception, action, and language interact with and influence each other? Such questions are surely amongst the most important in Cognitive Science and Artificial Intelligence (AI). Language, after all, is a central aspect of the human mind -indeed it may be what distinguishes us from other species.There is sometimes a tendency in the academic world to study language in isolation, as a formal system with rules for well-constructed sentences; or to focus on how language relates to formal notations such as symbolic logic. Language in the WorldHow does language relate to the non-linguistic world? If an agent is able to communicate linguistically and is also able to directly perceive and/or act on the world, how do perception, action, and language interact with and influence each other? Such questions are surely amongst the most important in Cognitive Science and Artificial Intelligence (AI). Language, after all, is a central aspect of the human mind -indeed it may be what distinguishes us from other species.There is sometimes a tendency in the academic world to study language in isolation, as a formal system with rules for well-constructed sentences; or to focus on how language relates to formal notations such as symbolic logic. But language did not evolve as an isolated system or as a way of communicating symbolic logic; it presumably evolved as a mechanism for exchanging information about the world, ultimately providing the medium for cultural transmission across generations."}, {"paper_id": "1362718", "adju_relevance": 0, "title": "Source-code queries with graph databases - with application to programming language usage and evolution", "background_label": "AbstractProgram querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA.The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held-with consequent limitations on the expressiveness of queries.", "method_label": "The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses.We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail.", "result_label": "It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or dataflow levels.We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail.", "abstract": "AbstractProgram querying and analysis tools are of growing importance, and occur in two main variants. AbstractProgram querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. AbstractProgram querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. AbstractProgram querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA.The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held-with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses.We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or dataflow levels.We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail."}, {"paper_id": "11131225", "adju_relevance": 0, "title": "Learning sparse metrics via linear programming", "background_label": "Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. However, it is often the case that (1) the correct distance is unknown or chosen by hand, and (2) its calculation is computationally expensive (e.g., such as for large dimensional objects). This method allows learning unknown distance functions (or approximating known functions) with the additional property of reducing distance computation time.", "method_label": "In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships. Unlike other popular embedding algorithms, this method can easily generalize to new points, does not have local minima, and explicitly models computational efficiency by finding a mapping that is sparse, i.e. one that depends on a small subset of features or dimensions.", "result_label": "Experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets.", "abstract": "Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. However, it is often the case that (1) the correct distance is unknown or chosen by hand, and (2) its calculation is computationally expensive (e.g., such as for large dimensional objects). In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. However, it is often the case that (1) the correct distance is unknown or chosen by hand, and (2) its calculation is computationally expensive (e.g., such as for large dimensional objects). This method allows learning unknown distance functions (or approximating known functions) with the additional property of reducing distance computation time. In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships. In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships. Unlike other popular embedding algorithms, this method can easily generalize to new points, does not have local minima, and explicitly models computational efficiency by finding a mapping that is sparse, i.e. In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships. Unlike other popular embedding algorithms, this method can easily generalize to new points, does not have local minima, and explicitly models computational efficiency by finding a mapping that is sparse, i.e. one that depends on a small subset of features or dimensions. Experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets."}, {"paper_id": "66993832", "adju_relevance": 0, "title": "Learning Python: Powerful Object-Oriented Programming", "background_label": "The authors of Learning Python show you enough essentials of the Python scripting language to enable you to begin solving problems right away, then reveal more powerful aspects of the language one at a time. This approach is sure to appeal to programmers and system administrators who have urgent problems and a preference for learning by semi-guided experimentation.", "method_label": "First off, Learning Python shows the relationships among Python scripts and their interpreter (in a mostly platform-neutral way). Then, the authors address the mechanics of the language itself, providing illustrations of how Python conceives of numbers, strings, and other objects as well as the operators you use to work with them. Dictionaries, lists, tuples, and other data structures specific to Python receive plenty of attention including complete examples. Authors Mark Lutz and David Ascher build on that fundamental information in their discussions of functions and modules, which evolve into coverage of namespaces, classes, and the object-oriented aspects of Python programming. There's also information on creating graphical user interfaces (GUIs) for Python applications with Tkinter.", "result_label": "In addition to its careful expository prose, Learning Python includes exercises that both test your Python skills and help reveal more elusive truths about the language.", "abstract": "The authors of Learning Python show you enough essentials of the Python scripting language to enable you to begin solving problems right away, then reveal more powerful aspects of the language one at a time. The authors of Learning Python show you enough essentials of the Python scripting language to enable you to begin solving problems right away, then reveal more powerful aspects of the language one at a time. This approach is sure to appeal to programmers and system administrators who have urgent problems and a preference for learning by semi-guided experimentation. First off, Learning Python shows the relationships among Python scripts and their interpreter (in a mostly platform-neutral way). First off, Learning Python shows the relationships among Python scripts and their interpreter (in a mostly platform-neutral way). Then, the authors address the mechanics of the language itself, providing illustrations of how Python conceives of numbers, strings, and other objects as well as the operators you use to work with them. First off, Learning Python shows the relationships among Python scripts and their interpreter (in a mostly platform-neutral way). Then, the authors address the mechanics of the language itself, providing illustrations of how Python conceives of numbers, strings, and other objects as well as the operators you use to work with them. Dictionaries, lists, tuples, and other data structures specific to Python receive plenty of attention including complete examples. First off, Learning Python shows the relationships among Python scripts and their interpreter (in a mostly platform-neutral way). Then, the authors address the mechanics of the language itself, providing illustrations of how Python conceives of numbers, strings, and other objects as well as the operators you use to work with them. Dictionaries, lists, tuples, and other data structures specific to Python receive plenty of attention including complete examples. Authors Mark Lutz and David Ascher build on that fundamental information in their discussions of functions and modules, which evolve into coverage of namespaces, classes, and the object-oriented aspects of Python programming. First off, Learning Python shows the relationships among Python scripts and their interpreter (in a mostly platform-neutral way). Then, the authors address the mechanics of the language itself, providing illustrations of how Python conceives of numbers, strings, and other objects as well as the operators you use to work with them. Dictionaries, lists, tuples, and other data structures specific to Python receive plenty of attention including complete examples. Authors Mark Lutz and David Ascher build on that fundamental information in their discussions of functions and modules, which evolve into coverage of namespaces, classes, and the object-oriented aspects of Python programming. There's also information on creating graphical user interfaces (GUIs) for Python applications with Tkinter. In addition to its careful expository prose, Learning Python includes exercises that both test your Python skills and help reveal more elusive truths about the language."}, {"paper_id": "14812404", "adju_relevance": 0, "title": "Lost in translation: formalizing proposed extensions to c#", "background_label": "Current real-world software applications typically involve heavy use of relational and XML data and their query languages. Unfortunately object-oriented languages and database query languages are based on different semantic foundations and optimization strategies. The resulting ''ROX (Relations, Objects, XML) impedance mismatc'' makes life very difficult for developers. Microsoft Corporation is developing extensions to the .NET framework to facilitate easier processing of non-object-oriented data models. Part of this project (known as \"LINQ\") includes various extensions to the .NET languages to leverage this support.", "abstract": "Current real-world software applications typically involve heavy use of relational and XML data and their query languages. Current real-world software applications typically involve heavy use of relational and XML data and their query languages. Unfortunately object-oriented languages and database query languages are based on different semantic foundations and optimization strategies. Current real-world software applications typically involve heavy use of relational and XML data and their query languages. Unfortunately object-oriented languages and database query languages are based on different semantic foundations and optimization strategies. The resulting ''ROX (Relations, Objects, XML) impedance mismatc'' makes life very difficult for developers. Current real-world software applications typically involve heavy use of relational and XML data and their query languages. Unfortunately object-oriented languages and database query languages are based on different semantic foundations and optimization strategies. The resulting ''ROX (Relations, Objects, XML) impedance mismatc'' makes life very difficult for developers. Microsoft Corporation is developing extensions to the .NET framework to facilitate easier processing of non-object-oriented data models. Current real-world software applications typically involve heavy use of relational and XML data and their query languages. Unfortunately object-oriented languages and database query languages are based on different semantic foundations and optimization strategies. The resulting ''ROX (Relations, Objects, XML) impedance mismatc'' makes life very difficult for developers. Microsoft Corporation is developing extensions to the .NET framework to facilitate easier processing of non-object-oriented data models. Part of this project (known as \"LINQ\") includes various extensions to the .NET languages to leverage this support."}, {"paper_id": "14412362", "adju_relevance": 0, "title": "PAL\u2014a language designed for teaching programming linguistics", "background_label": "This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. In the present case, there are two important considerations: control and specification. There is no PAL Users Group or Committee of Vested Interests concerned with retaining upward compatibility with what was done last year (or last month). This doesn't mean we change the specifications of the language every few weeks (our students are, in a real sense, our Committee of Vested Interests), but it does mean we can make decisions on changes solely on technical grounds. For example, the language almost demands interpretive execution.", "method_label": "Let us consider each of these in turn. More important, though, we can design the language to meet the criteria we think important. Since no one writes production programs in PAL we are able to put up with inefficiencies in the implementation that would otherwise be intolerable.", "result_label": "By virtue of our having designed PAL, it is ours. Thus we have designed our own language so that we will have control over it.", "abstract": "This paper describes PAL\u2014a new computer language. This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. In the present case, there are two important considerations: control and specification. Let us consider each of these in turn. By virtue of our having designed PAL, it is ours. This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. In the present case, there are two important considerations: control and specification. There is no PAL Users Group or Committee of Vested Interests concerned with retaining upward compatibility with what was done last year (or last month). This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. In the present case, there are two important considerations: control and specification. There is no PAL Users Group or Committee of Vested Interests concerned with retaining upward compatibility with what was done last year (or last month). This doesn't mean we change the specifications of the language every few weeks (our students are, in a real sense, our Committee of Vested Interests), but it does mean we can make decisions on changes solely on technical grounds. Let us consider each of these in turn. More important, though, we can design the language to meet the criteria we think important. This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. In the present case, there are two important considerations: control and specification. There is no PAL Users Group or Committee of Vested Interests concerned with retaining upward compatibility with what was done last year (or last month). This doesn't mean we change the specifications of the language every few weeks (our students are, in a real sense, our Committee of Vested Interests), but it does mean we can make decisions on changes solely on technical grounds. For example, the language almost demands interpretive execution. Let us consider each of these in turn. More important, though, we can design the language to meet the criteria we think important. Since no one writes production programs in PAL we are able to put up with inefficiencies in the implementation that would otherwise be intolerable. By virtue of our having designed PAL, it is ours. Thus we have designed our own language so that we will have control over it."}, {"paper_id": "12681857", "adju_relevance": 0, "title": "Bricklayer: An Authentic Introduction to the Functional Programming Language SML", "background_label": "Functional programming languages are seen by many as instrumental to effectively utilizing the computational power of multi-core platforms. As a result, there is growing interest to introduce functional programming and functional thinking as early as possible within the computer science curriculum. Bricklayer is an API, written in SML, that provides a set of abstractions for creating LEGO artifacts which can be viewed using LEGO Digital Designer.", "abstract": "Functional programming languages are seen by many as instrumental to effectively utilizing the computational power of multi-core platforms. Functional programming languages are seen by many as instrumental to effectively utilizing the computational power of multi-core platforms. As a result, there is growing interest to introduce functional programming and functional thinking as early as possible within the computer science curriculum. Functional programming languages are seen by many as instrumental to effectively utilizing the computational power of multi-core platforms. As a result, there is growing interest to introduce functional programming and functional thinking as early as possible within the computer science curriculum. Bricklayer is an API, written in SML, that provides a set of abstractions for creating LEGO artifacts which can be viewed using LEGO Digital Designer."}, {"paper_id": "29783854", "adju_relevance": 0, "title": "Learning programming by programming: a case study", "background_label": "Programming is a challenging field of computer science for both to teach and learn. Although studied extensively, a definite method for teaching programming is yet to be found.", "abstract": "Programming is a challenging field of computer science for both to teach and learn. Programming is a challenging field of computer science for both to teach and learn. Although studied extensively, a definite method for teaching programming is yet to be found."}, {"paper_id": "121303955", "adju_relevance": 0, "title": "Exquisitor: Interactive Learning at Large", "background_label": "Increasing scale is a dominant trend in today's multimedia collections, which especially impacts interactive applications. To facilitate interactive exploration of large multimedia collections, new approaches are needed that are capable of learning on the fly new analytic categories based on the visual and textual content. To facilitate general use on standard desktops, laptops, and mobile devices, they must furthermore work with limited computing resources.", "method_label": "We present Exquisitor, a highly scalable interactive learning approach, capable of intelligent exploration of the large-scale YFCC100M image collection with extremely efficient responses from the interactive classifier. Based on relevance feedback from the user on previously suggested items, Exquisitor uses semantic features, extracted from both visual and text attributes, to suggest relevant media items to the user. Exquisitor builds upon the state of the art in large-scale data representation, compression and indexing, introducing a cluster-based retrieval mechanism that facilitates the efficient suggestions. With Exquisitor, each interaction round over the full YFCC100M collection is completed in less than 0.3 seconds using a single CPU core.", "result_label": "That is 4x less time using 16x smaller computational resources than the most efficient state-of-the-art method, with a positive impact on result quality. These results open up many interesting research avenues, both for exploration of industry-scale media collections and for media exploration on mobile devices.", "abstract": "Increasing scale is a dominant trend in today's multimedia collections, which especially impacts interactive applications. Increasing scale is a dominant trend in today's multimedia collections, which especially impacts interactive applications. To facilitate interactive exploration of large multimedia collections, new approaches are needed that are capable of learning on the fly new analytic categories based on the visual and textual content. Increasing scale is a dominant trend in today's multimedia collections, which especially impacts interactive applications. To facilitate interactive exploration of large multimedia collections, new approaches are needed that are capable of learning on the fly new analytic categories based on the visual and textual content. To facilitate general use on standard desktops, laptops, and mobile devices, they must furthermore work with limited computing resources. We present Exquisitor, a highly scalable interactive learning approach, capable of intelligent exploration of the large-scale YFCC100M image collection with extremely efficient responses from the interactive classifier. We present Exquisitor, a highly scalable interactive learning approach, capable of intelligent exploration of the large-scale YFCC100M image collection with extremely efficient responses from the interactive classifier. Based on relevance feedback from the user on previously suggested items, Exquisitor uses semantic features, extracted from both visual and text attributes, to suggest relevant media items to the user. We present Exquisitor, a highly scalable interactive learning approach, capable of intelligent exploration of the large-scale YFCC100M image collection with extremely efficient responses from the interactive classifier. Based on relevance feedback from the user on previously suggested items, Exquisitor uses semantic features, extracted from both visual and text attributes, to suggest relevant media items to the user. Exquisitor builds upon the state of the art in large-scale data representation, compression and indexing, introducing a cluster-based retrieval mechanism that facilitates the efficient suggestions. We present Exquisitor, a highly scalable interactive learning approach, capable of intelligent exploration of the large-scale YFCC100M image collection with extremely efficient responses from the interactive classifier. Based on relevance feedback from the user on previously suggested items, Exquisitor uses semantic features, extracted from both visual and text attributes, to suggest relevant media items to the user. Exquisitor builds upon the state of the art in large-scale data representation, compression and indexing, introducing a cluster-based retrieval mechanism that facilitates the efficient suggestions. With Exquisitor, each interaction round over the full YFCC100M collection is completed in less than 0.3 seconds using a single CPU core. That is 4x less time using 16x smaller computational resources than the most efficient state-of-the-art method, with a positive impact on result quality. That is 4x less time using 16x smaller computational resources than the most efficient state-of-the-art method, with a positive impact on result quality. These results open up many interesting research avenues, both for exploration of industry-scale media collections and for media exploration on mobile devices."}, {"paper_id": "1777833", "adju_relevance": 0, "title": "Evaluating interactive support for secure programming", "background_label": "Implementing secure code is an important and oft-overlooked non-functional requirement. Secure programming errors are a subset of program errors that result in many common privacy and security breaches in commercial software. We are seeking to provide interactive support for secure programming in the development environment.", "method_label": "In this paper, we have evaluated our prototype tool, ASIDE, which provides real-time warnings and code generation to reduce secure programming errors introduced by programmers. We evaluate the potential use and effectiveness of ASIDE on both novice and professional developers in two comparison user studies.", "result_label": "Our results demonstrate that the interactive support can help address this important non-functional requirement, and suggest guidelines for such tools to support programmers.", "abstract": "Implementing secure code is an important and oft-overlooked non-functional requirement. Implementing secure code is an important and oft-overlooked non-functional requirement. Secure programming errors are a subset of program errors that result in many common privacy and security breaches in commercial software. Implementing secure code is an important and oft-overlooked non-functional requirement. Secure programming errors are a subset of program errors that result in many common privacy and security breaches in commercial software. We are seeking to provide interactive support for secure programming in the development environment. In this paper, we have evaluated our prototype tool, ASIDE, which provides real-time warnings and code generation to reduce secure programming errors introduced by programmers. In this paper, we have evaluated our prototype tool, ASIDE, which provides real-time warnings and code generation to reduce secure programming errors introduced by programmers. We evaluate the potential use and effectiveness of ASIDE on both novice and professional developers in two comparison user studies. Our results demonstrate that the interactive support can help address this important non-functional requirement, and suggest guidelines for such tools to support programmers."}, {"paper_id": "7451112", "adju_relevance": 0, "title": "LISA: An Interactive Environment for Programming Language Development", "background_label": "The LISA system is an interactive environment for programming language development. From the formal language specifications of a particular programming language LISA produces a language specific environment that includes editors (a language-knowledgable editor and a structured editor), a compiler/interpreter and other graphic tools.", "method_label": "The LISA is a set of related tools such as scanner generators, parser generators, compiler generators, graphic tools, editors and conversion tools, which are integrated by well-designed interfaces.", "abstract": " The LISA system is an interactive environment for programming language development. The LISA system is an interactive environment for programming language development. From the formal language specifications of a particular programming language LISA produces a language specific environment that includes editors (a language-knowledgable editor and a structured editor), a compiler/interpreter and other graphic tools. The LISA is a set of related tools such as scanner generators, parser generators, compiler generators, graphic tools, editors and conversion tools, which are integrated by well-designed interfaces."}, {"paper_id": "7271567", "adju_relevance": 0, "title": "A visual programming language for drawing and executing flowcharts", "background_label": "With recent advances in graphical user interfaces, more and more tasks on computers have become easier to perform. Out of the belief that creating computer programs can also be one of them, visual programming languages (VPLs) have emerged.", "abstract": "With recent advances in graphical user interfaces, more and more tasks on computers have become easier to perform. With recent advances in graphical user interfaces, more and more tasks on computers have become easier to perform. Out of the belief that creating computer programs can also be one of them, visual programming languages (VPLs) have emerged."}, {"paper_id": "17449634", "adju_relevance": 0, "title": "Language Transfer Hypotheses with Linear SVM Weights", "background_label": "Language transfer, the characteristic second language usage patterns caused by native language interference, is investigated by Second Language Acquisition (SLA) researchers seeking to find overused and underused linguistic features.", "method_label": "In this paper we develop and present a methodology for deriving ranked lists of such features. Using very large learner data, we show our method\u2019s ability to find relevant candidates using sophisticated linguistic features. To illustrate its applicability to SLA research, we formulate plausible language transfer hypotheses supported by current evidence.", "result_label": "This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a per-native language basis.", "abstract": "Language transfer, the characteristic second language usage patterns caused by native language interference, is investigated by Second Language Acquisition (SLA) researchers seeking to find overused and underused linguistic features. In this paper we develop and present a methodology for deriving ranked lists of such features. In this paper we develop and present a methodology for deriving ranked lists of such features. Using very large learner data, we show our method\u2019s ability to find relevant candidates using sophisticated linguistic features. In this paper we develop and present a methodology for deriving ranked lists of such features. Using very large learner data, we show our method\u2019s ability to find relevant candidates using sophisticated linguistic features. To illustrate its applicability to SLA research, we formulate plausible language transfer hypotheses supported by current evidence. This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a per-native language basis."}, {"paper_id": "62016977", "adju_relevance": 0, "title": "On the naturalness of software", "background_label": "Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations \u2014 and thus, like natural language, it is also likely to be repetitive and predictable.", "result_label": "This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We conclude the paper by laying out a vision for future research in this area.", "method_label": "We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability.", "abstract": "Natural languages like English are rich, complex, and powerful. Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations \u2014 and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We conclude the paper by laying out a vision for future research in this area."}, {"paper_id": "18412720", "adju_relevance": 0, "title": "Composing DTI Visualizations with End-user Programming", "background_label": "We present the design and prototype implementation of a scientific visualization language called Zifazah for composing 3D visualizations of diffusion tensor magnetic resonance imaging (DT-MRI or DTI) data.", "abstract": "We present the design and prototype implementation of a scientific visualization language called Zifazah for composing 3D visualizations of diffusion tensor magnetic resonance imaging (DT-MRI or DTI) data."}, {"paper_id": "7632414", "adju_relevance": 0, "title": "Pabble: Parameterised Scribble for Parallel Programming", "background_label": "Many parallel and distributed message-passing programs are written in a parametric way over available resources, in particular the number of nodes and their topologies, so that a single parallel program can scale over different environments.", "abstract": "Many parallel and distributed message-passing programs are written in a parametric way over available resources, in particular the number of nodes and their topologies, so that a single parallel program can scale over different environments."}, {"paper_id": "15219837", "adju_relevance": 0, "title": "Meaningful learning in the tutoring system for programming", "background_label": "Tutoring systems for programming helps students to understand features of target programming language, and develops their general problem solving skill. Our system guides novices in learning syntax and semantics of programming language, problem decomposition, program design and testing.", "method_label": "The notional machine defined by programming language and its verbal description of instruction actions helps students to understand semantics of instructions.", "result_label": "Advancement through the course material controlled by computer teacher supports connection of new concepts to the present studentpsilas knowledge.", "abstract": "Tutoring systems for programming helps students to understand features of target programming language, and develops their general problem solving skill. Tutoring systems for programming helps students to understand features of target programming language, and develops their general problem solving skill. Our system guides novices in learning syntax and semantics of programming language, problem decomposition, program design and testing. The notional machine defined by programming language and its verbal description of instruction actions helps students to understand semantics of instructions. Advancement through the course material controlled by computer teacher supports connection of new concepts to the present studentpsilas knowledge."}, {"paper_id": "53743215", "adju_relevance": 0, "title": "Parson's programming puzzles: a fun and effective learning tool for first programming courses", "background_label": "Mastery of basic syntactic and logical constructs is an essential part of learning to program. Unfortunately, practice exercises for programming basics can be very tedious, making it difficult to motivate students.", "abstract": "Mastery of basic syntactic and logical constructs is an essential part of learning to program. Mastery of basic syntactic and logical constructs is an essential part of learning to program. Unfortunately, practice exercises for programming basics can be very tedious, making it difficult to motivate students."}, {"paper_id": "855173", "adju_relevance": 0, "title": "Juniper: a functional reactive programming language for the Arduino", "background_label": "Juniper provides a number of high level features, including parametric polymorphic functions, anonymous functions, automatic memory management, and immutable data structures.", "method_label": "Also included is a standard library which offers many useful FRP signal processing functions. Juniper is translated to standard C++ and compiled with the existing Arduino development tools, allowing Juniper programs to fit on resource-constrained devices, and enabling seamless interoperability with existing C++ libraries for these devices.", "abstract": " Juniper provides a number of high level features, including parametric polymorphic functions, anonymous functions, automatic memory management, and immutable data structures. Also included is a standard library which offers many useful FRP signal processing functions. Also included is a standard library which offers many useful FRP signal processing functions. Juniper is translated to standard C++ and compiled with the existing Arduino development tools, allowing Juniper programs to fit on resource-constrained devices, and enabling seamless interoperability with existing C++ libraries for these devices."}, {"paper_id": "3588950", "adju_relevance": 0, "title": "Towards adaptive programming: integrating reinforcement learning into a programming language", "background_label": "Current programming languages and software engineering paradigms are proving insufficient for building intelligent multi-agent systems--such as interactive games and narratives--where developers are called upon to write increasingly complex behavior for agents in dynamic environments. A promising solution is to build adaptive systems; that is, to develop software written specifically to adapt to its environment by changing its behavior in response to what it observes in the world.", "method_label": "In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at code-writing time, leaving the run-time system to learn the rest. Partial programming enables programmers to more easily encode software agents that are difficult to write in existing languages that do not offer language-level support for adaptivity.", "result_label": "We motivate the use of partial programming with an example agent coded in a cutting-edge, but non-adaptive agent programming language (ABL), and show how A2BL can encode the same agent much more naturally.", "abstract": "Current programming languages and software engineering paradigms are proving insufficient for building intelligent multi-agent systems--such as interactive games and narratives--where developers are called upon to write increasingly complex behavior for agents in dynamic environments. Current programming languages and software engineering paradigms are proving insufficient for building intelligent multi-agent systems--such as interactive games and narratives--where developers are called upon to write increasingly complex behavior for agents in dynamic environments. A promising solution is to build adaptive systems; that is, to develop software written specifically to adapt to its environment by changing its behavior in response to what it observes in the world. In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at code-writing time, leaving the run-time system to learn the rest. In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at code-writing time, leaving the run-time system to learn the rest. Partial programming enables programmers to more easily encode software agents that are difficult to write in existing languages that do not offer language-level support for adaptivity. We motivate the use of partial programming with an example agent coded in a cutting-edge, but non-adaptive agent programming language (ABL), and show how A2BL can encode the same agent much more naturally."}, {"paper_id": "11777029", "adju_relevance": 0, "title": "A programming language", "background_label": "The paper describes a succinct problem-oriented programming language. The language is broad in scope, having been developed for, and applied effectively in, such diverse areas as microprogramming, switching theory, operations research, information retrieval, sorting theory, structure of compilers, search procedures, and language translation. The language permits a high degree of useful formalism.", "method_label": "It relies heavily on a systematic extension of a small set of basic operations to vectors, matrices, and trees, and on a family of flexible selection operations controlled by logical vectors.", "result_label": "Illustrations are drawn from a variety of applications.", "abstract": "The paper describes a succinct problem-oriented programming language. The paper describes a succinct problem-oriented programming language. The language is broad in scope, having been developed for, and applied effectively in, such diverse areas as microprogramming, switching theory, operations research, information retrieval, sorting theory, structure of compilers, search procedures, and language translation. The paper describes a succinct problem-oriented programming language. The language is broad in scope, having been developed for, and applied effectively in, such diverse areas as microprogramming, switching theory, operations research, information retrieval, sorting theory, structure of compilers, search procedures, and language translation. The language permits a high degree of useful formalism. It relies heavily on a systematic extension of a small set of basic operations to vectors, matrices, and trees, and on a family of flexible selection operations controlled by logical vectors. Illustrations are drawn from a variety of applications."}, {"paper_id": "43081235", "adju_relevance": 0, "title": "Design and implementation of an intelligent web-based interactive language learning system", "method_label": "We designed two kinds of learning environments: (1) interactive English writing environment and (2) mining movies for real English. These are intended to improve learners\u2019 basic language skills such as listening, reading, and writing. In addition, the system also offers authoring tools that facilitate teachers\u2019 content preparation. The system not only provides multimedia learning environments for users, but also builds a learner corpus, an archive of annotated English texts written by learners for whom English is a second language.", "result_label": "Further analysis of the learner corpus creates the potential to detect the users\u2019 persistent errors and then provide adequate help to the users.", "abstract": " We designed two kinds of learning environments: (1) interactive English writing environment and (2) mining movies for real English. We designed two kinds of learning environments: (1) interactive English writing environment and (2) mining movies for real English. These are intended to improve learners\u2019 basic language skills such as listening, reading, and writing. We designed two kinds of learning environments: (1) interactive English writing environment and (2) mining movies for real English. These are intended to improve learners\u2019 basic language skills such as listening, reading, and writing. In addition, the system also offers authoring tools that facilitate teachers\u2019 content preparation. We designed two kinds of learning environments: (1) interactive English writing environment and (2) mining movies for real English. These are intended to improve learners\u2019 basic language skills such as listening, reading, and writing. In addition, the system also offers authoring tools that facilitate teachers\u2019 content preparation. The system not only provides multimedia learning environments for users, but also builds a learner corpus, an archive of annotated English texts written by learners for whom English is a second language. Further analysis of the learner corpus creates the potential to detect the users\u2019 persistent errors and then provide adequate help to the users."}, {"paper_id": "16141377", "adju_relevance": 0, "title": "WinHIPE: an IDE for functional programming based on rewriting and visualization", "background_label": "The article describes an IDE for functional programming, called WinHIPE. It provides an interactive and flexible tracer, as well as a powerful visualization and animation system.", "method_label": "The former tool is based on the rewriting model of evaluation, and the latter provides automatic generation of visualizations and animations, friendly support for customization, maintenance and exportation of animations to the Web, and facilities to cope with large scale. Its main advantage over other visualization systems is an effortless approach to animation creation and maintenance, based on generating visualizations and animations automatically, as a side effect of program execution.", "result_label": "Finally, we briefly describe our experience using the system during several years in educational settings.", "abstract": "The article describes an IDE for functional programming, called WinHIPE. The article describes an IDE for functional programming, called WinHIPE. It provides an interactive and flexible tracer, as well as a powerful visualization and animation system. The former tool is based on the rewriting model of evaluation, and the latter provides automatic generation of visualizations and animations, friendly support for customization, maintenance and exportation of animations to the Web, and facilities to cope with large scale. The former tool is based on the rewriting model of evaluation, and the latter provides automatic generation of visualizations and animations, friendly support for customization, maintenance and exportation of animations to the Web, and facilities to cope with large scale. Its main advantage over other visualization systems is an effortless approach to animation creation and maintenance, based on generating visualizations and animations automatically, as a side effect of program execution. Finally, we briefly describe our experience using the system during several years in educational settings."}, {"paper_id": "61849710", "adju_relevance": 0, "title": "A Language for Generic Programming", "background_label": "The past decade of software library construction has demonstrated that the discipline of generic programming is an effective approach to the design and implementation of large-scale software libraries. At the heart of generic programming is a semi-formal interface specification language for generic components. Many programming languages have features for describing interfaces, but none of them match the generic programming specification language, and none are as suitable for specifying generic components. This lack of language support impedes the current practice of generic programming.", "abstract": "The past decade of software library construction has demonstrated that the discipline of generic programming is an effective approach to the design and implementation of large-scale software libraries. The past decade of software library construction has demonstrated that the discipline of generic programming is an effective approach to the design and implementation of large-scale software libraries. At the heart of generic programming is a semi-formal interface specification language for generic components. The past decade of software library construction has demonstrated that the discipline of generic programming is an effective approach to the design and implementation of large-scale software libraries. At the heart of generic programming is a semi-formal interface specification language for generic components. Many programming languages have features for describing interfaces, but none of them match the generic programming specification language, and none are as suitable for specifying generic components. The past decade of software library construction has demonstrated that the discipline of generic programming is an effective approach to the design and implementation of large-scale software libraries. At the heart of generic programming is a semi-formal interface specification language for generic components. Many programming languages have features for describing interfaces, but none of them match the generic programming specification language, and none are as suitable for specifying generic components. This lack of language support impedes the current practice of generic programming."}, {"paper_id": "9491703", "adju_relevance": 0, "title": "MyProLang - My Programming Language: A Template-Driven Automatic Natural Programming Language", "background_label": "Modern computer programming languages are governed by complex syntactic rules. They are unlike natural languages; they require extensive manual work and a significant amount of learning and practicing for an individual to become skilled at and to write correct programs. Computer programming is a difficult, complicated, unfamiliar, non-automated, and a challenging discipline for everyone; especially, for students, new programmers and end-users.", "abstract": "Modern computer programming languages are governed by complex syntactic rules. Modern computer programming languages are governed by complex syntactic rules. They are unlike natural languages; they require extensive manual work and a significant amount of learning and practicing for an individual to become skilled at and to write correct programs. Modern computer programming languages are governed by complex syntactic rules. They are unlike natural languages; they require extensive manual work and a significant amount of learning and practicing for an individual to become skilled at and to write correct programs. Computer programming is a difficult, complicated, unfamiliar, non-automated, and a challenging discipline for everyone; especially, for students, new programmers and end-users."}, {"paper_id": "7626370", "adju_relevance": 0, "title": "A decentralized approach for programming interactive applications with JavaScript and blockly", "background_label": "We present a decentralized-control methodology and a tool-set for developing interactive user interfaces.", "abstract": "We present a decentralized-control methodology and a tool-set for developing interactive user interfaces."}, {"paper_id": "15425134", "adju_relevance": 0, "title": "Cross-Language Code Analysis and Refactoring", "background_label": "Software composed of artifacts written in multiple (programming) languages is pervasive in today's enterprise, desktop, and mobile applications. Since they form one system, artifacts from different languages reference one another, thus creating what we call semantic cross-language links. By their very nature, such links are out of scope of the individual programming language, they are ignored by most language-specific tools and are often only established -- and checked for errors -- at runtime. This is unfortunate since it requires additional testing, leads to brittle code, and lessens maintainability.", "abstract": "Software composed of artifacts written in multiple (programming) languages is pervasive in today's enterprise, desktop, and mobile applications. Software composed of artifacts written in multiple (programming) languages is pervasive in today's enterprise, desktop, and mobile applications. Since they form one system, artifacts from different languages reference one another, thus creating what we call semantic cross-language links. Software composed of artifacts written in multiple (programming) languages is pervasive in today's enterprise, desktop, and mobile applications. Since they form one system, artifacts from different languages reference one another, thus creating what we call semantic cross-language links. By their very nature, such links are out of scope of the individual programming language, they are ignored by most language-specific tools and are often only established -- and checked for errors -- at runtime. Software composed of artifacts written in multiple (programming) languages is pervasive in today's enterprise, desktop, and mobile applications. Since they form one system, artifacts from different languages reference one another, thus creating what we call semantic cross-language links. By their very nature, such links are out of scope of the individual programming language, they are ignored by most language-specific tools and are often only established -- and checked for errors -- at runtime. This is unfortunate since it requires additional testing, leads to brittle code, and lessens maintainability."}, {"paper_id": "1942340", "adju_relevance": 0, "title": "Modern Natural Language Interfaces To Databases: Composing Statistical Parsing With Semantic Tractability", "background_label": "Natural Language Interfaces to Databases (NLIs) can benefit from the advances in statistical parsing over the last fifteen years or so. However, statistical parsers require training on a massive, labeled corpus, and manually creating such a corpus for each database is prohibitively expensive.", "abstract": "Natural Language Interfaces to Databases (NLIs) can benefit from the advances in statistical parsing over the last fifteen years or so. Natural Language Interfaces to Databases (NLIs) can benefit from the advances in statistical parsing over the last fifteen years or so. However, statistical parsers require training on a massive, labeled corpus, and manually creating such a corpus for each database is prohibitively expensive."}, {"paper_id": "53280703", "adju_relevance": 0, "title": "Computational Thinking with the Web Crowd using CodeMapper", "background_label": "It has been argued that computational thinking should precede computer programming in the course of a career in computing. This argument is the basis for the slogan\"logic first, syntax later\"and the development of many cryptic syntax removed programming languages such as Scratch!, Blockly and Visual Logic.", "abstract": "It has been argued that computational thinking should precede computer programming in the course of a career in computing. It has been argued that computational thinking should precede computer programming in the course of a career in computing. This argument is the basis for the slogan\"logic first, syntax later\"and the development of many cryptic syntax removed programming languages such as Scratch!, Blockly and Visual Logic."}, {"paper_id": "15539014", "adju_relevance": 0, "title": "Lexical triggers and latent semantic analysis for cross-lingual language model adaptation", "background_label": "In-domain texts for estimating statistical language models are not easily found for most languages of the world.", "abstract": "In-domain texts for estimating statistical language models are not easily found for most languages of the world."}, {"paper_id": "13465688", "adju_relevance": 0, "title": "Assessing automatic text classification for interactive language learning", "method_label": "We describe the tool that we developed on the basis of these design options and provide an assessment of its functioning. This tool is suitable to be used by students taking courses of Portuguese as a second language, as well as by expert instructors selecting items of exam that are aimed at assessing and certifying the language level of these students. It is an instrument that aims at supporting humans in their language level classification judgments by providing conditions to more consistent and objectively sustained judgments across different occasions and input texts and across different human classifiers.", "result_label": "Its design principles and underlying language technology can be applied to develop similar tools for any other language.", "abstract": " We describe the tool that we developed on the basis of these design options and provide an assessment of its functioning. We describe the tool that we developed on the basis of these design options and provide an assessment of its functioning. This tool is suitable to be used by students taking courses of Portuguese as a second language, as well as by expert instructors selecting items of exam that are aimed at assessing and certifying the language level of these students. We describe the tool that we developed on the basis of these design options and provide an assessment of its functioning. This tool is suitable to be used by students taking courses of Portuguese as a second language, as well as by expert instructors selecting items of exam that are aimed at assessing and certifying the language level of these students. It is an instrument that aims at supporting humans in their language level classification judgments by providing conditions to more consistent and objectively sustained judgments across different occasions and input texts and across different human classifiers. Its design principles and underlying language technology can be applied to develop similar tools for any other language."}, {"paper_id": "1691990", "adju_relevance": 0, "title": "Language Networks as Models of Cognition: Understanding Cognition through Language", "background_label": "Language is inherently cognitive and distinctly human. Separating the object of language from the human mind that processes and creates language fails to capture the full language system. Linguistics traditionally has focused on the study of language as a static representation, removed from the human mind. Network analysis has traditionally been focused on the properties and structure that emerge from network representations. In contrast, psycholinguistic research has focused on the process of language without committing to a representation. However, by considering language networks as approximations of the cognitive system we can take the strength of each of these approaches to study human performance and cognition as related to language.", "result_label": "Both disciplines could gain from looking at language as a cognitive process.", "abstract": "Language is inherently cognitive and distinctly human. Language is inherently cognitive and distinctly human. Separating the object of language from the human mind that processes and creates language fails to capture the full language system. Language is inherently cognitive and distinctly human. Separating the object of language from the human mind that processes and creates language fails to capture the full language system. Linguistics traditionally has focused on the study of language as a static representation, removed from the human mind. Language is inherently cognitive and distinctly human. Separating the object of language from the human mind that processes and creates language fails to capture the full language system. Linguistics traditionally has focused on the study of language as a static representation, removed from the human mind. Network analysis has traditionally been focused on the properties and structure that emerge from network representations. Both disciplines could gain from looking at language as a cognitive process. Language is inherently cognitive and distinctly human. Separating the object of language from the human mind that processes and creates language fails to capture the full language system. Linguistics traditionally has focused on the study of language as a static representation, removed from the human mind. Network analysis has traditionally been focused on the properties and structure that emerge from network representations. In contrast, psycholinguistic research has focused on the process of language without committing to a representation. Language is inherently cognitive and distinctly human. Separating the object of language from the human mind that processes and creates language fails to capture the full language system. Linguistics traditionally has focused on the study of language as a static representation, removed from the human mind. Network analysis has traditionally been focused on the properties and structure that emerge from network representations. In contrast, psycholinguistic research has focused on the process of language without committing to a representation. However, by considering language networks as approximations of the cognitive system we can take the strength of each of these approaches to study human performance and cognition as related to language."}, {"paper_id": "497108", "adju_relevance": 0, "title": "Learning a Neural Semantic Parser from User Feedback", "background_label": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention.", "method_label": "To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers.", "result_label": "Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "abstract": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch."}, {"paper_id": "6674405", "adju_relevance": 0, "title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification", "background_label": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities.", "abstract": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities."}, {"paper_id": "8034253", "adju_relevance": 0, "title": "BioRuby: bioinformatics software for the Ruby programming language", "background_label": "SUMMARY The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. BioRuby comes with a tutorial, documentation and an interactive environment, which can be used in the shell, and in the web browser.", "method_label": "AVAILABILITY BioRuby is free and open source software, made available under the Ruby license. BioRuby runs on all platforms that support Ruby, including Linux, Mac OS X and Windows. And, with JRuby, BioRuby runs on the Java Virtual Machine.", "abstract": "SUMMARY The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. SUMMARY The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. SUMMARY The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. BioRuby comes with a tutorial, documentation and an interactive environment, which can be used in the shell, and in the web browser. AVAILABILITY BioRuby is free and open source software, made available under the Ruby license. AVAILABILITY BioRuby is free and open source software, made available under the Ruby license. BioRuby runs on all platforms that support Ruby, including Linux, Mac OS X and Windows. AVAILABILITY BioRuby is free and open source software, made available under the Ruby license. BioRuby runs on all platforms that support Ruby, including Linux, Mac OS X and Windows. And, with JRuby, BioRuby runs on the Java Virtual Machine."}, {"paper_id": "5088062", "adju_relevance": 0, "title": "Polymonadic Programming", "background_label": "Monads are a popular tool for the working functional programmer to structure effectful computations. This paper presents polymonads, a generalization of monads. Polymonads give the familiar monadic bind the more general type forall a,b. L a ->(a ->M b) ->N b, to compose computations with three different kinds of effects, rather than just one. Polymonads subsume monads and parameterized monads, and can express other constructions, including precise type-and-effect systems and information flow tracking; more generally, polymonads correspond to Tate's productoid semantic model.", "method_label": "We show how to equip a core language (called lambda-PM) with syntactic support for programming with polymonads. Type inference and elaboration in lambda-PM allows programmers to write polymonadic code directly in an ML-like syntax--our algorithms compute principal types and produce elaborated programs wherein the binds appear explicitly.", "result_label": "Furthermore, we prove that the elaboration is coherent: no matter which (type-correct) binds are chosen, the elaborated program's semantics will be the same. Pleasingly, the inferred types are easy to read: the polymonad laws justify (sometimes dramatic) simplifications, but with no effect on a type's generality.", "abstract": "Monads are a popular tool for the working functional programmer to structure effectful computations. Monads are a popular tool for the working functional programmer to structure effectful computations. This paper presents polymonads, a generalization of monads. Monads are a popular tool for the working functional programmer to structure effectful computations. This paper presents polymonads, a generalization of monads. Polymonads give the familiar monadic bind the more general type forall a,b. Monads are a popular tool for the working functional programmer to structure effectful computations. This paper presents polymonads, a generalization of monads. Polymonads give the familiar monadic bind the more general type forall a,b. L a ->(a ->M b) ->N b, to compose computations with three different kinds of effects, rather than just one. Monads are a popular tool for the working functional programmer to structure effectful computations. This paper presents polymonads, a generalization of monads. Polymonads give the familiar monadic bind the more general type forall a,b. L a ->(a ->M b) ->N b, to compose computations with three different kinds of effects, rather than just one. Polymonads subsume monads and parameterized monads, and can express other constructions, including precise type-and-effect systems and information flow tracking; more generally, polymonads correspond to Tate's productoid semantic model. We show how to equip a core language (called lambda-PM) with syntactic support for programming with polymonads. We show how to equip a core language (called lambda-PM) with syntactic support for programming with polymonads. Type inference and elaboration in lambda-PM allows programmers to write polymonadic code directly in an ML-like syntax--our algorithms compute principal types and produce elaborated programs wherein the binds appear explicitly. Furthermore, we prove that the elaboration is coherent: no matter which (type-correct) binds are chosen, the elaborated program's semantics will be the same. Furthermore, we prove that the elaboration is coherent: no matter which (type-correct) binds are chosen, the elaborated program's semantics will be the same. Pleasingly, the inferred types are easy to read: the polymonad laws justify (sometimes dramatic) simplifications, but with no effect on a type's generality."}, {"paper_id": "16686628", "adju_relevance": 0, "title": "Enabling Learning by Teaching: Intuitive Composing of E-Learning Modules", "method_label": "We try to empower and liberate non-technical module users by lowering the bar for turning them into module authors, a bar previously set far too high. In turn, this stimulates learning through teaching. By making a damn fine piece of software, we furthermore make module authoring more pleasant for experienced authors as well. We propose a system that initially enables users to easily compose H5P modules. These modules are successively easy to share and modify.", "result_label": "Through gamification we encourage authors to share their work, and to improve the works of others.", "abstract": " We try to empower and liberate non-technical module users by lowering the bar for turning them into module authors, a bar previously set far too high. We try to empower and liberate non-technical module users by lowering the bar for turning them into module authors, a bar previously set far too high. In turn, this stimulates learning through teaching. We try to empower and liberate non-technical module users by lowering the bar for turning them into module authors, a bar previously set far too high. In turn, this stimulates learning through teaching. By making a damn fine piece of software, we furthermore make module authoring more pleasant for experienced authors as well. We try to empower and liberate non-technical module users by lowering the bar for turning them into module authors, a bar previously set far too high. In turn, this stimulates learning through teaching. By making a damn fine piece of software, we furthermore make module authoring more pleasant for experienced authors as well. We propose a system that initially enables users to easily compose H5P modules. We try to empower and liberate non-technical module users by lowering the bar for turning them into module authors, a bar previously set far too high. In turn, this stimulates learning through teaching. By making a damn fine piece of software, we furthermore make module authoring more pleasant for experienced authors as well. We propose a system that initially enables users to easily compose H5P modules. These modules are successively easy to share and modify. Through gamification we encourage authors to share their work, and to improve the works of others."}, {"paper_id": "20191900", "adju_relevance": 0, "title": "An N-Gram-and-Wikipedia joint approach to Natural Language Identification", "background_label": "Natural Language Identification is the process of detecting and determining in which language or languages a given piece of text is written. As one of the key steps in Computational Linguistics/Natural Language Processing(NLP) tasks, such as Machine Translation, Multi-lingual Information Retrieval and Processing of Language Resources, Natural Language Identification has drawn widespread attention and extensive research, making it one of the few relatively well studied sub-fields in the whole NLP field. However, various problems remain far from resolved in this field. Current non-computational approaches require researchers possess sufficient prior linguistic knowledge about the languages to be identified, while current computational (statistical) approaches demand large-scale training set for each to-be-identified language. Apparently, drawbacks for both are that, few computer scientists are equipped with sufficient knowledge in Linguistics, and the size of the training set may get endlessly larger in pursuit of higher accuracy and the ability to process more languages.", "method_label": "Also, faced with multi-lingual documents on the Internet, neither approach can render satisfactory results.", "abstract": "Natural Language Identification is the process of detecting and determining in which language or languages a given piece of text is written. Natural Language Identification is the process of detecting and determining in which language or languages a given piece of text is written. As one of the key steps in Computational Linguistics/Natural Language Processing(NLP) tasks, such as Machine Translation, Multi-lingual Information Retrieval and Processing of Language Resources, Natural Language Identification has drawn widespread attention and extensive research, making it one of the few relatively well studied sub-fields in the whole NLP field. Natural Language Identification is the process of detecting and determining in which language or languages a given piece of text is written. As one of the key steps in Computational Linguistics/Natural Language Processing(NLP) tasks, such as Machine Translation, Multi-lingual Information Retrieval and Processing of Language Resources, Natural Language Identification has drawn widespread attention and extensive research, making it one of the few relatively well studied sub-fields in the whole NLP field. However, various problems remain far from resolved in this field. Natural Language Identification is the process of detecting and determining in which language or languages a given piece of text is written. As one of the key steps in Computational Linguistics/Natural Language Processing(NLP) tasks, such as Machine Translation, Multi-lingual Information Retrieval and Processing of Language Resources, Natural Language Identification has drawn widespread attention and extensive research, making it one of the few relatively well studied sub-fields in the whole NLP field. However, various problems remain far from resolved in this field. Current non-computational approaches require researchers possess sufficient prior linguistic knowledge about the languages to be identified, while current computational (statistical) approaches demand large-scale training set for each to-be-identified language. Natural Language Identification is the process of detecting and determining in which language or languages a given piece of text is written. As one of the key steps in Computational Linguistics/Natural Language Processing(NLP) tasks, such as Machine Translation, Multi-lingual Information Retrieval and Processing of Language Resources, Natural Language Identification has drawn widespread attention and extensive research, making it one of the few relatively well studied sub-fields in the whole NLP field. However, various problems remain far from resolved in this field. Current non-computational approaches require researchers possess sufficient prior linguistic knowledge about the languages to be identified, while current computational (statistical) approaches demand large-scale training set for each to-be-identified language. Apparently, drawbacks for both are that, few computer scientists are equipped with sufficient knowledge in Linguistics, and the size of the training set may get endlessly larger in pursuit of higher accuracy and the ability to process more languages. Also, faced with multi-lingual documents on the Internet, neither approach can render satisfactory results."}, {"paper_id": "7571487", "adju_relevance": 0, "title": "A model-based approach to language integration", "background_label": "The interactions of several languages within a software system pose a number of problems. There is several anecdotal and empirical evidence supporting such concerns.", "abstract": "The interactions of several languages within a software system pose a number of problems. The interactions of several languages within a software system pose a number of problems. There is several anecdotal and empirical evidence supporting such concerns."}, {"paper_id": "3935841", "adju_relevance": 0, "title": "Synthesizing Bijective Lenses", "background_label": "Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. However, these domain-specific languages can be difficult to program in, requiring programmers to manage fiddly details while working in a complex type system. The output is a well-typed program in Boomerang (a bidirectional language based on the theory of lenses). The main technical challenge involves navigating the vast program search space efficiently enough. Unlike most prior work on type-directed synthesis, our system operates in the context of a language with a rich equivalence relation on types (the theory of regular expressions).", "method_label": "To solve this, we present Optician, a tool for type-directed synthesis of bijective string transformers. We synthesize terms of a equivalent language and convert those generated terms into our lens language. We prove the correctness of our synthesis algorithm. We also demonstrate empirically that our new language changes the synthesis problem from one that admits intractable solutions to one that admits highly efficient solutions.", "result_label": "The inputs to Optician are two ordinary regular expressions representing two data formats and a few concrete examples for disambiguation. We evaluate Optician on a benchmark suite of 39 examples including both microbenchmarks and realistic examples derived from other data management systems including Flash Fill, a tool for synthesizing string transformations in spreadsheets, and Augeas, a tool for bidirectional processing of Linux system configuration files.", "abstract": "Bidirectional transformations between different data representations occur frequently in modern software systems. Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. However, these domain-specific languages can be difficult to program in, requiring programmers to manage fiddly details while working in a complex type system. To solve this, we present Optician, a tool for type-directed synthesis of bijective string transformers. The inputs to Optician are two ordinary regular expressions representing two data formats and a few concrete examples for disambiguation. Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. However, these domain-specific languages can be difficult to program in, requiring programmers to manage fiddly details while working in a complex type system. The output is a well-typed program in Boomerang (a bidirectional language based on the theory of lenses). Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. However, these domain-specific languages can be difficult to program in, requiring programmers to manage fiddly details while working in a complex type system. The output is a well-typed program in Boomerang (a bidirectional language based on the theory of lenses). The main technical challenge involves navigating the vast program search space efficiently enough. Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. However, these domain-specific languages can be difficult to program in, requiring programmers to manage fiddly details while working in a complex type system. The output is a well-typed program in Boomerang (a bidirectional language based on the theory of lenses). The main technical challenge involves navigating the vast program search space efficiently enough. Unlike most prior work on type-directed synthesis, our system operates in the context of a language with a rich equivalence relation on types (the theory of regular expressions). To solve this, we present Optician, a tool for type-directed synthesis of bijective string transformers. We synthesize terms of a equivalent language and convert those generated terms into our lens language. To solve this, we present Optician, a tool for type-directed synthesis of bijective string transformers. We synthesize terms of a equivalent language and convert those generated terms into our lens language. We prove the correctness of our synthesis algorithm. To solve this, we present Optician, a tool for type-directed synthesis of bijective string transformers. We synthesize terms of a equivalent language and convert those generated terms into our lens language. We prove the correctness of our synthesis algorithm. We also demonstrate empirically that our new language changes the synthesis problem from one that admits intractable solutions to one that admits highly efficient solutions. The inputs to Optician are two ordinary regular expressions representing two data formats and a few concrete examples for disambiguation. We evaluate Optician on a benchmark suite of 39 examples including both microbenchmarks and realistic examples derived from other data management systems including Flash Fill, a tool for synthesizing string transformations in spreadsheets, and Augeas, a tool for bidirectional processing of Linux system configuration files."}, {"paper_id": "7028302", "adju_relevance": 0, "title": "Probabilistic programming in Python using PyMC3", "background_label": "ABSTRACTProbabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available.", "method_label": "PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model.", "result_label": "This paper is a tutorial-style introduction to this software package.", "abstract": "ABSTRACTProbabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. ABSTRACTProbabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. ABSTRACTProbabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package."}, {"paper_id": "14524541", "adju_relevance": 0, "title": "COOL: An object-based language for parallel programming", "background_label": "Effectively using shared-memory multiprocessors requires substantial programming effort.", "abstract": "Effectively using shared-memory multiprocessors requires substantial programming effort."}, {"paper_id": "38494544", "adju_relevance": 0, "title": "Guiding Reinforcement Learning Exploration Using Natural Language", "background_label": "In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments.", "method_label": "This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions.", "result_label": "This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.", "abstract": "In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions. This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping."}, {"paper_id": "6429546", "adju_relevance": 0, "title": "TerpreT: A Probabilistic Programming Language for Program Induction", "background_label": "We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs.", "abstract": "We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs."}, {"paper_id": "14920302", "adju_relevance": 0, "title": "Cedalion: a language for language oriented programming", "background_label": "Language Oriented Programming (LOP) is a paradigm that puts domain specific programming languages (DSLs) at the center of the software development process. Currently, there are three main approaches to LOP: (1) the use of internal DSLs, implemented as libraries in a given host language; (2) the use of external DSLs, implemented as interpreters or compilers in an external language; and (3) the use of language workbenches, which are integrated development environments (IDEs) for defining and using external DSLs.", "method_label": "In this paper, we contribute: (4) a novel language-oriented approach to LOP for defining and using internal DSLs. While language workbenches adapt internal DSL features to overcome some of the limitations of external DSLs, our approach adapts language workbench features to overcome some of the limitations of internal DSLs. We introduce Cedalion, an LOP host language for internal DSLs, featuring static validation and projectional editing.", "result_label": "To validate our approach we present a case study in which Cedalion was used by biologists in designing a DNA microarray for molecular Biology research.", "abstract": "Language Oriented Programming (LOP) is a paradigm that puts domain specific programming languages (DSLs) at the center of the software development process. Language Oriented Programming (LOP) is a paradigm that puts domain specific programming languages (DSLs) at the center of the software development process. Currently, there are three main approaches to LOP: (1) the use of internal DSLs, implemented as libraries in a given host language; (2) the use of external DSLs, implemented as interpreters or compilers in an external language; and (3) the use of language workbenches, which are integrated development environments (IDEs) for defining and using external DSLs. In this paper, we contribute: (4) a novel language-oriented approach to LOP for defining and using internal DSLs. In this paper, we contribute: (4) a novel language-oriented approach to LOP for defining and using internal DSLs. While language workbenches adapt internal DSL features to overcome some of the limitations of external DSLs, our approach adapts language workbench features to overcome some of the limitations of internal DSLs. In this paper, we contribute: (4) a novel language-oriented approach to LOP for defining and using internal DSLs. While language workbenches adapt internal DSL features to overcome some of the limitations of external DSLs, our approach adapts language workbench features to overcome some of the limitations of internal DSLs. We introduce Cedalion, an LOP host language for internal DSLs, featuring static validation and projectional editing. To validate our approach we present a case study in which Cedalion was used by biologists in designing a DNA microarray for molecular Biology research."}, {"paper_id": "2435394", "adju_relevance": 0, "title": "Modeling Language Variability", "background_label": "A systematic way of defining variants of a modeling language is useful for adapting the language to domain or project specific needs. Variants can be obtained by adapting the syntax or semantics of the language.", "abstract": "A systematic way of defining variants of a modeling language is useful for adapting the language to domain or project specific needs. A systematic way of defining variants of a modeling language is useful for adapting the language to domain or project specific needs. Variants can be obtained by adapting the syntax or semantics of the language."}, {"paper_id": "18534583", "adju_relevance": 0, "title": "Tools for Authoring Tutorial Dialogue Knowledge", "background_label": "The current generation of intelligent tutoring systems (ITS) have successfully produced learning gains without the use of natural language technology, but the goal for the next generation is to add natural language dialogue capabilities. Since it is already a tremendous effort to add domain and pedagogical knowledge to the current generation of ITSs, adding natural language dialogue capabilities can further increase the development time by requiring that language knowledge also be engineered.", "abstract": "The current generation of intelligent tutoring systems (ITS) have successfully produced learning gains without the use of natural language technology, but the goal for the next generation is to add natural language dialogue capabilities. The current generation of intelligent tutoring systems (ITS) have successfully produced learning gains without the use of natural language technology, but the goal for the next generation is to add natural language dialogue capabilities. Since it is already a tremendous effort to add domain and pedagogical knowledge to the current generation of ITSs, adding natural language dialogue capabilities can further increase the development time by requiring that language knowledge also be engineered."}, {"paper_id": "16488741", "adju_relevance": 0, "title": "A machine vision extension for the Ruby programming language", "background_label": "Dynamically typed scripting languages have become popular in recent years. Although interpreted languages allow for substantial reduction of software development time, they are often rejected due to performance concerns.", "abstract": "Dynamically typed scripting languages have become popular in recent years. Dynamically typed scripting languages have become popular in recent years. Although interpreted languages allow for substantial reduction of software development time, they are often rejected due to performance concerns."}, {"paper_id": "14552146", "adju_relevance": 0, "title": "TACO: prototyping high-level object-oriented programming constructs by means of template based programming techniques", "background_label": "Taco (Topologies and Collections) is a template based object platform for cluster architectures, that provides the flavour of distributed data-parallel programming based on distributed object groups.", "abstract": "Taco (Topologies and Collections) is a template based object platform for cluster architectures, that provides the flavour of distributed data-parallel programming based on distributed object groups."}, {"paper_id": "202537512", "adju_relevance": 0, "title": "MULE: Multimodal Universal Language Embedding", "background_label": "Existing vision-language methods typically support two languages at a time at most.", "abstract": "Existing vision-language methods typically support two languages at a time at most."}, {"paper_id": "61292926", "adju_relevance": 0, "title": "The Go Programming Language", "background_label": "The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The book does not assume prior knowledge of Go nor experience with any specific language, so youll find it accessible whether youre most comfortable with JavaScript, Ruby, Python, Java, or C++. Two chapters on concurrency present in-depth approaches to this increasingly important topic. The first, which covers the basic mechanisms of goroutines and channels, illustrates the style known as communicating sequential processes for which Go is renowned. The second covers more traditional aspects of concurrency with shared variables. These chapters provide a solid foundation for programmers encountering concurrency for the first time.", "method_label": "The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. The final two chapters explore lower-level features of Go. One covers the art of metaprogramming using reflection. The other shows how to use the unsafe package to step outside the type system for special situations, and how to use the cgo tool to create Go bindings for C libraries. The book features hundreds of interesting and practical examples of well-written Go code that cover the whole language, its most important packages, and a wide range of applications. Each chapter has exercises to test your understanding and explore extensions and alternatives.", "result_label": "They explain the key principles of encapsulation, composition, and substitutability using realistic examples.", "abstract": "The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The book does not assume prior knowledge of Go nor experience with any specific language, so youll find it accessible whether youre most comfortable with JavaScript, Ruby, Python, Java, or C++. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. They explain the key principles of encapsulation, composition, and substitutability using realistic examples. The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The book does not assume prior knowledge of Go nor experience with any specific language, so youll find it accessible whether youre most comfortable with JavaScript, Ruby, Python, Java, or C++. Two chapters on concurrency present in-depth approaches to this increasingly important topic. The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The book does not assume prior knowledge of Go nor experience with any specific language, so youll find it accessible whether youre most comfortable with JavaScript, Ruby, Python, Java, or C++. Two chapters on concurrency present in-depth approaches to this increasingly important topic. The first, which covers the basic mechanisms of goroutines and channels, illustrates the style known as communicating sequential processes for which Go is renowned. The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The book does not assume prior knowledge of Go nor experience with any specific language, so youll find it accessible whether youre most comfortable with JavaScript, Ruby, Python, Java, or C++. Two chapters on concurrency present in-depth approaches to this increasingly important topic. The first, which covers the basic mechanisms of goroutines and channels, illustrates the style known as communicating sequential processes for which Go is renowned. The second covers more traditional aspects of concurrency with shared variables. The Go Programming Language is the authoritative resource for any programmer who wants to learn Go. It shows how to write clear and idiomatic Go to solve real-world problems. The book does not assume prior knowledge of Go nor experience with any specific language, so youll find it accessible whether youre most comfortable with JavaScript, Ruby, Python, Java, or C++. Two chapters on concurrency present in-depth approaches to this increasingly important topic. The first, which covers the basic mechanisms of goroutines and channels, illustrates the style known as communicating sequential processes for which Go is renowned. The second covers more traditional aspects of concurrency with shared variables. These chapters provide a solid foundation for programmers encountering concurrency for the first time. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. The final two chapters explore lower-level features of Go. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. The final two chapters explore lower-level features of Go. One covers the art of metaprogramming using reflection. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. The final two chapters explore lower-level features of Go. One covers the art of metaprogramming using reflection. The other shows how to use the unsafe package to step outside the type system for special situations, and how to use the cgo tool to create Go bindings for C libraries. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. The final two chapters explore lower-level features of Go. One covers the art of metaprogramming using reflection. The other shows how to use the unsafe package to step outside the type system for special situations, and how to use the cgo tool to create Go bindings for C libraries. The book features hundreds of interesting and practical examples of well-written Go code that cover the whole language, its most important packages, and a wide range of applications. The first chapter is a tutorial on the basic concepts of Go, introduced through programs for file I/O and text processing, simple graphics, and web clients and servers. Early chapters cover the structural elements of Go programs: syntax, control flow, data types, and the organization of a program into packages, files, and functions. The examples illustrate many packages from the standard library and show how to create new ones of your own. Later chapters explain the package mechanism in more detail, and how to build, test, and maintain projects using the go tool. The chapters on methods and interfaces introduce Gos unconventional approach to object-oriented programming, in which methods can be declared on any type and interfaces are implicitly satisfied. The final two chapters explore lower-level features of Go. One covers the art of metaprogramming using reflection. The other shows how to use the unsafe package to step outside the type system for special situations, and how to use the cgo tool to create Go bindings for C libraries. The book features hundreds of interesting and practical examples of well-written Go code that cover the whole language, its most important packages, and a wide range of applications. Each chapter has exercises to test your understanding and explore extensions and alternatives."}, {"paper_id": "57115900", "adju_relevance": 0, "title": "Developing the distributed component of a framework for processing intensional programming languages", "background_label": "Based on a simple non-procedural language with temporal logic operators, Lucid underlies a family of multi-dimensional programming languages based on intensional logic. Intension is a concept rooted in an aspect of natural language called \u201cintensional context\u201d, in which the meaning of a statement (extension) depends on the context in which it is uttered (intension). The implicit temporal feature of Lucid makes it suitable for use as a means of describing dynamic systems. In the past, experiments have been performed and real applications have been developed with programs written in Lucid. However, these systems focused mainly on improving the execution performance of one dialect of Lucid and not address the problem of interpreting variants of Lucid.", "method_label": "The GIPSY system is designed to not only process current Lucid variants efficiently but also to be modified easily to accept new dialects of Lucid. In the thesis, we discuss the essence of executing intensional programming languages using the eduction (also called demand-driven or lazy) execution model; describe experiments with different approaches to interpreting programs written in Lucid; and focuses on execution over a network of processors. We describe the implementation of a prototype for executing Lucid programs in a distributed environment. We also explore the advantages of applying the object concept to distributed systems and describe experiments with these methods. In addition, the thesis includes estimates of the impact of integrating computation functions into the Lucid code and proposes an advanced execution model consisting of self-contained and intelligent clients associated with a meta-level resource management.", "abstract": "Based on a simple non-procedural language with temporal logic operators, Lucid underlies a family of multi-dimensional programming languages based on intensional logic. Based on a simple non-procedural language with temporal logic operators, Lucid underlies a family of multi-dimensional programming languages based on intensional logic. Intension is a concept rooted in an aspect of natural language called \u201cintensional context\u201d, in which the meaning of a statement (extension) depends on the context in which it is uttered (intension). Based on a simple non-procedural language with temporal logic operators, Lucid underlies a family of multi-dimensional programming languages based on intensional logic. Intension is a concept rooted in an aspect of natural language called \u201cintensional context\u201d, in which the meaning of a statement (extension) depends on the context in which it is uttered (intension). The implicit temporal feature of Lucid makes it suitable for use as a means of describing dynamic systems. Based on a simple non-procedural language with temporal logic operators, Lucid underlies a family of multi-dimensional programming languages based on intensional logic. Intension is a concept rooted in an aspect of natural language called \u201cintensional context\u201d, in which the meaning of a statement (extension) depends on the context in which it is uttered (intension). The implicit temporal feature of Lucid makes it suitable for use as a means of describing dynamic systems. In the past, experiments have been performed and real applications have been developed with programs written in Lucid. Based on a simple non-procedural language with temporal logic operators, Lucid underlies a family of multi-dimensional programming languages based on intensional logic. Intension is a concept rooted in an aspect of natural language called \u201cintensional context\u201d, in which the meaning of a statement (extension) depends on the context in which it is uttered (intension). The implicit temporal feature of Lucid makes it suitable for use as a means of describing dynamic systems. In the past, experiments have been performed and real applications have been developed with programs written in Lucid. However, these systems focused mainly on improving the execution performance of one dialect of Lucid and not address the problem of interpreting variants of Lucid. The GIPSY system is designed to not only process current Lucid variants efficiently but also to be modified easily to accept new dialects of Lucid. The GIPSY system is designed to not only process current Lucid variants efficiently but also to be modified easily to accept new dialects of Lucid. In the thesis, we discuss the essence of executing intensional programming languages using the eduction (also called demand-driven or lazy) execution model; describe experiments with different approaches to interpreting programs written in Lucid; and focuses on execution over a network of processors. The GIPSY system is designed to not only process current Lucid variants efficiently but also to be modified easily to accept new dialects of Lucid. In the thesis, we discuss the essence of executing intensional programming languages using the eduction (also called demand-driven or lazy) execution model; describe experiments with different approaches to interpreting programs written in Lucid; and focuses on execution over a network of processors. We describe the implementation of a prototype for executing Lucid programs in a distributed environment. The GIPSY system is designed to not only process current Lucid variants efficiently but also to be modified easily to accept new dialects of Lucid. In the thesis, we discuss the essence of executing intensional programming languages using the eduction (also called demand-driven or lazy) execution model; describe experiments with different approaches to interpreting programs written in Lucid; and focuses on execution over a network of processors. We describe the implementation of a prototype for executing Lucid programs in a distributed environment. We also explore the advantages of applying the object concept to distributed systems and describe experiments with these methods. The GIPSY system is designed to not only process current Lucid variants efficiently but also to be modified easily to accept new dialects of Lucid. In the thesis, we discuss the essence of executing intensional programming languages using the eduction (also called demand-driven or lazy) execution model; describe experiments with different approaches to interpreting programs written in Lucid; and focuses on execution over a network of processors. We describe the implementation of a prototype for executing Lucid programs in a distributed environment. We also explore the advantages of applying the object concept to distributed systems and describe experiments with these methods. In addition, the thesis includes estimates of the impact of integrating computation functions into the Lucid code and proposes an advanced execution model consisting of self-contained and intelligent clients associated with a meta-level resource management."}, {"paper_id": "5999738", "adju_relevance": 0, "title": "Integration of Heterogeneous Modeling Languages via Extensible and Composable Language Components", "background_label": "Effective model-driven engineering of complex systems requires to appropriately describe different specific system aspects. To this end, efficient integration of different heterogeneous modeling languages is essential. Modeling language integaration is onerous and requires in-depth conceptual and technical knowledge and ef- fort. Traditional modeling lanugage integration approches require language engineers to compose monolithic language aggregates for a specific task or project. Adapting these aggregates cannot be to different contexts requires vast effort and makes these hardly reusable.", "method_label": "This contribution presents a method for the engineering of grammar-based language components that can be independently developed, are syntactically composable, and ultimately reusable. To this end, it introduces the concepts of language aggregation, language embed- ding, and language inheritance, as well as their realization in the language workbench MontiCore.", "result_label": "The result is a generalizable, systematic, and efficient syntax-oriented composition of languages that allows the agile employment of modeling languages efficiently tailored for individual software projects.", "abstract": "Effective model-driven engineering of complex systems requires to appropriately describe different specific system aspects. Effective model-driven engineering of complex systems requires to appropriately describe different specific system aspects. To this end, efficient integration of different heterogeneous modeling languages is essential. Effective model-driven engineering of complex systems requires to appropriately describe different specific system aspects. To this end, efficient integration of different heterogeneous modeling languages is essential. Modeling language integaration is onerous and requires in-depth conceptual and technical knowledge and ef- fort. Effective model-driven engineering of complex systems requires to appropriately describe different specific system aspects. To this end, efficient integration of different heterogeneous modeling languages is essential. Modeling language integaration is onerous and requires in-depth conceptual and technical knowledge and ef- fort. Traditional modeling lanugage integration approches require language engineers to compose monolithic language aggregates for a specific task or project. Effective model-driven engineering of complex systems requires to appropriately describe different specific system aspects. To this end, efficient integration of different heterogeneous modeling languages is essential. Modeling language integaration is onerous and requires in-depth conceptual and technical knowledge and ef- fort. Traditional modeling lanugage integration approches require language engineers to compose monolithic language aggregates for a specific task or project. Adapting these aggregates cannot be to different contexts requires vast effort and makes these hardly reusable. This contribution presents a method for the engineering of grammar-based language components that can be independently developed, are syntactically composable, and ultimately reusable. This contribution presents a method for the engineering of grammar-based language components that can be independently developed, are syntactically composable, and ultimately reusable. To this end, it introduces the concepts of language aggregation, language embed- ding, and language inheritance, as well as their realization in the language workbench MontiCore. The result is a generalizable, systematic, and efficient syntax-oriented composition of languages that allows the agile employment of modeling languages efficiently tailored for individual software projects."}, {"paper_id": "53235786", "adju_relevance": 0, "title": "Syntactic Manipulation for Generating more Diverse and Interesting Texts", "background_label": "AbstractNatural Language Generation plays an important role in the domain of dialogue systems as it determines how users perceive the system. Recently, deep-learning based systems have been proposed to tackle this task, as they generalize better and require less amounts of manual effort to implement them for new domains. However, deep learning systems usually adapt a very homogeneous sounding writing style which expresses little variation.", "method_label": "In this work, we present our system for Natural Language Generation where we control various aspects of the surface realization in order to increase the lexical variability of the utterances, such that they sound more diverse and interesting. For this, we use a Semantically Controlled Long Short-term Memory Network (SC-LSTM), and apply its specialized cell to control various syntactic features of the generated texts.", "result_label": "We present an in-depth human evaluation where we show the effects of these surface manipulation on the perception of potential users.", "abstract": "AbstractNatural Language Generation plays an important role in the domain of dialogue systems as it determines how users perceive the system. AbstractNatural Language Generation plays an important role in the domain of dialogue systems as it determines how users perceive the system. Recently, deep-learning based systems have been proposed to tackle this task, as they generalize better and require less amounts of manual effort to implement them for new domains. AbstractNatural Language Generation plays an important role in the domain of dialogue systems as it determines how users perceive the system. Recently, deep-learning based systems have been proposed to tackle this task, as they generalize better and require less amounts of manual effort to implement them for new domains. However, deep learning systems usually adapt a very homogeneous sounding writing style which expresses little variation. In this work, we present our system for Natural Language Generation where we control various aspects of the surface realization in order to increase the lexical variability of the utterances, such that they sound more diverse and interesting. In this work, we present our system for Natural Language Generation where we control various aspects of the surface realization in order to increase the lexical variability of the utterances, such that they sound more diverse and interesting. For this, we use a Semantically Controlled Long Short-term Memory Network (SC-LSTM), and apply its specialized cell to control various syntactic features of the generated texts. We present an in-depth human evaluation where we show the effects of these surface manipulation on the perception of potential users."}, {"paper_id": "6401679", "adju_relevance": 0, "title": "Semantic Parsing on Freebase from Question-Answer Pairs", "background_label": "AbstractIn this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs.", "abstract": "AbstractIn this paper, we train a semantic parser that scales up to Freebase. AbstractIn this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs."}, {"paper_id": "34765991", "adju_relevance": 0, "title": "Towards Interactive Object-Oriented Programming", "background_label": "To represent interactive objects, we propose a choice-disjunctive declaration statement of the form S R where S;R are the (procedure or field) declaration statements within a class.", "method_label": "This statement has the following semantics: request the user to choose one between S and R when an object of this class is created.", "result_label": "This statement is useful for representing interactive objects that require interactions with the user.", "abstract": "To represent interactive objects, we propose a choice-disjunctive declaration statement of the form S R where S;R are the (procedure or field) declaration statements within a class. This statement has the following semantics: request the user to choose one between S and R when an object of this class is created. This statement is useful for representing interactive objects that require interactions with the user."}, {"paper_id": "15904896", "adju_relevance": 0, "title": "Flapjax: a programming language for Ajax applications", "background_label": "This paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces.", "abstract": "This paper presents Flapjax, a language designed for contemporary Web applications. This paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces."}, {"paper_id": "60752931", "adju_relevance": 0, "title": "Programming with sets; an introduction to SETL", "background_label": "The programming language SETL is a relatively new member of the so-called \"very-high-level\" class of languages, some of whose other well-known mem bers are LISP, APL, SNOBOL, and PROLOG.", "abstract": "The programming language SETL is a relatively new member of the so-called \"very-high-level\" class of languages, some of whose other well-known mem bers are LISP, APL, SNOBOL, and PROLOG."}, {"paper_id": "155092173", "adju_relevance": 0, "title": "Dual Supervised Learning for Natural Language Understanding and Generation", "background_label": "Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP field.", "abstract": "Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP field."}, {"paper_id": "14902296", "adju_relevance": 0, "title": "Toque: designing a cooking-based programming language for and with children", "background_label": "An intergenerational design team of children (ages 7-11 years old) along with graduate students and faculty in computer science and information studies developed a programming language for children, Toque. Concrete real-world cooking scenarios were used as programming metaphors to support an accessible programming learning experience.", "method_label": "The Wiimote and Nunchuk were used as physical programming input devices. The programs that were created were pictorial recipes which dynamically controlled animations of an on-screen chef preparing virtual dishes in a graphical kitchen environment. Through multiple design sessions, programming strategies were explored, cooking metaphors were developed and, prototypes of the Toque environment were iterated.", "result_label": "Results of these design experiences have shown us the importance of pair-programming, programming by storytelling, parallel programming, function-argument relationships, and the role of tangibility in overcoming challenges with constraints imposed by the system design.", "abstract": "An intergenerational design team of children (ages 7-11 years old) along with graduate students and faculty in computer science and information studies developed a programming language for children, Toque. An intergenerational design team of children (ages 7-11 years old) along with graduate students and faculty in computer science and information studies developed a programming language for children, Toque. Concrete real-world cooking scenarios were used as programming metaphors to support an accessible programming learning experience. The Wiimote and Nunchuk were used as physical programming input devices. The Wiimote and Nunchuk were used as physical programming input devices. The programs that were created were pictorial recipes which dynamically controlled animations of an on-screen chef preparing virtual dishes in a graphical kitchen environment. The Wiimote and Nunchuk were used as physical programming input devices. The programs that were created were pictorial recipes which dynamically controlled animations of an on-screen chef preparing virtual dishes in a graphical kitchen environment. Through multiple design sessions, programming strategies were explored, cooking metaphors were developed and, prototypes of the Toque environment were iterated. Results of these design experiences have shown us the importance of pair-programming, programming by storytelling, parallel programming, function-argument relationships, and the role of tangibility in overcoming challenges with constraints imposed by the system design."}, {"paper_id": "57653087", "adju_relevance": 0, "title": "Amphion: Automatic Programming for the NAIF Toolkit", "background_label": "A synchronizing check relay is used in an electrical transmission network for controlling a circuit breaker to interconnect first and second transmission lines to establish power flow therebetween and includes an input circuit for receiving signals from first and second transmission lines.", "method_label": "A phase difference generator generates a phase difference signal proportional to a phase difference between said signals from said first and second transmission lines. A phase difference comparator receives the phase difference signal and produces a phase output signal at a selected phase condition. An adjustable timer produces a timer signal after a selected time interval in response to the phase comparator to insure a proper phase difference between the transmission lines. The voltage sensor produces a breaker closing inhibit signal when one of the alternating current voltages is outside a reference limit.", "result_label": "A control circuit is responsive to the phase output signal, the time signal and the breaker closing inhibit signal for providing a control response to the circuit breaker.", "abstract": "A synchronizing check relay is used in an electrical transmission network for controlling a circuit breaker to interconnect first and second transmission lines to establish power flow therebetween and includes an input circuit for receiving signals from first and second transmission lines. A phase difference generator generates a phase difference signal proportional to a phase difference between said signals from said first and second transmission lines. A phase difference generator generates a phase difference signal proportional to a phase difference between said signals from said first and second transmission lines. A phase difference comparator receives the phase difference signal and produces a phase output signal at a selected phase condition. A phase difference generator generates a phase difference signal proportional to a phase difference between said signals from said first and second transmission lines. A phase difference comparator receives the phase difference signal and produces a phase output signal at a selected phase condition. An adjustable timer produces a timer signal after a selected time interval in response to the phase comparator to insure a proper phase difference between the transmission lines. A phase difference generator generates a phase difference signal proportional to a phase difference between said signals from said first and second transmission lines. A phase difference comparator receives the phase difference signal and produces a phase output signal at a selected phase condition. An adjustable timer produces a timer signal after a selected time interval in response to the phase comparator to insure a proper phase difference between the transmission lines. The voltage sensor produces a breaker closing inhibit signal when one of the alternating current voltages is outside a reference limit. A control circuit is responsive to the phase output signal, the time signal and the breaker closing inhibit signal for providing a control response to the circuit breaker."}, {"paper_id": "8441637", "adju_relevance": 0, "title": "Robust Interactive Learning", "background_label": "In this paper we propose and study a generalization of the standard active-learning model where a more general type of query, class conditional query, is allowed. Such queries have been quite useful in applications, but have been lacking theoretical understanding.", "abstract": "In this paper we propose and study a generalization of the standard active-learning model where a more general type of query, class conditional query, is allowed. In this paper we propose and study a generalization of the standard active-learning model where a more general type of query, class conditional query, is allowed. Such queries have been quite useful in applications, but have been lacking theoretical understanding."}, {"paper_id": "3250710", "adju_relevance": 0, "title": "Teaching and learning programming and software engineering via interactive gaming", "background_label": "Massive Open Online Courses (MOOCs) have recently gained high popularity among various universities and even in global societies. A critical factor for their success in teaching and learning effectiveness is assignment grading. Traditional ways of assignment grading are not scalable and do not give timely or interactive feedback to students.", "abstract": "Massive Open Online Courses (MOOCs) have recently gained high popularity among various universities and even in global societies. Massive Open Online Courses (MOOCs) have recently gained high popularity among various universities and even in global societies. A critical factor for their success in teaching and learning effectiveness is assignment grading. Massive Open Online Courses (MOOCs) have recently gained high popularity among various universities and even in global societies. A critical factor for their success in teaching and learning effectiveness is assignment grading. Traditional ways of assignment grading are not scalable and do not give timely or interactive feedback to students."}, {"paper_id": "58028927", "adju_relevance": 0, "title": "Automatic Keyboard Layout Design for Low-Resource Latin-Script Languages", "background_label": "For many speakers, one of the barriers in accessing and creating text content on the web is the absence of input tools for their language. Ease in typing in these languages would lower technological barriers to online communication and collaboration, likely leading to the creation of more web content.", "method_label": "Unfortunately, it can be time-consuming to develop layouts manually even for language communities that use a keyboard layout very similar to English; starting from scratch requires many configuration files to describe multiple possible behaviors for each key. With our approach, we only need a small amount of data in each language to generate keyboard layouts with very little human effort. This process can help serve speakers of low-resource languages in a scalable way, allowing us to develop input tools for more languages.", "result_label": "Having input tools that reflect the linguistic diversity of the world will let as many people as possible use technology to learn, communicate, and express themselves in their own native languages.", "abstract": " For many speakers, one of the barriers in accessing and creating text content on the web is the absence of input tools for their language. For many speakers, one of the barriers in accessing and creating text content on the web is the absence of input tools for their language. Ease in typing in these languages would lower technological barriers to online communication and collaboration, likely leading to the creation of more web content. Unfortunately, it can be time-consuming to develop layouts manually even for language communities that use a keyboard layout very similar to English; starting from scratch requires many configuration files to describe multiple possible behaviors for each key. Unfortunately, it can be time-consuming to develop layouts manually even for language communities that use a keyboard layout very similar to English; starting from scratch requires many configuration files to describe multiple possible behaviors for each key. With our approach, we only need a small amount of data in each language to generate keyboard layouts with very little human effort. Unfortunately, it can be time-consuming to develop layouts manually even for language communities that use a keyboard layout very similar to English; starting from scratch requires many configuration files to describe multiple possible behaviors for each key. With our approach, we only need a small amount of data in each language to generate keyboard layouts with very little human effort. This process can help serve speakers of low-resource languages in a scalable way, allowing us to develop input tools for more languages. Having input tools that reflect the linguistic diversity of the world will let as many people as possible use technology to learn, communicate, and express themselves in their own native languages."}, {"paper_id": "13856098", "adju_relevance": 0, "title": "DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning", "background_label": "Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Existing approaches mine API mappings from projects that have corresponding versions in two languages. They rely on the sparse availability of bilingual projects, thus producing a limited number of API mappings.", "abstract": "Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Existing approaches mine API mappings from projects that have corresponding versions in two languages. Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Existing approaches mine API mappings from projects that have corresponding versions in two languages. They rely on the sparse availability of bilingual projects, thus producing a limited number of API mappings."}, {"paper_id": "52098336", "adju_relevance": 0, "title": "Natural Language Data Management and Interfaces", "background_label": "Abstract The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated with publishing, and the progress on the digitization of printed texts. This growth combined with the proliferation of natural language systems for search and retrieving information provides tremendous opportunities for studying some of the areas where database systems and natural language processing systems overlap.", "abstract": "Abstract The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated with publishing, and the progress on the digitization of printed texts. Abstract The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated with publishing, and the progress on the digitization of printed texts. This growth combined with the proliferation of natural language systems for search and retrieving information provides tremendous opportunities for studying some of the areas where database systems and natural language processing systems overlap."}, {"paper_id": "123556377", "adju_relevance": 0, "title": "Introduction to algorithms", "background_label": "Each language is either designed for a class of problems or supports a different style of programming. In other words, a programming language turns the computer into a \u2018virtual machine\u2019 whose features and capabilities are unlimited.", "method_label": "In this article, we illustrate these aspects through a language similar tologo. Programs are developed to draw geometric pictures using this language.", "abstract": " Each language is either designed for a class of problems or supports a different style of programming. Each language is either designed for a class of problems or supports a different style of programming. In other words, a programming language turns the computer into a \u2018virtual machine\u2019 whose features and capabilities are unlimited. In this article, we illustrate these aspects through a language similar tologo. In this article, we illustrate these aspects through a language similar tologo. Programs are developed to draw geometric pictures using this language."}, {"paper_id": "11761105", "adju_relevance": 0, "title": "Programming Pluralism: Using Learning Analytics to Detect Patterns in the Learning of Computer Programming", "background_label": "New high-frequency, automated data collection and analysis algorithms could offer new insights into complex learning processes, especially for tasks in which students have opportunities to generate unique open-ended artifacts such as computer programs. These approaches should be particularly useful because the need for scalable project-based and student-centered learning is growing considerably.", "abstract": "New high-frequency, automated data collection and analysis algorithms could offer new insights into complex learning processes, especially for tasks in which students have opportunities to generate unique open-ended artifacts such as computer programs. New high-frequency, automated data collection and analysis algorithms could offer new insights into complex learning processes, especially for tasks in which students have opportunities to generate unique open-ended artifacts such as computer programs. These approaches should be particularly useful because the need for scalable project-based and student-centered learning is growing considerably."}, {"paper_id": "58407336", "adju_relevance": 0, "title": "Computer-Assisted Language Learning: Context and Conceptualization", "background_label": "So far the development of Computer-Assisted Language Learning (CALL) has been fragmented. The points of departure for CALL projects have been enormously varied, and when the projects have been written up, they rarely refer to those that have gone before.", "abstract": "So far the development of Computer-Assisted Language Learning (CALL) has been fragmented. So far the development of Computer-Assisted Language Learning (CALL) has been fragmented. The points of departure for CALL projects have been enormously varied, and when the projects have been written up, they rarely refer to those that have gone before."}, {"paper_id": "16946662", "adju_relevance": 0, "title": "Locating and Reusing Sundry NLP Flotsam in an e-Learning Application", "background_label": "We describe the background and motivation for an e-learning project\u2014IT-based Collaborative Learning in Grammar\u2014where NLP resource reuse has become an important issue. The resources are of several kinds: POS-tagged and syntactically annotated corpora (treebanks), parsing systems and grammar writer\u2019 s workbenches, and visulization and manipulation tools for linguistically annotated corpora. Our experience thus far has been that although there are a number of such resources available e.g. on the Web, as a rule, numerous incompatibilities and lack of standardization at all levels\u2014markup formats, linguistic annotation schemes, grammatical framework, software APIs, etc.\u2014make the reuse of these resources into a non-trivial endeavor.", "abstract": "We describe the background and motivation for an e-learning project\u2014IT-based Collaborative Learning in Grammar\u2014where NLP resource reuse has become an important issue. We describe the background and motivation for an e-learning project\u2014IT-based Collaborative Learning in Grammar\u2014where NLP resource reuse has become an important issue. The resources are of several kinds: POS-tagged and syntactically annotated corpora (treebanks), parsing systems and grammar writer\u2019 s workbenches, and visulization and manipulation tools for linguistically annotated corpora. We describe the background and motivation for an e-learning project\u2014IT-based Collaborative Learning in Grammar\u2014where NLP resource reuse has become an important issue. The resources are of several kinds: POS-tagged and syntactically annotated corpora (treebanks), parsing systems and grammar writer\u2019 s workbenches, and visulization and manipulation tools for linguistically annotated corpora. Our experience thus far has been that although there are a number of such resources available e.g. We describe the background and motivation for an e-learning project\u2014IT-based Collaborative Learning in Grammar\u2014where NLP resource reuse has become an important issue. The resources are of several kinds: POS-tagged and syntactically annotated corpora (treebanks), parsing systems and grammar writer\u2019 s workbenches, and visulization and manipulation tools for linguistically annotated corpora. Our experience thus far has been that although there are a number of such resources available e.g. on the Web, as a rule, numerous incompatibilities and lack of standardization at all levels\u2014markup formats, linguistic annotation schemes, grammatical framework, software APIs, etc.\u2014make the reuse of these resources into a non-trivial endeavor."}, {"paper_id": "54524760", "adju_relevance": 0, "title": "The Now-or-Never bottleneck: A fundamental constraint on language.", "background_label": "Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this \"Now-or-Never\" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This is \"Chunk-and-Pass\" processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning.", "result_label": "This observation has strong implications for the nature of language processing: (1) the language system must \"eagerly\" recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with \"Right-First-Time\"; once the original input is lost, there is no way for the language system to recover.", "abstract": "Memory is fleeting. Memory is fleeting. New material rapidly obliterates previous material. Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this \"Now-or-Never\" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must \"eagerly\" recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with \"Right-First-Time\"; once the original input is lost, there is no way for the language system to recover. Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this \"Now-or-Never\" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This is \"Chunk-and-Pass\" processing. Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this \"Now-or-Never\" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This is \"Chunk-and-Pass\" processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this \"Now-or-Never\" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This is \"Chunk-and-Pass\" processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this \"Now-or-Never\" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This is \"Chunk-and-Pass\" processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning."}, {"paper_id": "13383837", "adju_relevance": 0, "title": "Declarative Layer Composition with The JCop Programming Language", "background_label": "Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. With context-oriented programming (COP), context-dependent behavior can be explicitly modularized and dynamically activated. The available COP implementations offer language constructs that allow to describe context-dependent functionality and to specify for which control flows this functionality should be executed. Since these language constructs require modifications to the source code, the contemporary approaches limit the use of COP to program parts whose source code is accessible to the developer (the user code). Instead, context-dependent behavior is addressed whenever a control flow from the framework code enters the user code. Context composition must be addressed at any of these control flow entry points, which may lead to a redundant specification of this functionality.", "result_label": "The dynamic control over context-dependent behavior in frameworks cannot be directly addressed by COP as this would imply changes to the source code.", "abstract": "Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. With context-oriented programming (COP), context-dependent behavior can be explicitly modularized and dynamically activated. Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. With context-oriented programming (COP), context-dependent behavior can be explicitly modularized and dynamically activated. The available COP implementations offer language constructs that allow to describe context-dependent functionality and to specify for which control flows this functionality should be executed. Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. With context-oriented programming (COP), context-dependent behavior can be explicitly modularized and dynamically activated. The available COP implementations offer language constructs that allow to describe context-dependent functionality and to specify for which control flows this functionality should be executed. Since these language constructs require modifications to the source code, the contemporary approaches limit the use of COP to program parts whose source code is accessible to the developer (the user code). The dynamic control over context-dependent behavior in frameworks cannot be directly addressed by COP as this would imply changes to the source code. Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. With context-oriented programming (COP), context-dependent behavior can be explicitly modularized and dynamically activated. The available COP implementations offer language constructs that allow to describe context-dependent functionality and to specify for which control flows this functionality should be executed. Since these language constructs require modifications to the source code, the contemporary approaches limit the use of COP to program parts whose source code is accessible to the developer (the user code). Instead, context-dependent behavior is addressed whenever a control flow from the framework code enters the user code. Abstract Program behavior that relies on contextual information, such as physical location or network accessibility, is common in today's applications, yet its representation at the source code level is not sufficiently supported by programming languages. With context-oriented programming (COP), context-dependent behavior can be explicitly modularized and dynamically activated. The available COP implementations offer language constructs that allow to describe context-dependent functionality and to specify for which control flows this functionality should be executed. Since these language constructs require modifications to the source code, the contemporary approaches limit the use of COP to program parts whose source code is accessible to the developer (the user code). Instead, context-dependent behavior is addressed whenever a control flow from the framework code enters the user code. Context composition must be addressed at any of these control flow entry points, which may lead to a redundant specification of this functionality."}, {"paper_id": "470312", "adju_relevance": 0, "title": "On the Formal Semantics of Speech-Act Based Communication in an Agent-Oriented Programming Language", "background_label": "Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. In part, the problem is that speech-act semantics typically make reference to the\"mental states\"of agents (their beliefs, desires, and intentions), and there is in general no way to attribute such attitudes to arbitrary computational agents. In addition, agent programming languages have only had their semantics formalised for abstract, stand-alone versions, neglecting aspects such as communication primitives. With respect to communication, implemented agent programming languages have tended to be rather ad hoc.", "abstract": "Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. In part, the problem is that speech-act semantics typically make reference to the\"mental states\"of agents (their beliefs, desires, and intentions), and there is in general no way to attribute such attitudes to arbitrary computational agents. Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. In part, the problem is that speech-act semantics typically make reference to the\"mental states\"of agents (their beliefs, desires, and intentions), and there is in general no way to attribute such attitudes to arbitrary computational agents. In addition, agent programming languages have only had their semantics formalised for abstract, stand-alone versions, neglecting aspects such as communication primitives. Research on agent communication languages has typically taken the speech acts paradigm as its starting point. Despite their manifest attractions, speech-act models of communication have several serious disadvantages as a foundation for communication in artificial agent systems. In particular, it has proved to be extremely difficult to give a satisfactory semantics to speech-act based agent communication languages. In part, the problem is that speech-act semantics typically make reference to the\"mental states\"of agents (their beliefs, desires, and intentions), and there is in general no way to attribute such attitudes to arbitrary computational agents. In addition, agent programming languages have only had their semantics formalised for abstract, stand-alone versions, neglecting aspects such as communication primitives. With respect to communication, implemented agent programming languages have tended to be rather ad hoc."}, {"paper_id": "59745579", "adju_relevance": 0, "title": "APPLAB -- A Laboratory For Application Languages", "background_label": ") is an environment thatsupports the interactive development of application languages.", "method_label": "Thesystem allows the language designer to edit a language description andsimultaneously edit a program in the new (changing) language. Thesystem also supports the design of static semantics for the language.APPLAB is used in a case study looking at the different levels of pro-gramming involved in programming industrial robots.", "abstract": ") is an environment thatsupports the interactive development of application languages. Thesystem allows the language designer to edit a language description andsimultaneously edit a program in the new (changing) language. Thesystem allows the language designer to edit a language description andsimultaneously edit a program in the new (changing) language. Thesystem also supports the design of static semantics for the language.APPLAB is used in a case study looking at the different levels of pro-gramming involved in programming industrial robots."}, {"paper_id": "123308915", "adju_relevance": 0, "title": "SEMANTICS OF QUANTUM PROGRAMMING LANGUAGE LANQ", "background_label": "We show a memory model of an imperative concurrent quantum programming language LanQ.", "method_label": "The memory model is used to specify the shape of semantical structure upon which the language operational semantics is defined. We also outline the language abilities in the area of formal verification on an example implementation of teleportation protocol.", "abstract": "We show a memory model of an imperative concurrent quantum programming language LanQ. The memory model is used to specify the shape of semantical structure upon which the language operational semantics is defined. The memory model is used to specify the shape of semantical structure upon which the language operational semantics is defined. We also outline the language abilities in the area of formal verification on an example implementation of teleportation protocol."}, {"paper_id": "16098539", "adju_relevance": 0, "title": "Online Pair-Programming for Learning Programming of Novices", "abstract": ""}, {"paper_id": "3648651", "adju_relevance": 0, "title": "Structures, Not Strings: Linguistics as Part of the Cognitive Sciences.", "background_label": "There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what 'language' is. Sharpening the core question--what is language?--and paying close attention to the basic property of the language faculty and its biological foundations makes it clear how linguistics is firmly positioned within the cognitive sciences.", "result_label": "Here we will show how recent developments in generative grammar, taking language as a computational cognitive mechanism seriously, allow us to address issues left unexplained in the increasingly popular surface-oriented approaches to language.", "abstract": "There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what 'language' is. There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what 'language' is. Sharpening the core question--what is language?--and paying close attention to the basic property of the language faculty and its biological foundations makes it clear how linguistics is firmly positioned within the cognitive sciences. Here we will show how recent developments in generative grammar, taking language as a computational cognitive mechanism seriously, allow us to address issues left unexplained in the increasingly popular surface-oriented approaches to language."}, {"paper_id": "6098470", "adju_relevance": 0, "title": "Deep Syntax Language Models and Statistical Machine Translation", "background_label": "AbstractHierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005) , for example.", "method_label": "An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model.", "result_label": "We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems.", "abstract": "AbstractHierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. AbstractHierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005) , for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems."}, {"paper_id": "34352074", "adju_relevance": 0, "title": "Promoin: An Interactive System for Multiobjective Programming", "background_label": "The interactive system PROMOIN is presented in this paper.", "abstract": "The interactive system PROMOIN is presented in this paper."}, {"paper_id": "14475079", "adju_relevance": 0, "title": "CodeMend: Assisting Interactive Programming with Bimodal Embedding", "background_label": "Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. To address this problem, we present CodeMend, a system to support finding and integration of code. CodeMend leverages a neural embedding model to jointly model natural language and code as mined from large Web and code datasets.", "method_label": "We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. The system makes salient the relevant API functions, the lines in the end-user's program that should be changed, as well as proposing the actual change.", "result_label": "We demonstrate the utility and accuracy of CodeMend through lab and simulation studies.", "abstract": "Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. To address this problem, we present CodeMend, a system to support finding and integration of code. Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. To address this problem, we present CodeMend, a system to support finding and integration of code. CodeMend leverages a neural embedding model to jointly model natural language and code as mined from large Web and code datasets. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. The system makes salient the relevant API functions, the lines in the end-user's program that should be changed, as well as proposing the actual change. We demonstrate the utility and accuracy of CodeMend through lab and simulation studies."}, {"paper_id": "2119453", "adju_relevance": 0, "title": "Marco: Safe, Expressive Macros for Any Language \u22c6", "background_label": "Abstract. Macros improve expressiveness, concision, abstraction, and language interoperability without changing the programming language itself. They are indispensable for building increasingly prevalent multilingual applications. Unfortunately, existing macro systems are wellencapsulated but unsafe (e.g., the C preprocessor) or are safe but tightlyintegrated with the language implementation (e.g., Scheme macros).", "abstract": "Abstract. Abstract. Macros improve expressiveness, concision, abstraction, and language interoperability without changing the programming language itself. Abstract. Macros improve expressiveness, concision, abstraction, and language interoperability without changing the programming language itself. They are indispensable for building increasingly prevalent multilingual applications. Abstract. Macros improve expressiveness, concision, abstraction, and language interoperability without changing the programming language itself. They are indispensable for building increasingly prevalent multilingual applications. Unfortunately, existing macro systems are wellencapsulated but unsafe (e.g., the C preprocessor) or are safe but tightlyintegrated with the language implementation (e.g., Scheme macros)."}, {"paper_id": "2921786", "adju_relevance": 0, "title": "Gated-Attention Architectures for Task-Oriented Language Grounding", "background_label": "To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding.", "abstract": "To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding."}, {"paper_id": "1981910", "adju_relevance": 0, "title": "Decentralized language learning through acting", "method_label": "Our reinforcement-learning method is based on Bayesian filtering and has been adapted for a decentralized control process. Designing intelligent agents able to adapt their mutual interpretation of messages exchanged, in order to improve overall task-oriented performance, introduces an essential cognitive capability that can upgrade the current state of the art in multi-agent and human-machine systems to the next level.", "result_label": "Empirical results shed light on the complexity of the learning problem, and on factors affecting the speed of convergence. Learning to communicate while acting will add to the robustness and flexibility of these systems and hence to a more efficient and productive performance.", "abstract": " Our reinforcement-learning method is based on Bayesian filtering and has been adapted for a decentralized control process. Empirical results shed light on the complexity of the learning problem, and on factors affecting the speed of convergence. Our reinforcement-learning method is based on Bayesian filtering and has been adapted for a decentralized control process. Designing intelligent agents able to adapt their mutual interpretation of messages exchanged, in order to improve overall task-oriented performance, introduces an essential cognitive capability that can upgrade the current state of the art in multi-agent and human-machine systems to the next level. Empirical results shed light on the complexity of the learning problem, and on factors affecting the speed of convergence. Learning to communicate while acting will add to the robustness and flexibility of these systems and hence to a more efficient and productive performance."}, {"paper_id": "15103971", "adju_relevance": 0, "title": "Language identification of names with SVMs", "background_label": "AbstractThe task of identifying the language of text or utterances has a number of applications in natural language processing. Language identification has traditionally been approached with character-level language models. However, the language model approach crucially depends on the length of the text in question.", "abstract": "AbstractThe task of identifying the language of text or utterances has a number of applications in natural language processing. AbstractThe task of identifying the language of text or utterances has a number of applications in natural language processing. Language identification has traditionally been approached with character-level language models. AbstractThe task of identifying the language of text or utterances has a number of applications in natural language processing. Language identification has traditionally been approached with character-level language models. However, the language model approach crucially depends on the length of the text in question."}, {"paper_id": "146468954", "adju_relevance": 0, "title": "Constructing a Language: A Usage-Based Theory of Language Acquisition", "background_label": "Drawing together a vast body of empirical research in cognitive science, linguistics, and developmental psychology, Michael Tomasello demonstrates that we don't need a self-contained \"language instinct\" to explain how children learn language. Their linguistic ability is interwoven with other cognitive abilities.", "abstract": "Drawing together a vast body of empirical research in cognitive science, linguistics, and developmental psychology, Michael Tomasello demonstrates that we don't need a self-contained \"language instinct\" to explain how children learn language. Drawing together a vast body of empirical research in cognitive science, linguistics, and developmental psychology, Michael Tomasello demonstrates that we don't need a self-contained \"language instinct\" to explain how children learn language. Their linguistic ability is interwoven with other cognitive abilities."}, {"paper_id": "10023477", "adju_relevance": 0, "title": "Scripting embodied agents behaviour with CML: character markup language", "background_label": "Embodied agents present ongoing challenging agenda for research in multi-modal user interfaces and human-computer-interaction. Such agent metaphors will only be widely applicable to online applications when there is a standardised way to map underlying engines with the visual presentation of the agents.", "abstract": "Embodied agents present ongoing challenging agenda for research in multi-modal user interfaces and human-computer-interaction. Embodied agents present ongoing challenging agenda for research in multi-modal user interfaces and human-computer-interaction. Such agent metaphors will only be widely applicable to online applications when there is a standardised way to map underlying engines with the visual presentation of the agents."}, {"paper_id": "1308791", "adju_relevance": 0, "title": "Multilingual Text Entry using Automatic Language Detection", "background_label": "AbstractComputer users increasingly need to produce text written in multiple languages. However, typical computer interfaces require the user to change the text entry software each time a different language is used.", "abstract": "AbstractComputer users increasingly need to produce text written in multiple languages. AbstractComputer users increasingly need to produce text written in multiple languages. However, typical computer interfaces require the user to change the text entry software each time a different language is used."}, {"paper_id": "62034515", "adju_relevance": 0, "title": "Flapjax: a programming language for Ajax applications", "background_label": "This paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces.", "abstract": "This paper presents Flapjax, a language designed for contemporary Web applications. This paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces."}, {"paper_id": "148068080", "adju_relevance": 0, "title": "Why Only Us: Language and Evolution", "background_label": "We are born crying, but those cries signal the first stirring of language. Within a year or so, infants master the sound system of their language; a few years after that, they are engaging in conversations. This remarkable, species-specific ability to acquire any human language -- \"the language faculty\" -- raises important biological questions about language, including how it has evolved.", "abstract": "We are born crying, but those cries signal the first stirring of language. We are born crying, but those cries signal the first stirring of language. Within a year or so, infants master the sound system of their language; a few years after that, they are engaging in conversations. We are born crying, but those cries signal the first stirring of language. Within a year or so, infants master the sound system of their language; a few years after that, they are engaging in conversations. This remarkable, species-specific ability to acquire any human language -- \"the language faculty\" -- raises important biological questions about language, including how it has evolved."}, {"paper_id": "27267084", "adju_relevance": 0, "title": "GENERATING AMERICAN SIGN LANGUAGE CLASSIFIER PREDICATES FOR ENGLISH-TO-ASL MACHINE TRANSLATION", "background_label": "A majority of deaf 18-year-olds in the United States have an English reading level below that of a typical 10-year-old student, and so machine translation (MT) software that could translate English text into American Sign Language (ASL) animations could significantly improve these individuals' access to information, communication, and services. Previous English-to-ASL MT projects have made limited progress by restricting their output to subsets of ASL phenomena---thus avoiding important linguistic and animation issues. CPs are frequent in ASL and are necessary for conveying many concepts. The classifier predicate generator inside this design has a planning-based architecture that uses a 3D \"visualization\" model of the arrangement of objects in a scene discussed by the English input text. This generator would be one pathway in a multi-path English-to-ASL MT design; a separate processing pathway would be used to generate classifier predicates, to generate other ASL sentences, and to generate animations of Signed English (if the system lacked lexical resources for some input).", "method_label": "None of these systems have shown how to generate classifier predicates (CPs), a phenomenon in which signers use special hand movements to indicate the location and movement of invisible objects (representing entities under discussion) in space around their bodies. Instead of representing the ASL animation as a string (of individual signs to perform), this system encodes the multimodal language signal as multiple channels that are hierarchically structured and coordinated over time. While this design feature and others have been prompted by the unique requirements of generating a sign language, these technologies have applications for the machine translation of written languages, the representation of other multimodal language signals, and the production of meaningful gestures by other animated virtual human characters. To evaluate the functionality and scalability of the most novel portion of this English-to-ASL MT design, this project has implemented a prototype-version of the planning-based classifier predicate generator.", "result_label": "This project has created an English-to-ASL MT design capable of producing classifier predicates. The classifier predicate animations produced by the system have been shown to native ASL signers to evaluate the output.", "abstract": "A majority of deaf 18-year-olds in the United States have an English reading level below that of a typical 10-year-old student, and so machine translation (MT) software that could translate English text into American Sign Language (ASL) animations could significantly improve these individuals' access to information, communication, and services. A majority of deaf 18-year-olds in the United States have an English reading level below that of a typical 10-year-old student, and so machine translation (MT) software that could translate English text into American Sign Language (ASL) animations could significantly improve these individuals' access to information, communication, and services. Previous English-to-ASL MT projects have made limited progress by restricting their output to subsets of ASL phenomena---thus avoiding important linguistic and animation issues. None of these systems have shown how to generate classifier predicates (CPs), a phenomenon in which signers use special hand movements to indicate the location and movement of invisible objects (representing entities under discussion) in space around their bodies. A majority of deaf 18-year-olds in the United States have an English reading level below that of a typical 10-year-old student, and so machine translation (MT) software that could translate English text into American Sign Language (ASL) animations could significantly improve these individuals' access to information, communication, and services. Previous English-to-ASL MT projects have made limited progress by restricting their output to subsets of ASL phenomena---thus avoiding important linguistic and animation issues. CPs are frequent in ASL and are necessary for conveying many concepts. This project has created an English-to-ASL MT design capable of producing classifier predicates. A majority of deaf 18-year-olds in the United States have an English reading level below that of a typical 10-year-old student, and so machine translation (MT) software that could translate English text into American Sign Language (ASL) animations could significantly improve these individuals' access to information, communication, and services. Previous English-to-ASL MT projects have made limited progress by restricting their output to subsets of ASL phenomena---thus avoiding important linguistic and animation issues. CPs are frequent in ASL and are necessary for conveying many concepts. The classifier predicate generator inside this design has a planning-based architecture that uses a 3D \"visualization\" model of the arrangement of objects in a scene discussed by the English input text. A majority of deaf 18-year-olds in the United States have an English reading level below that of a typical 10-year-old student, and so machine translation (MT) software that could translate English text into American Sign Language (ASL) animations could significantly improve these individuals' access to information, communication, and services. Previous English-to-ASL MT projects have made limited progress by restricting their output to subsets of ASL phenomena---thus avoiding important linguistic and animation issues. CPs are frequent in ASL and are necessary for conveying many concepts. The classifier predicate generator inside this design has a planning-based architecture that uses a 3D \"visualization\" model of the arrangement of objects in a scene discussed by the English input text. This generator would be one pathway in a multi-path English-to-ASL MT design; a separate processing pathway would be used to generate classifier predicates, to generate other ASL sentences, and to generate animations of Signed English (if the system lacked lexical resources for some input). None of these systems have shown how to generate classifier predicates (CPs), a phenomenon in which signers use special hand movements to indicate the location and movement of invisible objects (representing entities under discussion) in space around their bodies. Instead of representing the ASL animation as a string (of individual signs to perform), this system encodes the multimodal language signal as multiple channels that are hierarchically structured and coordinated over time. None of these systems have shown how to generate classifier predicates (CPs), a phenomenon in which signers use special hand movements to indicate the location and movement of invisible objects (representing entities under discussion) in space around their bodies. Instead of representing the ASL animation as a string (of individual signs to perform), this system encodes the multimodal language signal as multiple channels that are hierarchically structured and coordinated over time. While this design feature and others have been prompted by the unique requirements of generating a sign language, these technologies have applications for the machine translation of written languages, the representation of other multimodal language signals, and the production of meaningful gestures by other animated virtual human characters. None of these systems have shown how to generate classifier predicates (CPs), a phenomenon in which signers use special hand movements to indicate the location and movement of invisible objects (representing entities under discussion) in space around their bodies. Instead of representing the ASL animation as a string (of individual signs to perform), this system encodes the multimodal language signal as multiple channels that are hierarchically structured and coordinated over time. While this design feature and others have been prompted by the unique requirements of generating a sign language, these technologies have applications for the machine translation of written languages, the representation of other multimodal language signals, and the production of meaningful gestures by other animated virtual human characters. To evaluate the functionality and scalability of the most novel portion of this English-to-ASL MT design, this project has implemented a prototype-version of the planning-based classifier predicate generator. This project has created an English-to-ASL MT design capable of producing classifier predicates. The classifier predicate animations produced by the system have been shown to native ASL signers to evaluate the output."}, {"paper_id": "7867412", "adju_relevance": 0, "title": "An interactive semantics of logic programming", "method_label": "The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. As main contributions, we mention the finitary presentation of abstract unification, and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming. Moreover, the compositionality of the tile semantics is guaranteed by standard results, as it reduces to check that the tile systems associated to logic programs enjoy the tile decomposition property.", "result_label": "An extension of the approach for handling constraint systems is also discussed.", "abstract": " The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. As main contributions, we mention the finitary presentation of abstract unification, and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming. The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. As main contributions, we mention the finitary presentation of abstract unification, and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming. Moreover, the compositionality of the tile semantics is guaranteed by standard results, as it reduces to check that the tile systems associated to logic programs enjoy the tile decomposition property. An extension of the approach for handling constraint systems is also discussed."}, {"paper_id": "37390552", "adju_relevance": 0, "title": "Learning with Latent Language", "background_label": "The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies?", "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies?"}, {"paper_id": "55996011", "adju_relevance": 0, "title": "Probabilistic Programming in Python using PyMC", "background_label": "Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987).", "method_label": "Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython.", "result_label": "These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.", "abstract": "Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis."}, {"paper_id": "11017969", "adju_relevance": 0, "title": "Learning Phrasal Lexicons for Robotic Commands using Crowdsourcing", "background_label": "Robotic commands in natural language usually contain lots of spatial descriptions which are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions.", "method_label": "To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language.", "abstract": "Robotic commands in natural language usually contain lots of spatial descriptions which are semantically similar but syntactically different. Robotic commands in natural language usually contain lots of spatial descriptions which are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language."}, {"paper_id": "146121146", "adju_relevance": 0, "title": "TryLinks: An interactive tutorial system for a cross-tier Web programming language", "background_label": "Links is a web programming language under development in Edinburgh aimed at simplifying web development. Conventional multi-tier applications involve programming in several languages for different layers, and the mismatches between these layers and abstractions need to be handled by the programmer, which can lead to costly errors or security vulnerabilities. In Links, programs combine all of the code of a web application in a single program, and the implementation generates appropriate JavaScript and HTML for the client, and SQL queries for the database. However, installing and using Links is non-trivial, making it difficult for new programmers to get started and learn about Links.", "abstract": "Links is a web programming language under development in Edinburgh aimed at simplifying web development. Links is a web programming language under development in Edinburgh aimed at simplifying web development. Conventional multi-tier applications involve programming in several languages for different layers, and the mismatches between these layers and abstractions need to be handled by the programmer, which can lead to costly errors or security vulnerabilities. Links is a web programming language under development in Edinburgh aimed at simplifying web development. Conventional multi-tier applications involve programming in several languages for different layers, and the mismatches between these layers and abstractions need to be handled by the programmer, which can lead to costly errors or security vulnerabilities. In Links, programs combine all of the code of a web application in a single program, and the implementation generates appropriate JavaScript and HTML for the client, and SQL queries for the database. Links is a web programming language under development in Edinburgh aimed at simplifying web development. Conventional multi-tier applications involve programming in several languages for different layers, and the mismatches between these layers and abstractions need to be handled by the programmer, which can lead to costly errors or security vulnerabilities. In Links, programs combine all of the code of a web application in a single program, and the implementation generates appropriate JavaScript and HTML for the client, and SQL queries for the database. However, installing and using Links is non-trivial, making it difficult for new programmers to get started and learn about Links."}, {"paper_id": "11730229", "adju_relevance": 0, "title": "Interactive Learning of State Representation through Natural Language Instruction and Explanation", "background_label": "One significant simplification in most previous work on robot learning is the closed-world assumption where the robot is assumed to know ahead of time a complete set of predicates describing the state of the physical world. However, robots are not likely to have a complete model of the world especially when learning a new task.", "abstract": "One significant simplification in most previous work on robot learning is the closed-world assumption where the robot is assumed to know ahead of time a complete set of predicates describing the state of the physical world. One significant simplification in most previous work on robot learning is the closed-world assumption where the robot is assumed to know ahead of time a complete set of predicates describing the state of the physical world. However, robots are not likely to have a complete model of the world especially when learning a new task."}, {"paper_id": "9027681", "adju_relevance": 0, "title": "Compositional Semantic Parsing on Semi-Structured Tables", "background_label": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision.", "method_label": "The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines.", "result_label": "For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.", "abstract": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available."}, {"paper_id": "122872326", "adju_relevance": 0, "title": "Interactive Goal Programming", "background_label": "The use of goal programming for the solution of the multiple criteria problem is discussed.", "method_label": "An extension of this use is presented in the form of an algorithm which requires interaction with the relevant decision-maker in order to obtain certain information regarding his utility function defined over the permissible values of the criteria. This algorithm provides a bridge between goal programming and recently suggested interactive strategies for the optimization of the multiple criteria problem.", "abstract": "The use of goal programming for the solution of the multiple criteria problem is discussed. An extension of this use is presented in the form of an algorithm which requires interaction with the relevant decision-maker in order to obtain certain information regarding his utility function defined over the permissible values of the criteria. An extension of this use is presented in the form of an algorithm which requires interaction with the relevant decision-maker in order to obtain certain information regarding his utility function defined over the permissible values of the criteria. This algorithm provides a bridge between goal programming and recently suggested interactive strategies for the optimization of the multiple criteria problem."}, {"paper_id": "145287916", "adju_relevance": 0, "title": "Agent-based models of language competition", "background_label": "Abstract This article uses computer simulations as a means of assessing two different models for the competition between two languages from the interdisciplinary perspective of complex systems. These models allow for the analysis of the role of bilingual speakers and they consider two of the basic factors determining the use of each language: their relative prestige and the language loyalty of their speakers.", "abstract": "Abstract This article uses computer simulations as a means of assessing two different models for the competition between two languages from the interdisciplinary perspective of complex systems. Abstract This article uses computer simulations as a means of assessing two different models for the competition between two languages from the interdisciplinary perspective of complex systems. These models allow for the analysis of the role of bilingual speakers and they consider two of the basic factors determining the use of each language: their relative prestige and the language loyalty of their speakers."}, {"paper_id": "60903435", "adju_relevance": 0, "title": "Learning Basic Programming Concepts by Creating Games with Scratch Programming Environment", "background_label": "Abstract A number of researchers have documented several difficulties faced by learners of basic programming concepts. Among the suggested pedagogical solutions to overcome these difficulties is the use of serious games inthe learning process. In fact, these games are more likely to boost the motivation of students and allow them to develop their knowledge efficiently.", "abstract": "Abstract A number of researchers have documented several difficulties faced by learners of basic programming concepts. Abstract A number of researchers have documented several difficulties faced by learners of basic programming concepts. Among the suggested pedagogical solutions to overcome these difficulties is the use of serious games inthe learning process. Abstract A number of researchers have documented several difficulties faced by learners of basic programming concepts. Among the suggested pedagogical solutions to overcome these difficulties is the use of serious games inthe learning process. In fact, these games are more likely to boost the motivation of students and allow them to develop their knowledge efficiently."}, {"paper_id": "145088158", "adju_relevance": 0, "title": "Children creating core properties of language: Evidence from an emerging sign language in Nicaragua", "background_label": "A new sign language has been created by deaf Nicaraguans over the past 25 years, providing an opportunity to observe the inception of universal hallmarks of language. We found that in their initial creation of the language, children analyzed complex events into basic elements and sequenced these elements into hierarchically structured expressions according to principles not observed in gestures accompanying speech in the surrounding language.", "method_label": "Successive cohorts of learners extended this procedure, transforming Nicaraguan signing from its early gestural form into a linguistic system. We propose that this early segmentation and recombination reflect mechanisms with which children learn, and thereby perpetuate, language.", "result_label": "Thus, children naturally possess learning abilities capable of giving language its fundamental structure.", "abstract": "A new sign language has been created by deaf Nicaraguans over the past 25 years, providing an opportunity to observe the inception of universal hallmarks of language. A new sign language has been created by deaf Nicaraguans over the past 25 years, providing an opportunity to observe the inception of universal hallmarks of language. We found that in their initial creation of the language, children analyzed complex events into basic elements and sequenced these elements into hierarchically structured expressions according to principles not observed in gestures accompanying speech in the surrounding language. Successive cohorts of learners extended this procedure, transforming Nicaraguan signing from its early gestural form into a linguistic system. Successive cohorts of learners extended this procedure, transforming Nicaraguan signing from its early gestural form into a linguistic system. We propose that this early segmentation and recombination reflect mechanisms with which children learn, and thereby perpetuate, language. Thus, children naturally possess learning abilities capable of giving language its fundamental structure."}, {"paper_id": "15544018", "adju_relevance": 0, "title": "Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System", "background_label": "We present NaturalOWL, a natural language generation system that produces texts describing individuals or classes of OWL ontologies.", "abstract": "We present NaturalOWL, a natural language generation system that produces texts describing individuals or classes of OWL ontologies."}, {"paper_id": "29379427", "adju_relevance": 0, "title": "Building Statistical Language Models of code", "background_label": "We present the Source Code Statistical Language Model data analysis pattern. Statistical language models have been an enabling tool for a wide array of important language technologies. Speech recognition, machine translation, and document summarization (to name a few) all rely on statistical language models to assign probability estimates to natural language utterances or sentences.", "method_label": "In this data analysis pattern, we describe the process of building n-gram language models over software source files.", "result_label": "We hope that by introducing the empirical software engineering community to best practices that have been established over the years in research for natural languages, statistical language models can become a tool that SE researchers are able to use to explore new research directions.", "abstract": "We present the Source Code Statistical Language Model data analysis pattern. We present the Source Code Statistical Language Model data analysis pattern. Statistical language models have been an enabling tool for a wide array of important language technologies. We present the Source Code Statistical Language Model data analysis pattern. Statistical language models have been an enabling tool for a wide array of important language technologies. Speech recognition, machine translation, and document summarization (to name a few) all rely on statistical language models to assign probability estimates to natural language utterances or sentences. In this data analysis pattern, we describe the process of building n-gram language models over software source files. We hope that by introducing the empirical software engineering community to best practices that have been established over the years in research for natural languages, statistical language models can become a tool that SE researchers are able to use to explore new research directions."}, {"paper_id": "14892379", "adju_relevance": 0, "title": "PIDoc: Wiki style Literate Programming for Prolog", "background_label": "This document introduces PlDoc, a literate programming system for Prolog. Starting point for PlDoc was minimal distraction from the programming task and maximal immediate reward, attempting to seduce the programmer to use the system.", "method_label": "Minimal distraction is achieved using structured comments that are as closely as possible related to common Prolog documentation practices. Immediate reward is provided by a web interface powered from the Prolog development environment that integrates searching and browsing application and system documentation.", "result_label": "When accessed from localhost, it is possible to go from documentation shown in a browser to the source code displayed in the user's editor of choice.", "abstract": "This document introduces PlDoc, a literate programming system for Prolog. This document introduces PlDoc, a literate programming system for Prolog. Starting point for PlDoc was minimal distraction from the programming task and maximal immediate reward, attempting to seduce the programmer to use the system. Minimal distraction is achieved using structured comments that are as closely as possible related to common Prolog documentation practices. Minimal distraction is achieved using structured comments that are as closely as possible related to common Prolog documentation practices. Immediate reward is provided by a web interface powered from the Prolog development environment that integrates searching and browsing application and system documentation. When accessed from localhost, it is possible to go from documentation shown in a browser to the source code displayed in the user's editor of choice."}, {"paper_id": "8518817", "adju_relevance": 0, "title": "Learning abstract visual concepts via probabilistic program induction in a Language of Thought", "background_label": "The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood.", "abstract": "The ability to learn abstract concepts is a powerful component of human cognition. The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "70125628", "adju_relevance": 0, "title": "Learning Libraries of Subroutines for Neurally\u2013Guided Bayesian Program Learning", "background_label": "Successful approaches to program induction require a hand-engineered domain-specific language (DSL), constraining the space of allowed programs and imparting prior knowledge of the domain.", "method_label": "We contribute a program induction algorithm that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We use our model to synthesize functions on lists, edit text, and solve symbolic regression problems, showing how the model learns a domain-specific library of program components for expressing solutions to problems in the domain.", "abstract": "Successful approaches to program induction require a hand-engineered domain-specific language (DSL), constraining the space of allowed programs and imparting prior knowledge of the domain. We contribute a program induction algorithm that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We contribute a program induction algorithm that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We use our model to synthesize functions on lists, edit text, and solve symbolic regression problems, showing how the model learns a domain-specific library of program components for expressing solutions to problems in the domain."}, {"paper_id": "842659", "adju_relevance": 0, "title": "An incrementally extensible document retrieval system based on linguistic and logical principles", "background_label": "Most natural language based document retrieval systems use the syntax structures of constituent phrases of documents as index terms. Many of these systems also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structures. However, the retrieval performance of such systems remains fairly disappointing. Some systems therefore use a meaning representation language to index and retrieve documents.", "method_label": "In this paper, a system is presented that uses Horn Clause Logic as meaning representation language, employs advanced techniques from natural Language Processing to achieve incremental extensibility, and uses methods from Logic Programming to achieve robustness in the face of insufficient data.", "abstract": "Most natural language based document retrieval systems use the syntax structures of constituent phrases of documents as index terms. Most natural language based document retrieval systems use the syntax structures of constituent phrases of documents as index terms. Many of these systems also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structures. Most natural language based document retrieval systems use the syntax structures of constituent phrases of documents as index terms. Many of these systems also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structures. However, the retrieval performance of such systems remains fairly disappointing. Most natural language based document retrieval systems use the syntax structures of constituent phrases of documents as index terms. Many of these systems also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structures. However, the retrieval performance of such systems remains fairly disappointing. Some systems therefore use a meaning representation language to index and retrieve documents. In this paper, a system is presented that uses Horn Clause Logic as meaning representation language, employs advanced techniques from natural Language Processing to achieve incremental extensibility, and uses methods from Logic Programming to achieve robustness in the face of insufficient data."}, {"paper_id": "15542847", "adju_relevance": 0, "title": "Naturalizing consciousness: a theoretical framework.", "background_label": "Consciousness has a number of apparently disparate properties, some of which seem to be highly complex and even inaccessible to outside observation. To place these properties within a biological framework requires a theory based on a set of evolutionary and developmental principles.", "abstract": "Consciousness has a number of apparently disparate properties, some of which seem to be highly complex and even inaccessible to outside observation. Consciousness has a number of apparently disparate properties, some of which seem to be highly complex and even inaccessible to outside observation. To place these properties within a biological framework requires a theory based on a set of evolutionary and developmental principles."}, {"paper_id": "11341005", "adju_relevance": 0, "title": "Learning with dynamic programming", "background_label": "We consider the role of dynamic programming in sequential learning problems. These problems require deciding which information to collect in order to best support later actions. Such problems are ubiquitous, appearing in simulation, global optimization, revenue management, and many other areas.", "method_label": "Dynamic programming oers a coherent framework for understanding and solving Bayesian formulations of these problems. We present the dynamic programming formulation applied to a canonical problem, Bernoulli ranking and selection. We then review other sequential learning problems from the literature and the role of dynamic programming in their analysis.", "abstract": "We consider the role of dynamic programming in sequential learning problems. We consider the role of dynamic programming in sequential learning problems. These problems require deciding which information to collect in order to best support later actions. We consider the role of dynamic programming in sequential learning problems. These problems require deciding which information to collect in order to best support later actions. Such problems are ubiquitous, appearing in simulation, global optimization, revenue management, and many other areas. Dynamic programming oers a coherent framework for understanding and solving Bayesian formulations of these problems. Dynamic programming oers a coherent framework for understanding and solving Bayesian formulations of these problems. We present the dynamic programming formulation applied to a canonical problem, Bernoulli ranking and selection. Dynamic programming oers a coherent framework for understanding and solving Bayesian formulations of these problems. We present the dynamic programming formulation applied to a canonical problem, Bernoulli ranking and selection. We then review other sequential learning problems from the literature and the role of dynamic programming in their analysis."}, {"paper_id": "60529032", "adju_relevance": 0, "title": "Programming in MacScheme", "background_label": "\"Programming in MacScheme\" is an introduction to programming using the Scheme Language on an Apple Macintosh. It assumes no previous programming experience on the part of the reader, but covers all the basics of the language and many advanced topics as well. Like all good computer language texts, it gets the learner on the machine early and provides an abundance of examples and exercises.Michael Eisenberg is a Ph.D candidate in Computer Science at MIT.", "method_label": "William Clinger is Assistant Professor of Computer Science at the University of Oregon.", "result_label": "Ann Hartheimer is a programmer and writer.Harold Abelson is an Associate Professor of Computer Science and Engineering at MIT.", "abstract": "\"Programming in MacScheme\" is an introduction to programming using the Scheme Language on an Apple Macintosh. \"Programming in MacScheme\" is an introduction to programming using the Scheme Language on an Apple Macintosh. It assumes no previous programming experience on the part of the reader, but covers all the basics of the language and many advanced topics as well. \"Programming in MacScheme\" is an introduction to programming using the Scheme Language on an Apple Macintosh. It assumes no previous programming experience on the part of the reader, but covers all the basics of the language and many advanced topics as well. Like all good computer language texts, it gets the learner on the machine early and provides an abundance of examples and exercises.Michael Eisenberg is a Ph.D candidate in Computer Science at MIT. William Clinger is Assistant Professor of Computer Science at the University of Oregon. Ann Hartheimer is a programmer and writer.Harold Abelson is an Associate Professor of Computer Science and Engineering at MIT."}, {"paper_id": "1278819", "adju_relevance": 0, "title": "Sign Language Lexical Recognition With Propositional Dynamic Logic", "background_label": "SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations.", "method_label": "Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora.", "result_label": "We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages.", "abstract": " SLs are visual, complete, standalone languages which are just as expressive as oral languages. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages."}, {"paper_id": "1735632", "adju_relevance": 0, "title": "Adaptive Statistical Language Modeling: A Maximum Entropy Approach", "background_label": "Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge.", "abstract": "Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge."}, {"paper_id": "1335463", "adju_relevance": 0, "title": "Language composition untangled", "background_label": "In language-oriented programming and modeling, software developers are largely concerned with the definition of domain-specific languages (DSLs) and their composition. While various implementation techniques and frameworks exist for defining DSLs, language composition has not obtained enough attention and is not well-enough understood. In particular, there is a lack of precise terminology for describing observations about language composition in theory and in existing language-development systems.", "method_label": "To clarify the issue, we specify five forms of language composition: language extension, language restriction, language unification, self-extension, and extension composition. We illustrate this classification by various examples and apply it to discuss the performance of different language-development systems with respect to language composition.", "result_label": "We hope that the terminology provided by our classification will enable more precise communication on language composition.", "abstract": "In language-oriented programming and modeling, software developers are largely concerned with the definition of domain-specific languages (DSLs) and their composition. In language-oriented programming and modeling, software developers are largely concerned with the definition of domain-specific languages (DSLs) and their composition. While various implementation techniques and frameworks exist for defining DSLs, language composition has not obtained enough attention and is not well-enough understood. In language-oriented programming and modeling, software developers are largely concerned with the definition of domain-specific languages (DSLs) and their composition. While various implementation techniques and frameworks exist for defining DSLs, language composition has not obtained enough attention and is not well-enough understood. In particular, there is a lack of precise terminology for describing observations about language composition in theory and in existing language-development systems. To clarify the issue, we specify five forms of language composition: language extension, language restriction, language unification, self-extension, and extension composition. To clarify the issue, we specify five forms of language composition: language extension, language restriction, language unification, self-extension, and extension composition. We illustrate this classification by various examples and apply it to discuss the performance of different language-development systems with respect to language composition. We hope that the terminology provided by our classification will enable more precise communication on language composition."}, {"paper_id": "15767190", "adju_relevance": 0, "title": "Cross-lingual Text Classification Using Topic-Dependent Word Probabilities", "background_label": "Cross-lingual text classification is a major challenge in natural language processing, since often training data is available in only one language (target language), but not available for the language of the document we want to classify (source language).", "method_label": "Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words. Using the derived translation probabilities, we then calculate the expected word frequency of each word type in the target language. Finally, these expected word frequencies can be used to classify the source text with any classifier that was trained using only target language documents.", "result_label": "Our experiments confirm the usefulness of our proposed method.", "abstract": "Cross-lingual text classification is a major challenge in natural language processing, since often training data is available in only one language (target language), but not available for the language of the document we want to classify (source language). Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words. Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words. Using the derived translation probabilities, we then calculate the expected word frequency of each word type in the target language. Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words. Using the derived translation probabilities, we then calculate the expected word frequency of each word type in the target language. Finally, these expected word frequencies can be used to classify the source text with any classifier that was trained using only target language documents. Our experiments confirm the usefulness of our proposed method."}, {"paper_id": "8292072", "adju_relevance": 0, "title": "Visualizing Natural Language Descriptions: A Survey", "background_label": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation.", "abstract": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation."}, {"paper_id": "16895378", "adju_relevance": 0, "title": "Automatic segmentation and identification of mixed-language speech using delta-BIC and LSA-based GMMs", "method_label": "A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A VQ-based bi-gram model is used to characterize the acoustic-phonetic dynamics of two consecutive codewords in a language. Accordingly the language-specific acoustic-phonetic property of sequence of phones was integrated in the identification process. A Gaussian mixture model (GMM) is used to model codeword occurrence vectors orthonormally transformed using latent semantic analysis (LSA) for each language-dependent segment. A filtering method is used to smooth the hypothesized language sequence and thus eliminate noise-like components of the detected language sequence generated by the maximum likelihood estimation. Finally, a dynamic programming method is used to determine globally the language boundaries.", "result_label": "Experimental results show that for Mandarin, English, and Taiwanese, a recall rate of 0.87 for language boundary segmentation was obtained. Based on this recall rate, the proposed approach achieved language identification accuracies of 92.1% and 74.9% for single-language and mixed-language speech, respectively.", "abstract": " A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A VQ-based bi-gram model is used to characterize the acoustic-phonetic dynamics of two consecutive codewords in a language. A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A VQ-based bi-gram model is used to characterize the acoustic-phonetic dynamics of two consecutive codewords in a language. Accordingly the language-specific acoustic-phonetic property of sequence of phones was integrated in the identification process. A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A VQ-based bi-gram model is used to characterize the acoustic-phonetic dynamics of two consecutive codewords in a language. Accordingly the language-specific acoustic-phonetic property of sequence of phones was integrated in the identification process. A Gaussian mixture model (GMM) is used to model codeword occurrence vectors orthonormally transformed using latent semantic analysis (LSA) for each language-dependent segment. A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A VQ-based bi-gram model is used to characterize the acoustic-phonetic dynamics of two consecutive codewords in a language. Accordingly the language-specific acoustic-phonetic property of sequence of phones was integrated in the identification process. A Gaussian mixture model (GMM) is used to model codeword occurrence vectors orthonormally transformed using latent semantic analysis (LSA) for each language-dependent segment. A filtering method is used to smooth the hypothesized language sequence and thus eliminate noise-like components of the detected language sequence generated by the maximum likelihood estimation. A delta Bayesian information criterion (delta-BIC) is firstly applied to segment the input speech utterance into a sequence of language-dependent segments using acoustic features. A VQ-based bi-gram model is used to characterize the acoustic-phonetic dynamics of two consecutive codewords in a language. Accordingly the language-specific acoustic-phonetic property of sequence of phones was integrated in the identification process. A Gaussian mixture model (GMM) is used to model codeword occurrence vectors orthonormally transformed using latent semantic analysis (LSA) for each language-dependent segment. A filtering method is used to smooth the hypothesized language sequence and thus eliminate noise-like components of the detected language sequence generated by the maximum likelihood estimation. Finally, a dynamic programming method is used to determine globally the language boundaries. Experimental results show that for Mandarin, English, and Taiwanese, a recall rate of 0.87 for language boundary segmentation was obtained. Experimental results show that for Mandarin, English, and Taiwanese, a recall rate of 0.87 for language boundary segmentation was obtained. Based on this recall rate, the proposed approach achieved language identification accuracies of 92.1% and 74.9% for single-language and mixed-language speech, respectively."}, {"paper_id": "20239810", "adju_relevance": 0, "title": "Sign Language Writing Acquisition -- Technology for a Writing System", "background_label": "The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. However, Deaf children should no longer be expected to access academic learning using the oral language [1][4][6]. Rather they need to have access to a writing system in/for Sign Language. Writing systems (sequence of characters to represent a language) store and retrieve vital information for literature, science, knowledge creation, information dissemination, communication etc. SignWriting is a writing system deemed adequate to the spatial-visual nature of Sign Languages. However, the existing learning methodologies and computational technologies fail to help the Deaf (they lack usability, and/or are one-to-one translation from the oral language etc.).", "abstract": "The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. However, Deaf children should no longer be expected to access academic learning using the oral language [1][4][6]. The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. However, Deaf children should no longer be expected to access academic learning using the oral language [1][4][6]. Rather they need to have access to a writing system in/for Sign Language. The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. However, Deaf children should no longer be expected to access academic learning using the oral language [1][4][6]. Rather they need to have access to a writing system in/for Sign Language. Writing systems (sequence of characters to represent a language) store and retrieve vital information for literature, science, knowledge creation, information dissemination, communication etc. The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. However, Deaf children should no longer be expected to access academic learning using the oral language [1][4][6]. Rather they need to have access to a writing system in/for Sign Language. Writing systems (sequence of characters to represent a language) store and retrieve vital information for literature, science, knowledge creation, information dissemination, communication etc. SignWriting is a writing system deemed adequate to the spatial-visual nature of Sign Languages. The Deaf use Sign Language for intellectual development, communication and other language dependent tasks, including the learning of the oral language in which they are immersed. However, Deaf children should no longer be expected to access academic learning using the oral language [1][4][6]. Rather they need to have access to a writing system in/for Sign Language. Writing systems (sequence of characters to represent a language) store and retrieve vital information for literature, science, knowledge creation, information dissemination, communication etc. SignWriting is a writing system deemed adequate to the spatial-visual nature of Sign Languages. However, the existing learning methodologies and computational technologies fail to help the Deaf (they lack usability, and/or are one-to-one translation from the oral language etc.)."}, {"paper_id": "586525", "adju_relevance": 0, "title": "Interactive multi-modal robot programming", "abstract": ""}, {"paper_id": "17698747", "adju_relevance": 0, "title": "Implementing aspect-oriented programming constructs as modular language extensions", "background_label": "AbstractExtensible programming languages and their compilers are experimental systems that use highly modular specifications of languages and language extensions in order to allow a variety of language features to be easily imported, by the programmer, into his or her programming environment. Forwarding is designed such that no additional attribute definitions need to be written when combining a \"host\" language with language extensions (specified as attribute grammars) thus allowing for the modular composition of language features. This means that programmers can remain unaware of the underlying attribute grammars when building customized languages by importing language extensions.", "method_label": "Our framework for extensible languages is based on higher-order attribute grammars extended with a mechanism called \"forwarding\" that mimics a simple rewriting process. This paper shows how aspects and the aspect weaving process from Aspect-Oriented Programming can be specified as a modular language extension and imported into an extensible host language.", "result_label": "This paper also illustrates how an extensible compiler framework exposes its underlying semantic analyses and how this can provide a convenient arena for researchers to explore new aspect-oriented language features.", "abstract": "AbstractExtensible programming languages and their compilers are experimental systems that use highly modular specifications of languages and language extensions in order to allow a variety of language features to be easily imported, by the programmer, into his or her programming environment. Our framework for extensible languages is based on higher-order attribute grammars extended with a mechanism called \"forwarding\" that mimics a simple rewriting process. AbstractExtensible programming languages and their compilers are experimental systems that use highly modular specifications of languages and language extensions in order to allow a variety of language features to be easily imported, by the programmer, into his or her programming environment. Forwarding is designed such that no additional attribute definitions need to be written when combining a \"host\" language with language extensions (specified as attribute grammars) thus allowing for the modular composition of language features. AbstractExtensible programming languages and their compilers are experimental systems that use highly modular specifications of languages and language extensions in order to allow a variety of language features to be easily imported, by the programmer, into his or her programming environment. Forwarding is designed such that no additional attribute definitions need to be written when combining a \"host\" language with language extensions (specified as attribute grammars) thus allowing for the modular composition of language features. This means that programmers can remain unaware of the underlying attribute grammars when building customized languages by importing language extensions. Our framework for extensible languages is based on higher-order attribute grammars extended with a mechanism called \"forwarding\" that mimics a simple rewriting process. This paper shows how aspects and the aspect weaving process from Aspect-Oriented Programming can be specified as a modular language extension and imported into an extensible host language. This paper also illustrates how an extensible compiler framework exposes its underlying semantic analyses and how this can provide a convenient arena for researchers to explore new aspect-oriented language features."}, {"paper_id": "17493942", "adju_relevance": 0, "title": "Fly out-smarts man", "background_label": "Precopulatory courtship is a high-cost, non-well understood animal world mystery. Drosophila's (=D. 's) precopulatory courtship not only shows marked structural similarities with mammalian courtship, but also with human spoken language.", "abstract": "Precopulatory courtship is a high-cost, non-well understood animal world mystery. Precopulatory courtship is a high-cost, non-well understood animal world mystery. Drosophila's (=D. Precopulatory courtship is a high-cost, non-well understood animal world mystery. Drosophila's (=D. 's) precopulatory courtship not only shows marked structural similarities with mammalian courtship, but also with human spoken language."}, {"paper_id": "67280741", "adju_relevance": 0, "title": "A Basic Language Technology Toolkit for Quechua", "background_label": "In this thesis, we describe the development of several natural language processing tools and resources for the Andean language Cuzco Quechua as part of the SQUOIA project at the University of Zurich.", "abstract": "In this thesis, we describe the development of several natural language processing tools and resources for the Andean language Cuzco Quechua as part of the SQUOIA project at the University of Zurich."}, {"paper_id": "60925535", "adju_relevance": 0, "title": "SISAL: streams and iteration in a single-assignment language. Language reference manual, Version 1. 1", "background_label": "Many multi-processor systems are currently under study by various groups around the world. The understanding and exploitation of parallelism in these systems is a primary goal of these studies.", "abstract": "Many multi-processor systems are currently under study by various groups around the world. Many multi-processor systems are currently under study by various groups around the world. The understanding and exploitation of parallelism in these systems is a primary goal of these studies."}, {"paper_id": "1467233", "adju_relevance": 0, "title": "Learning Lexical Entries for Robotic Commands using Crowdsourcing", "background_label": "Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions.", "method_label": "To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language.", "abstract": "Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language."}, {"paper_id": "17390526", "adju_relevance": 0, "title": "Extending XML-RL with update", "background_label": "With the extensive use of XML in applications over the Web, how to update XML data is becoming an important issue because the role of XML has expanded beyond traditional applications, in which XML is used as a mean for data representation and exchange on the Web.", "abstract": "With the extensive use of XML in applications over the Web, how to update XML data is becoming an important issue because the role of XML has expanded beyond traditional applications, in which XML is used as a mean for data representation and exchange on the Web."}, {"paper_id": "14855025", "adju_relevance": 0, "title": "High-performance cross-language interoperability in a multi-language runtime", "background_label": "Programmers combine different programming languages because it allows them to use the most suitable language for a given problem, to gradually migrate existing projects from one language to another, or to reuse existing source code. However, existing cross-language mechanisms suffer from complex interfaces, insufficient flexibility, or poor performance.", "abstract": "Programmers combine different programming languages because it allows them to use the most suitable language for a given problem, to gradually migrate existing projects from one language to another, or to reuse existing source code. Programmers combine different programming languages because it allows them to use the most suitable language for a given problem, to gradually migrate existing projects from one language to another, or to reuse existing source code. However, existing cross-language mechanisms suffer from complex interfaces, insufficient flexibility, or poor performance."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "201668540", "adju_relevance": 0, "title": "Interactive Language Learning by Question Answering", "background_label": "Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching.", "abstract": "Humans observe and interact with the world to acquire knowledge. Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching."}, {"paper_id": "13148301", "adju_relevance": 0, "title": "Montages/Gem-Mex: a meta visual programming generator", "background_label": "Last decade witnessed a disappointing lack in technology transfer from formal semantics to language design. Research in formal semantics has developed increasingly complex concepts and notation, at the expense of calculational clarity and applicability in the development of languages. Montages is a visual domain-specific formalism for specifying all the aspects of a programming language.", "method_label": "It is intelligible to a broad range of people involved in the language life cycle, from design to programming language descriptions are fed to a rapid prototyping tool, called Gem-Mex, which generates a visual programming environment for the given language. Gem-Mex consists of a graphical front-end which allows a comfortable editing of the visual components of the specification. Starting from these visual descriptions the tool is able to generate in an automatic way high-quality documents, type-checkers, interpreters and a visual symbolic debugger.", "result_label": "All these products form a powerful suite where the programmer can write, execute, animate and debug programs written in the specified language.", "abstract": "Last decade witnessed a disappointing lack in technology transfer from formal semantics to language design. Last decade witnessed a disappointing lack in technology transfer from formal semantics to language design. Research in formal semantics has developed increasingly complex concepts and notation, at the expense of calculational clarity and applicability in the development of languages. Last decade witnessed a disappointing lack in technology transfer from formal semantics to language design. Research in formal semantics has developed increasingly complex concepts and notation, at the expense of calculational clarity and applicability in the development of languages. Montages is a visual domain-specific formalism for specifying all the aspects of a programming language. It is intelligible to a broad range of people involved in the language life cycle, from design to programming language descriptions are fed to a rapid prototyping tool, called Gem-Mex, which generates a visual programming environment for the given language. It is intelligible to a broad range of people involved in the language life cycle, from design to programming language descriptions are fed to a rapid prototyping tool, called Gem-Mex, which generates a visual programming environment for the given language. Gem-Mex consists of a graphical front-end which allows a comfortable editing of the visual components of the specification. It is intelligible to a broad range of people involved in the language life cycle, from design to programming language descriptions are fed to a rapid prototyping tool, called Gem-Mex, which generates a visual programming environment for the given language. Gem-Mex consists of a graphical front-end which allows a comfortable editing of the visual components of the specification. Starting from these visual descriptions the tool is able to generate in an automatic way high-quality documents, type-checkers, interpreters and a visual symbolic debugger. All these products form a powerful suite where the programmer can write, execute, animate and debug programs written in the specified language."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "1629069", "adju_relevance": 0, "title": "Learning SQL Programming with Interactive Tools: From Integration to Personalization", "background_label": "Rich, interactive eLearning tools receive a lot of attention nowadays from both practitioners and researchers. However, broader dissemination of these tools is hindered by the technical difficulties of their integration into existing platforms.", "abstract": "Rich, interactive eLearning tools receive a lot of attention nowadays from both practitioners and researchers. Rich, interactive eLearning tools receive a lot of attention nowadays from both practitioners and researchers. However, broader dissemination of these tools is hindered by the technical difficulties of their integration into existing platforms."}, {"paper_id": "7699915", "adju_relevance": 0, "title": "Multilingual CALL Framework for Automatic Language Exercise Generation from Free Text", "background_label": "This paper describes a web-based application to design and answer exercises for language learning. It is available in  Basque, Spanish, English, and French.", "method_label": "Based on open-source Natural Language Processing (NLP) technology such as  word embedding models and word sense disambiguation, the system enables users to create automatically and in real time three types of exercises, namely, Fill-in-the-Gaps, Multiple Choice, and Shuffled Sentences questionnaires. These are generated from texts of the users\u2019 own choice, so they can train their language skills with  content of their particular interest.", "abstract": "This paper describes a web-based application to design and answer exercises for language learning. This paper describes a web-based application to design and answer exercises for language learning. It is available in  Basque, Spanish, English, and French. Based on open-source Natural Language Processing (NLP) technology such as  word embedding models and word sense disambiguation, the system enables users to create automatically and in real time three types of exercises, namely, Fill-in-the-Gaps, Multiple Choice, and Shuffled Sentences questionnaires. Based on open-source Natural Language Processing (NLP) technology such as  word embedding models and word sense disambiguation, the system enables users to create automatically and in real time three types of exercises, namely, Fill-in-the-Gaps, Multiple Choice, and Shuffled Sentences questionnaires. These are generated from texts of the users\u2019 own choice, so they can train their language skills with  content of their particular interest."}, {"paper_id": "7375974", "adju_relevance": 0, "title": "Learning concepts through conversations in spoken dialogue systems", "background_label": "Spoken dialogue systems must be able to recover gracefully from unexpected user inputs. In many cases, these unexpected utterances may be within the scope of the system, but include previously unseen phrases that the system cannot interpret.", "abstract": "Spoken dialogue systems must be able to recover gracefully from unexpected user inputs. Spoken dialogue systems must be able to recover gracefully from unexpected user inputs. In many cases, these unexpected utterances may be within the scope of the system, but include previously unseen phrases that the system cannot interpret."}, {"paper_id": "202565569", "adju_relevance": 0, "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset", "background_label": "A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data.", "method_label": "To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken\"Wizard of Oz\"(WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is\"self-dialog\"in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally.", "result_label": "Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.", "abstract": "A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken\"Wizard of Oz\"(WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is\"self-dialog\"in which crowdsourced workers write the entire dialog themselves. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken\"Wizard of Oz\"(WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is\"self-dialog\"in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken\"Wizard of Oz\"(WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is\"self-dialog\"in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken\"Wizard of Oz\"(WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is\"self-dialog\"in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken\"Wizard of Oz\"(WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is\"self-dialog\"in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design."}, {"paper_id": "198147497", "adju_relevance": 0, "title": "Language Support for Adaptation: Intent-Driven Programming in FAST", "background_label": "Historically, programming language semantics has focused on assigning a precise mathematical meaning to programs. That meaning is a function from the program's input domain to its output domain determined solely by its syntactic structure. Such a semantics, fosters the development of portable applications which are oblivious to the performance characteristics and limitations (such as a maximum memory footprint) of particular hardware and software platforms.", "method_label": "This paper introduces the idea of intent-driven programming where the meaning of a program additionally depends on an accompanying intent specification expressing how the ordinary program meaning is dynamically modified during execution to satisfy additional properties expressed by the intent. These include both intensional properties---e.g., resource usage---and extensional properties---e.g., accuracy of the computed answer. To demonstrate the intent-driven programming model's value, this paper presents a general-purpose intent-driven programming language---called FAST---implemented as an extension of Swift. FAST consists of an intent compiler, a profiler, a general controller interface and a runtime module which supports interoperation with legacy C/C++ codes.", "result_label": "Compared to existing frameworks for adaptive computing, \\FAST{} supports dynamic adaptation to changes both in the operating environment and in the intent itself, and enables the mixing of procedural control and control based on feedback and optimization.", "abstract": "Historically, programming language semantics has focused on assigning a precise mathematical meaning to programs. Historically, programming language semantics has focused on assigning a precise mathematical meaning to programs. That meaning is a function from the program's input domain to its output domain determined solely by its syntactic structure. Historically, programming language semantics has focused on assigning a precise mathematical meaning to programs. That meaning is a function from the program's input domain to its output domain determined solely by its syntactic structure. Such a semantics, fosters the development of portable applications which are oblivious to the performance characteristics and limitations (such as a maximum memory footprint) of particular hardware and software platforms. This paper introduces the idea of intent-driven programming where the meaning of a program additionally depends on an accompanying intent specification expressing how the ordinary program meaning is dynamically modified during execution to satisfy additional properties expressed by the intent. This paper introduces the idea of intent-driven programming where the meaning of a program additionally depends on an accompanying intent specification expressing how the ordinary program meaning is dynamically modified during execution to satisfy additional properties expressed by the intent. These include both intensional properties---e.g., resource usage---and extensional properties---e.g., accuracy of the computed answer. This paper introduces the idea of intent-driven programming where the meaning of a program additionally depends on an accompanying intent specification expressing how the ordinary program meaning is dynamically modified during execution to satisfy additional properties expressed by the intent. These include both intensional properties---e.g., resource usage---and extensional properties---e.g., accuracy of the computed answer. To demonstrate the intent-driven programming model's value, this paper presents a general-purpose intent-driven programming language---called FAST---implemented as an extension of Swift. This paper introduces the idea of intent-driven programming where the meaning of a program additionally depends on an accompanying intent specification expressing how the ordinary program meaning is dynamically modified during execution to satisfy additional properties expressed by the intent. These include both intensional properties---e.g., resource usage---and extensional properties---e.g., accuracy of the computed answer. To demonstrate the intent-driven programming model's value, this paper presents a general-purpose intent-driven programming language---called FAST---implemented as an extension of Swift. FAST consists of an intent compiler, a profiler, a general controller interface and a runtime module which supports interoperation with legacy C/C++ codes. Compared to existing frameworks for adaptive computing, \\FAST{} supports dynamic adaptation to changes both in the operating environment and in the intent itself, and enables the mixing of procedural control and control based on feedback and optimization."}, {"paper_id": "16098120", "adju_relevance": 0, "title": "Language understanding as a step towards human level intelligence - automatizing the construction of the initial dictionary from example sentences", "background_label": "For a system to understand natural language, it needs to be able to take natural language text and answer questions given in natural language with respect to that text; it also needs to be able to follow instructions given in natural language. To achieve this, a system must be able to process natural language and be able to capture the knowledge within that text. Thus it needs to be able to translate natural language text into a formal language.", "method_label": "We discuss our approach to do this, where the translation is achieved by composing the meaning of words in a sentence. Our initial approach uses an inverse lambda method that we developed (and other methods) to learn meaning of words from meaning of sentences and an initial lexicon. We then present an improved method where the initial lexicon is also learned by analyzing the training sentence and meaning pairs.", "result_label": "We evaluate our methods and compare them with other existing methods on a corpora of database querying and robot command and control.", "abstract": "For a system to understand natural language, it needs to be able to take natural language text and answer questions given in natural language with respect to that text; it also needs to be able to follow instructions given in natural language. For a system to understand natural language, it needs to be able to take natural language text and answer questions given in natural language with respect to that text; it also needs to be able to follow instructions given in natural language. To achieve this, a system must be able to process natural language and be able to capture the knowledge within that text. For a system to understand natural language, it needs to be able to take natural language text and answer questions given in natural language with respect to that text; it also needs to be able to follow instructions given in natural language. To achieve this, a system must be able to process natural language and be able to capture the knowledge within that text. Thus it needs to be able to translate natural language text into a formal language. We discuss our approach to do this, where the translation is achieved by composing the meaning of words in a sentence. We discuss our approach to do this, where the translation is achieved by composing the meaning of words in a sentence. Our initial approach uses an inverse lambda method that we developed (and other methods) to learn meaning of words from meaning of sentences and an initial lexicon. We discuss our approach to do this, where the translation is achieved by composing the meaning of words in a sentence. Our initial approach uses an inverse lambda method that we developed (and other methods) to learn meaning of words from meaning of sentences and an initial lexicon. We then present an improved method where the initial lexicon is also learned by analyzing the training sentence and meaning pairs. We evaluate our methods and compare them with other existing methods on a corpora of database querying and robot command and control."}, {"paper_id": "2539078", "adju_relevance": 0, "title": "Continuous multilinguality with language vectors", "background_label": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language.", "method_label": "We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training.", "result_label": "In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.", "abstract": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages."}, {"paper_id": "11277176", "adju_relevance": 0, "title": "Saul: Towards Declarative Learning Based Programming.", "background_label": "We present Saul, a new probabilistic programming language designed to address some of the shortcomings of programming languages that aim at advancing and simplifying the development of AI systems.", "method_label": "Such languages need to interact with messy, naturally occurring data, to allow a programmer to specify what needs to be done at an appropriate level of abstraction rather than at the data level, to be developed on a solid theory that supports moving to and reasoning at this level of abstraction and, finally, to support flexible integration of these learning and inference models within an application program. Saul is an object-functional programming language written in Scala that facilitates these by (1) allowing a programmer to learn, name and manipulate named abstractions over relational data; (2) supporting seamless incorporation of trainable (probabilistic or discriminative) components into the program, and (3) providing a level of inference over trainable models to support composition and make decisions that respect domain and application constraints. Saul is developed over a declaratively defined relational data model, can use piecewise learned factor graphs with declaratively specified learning and inference objectives, and it supports inference over probabilistic models augmented with declarative knowledge-based constraints. We describe the key constructs of Saul and exemplify its use in developing applications that require relational feature engineering and structured output prediction.", "abstract": "We present Saul, a new probabilistic programming language designed to address some of the shortcomings of programming languages that aim at advancing and simplifying the development of AI systems. Such languages need to interact with messy, naturally occurring data, to allow a programmer to specify what needs to be done at an appropriate level of abstraction rather than at the data level, to be developed on a solid theory that supports moving to and reasoning at this level of abstraction and, finally, to support flexible integration of these learning and inference models within an application program. Such languages need to interact with messy, naturally occurring data, to allow a programmer to specify what needs to be done at an appropriate level of abstraction rather than at the data level, to be developed on a solid theory that supports moving to and reasoning at this level of abstraction and, finally, to support flexible integration of these learning and inference models within an application program. Saul is an object-functional programming language written in Scala that facilitates these by (1) allowing a programmer to learn, name and manipulate named abstractions over relational data; (2) supporting seamless incorporation of trainable (probabilistic or discriminative) components into the program, and (3) providing a level of inference over trainable models to support composition and make decisions that respect domain and application constraints. Such languages need to interact with messy, naturally occurring data, to allow a programmer to specify what needs to be done at an appropriate level of abstraction rather than at the data level, to be developed on a solid theory that supports moving to and reasoning at this level of abstraction and, finally, to support flexible integration of these learning and inference models within an application program. Saul is an object-functional programming language written in Scala that facilitates these by (1) allowing a programmer to learn, name and manipulate named abstractions over relational data; (2) supporting seamless incorporation of trainable (probabilistic or discriminative) components into the program, and (3) providing a level of inference over trainable models to support composition and make decisions that respect domain and application constraints. Saul is developed over a declaratively defined relational data model, can use piecewise learned factor graphs with declaratively specified learning and inference objectives, and it supports inference over probabilistic models augmented with declarative knowledge-based constraints. Such languages need to interact with messy, naturally occurring data, to allow a programmer to specify what needs to be done at an appropriate level of abstraction rather than at the data level, to be developed on a solid theory that supports moving to and reasoning at this level of abstraction and, finally, to support flexible integration of these learning and inference models within an application program. Saul is an object-functional programming language written in Scala that facilitates these by (1) allowing a programmer to learn, name and manipulate named abstractions over relational data; (2) supporting seamless incorporation of trainable (probabilistic or discriminative) components into the program, and (3) providing a level of inference over trainable models to support composition and make decisions that respect domain and application constraints. Saul is developed over a declaratively defined relational data model, can use piecewise learned factor graphs with declaratively specified learning and inference objectives, and it supports inference over probabilistic models augmented with declarative knowledge-based constraints. We describe the key constructs of Saul and exemplify its use in developing applications that require relational feature engineering and structured output prediction."}, {"paper_id": "8354088", "adju_relevance": 0, "title": "A Survey of Language Identification Techniques and Applications", "background_label": "Language Identification is the process of determining in which natural language the contents of the text is written. Language identification is always been an important research area which has been carried out from early 1970\u2019s. Still it is a fascinating field to be studied due to increased demand of natural language processing applications. In many applications, it works as a primary step of some larger process.", "method_label": "In this paper, a number of applications are outlined where language identification is working successfully. Language Identification can be done using two types of techniques: computational techniques and non-computational techniques. Computational techniques are based on statistical methods and requires large set of training data for each of the language while non-computational techniques require that researcher must have extensive knowledge about the language to-be-identified. In this paper, a brief review of the few papers is presented which outlines the various statistical and non-statistical techniques that have been applied by the different researchers for language identification.", "result_label": "Besides it, different researchers performed language identification for different type of documents such as monolingual, multilingual, long and short and for a particular set of languages.", "abstract": "Language Identification is the process of determining in which natural language the contents of the text is written. Language Identification is the process of determining in which natural language the contents of the text is written. Language identification is always been an important research area which has been carried out from early 1970\u2019s. Language Identification is the process of determining in which natural language the contents of the text is written. Language identification is always been an important research area which has been carried out from early 1970\u2019s. Still it is a fascinating field to be studied due to increased demand of natural language processing applications. Language Identification is the process of determining in which natural language the contents of the text is written. Language identification is always been an important research area which has been carried out from early 1970\u2019s. Still it is a fascinating field to be studied due to increased demand of natural language processing applications. In many applications, it works as a primary step of some larger process. In this paper, a number of applications are outlined where language identification is working successfully. In this paper, a number of applications are outlined where language identification is working successfully. Language Identification can be done using two types of techniques: computational techniques and non-computational techniques. In this paper, a number of applications are outlined where language identification is working successfully. Language Identification can be done using two types of techniques: computational techniques and non-computational techniques. Computational techniques are based on statistical methods and requires large set of training data for each of the language while non-computational techniques require that researcher must have extensive knowledge about the language to-be-identified. In this paper, a number of applications are outlined where language identification is working successfully. Language Identification can be done using two types of techniques: computational techniques and non-computational techniques. Computational techniques are based on statistical methods and requires large set of training data for each of the language while non-computational techniques require that researcher must have extensive knowledge about the language to-be-identified. In this paper, a brief review of the few papers is presented which outlines the various statistical and non-statistical techniques that have been applied by the different researchers for language identification. Besides it, different researchers performed language identification for different type of documents such as monolingual, multilingual, long and short and for a particular set of languages."}, {"paper_id": "61547537", "adju_relevance": 0, "title": "C# Programming Language", "background_label": "Based on my own experience, I can safely say that every .NET developer who reads this will have at least one aha moment and will be a better developer for it. From the Foreword by Don Box The popular C# programming language combines the high productivity of rapid application development languages with the raw power of C and C++. Written by Anders Hejlsberg, the languages architect, and his colleagues, Mads Torgersen, Scott Wiltamuth, and Peter Golde, this volume has been completely updated for C# 4.0. The book provides the complete specification of the language, along with descriptions, reference materials, code samples, and annotations from twelve prominent C# gurus. The many annotations bring a depth and breadth of understanding rarely found in any programming book.", "method_label": "Updated to cover the new features of C# 4.0, including dynamic binding, named and optional parameters, and covariant and contravariant generic types, this release takes the language to the next level by adding the ability to cleanly write programs that dont rely on static type definitions. This allows dynamic programming languages such as Python, Ruby, and JavaScript to feel native to C#. As the main text of the book introduces the concepts of the C# language, cogent annotations explain why they are important, how they are used, how they relate to other languages, and even how they evolved.", "result_label": "The C# Programming Language, Fourth Edition, continues to be the authoritative and annotated technical reference for C# 4.0.", "abstract": "Based on my own experience, I can safely say that every .NET developer who reads this will have at least one aha moment and will be a better developer for it. Based on my own experience, I can safely say that every .NET developer who reads this will have at least one aha moment and will be a better developer for it. From the Foreword by Don Box The popular C# programming language combines the high productivity of rapid application development languages with the raw power of C and C++. Updated to cover the new features of C# 4.0, including dynamic binding, named and optional parameters, and covariant and contravariant generic types, this release takes the language to the next level by adding the ability to cleanly write programs that dont rely on static type definitions. Updated to cover the new features of C# 4.0, including dynamic binding, named and optional parameters, and covariant and contravariant generic types, this release takes the language to the next level by adding the ability to cleanly write programs that dont rely on static type definitions. This allows dynamic programming languages such as Python, Ruby, and JavaScript to feel native to C#. The C# Programming Language, Fourth Edition, continues to be the authoritative and annotated technical reference for C# 4.0. Based on my own experience, I can safely say that every .NET developer who reads this will have at least one aha moment and will be a better developer for it. From the Foreword by Don Box The popular C# programming language combines the high productivity of rapid application development languages with the raw power of C and C++. Written by Anders Hejlsberg, the languages architect, and his colleagues, Mads Torgersen, Scott Wiltamuth, and Peter Golde, this volume has been completely updated for C# 4.0. Based on my own experience, I can safely say that every .NET developer who reads this will have at least one aha moment and will be a better developer for it. From the Foreword by Don Box The popular C# programming language combines the high productivity of rapid application development languages with the raw power of C and C++. Written by Anders Hejlsberg, the languages architect, and his colleagues, Mads Torgersen, Scott Wiltamuth, and Peter Golde, this volume has been completely updated for C# 4.0. The book provides the complete specification of the language, along with descriptions, reference materials, code samples, and annotations from twelve prominent C# gurus. Based on my own experience, I can safely say that every .NET developer who reads this will have at least one aha moment and will be a better developer for it. From the Foreword by Don Box The popular C# programming language combines the high productivity of rapid application development languages with the raw power of C and C++. Written by Anders Hejlsberg, the languages architect, and his colleagues, Mads Torgersen, Scott Wiltamuth, and Peter Golde, this volume has been completely updated for C# 4.0. The book provides the complete specification of the language, along with descriptions, reference materials, code samples, and annotations from twelve prominent C# gurus. The many annotations bring a depth and breadth of understanding rarely found in any programming book. Updated to cover the new features of C# 4.0, including dynamic binding, named and optional parameters, and covariant and contravariant generic types, this release takes the language to the next level by adding the ability to cleanly write programs that dont rely on static type definitions. This allows dynamic programming languages such as Python, Ruby, and JavaScript to feel native to C#. As the main text of the book introduces the concepts of the C# language, cogent annotations explain why they are important, how they are used, how they relate to other languages, and even how they evolved."}, {"paper_id": "12012819", "adju_relevance": 0, "title": "Natural Language Generation in Interactive Systems", "method_label": "Detailing the techniques and challenges of NLG for interactive applications, it focuses on the research into systems that model collaborativity and uncertainty, are capable of being scaled incrementally, and can engage with the user effectively. A range of real-world case studies is also included. The book and the accompanying website feature a comprehensive bibliography, and refer the reader to corpora, data, software and other resources for pursuing research on natural language generation and interactive systems, including dialog systems, multimodal interfaces and assistive technologies.", "result_label": "It is an ideal resource for students and researchers in computational linguistics, natural language processing and related fields.", "abstract": " Detailing the techniques and challenges of NLG for interactive applications, it focuses on the research into systems that model collaborativity and uncertainty, are capable of being scaled incrementally, and can engage with the user effectively. Detailing the techniques and challenges of NLG for interactive applications, it focuses on the research into systems that model collaborativity and uncertainty, are capable of being scaled incrementally, and can engage with the user effectively. A range of real-world case studies is also included. Detailing the techniques and challenges of NLG for interactive applications, it focuses on the research into systems that model collaborativity and uncertainty, are capable of being scaled incrementally, and can engage with the user effectively. A range of real-world case studies is also included. The book and the accompanying website feature a comprehensive bibliography, and refer the reader to corpora, data, software and other resources for pursuing research on natural language generation and interactive systems, including dialog systems, multimodal interfaces and assistive technologies. It is an ideal resource for students and researchers in computational linguistics, natural language processing and related fields."}, {"paper_id": "1789360", "adju_relevance": 0, "title": "Design and Implementation of Probabilistic Programming Language Anglican", "background_label": "Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages.", "method_label": "We introduce the programming language Anglican, outline our design choices, and discuss in depth the implementation of the Anglican language and runtime, including macro-based compilation, extended CPS-based evaluation model, and functional representations for probabilistic paradigms, such as a distribution, a random process, and an inference algorithm. We show that a probabilistic functional language can be implemented efficiently and integrated tightly with a conventional functional language with only moderate computational overhead.", "result_label": "We also demonstrate how advanced probabilistic modeling concepts are mapped naturally to the functional foundation.", "abstract": "Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages. We introduce the programming language Anglican, outline our design choices, and discuss in depth the implementation of the Anglican language and runtime, including macro-based compilation, extended CPS-based evaluation model, and functional representations for probabilistic paradigms, such as a distribution, a random process, and an inference algorithm. We introduce the programming language Anglican, outline our design choices, and discuss in depth the implementation of the Anglican language and runtime, including macro-based compilation, extended CPS-based evaluation model, and functional representations for probabilistic paradigms, such as a distribution, a random process, and an inference algorithm. We show that a probabilistic functional language can be implemented efficiently and integrated tightly with a conventional functional language with only moderate computational overhead. We also demonstrate how advanced probabilistic modeling concepts are mapped naturally to the functional foundation."}, {"paper_id": "18876193", "adju_relevance": 0, "title": "Templet: a Markup Language for Concurrent Programming", "background_label": "Special programming languages or extensions to sequential languages are usually designed to express the semantics of concurrent execution. Using libraries in C++, Java, C#, and other languages is more practical way of concurrent programming.", "method_label": "However, this method leads to an increase in workload of a manual coding. Besides, stock compilers can not detect semantic errors related to the programming model in such libraries. The new markup language and a special technique of automatic programming based on the marked code can solve these problems. The article provides a detailed specification of the markup language without discussing its implementation details.", "result_label": "The language is used for programming of current and prospective multi-core and many-core systems.", "abstract": " Special programming languages or extensions to sequential languages are usually designed to express the semantics of concurrent execution. Special programming languages or extensions to sequential languages are usually designed to express the semantics of concurrent execution. Using libraries in C++, Java, C#, and other languages is more practical way of concurrent programming. However, this method leads to an increase in workload of a manual coding. However, this method leads to an increase in workload of a manual coding. Besides, stock compilers can not detect semantic errors related to the programming model in such libraries. However, this method leads to an increase in workload of a manual coding. Besides, stock compilers can not detect semantic errors related to the programming model in such libraries. The new markup language and a special technique of automatic programming based on the marked code can solve these problems. However, this method leads to an increase in workload of a manual coding. Besides, stock compilers can not detect semantic errors related to the programming model in such libraries. The new markup language and a special technique of automatic programming based on the marked code can solve these problems. The article provides a detailed specification of the markup language without discussing its implementation details. The language is used for programming of current and prospective multi-core and many-core systems."}, {"paper_id": "20605616", "adju_relevance": 0, "title": "Visual Reasoning with Natural Language", "background_label": "Natural language provides a widely accessible and expressive interface for robotic agents. To understand language in complex environments, agents must reason about the full range of language inputs and their correspondence to the world. Such reasoning over language and vision is an open problem that is receiving increasing attention. While existing data sets focus on visual diversity, they do not display the full range of natural language expressions, such as counting, set reasoning, and comparisons.", "method_label": "We propose a simple task for natural language visual reasoning, where images are paired with descriptive statements. The task is to predict if a statement is true for the given scene.", "result_label": "This abstract describes our existing synthetic images corpus and our current work on collecting real vision data.", "abstract": "Natural language provides a widely accessible and expressive interface for robotic agents. Natural language provides a widely accessible and expressive interface for robotic agents. To understand language in complex environments, agents must reason about the full range of language inputs and their correspondence to the world. Natural language provides a widely accessible and expressive interface for robotic agents. To understand language in complex environments, agents must reason about the full range of language inputs and their correspondence to the world. Such reasoning over language and vision is an open problem that is receiving increasing attention. Natural language provides a widely accessible and expressive interface for robotic agents. To understand language in complex environments, agents must reason about the full range of language inputs and their correspondence to the world. Such reasoning over language and vision is an open problem that is receiving increasing attention. While existing data sets focus on visual diversity, they do not display the full range of natural language expressions, such as counting, set reasoning, and comparisons. We propose a simple task for natural language visual reasoning, where images are paired with descriptive statements. We propose a simple task for natural language visual reasoning, where images are paired with descriptive statements. The task is to predict if a statement is true for the given scene. This abstract describes our existing synthetic images corpus and our current work on collecting real vision data."}, {"paper_id": "54140293", "adju_relevance": 0, "title": "The Scheme Programming Language", "background_label": "This thoroughly updated edition of The Scheme Programming Language provides an introduction to Scheme and a definitive reference for standard Scheme, presented in a clear and concise manner. Written for professionals and students with some prior programming experience, it begins by leading the programmer gently through the basics of Scheme and continues with an introduction to some of the more advanced features of the language. Many exercises are presented to help reinforce the lessons learned, and answers to the exercises are given in a new appendix.", "method_label": "Most of the remaining chapters are dedicated to the reference material, which describes in detail the standard features of Scheme included in the Revised Report on Scheme and the ANSI/IEEE standard for Scheme.Numerous examples are presented throughout the introductory and reference portions of the text, and a unique set of extended example programs and applications, with additional exercises, are presented in the final chapter. Reinforcing the book's utility as a reference text are appendixes that present the formal syntax of Scheme, a summary of standard forms and procedures, and a bibliography of Scheme resources. The Scheme Programming Language stands alone as an introduction to and essential reference for Scheme programmers.", "result_label": "It is also useful as a supplementary text for any course that uses Scheme.The Scheme Programming Language is illustrated by artist Jean-Pierre Hebert, who writes Scheme programs to extend his ability to create sophisticated works of digital art.", "abstract": "This thoroughly updated edition of The Scheme Programming Language provides an introduction to Scheme and a definitive reference for standard Scheme, presented in a clear and concise manner. This thoroughly updated edition of The Scheme Programming Language provides an introduction to Scheme and a definitive reference for standard Scheme, presented in a clear and concise manner. Written for professionals and students with some prior programming experience, it begins by leading the programmer gently through the basics of Scheme and continues with an introduction to some of the more advanced features of the language. This thoroughly updated edition of The Scheme Programming Language provides an introduction to Scheme and a definitive reference for standard Scheme, presented in a clear and concise manner. Written for professionals and students with some prior programming experience, it begins by leading the programmer gently through the basics of Scheme and continues with an introduction to some of the more advanced features of the language. Many exercises are presented to help reinforce the lessons learned, and answers to the exercises are given in a new appendix. Most of the remaining chapters are dedicated to the reference material, which describes in detail the standard features of Scheme included in the Revised Report on Scheme and the ANSI/IEEE standard for Scheme.Numerous examples are presented throughout the introductory and reference portions of the text, and a unique set of extended example programs and applications, with additional exercises, are presented in the final chapter. Most of the remaining chapters are dedicated to the reference material, which describes in detail the standard features of Scheme included in the Revised Report on Scheme and the ANSI/IEEE standard for Scheme.Numerous examples are presented throughout the introductory and reference portions of the text, and a unique set of extended example programs and applications, with additional exercises, are presented in the final chapter. Reinforcing the book's utility as a reference text are appendixes that present the formal syntax of Scheme, a summary of standard forms and procedures, and a bibliography of Scheme resources. Most of the remaining chapters are dedicated to the reference material, which describes in detail the standard features of Scheme included in the Revised Report on Scheme and the ANSI/IEEE standard for Scheme.Numerous examples are presented throughout the introductory and reference portions of the text, and a unique set of extended example programs and applications, with additional exercises, are presented in the final chapter. Reinforcing the book's utility as a reference text are appendixes that present the formal syntax of Scheme, a summary of standard forms and procedures, and a bibliography of Scheme resources. The Scheme Programming Language stands alone as an introduction to and essential reference for Scheme programmers. It is also useful as a supplementary text for any course that uses Scheme.The Scheme Programming Language is illustrated by artist Jean-Pierre Hebert, who writes Scheme programs to extend his ability to create sophisticated works of digital art."}, {"paper_id": "12693371", "adju_relevance": 0, "title": "Experimental Multi-threading Support for the Julia Programming Language", "background_label": "Julia is a young programming language that is designed for technical computing. Although Julia is dynamically typed it is very fast and usually yields C speed by utilizing a just-in-time compiler. Still, Julia has a simple syntax that is similar to Matlab, which is widely known as an easy-to-use programming environment. While Julia is very versatile and provides asynchronous programming facilities in the form of tasks (coroutines) as well as distributed multi-process parallelism, one missing feature is shared memory multi-threading.", "abstract": "Julia is a young programming language that is designed for technical computing. Julia is a young programming language that is designed for technical computing. Although Julia is dynamically typed it is very fast and usually yields C speed by utilizing a just-in-time compiler. Julia is a young programming language that is designed for technical computing. Although Julia is dynamically typed it is very fast and usually yields C speed by utilizing a just-in-time compiler. Still, Julia has a simple syntax that is similar to Matlab, which is widely known as an easy-to-use programming environment. Julia is a young programming language that is designed for technical computing. Although Julia is dynamically typed it is very fast and usually yields C speed by utilizing a just-in-time compiler. Still, Julia has a simple syntax that is similar to Matlab, which is widely known as an easy-to-use programming environment. While Julia is very versatile and provides asynchronous programming facilities in the form of tasks (coroutines) as well as distributed multi-process parallelism, one missing feature is shared memory multi-threading."}, {"paper_id": "406436", "adju_relevance": 0, "title": "PISA: A Platform and Programming Language Independent Interface for Search Algorithms", "background_label": "This paper introduces an interface specification (PISA) that allows to separate the problem-specific part of an optimizer from the problem-independent part.", "abstract": "This paper introduces an interface specification (PISA) that allows to separate the problem-specific part of an optimizer from the problem-independent part."}, {"paper_id": "7403469", "adju_relevance": 0, "title": "Instant pickles: generating object-oriented pickler combinators for fast and extensible serialization", "background_label": "As more applications migrate to the cloud, and as \"big data\" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often under-considered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. Pickler combinators are a popular approach from functional programming; their composability alleviates some of the tedium of writing pickling code by hand, but they don't translate well to object-oriented programming due to qualities like open class hierarchies and subtyping polymorphism. Furthermore, both functional pickler combinators and popular, Java-based serialization frameworks tend to be tied to a specific pickle format, leaving programmers with no choice of how their data is persisted.", "method_label": "In this paper, we present object-oriented pickler combinators and a framework for generating them at compile-time, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. The static generation of OO picklers enables significant performance improvements, outperforming Java and Kryo in most of our benchmarks. In addition to high performance and the need for little to no boilerplate, our framework is extensible: using the type class pattern, users can provide both (1) custom, easily interchangeable pickle formats and (2) custom picklers, to override the default behavior of the pickling framework.", "result_label": "In benchmarks, we compare scala/pickling with other popular industrial frameworks, and present results on time, memory usage, and size when pickling/unpickling a number of data types used in real-world, large-scale distributed applications and frameworks.", "abstract": "As more applications migrate to the cloud, and as \"big data\" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. As more applications migrate to the cloud, and as \"big data\" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often under-considered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. As more applications migrate to the cloud, and as \"big data\" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often under-considered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. Pickler combinators are a popular approach from functional programming; their composability alleviates some of the tedium of writing pickling code by hand, but they don't translate well to object-oriented programming due to qualities like open class hierarchies and subtyping polymorphism. As more applications migrate to the cloud, and as \"big data\" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often under-considered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. Pickler combinators are a popular approach from functional programming; their composability alleviates some of the tedium of writing pickling code by hand, but they don't translate well to object-oriented programming due to qualities like open class hierarchies and subtyping polymorphism. Furthermore, both functional pickler combinators and popular, Java-based serialization frameworks tend to be tied to a specific pickle format, leaving programmers with no choice of how their data is persisted. In this paper, we present object-oriented pickler combinators and a framework for generating them at compile-time, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. In this paper, we present object-oriented pickler combinators and a framework for generating them at compile-time, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. The static generation of OO picklers enables significant performance improvements, outperforming Java and Kryo in most of our benchmarks. In this paper, we present object-oriented pickler combinators and a framework for generating them at compile-time, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. The static generation of OO picklers enables significant performance improvements, outperforming Java and Kryo in most of our benchmarks. In addition to high performance and the need for little to no boilerplate, our framework is extensible: using the type class pattern, users can provide both (1) custom, easily interchangeable pickle formats and (2) custom picklers, to override the default behavior of the pickling framework. In benchmarks, we compare scala/pickling with other popular industrial frameworks, and present results on time, memory usage, and size when pickling/unpickling a number of data types used in real-world, large-scale distributed applications and frameworks."}, {"paper_id": "7923374", "adju_relevance": 0, "title": "A stable programming language", "background_label": "AbstractIt is well-known that stable models (as dI-domains, qualitative domains and coherence spaces) are not fully abstract for the language PCF.", "abstract": "AbstractIt is well-known that stable models (as dI-domains, qualitative domains and coherence spaces) are not fully abstract for the language PCF."}, {"paper_id": "16624116", "adju_relevance": 0, "title": "Symbiotic reflection between an object-oriented and a logic programming language", "background_label": "Meta-programming is the act of using one system or language to reason about another one. Reflection describes systems that have access to and change a causally connected representation of themselves, hence leading to self-extensible systems . Up to now, most of the reflective languages have been implemented in the same paradigm.", "abstract": "Meta-programming is the act of using one system or language to reason about another one. Meta-programming is the act of using one system or language to reason about another one. Reflection describes systems that have access to and change a causally connected representation of themselves, hence leading to self-extensible systems . Meta-programming is the act of using one system or language to reason about another one. Reflection describes systems that have access to and change a causally connected representation of themselves, hence leading to self-extensible systems . Up to now, most of the reflective languages have been implemented in the same paradigm."}, {"paper_id": "7629822", "adju_relevance": 0, "title": "Study of Large Data Resources for Multilingual Training and System Porting", "background_label": "Abstract This study investigates the behavior of a feature extraction neural network model trained on a large amount of single language data (\u201csource language\u201d) on a set of under-resourced target languages.", "method_label": "The coverage of the source language acoustic space was changed in two ways: (1) by changing the amount of training data and (2) by altering the level of detail of acoustic units (by changing the triphone clustering). We observe the effect of these changes on the performance on target language in two scenarios: (1) the source-language NNs were used directly, (2) NNs were first ported to target language.", "result_label": "The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. For the first one, both source language characteristic have about the same effect. For the second scenario, the amount of data in source language is more important than the level of detail. The possibility to include large data into multilingual training set was also investigated. Our experiments point out possible risk of over-weighting the NNs towards the source language with large data. This degrades the performance on part of the target languages, compared to the setting where the amounts of data per language are balanced.", "abstract": "Abstract This study investigates the behavior of a feature extraction neural network model trained on a large amount of single language data (\u201csource language\u201d) on a set of under-resourced target languages. The coverage of the source language acoustic space was changed in two ways: (1) by changing the amount of training data and (2) by altering the level of detail of acoustic units (by changing the triphone clustering). The coverage of the source language acoustic space was changed in two ways: (1) by changing the amount of training data and (2) by altering the level of detail of acoustic units (by changing the triphone clustering). We observe the effect of these changes on the performance on target language in two scenarios: (1) the source-language NNs were used directly, (2) NNs were first ported to target language. The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. For the first one, both source language characteristic have about the same effect. The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. For the first one, both source language characteristic have about the same effect. For the second scenario, the amount of data in source language is more important than the level of detail. The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. For the first one, both source language characteristic have about the same effect. For the second scenario, the amount of data in source language is more important than the level of detail. The possibility to include large data into multilingual training set was also investigated. The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. For the first one, both source language characteristic have about the same effect. For the second scenario, the amount of data in source language is more important than the level of detail. The possibility to include large data into multilingual training set was also investigated. Our experiments point out possible risk of over-weighting the NNs towards the source language with large data. The results show that increasing coverage as well as level of detail on the source language improves the target language system performance in both scenarios. For the first one, both source language characteristic have about the same effect. For the second scenario, the amount of data in source language is more important than the level of detail. The possibility to include large data into multilingual training set was also investigated. Our experiments point out possible risk of over-weighting the NNs towards the source language with large data. This degrades the performance on part of the target languages, compared to the setting where the amounts of data per language are balanced."}, {"paper_id": "13467834", "adju_relevance": 0, "title": "Autonomous Semi-Reactive Agent Design Based on Incremental Inductive Learning in Logic Programming", "abstract": ""}, {"paper_id": "25434922", "adju_relevance": 0, "title": "Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game", "background_label": "Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting.", "abstract": "Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting."}, {"paper_id": "26553546", "adju_relevance": 0, "title": "Deep Reinforcement Learning for Programming Language Correction", "background_label": "Novice programmers often struggle with the formal syntax of programming languages.", "abstract": "Novice programmers often struggle with the formal syntax of programming languages."}, {"paper_id": "6527359", "adju_relevance": 0, "title": "Integrating whole body motion primitives and natural language for humanoid robots", "method_label": "This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. The association is defined by probability that the motion pattern generates the latent variable and probability that the latent variable generates the morpheme word. The natural language model represents order relation among the morpheme words via word classes by using hidden Markov model. The motion language model and the natural language model are equivalent to semantics and syntax respectively. Integration of the motion language model and the natural language model achieves linguistic interpretation of motion patterns by composing semantically and syntactically appropriate sentence. The efficient algorithm for the composition is proposed.", "result_label": "The validity of the motion language model, the natural language model and the integration is demonstrated by testing the implemented algorithm on human motion data.", "abstract": " This framework consists of two models : motion language model and natural language model. This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. The association is defined by probability that the motion pattern generates the latent variable and probability that the latent variable generates the morpheme word. This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. The association is defined by probability that the motion pattern generates the latent variable and probability that the latent variable generates the morpheme word. The natural language model represents order relation among the morpheme words via word classes by using hidden Markov model. This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. The association is defined by probability that the motion pattern generates the latent variable and probability that the latent variable generates the morpheme word. The natural language model represents order relation among the morpheme words via word classes by using hidden Markov model. The motion language model and the natural language model are equivalent to semantics and syntax respectively. This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. The association is defined by probability that the motion pattern generates the latent variable and probability that the latent variable generates the morpheme word. The natural language model represents order relation among the morpheme words via word classes by using hidden Markov model. The motion language model and the natural language model are equivalent to semantics and syntax respectively. Integration of the motion language model and the natural language model achieves linguistic interpretation of motion patterns by composing semantically and syntactically appropriate sentence. This framework consists of two models : motion language model and natural language model. In the motion language model, morpheme words are stochastically associated with symbolized motion patterns via latent variables. The association is defined by probability that the motion pattern generates the latent variable and probability that the latent variable generates the morpheme word. The natural language model represents order relation among the morpheme words via word classes by using hidden Markov model. The motion language model and the natural language model are equivalent to semantics and syntax respectively. Integration of the motion language model and the natural language model achieves linguistic interpretation of motion patterns by composing semantically and syntactically appropriate sentence. The efficient algorithm for the composition is proposed. The validity of the motion language model, the natural language model and the integration is demonstrated by testing the implemented algorithm on human motion data."}, {"paper_id": "10997980", "adju_relevance": 0, "title": "Natural Language Understanding with Distributed Representation", "background_label": "This is a lecture note for the course DS-GA 3001<Natural Language Understanding with Distributed Representation>at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing.", "method_label": "In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced.", "result_label": "On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.", "abstract": "This is a lecture note for the course DS-GA 3001<Natural Language Understanding with Distributed Representation>at the Center for Data Science , New York University in Fall, 2015. This is a lecture note for the course DS-GA 3001<Natural Language Understanding with Distributed Representation>at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding."}, {"paper_id": "58744159", "adju_relevance": 0, "title": "ConMan: A visual programming language for interactive graphics", "background_label": "A grinder-mixer having a trough-shaped mixing tank, a horizontal transfer and mixing auger seated along the bottom of the tank and extending into the hammer mill, and a vertical auger in the tank for elevating the material for recirculation or discharge. Mixing capacity is increased with the larger tank and bridging of material is minimized with steeply inclined tank walls, a centrally mounted agitator and the trough-mounted mixing auger.", "method_label": "Power train demands are reduced through utilizing only two auger conveyors to transfer material from the hammer mill, mix it in the tank and elevate it for recirculation or discharge.", "result_label": "Improved intermixing is provided through stepped pitch flights provided on the horizontal mixing auger.", "abstract": "A grinder-mixer having a trough-shaped mixing tank, a horizontal transfer and mixing auger seated along the bottom of the tank and extending into the hammer mill, and a vertical auger in the tank for elevating the material for recirculation or discharge. A grinder-mixer having a trough-shaped mixing tank, a horizontal transfer and mixing auger seated along the bottom of the tank and extending into the hammer mill, and a vertical auger in the tank for elevating the material for recirculation or discharge. Mixing capacity is increased with the larger tank and bridging of material is minimized with steeply inclined tank walls, a centrally mounted agitator and the trough-mounted mixing auger. Power train demands are reduced through utilizing only two auger conveyors to transfer material from the hammer mill, mix it in the tank and elevate it for recirculation or discharge. Improved intermixing is provided through stepped pitch flights provided on the horizontal mixing auger."}, {"paper_id": "8829260", "adju_relevance": 0, "title": "The \u00b5-TBL System: Logic Programming Tools for Transformation-Based Learning", "background_label": "AbstractThe #u-TBL system represents an attempt to use the search and database capabilities of the Prolog programming language to implement a generalized form of transformation-based learning. In the true spirit of logic-programming, the implementation is 'derived' from a declarative, logical interpretation of transformation rules.", "method_label": "The #-TBL system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including Constraint Grammar disambiguators as well as more traditional 'Brill-taggers'.", "result_label": "Results from a number of experiments and benchmarks are presented which show that the system is both flex-\" ible and efficient.", "abstract": "AbstractThe #u-TBL system represents an attempt to use the search and database capabilities of the Prolog programming language to implement a generalized form of transformation-based learning. AbstractThe #u-TBL system represents an attempt to use the search and database capabilities of the Prolog programming language to implement a generalized form of transformation-based learning. In the true spirit of logic-programming, the implementation is 'derived' from a declarative, logical interpretation of transformation rules. The #-TBL system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including Constraint Grammar disambiguators as well as more traditional 'Brill-taggers'. Results from a number of experiments and benchmarks are presented which show that the system is both flex-\" ible and efficient."}, {"paper_id": "46974778", "adju_relevance": 0, "title": "Linguistics and natural logic", "background_label": "Evidence is presented to show that the role of a generative grammar of a natural language is not merely to generate the grammatical sentences of that language, but also to relate them to their logical forms.", "abstract": "Evidence is presented to show that the role of a generative grammar of a natural language is not merely to generate the grammatical sentences of that language, but also to relate them to their logical forms."}, {"paper_id": "198147980", "adju_relevance": 0, "title": "Towards meta-interpretive learning of programming language semantics", "background_label": "We introduce a new application for inductive logic programming: learning the semantics of programming languages from example evaluations.", "method_label": "In this short paper, we explored a simplified task in this domain using the Metagol meta-interpretive learning system.", "result_label": "We highlighted the challenging aspects of this scenario, including abstracting over function symbols, nonterminating examples, and learning non-observed predicates, and proposed extensions to Metagol helpful for overcoming these challenges, which may prove useful in other domains.", "abstract": "We introduce a new application for inductive logic programming: learning the semantics of programming languages from example evaluations. In this short paper, we explored a simplified task in this domain using the Metagol meta-interpretive learning system. We highlighted the challenging aspects of this scenario, including abstracting over function symbols, nonterminating examples, and learning non-observed predicates, and proposed extensions to Metagol helpful for overcoming these challenges, which may prove useful in other domains."}, {"paper_id": "14798421", "adju_relevance": 0, "title": "Language Migration in non-CS Introductory Programming through Mutual Language Translation Environment", "background_label": "In the past decade, improvements have been made to the environments used for introductory programming education, including by the introduction of visual programming languages such as Squeak and Scratch. However, migration from these languages to text-based programming languages such as C and Java is still a problem.", "method_label": "Hence, using the OpenBlocks framework proposed at the Massachusetts Institute of Technology, we developed a system named BlockEditor, which can translate bidirectionally between Block (the block language used here) and Java. We conducted an empirical study of this system in an introductory programming course taken by approximately 100 university students not majoring in computer science. When students were given opportunities to select the language to solve their programming assignments, we traced their selection by tracking working time with BlockEditor or Java for each individual student.", "result_label": "The results illustrate the nature of the seamless migration from Block to Java, and show that there is great diversity in the timing and speed of migration to Java by each individual. Additionally, we found that students with low self-evaluation of their skill chose to use Block at a significantly higher rate than did students with high self-evaluation. This suggests that BlockEditor can act as scaffolding for students by promoting mixed programming between Block and Java in their migration phase.", "abstract": "In the past decade, improvements have been made to the environments used for introductory programming education, including by the introduction of visual programming languages such as Squeak and Scratch. In the past decade, improvements have been made to the environments used for introductory programming education, including by the introduction of visual programming languages such as Squeak and Scratch. However, migration from these languages to text-based programming languages such as C and Java is still a problem. Hence, using the OpenBlocks framework proposed at the Massachusetts Institute of Technology, we developed a system named BlockEditor, which can translate bidirectionally between Block (the block language used here) and Java. Hence, using the OpenBlocks framework proposed at the Massachusetts Institute of Technology, we developed a system named BlockEditor, which can translate bidirectionally between Block (the block language used here) and Java. We conducted an empirical study of this system in an introductory programming course taken by approximately 100 university students not majoring in computer science. Hence, using the OpenBlocks framework proposed at the Massachusetts Institute of Technology, we developed a system named BlockEditor, which can translate bidirectionally between Block (the block language used here) and Java. We conducted an empirical study of this system in an introductory programming course taken by approximately 100 university students not majoring in computer science. When students were given opportunities to select the language to solve their programming assignments, we traced their selection by tracking working time with BlockEditor or Java for each individual student. The results illustrate the nature of the seamless migration from Block to Java, and show that there is great diversity in the timing and speed of migration to Java by each individual. The results illustrate the nature of the seamless migration from Block to Java, and show that there is great diversity in the timing and speed of migration to Java by each individual. Additionally, we found that students with low self-evaluation of their skill chose to use Block at a significantly higher rate than did students with high self-evaluation. The results illustrate the nature of the seamless migration from Block to Java, and show that there is great diversity in the timing and speed of migration to Java by each individual. Additionally, we found that students with low self-evaluation of their skill chose to use Block at a significantly higher rate than did students with high self-evaluation. This suggests that BlockEditor can act as scaffolding for students by promoting mixed programming between Block and Java in their migration phase."}, {"paper_id": "39159868", "adju_relevance": 0, "title": "RESTFul POS tagging WEB service for Sinhala language", "background_label": "In the present context of human computer interaction, world has achieved tremendous progress in the field of Natural Language Processing (NLP) applications. The availability of lexical resources is vital to many natural language processing in the field of computation linguistics. But only few languages in the world have the advantage of having enough lexical resources. Though Sinhala language has a long history, Natural Language Processing and computational linguistic aided development of the language is far behind compared with other languages. Researches on NLP for Sinhala language can be pushed by creation of required lexical resources and tools.", "abstract": "In the present context of human computer interaction, world has achieved tremendous progress in the field of Natural Language Processing (NLP) applications. In the present context of human computer interaction, world has achieved tremendous progress in the field of Natural Language Processing (NLP) applications. The availability of lexical resources is vital to many natural language processing in the field of computation linguistics. In the present context of human computer interaction, world has achieved tremendous progress in the field of Natural Language Processing (NLP) applications. The availability of lexical resources is vital to many natural language processing in the field of computation linguistics. But only few languages in the world have the advantage of having enough lexical resources. In the present context of human computer interaction, world has achieved tremendous progress in the field of Natural Language Processing (NLP) applications. The availability of lexical resources is vital to many natural language processing in the field of computation linguistics. But only few languages in the world have the advantage of having enough lexical resources. Though Sinhala language has a long history, Natural Language Processing and computational linguistic aided development of the language is far behind compared with other languages. In the present context of human computer interaction, world has achieved tremendous progress in the field of Natural Language Processing (NLP) applications. The availability of lexical resources is vital to many natural language processing in the field of computation linguistics. But only few languages in the world have the advantage of having enough lexical resources. Though Sinhala language has a long history, Natural Language Processing and computational linguistic aided development of the language is far behind compared with other languages. Researches on NLP for Sinhala language can be pushed by creation of required lexical resources and tools."}, {"paper_id": "2617263", "adju_relevance": 0, "title": "Learning Task Formulations through Situated Interactive Instruction", "background_label": "We present an agent, called RosieTAG, which is implemented in Soar and interacts with an external robotic environment. Rosie learns new games through interactive instruction with a human via restricted natural language.", "method_label": "Instead of learning policy or strategy information as is common in other game learners, Rosie learns multiple game formulations (the objects, players, and rules of a game) and then uses its own general strategies to solve them.", "result_label": "We describe the structure and functionality of Rosie, and evaluate its competence, generality, communication efficiency, communication accessibility, and ability to continuously learn and accumulate new tasks and new task knowledge.", "abstract": "We present an agent, called RosieTAG, which is implemented in Soar and interacts with an external robotic environment. We present an agent, called RosieTAG, which is implemented in Soar and interacts with an external robotic environment. Rosie learns new games through interactive instruction with a human via restricted natural language. Instead of learning policy or strategy information as is common in other game learners, Rosie learns multiple game formulations (the objects, players, and rules of a game) and then uses its own general strategies to solve them. We describe the structure and functionality of Rosie, and evaluate its competence, generality, communication efficiency, communication accessibility, and ability to continuously learn and accumulate new tasks and new task knowledge."}, {"paper_id": "17781156", "adju_relevance": 0, "title": "SOUL and Smalltalk - Just Married: Evolution of the Interaction between a Logic and an Object-Oriented Language towards Symbiosis", "background_label": "The Smalltalk Open Unification Language is a Prolog-like language embedded in the object-oriented language Smalltalk [5]. Over the years, it has been used as a research platform for applying logic programming to a variety of problems in object-oriented software engineering, some examples are: representing domain knowledge explicitly [3]; reasoning about object-oriented design [15,14]; checking and enforcing programming patterns [11]; ; checking architectural conformance [16] and making the crosscuts in Aspect-Oriented Programming more robust [6]. These examples fit in the wider research of Declarative Meta Programming, where SOUL is used as a meta language to reason about Smalltalk code.", "method_label": "Recently, we explored a different usage of SOUL in connecting business rules and core application functionality [2], which involves reasoning about Smalltalk objects.", "result_label": "We found we had to improve on SOUL\u2019s existing mechanism for interacting with those objects because it was not transparent: it was clear from the SOUL code when rules were invoked and when messages were sent to objects, vice-versa solving queries from methods was rather clumsy.", "abstract": "The Smalltalk Open Unification Language is a Prolog-like language embedded in the object-oriented language Smalltalk [5]. The Smalltalk Open Unification Language is a Prolog-like language embedded in the object-oriented language Smalltalk [5]. Over the years, it has been used as a research platform for applying logic programming to a variety of problems in object-oriented software engineering, some examples are: representing domain knowledge explicitly [3]; reasoning about object-oriented design [15,14]; checking and enforcing programming patterns [11]; ; checking architectural conformance [16] and making the crosscuts in Aspect-Oriented Programming more robust [6]. The Smalltalk Open Unification Language is a Prolog-like language embedded in the object-oriented language Smalltalk [5]. Over the years, it has been used as a research platform for applying logic programming to a variety of problems in object-oriented software engineering, some examples are: representing domain knowledge explicitly [3]; reasoning about object-oriented design [15,14]; checking and enforcing programming patterns [11]; ; checking architectural conformance [16] and making the crosscuts in Aspect-Oriented Programming more robust [6]. These examples fit in the wider research of Declarative Meta Programming, where SOUL is used as a meta language to reason about Smalltalk code. Recently, we explored a different usage of SOUL in connecting business rules and core application functionality [2], which involves reasoning about Smalltalk objects. We found we had to improve on SOUL\u2019s existing mechanism for interacting with those objects because it was not transparent: it was clear from the SOUL code when rules were invoked and when messages were sent to objects, vice-versa solving queries from methods was rather clumsy."}, {"paper_id": "11430990", "adju_relevance": 0, "title": "Interactive machine learning", "background_label": "Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers.", "abstract": "Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers."}, {"paper_id": "52164739", "adju_relevance": 0, "title": "Localizing Moments in Video with Temporal Language", "background_label": "Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text.", "abstract": "Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text."}, {"paper_id": "62204437", "adju_relevance": 0, "title": "Interactive Multimedia-based Mobile Application for Learning Iban Language (I-MMAPS for Learning Iban Language)", "background_label": "Abstract The emergence of mobile technological devices has brought along new possibilities to all walks of life in various communities. As mobile devices with highest capabilities extend into all areas of human life, they also affected language learning and mobile education. There are many mobile applications created for most of the major languages in the world and much lesser amount is available for the far less used languages. Iban language is an example of those languages given far less attention as it is not as practical as other world languages.", "method_label": "Iban language has been chosen to be implemented as a multimedia-based mobile application to promote language learning due to its richness in unique culture and language. The application; I- MMAPPS for learning Iban language was designed and developed by adapting conversational method and constructivism learning theory to provide learners a different approach in learning language based on various environments and situations. Since Iban pronunciations are different from any Malaysians local dialects, thus the correct pronunciations are vital to assists the learner. 30 non native speakers were chosen to test the application.", "result_label": "The outcomes that this research presented have shown that, this application is a promising development in future research. Most of the respondents were satisfied when they learn Iban language using the application.", "abstract": "Abstract The emergence of mobile technological devices has brought along new possibilities to all walks of life in various communities. Abstract The emergence of mobile technological devices has brought along new possibilities to all walks of life in various communities. As mobile devices with highest capabilities extend into all areas of human life, they also affected language learning and mobile education. Abstract The emergence of mobile technological devices has brought along new possibilities to all walks of life in various communities. As mobile devices with highest capabilities extend into all areas of human life, they also affected language learning and mobile education. There are many mobile applications created for most of the major languages in the world and much lesser amount is available for the far less used languages. Abstract The emergence of mobile technological devices has brought along new possibilities to all walks of life in various communities. As mobile devices with highest capabilities extend into all areas of human life, they also affected language learning and mobile education. There are many mobile applications created for most of the major languages in the world and much lesser amount is available for the far less used languages. Iban language is an example of those languages given far less attention as it is not as practical as other world languages. Iban language has been chosen to be implemented as a multimedia-based mobile application to promote language learning due to its richness in unique culture and language. Iban language has been chosen to be implemented as a multimedia-based mobile application to promote language learning due to its richness in unique culture and language. The application; I- MMAPPS for learning Iban language was designed and developed by adapting conversational method and constructivism learning theory to provide learners a different approach in learning language based on various environments and situations. Iban language has been chosen to be implemented as a multimedia-based mobile application to promote language learning due to its richness in unique culture and language. The application; I- MMAPPS for learning Iban language was designed and developed by adapting conversational method and constructivism learning theory to provide learners a different approach in learning language based on various environments and situations. Since Iban pronunciations are different from any Malaysians local dialects, thus the correct pronunciations are vital to assists the learner. Iban language has been chosen to be implemented as a multimedia-based mobile application to promote language learning due to its richness in unique culture and language. The application; I- MMAPPS for learning Iban language was designed and developed by adapting conversational method and constructivism learning theory to provide learners a different approach in learning language based on various environments and situations. Since Iban pronunciations are different from any Malaysians local dialects, thus the correct pronunciations are vital to assists the learner. 30 non native speakers were chosen to test the application. The outcomes that this research presented have shown that, this application is a promising development in future research. The outcomes that this research presented have shown that, this application is a promising development in future research. Most of the respondents were satisfied when they learn Iban language using the application."}]