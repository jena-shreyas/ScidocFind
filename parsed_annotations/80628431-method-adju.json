[{"paper_id": "80628431", "title": "Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages", "background_label": "How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs' syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus.", "method_label": "We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages.", "result_label": "Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order.", "abstract": "How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs' syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order."}, {"paper_id": "52113185", "adju_relevance": 3, "title": "Targeted Syntactic Evaluation of Language Models", "background_label": "We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.", "method_label": "We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions.", "result_label": "Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.", "abstract": "We present a dataset for evaluating the grammaticality of the predictions of a language model. We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model."}, {"paper_id": "28123397", "adju_relevance": 2, "title": "Generating synthetic mobility traffic using RNNs", "background_label": "Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility. Privacy concerns today however, have restricted sharing of such datasets. This has led to the development of synthetic traffic generators, which simulate moving entities to create pseudo-realistic trajectory datasets. Existing work on traffic generation, superficially matches a-priori modeled mobility characteristics, which lacks realism and does not capture the substantive properties of human mobility. Critical applications however, require data that contains these complex, candid and hidden mobility patterns.", "abstract": "Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility. Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility. Privacy concerns today however, have restricted sharing of such datasets. Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility. Privacy concerns today however, have restricted sharing of such datasets. This has led to the development of synthetic traffic generators, which simulate moving entities to create pseudo-realistic trajectory datasets. Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility. Privacy concerns today however, have restricted sharing of such datasets. This has led to the development of synthetic traffic generators, which simulate moving entities to create pseudo-realistic trajectory datasets. Existing work on traffic generation, superficially matches a-priori modeled mobility characteristics, which lacks realism and does not capture the substantive properties of human mobility. Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility. Privacy concerns today however, have restricted sharing of such datasets. This has led to the development of synthetic traffic generators, which simulate moving entities to create pseudo-realistic trajectory datasets. Existing work on traffic generation, superficially matches a-priori modeled mobility characteristics, which lacks realism and does not capture the substantive properties of human mobility. Critical applications however, require data that contains these complex, candid and hidden mobility patterns."}, {"paper_id": "4460159", "adju_relevance": 2, "title": "Colorless green recurrent networks dream hierarchically", "background_label": "Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language.", "abstract": "Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language."}, {"paper_id": "49363457", "adju_relevance": 2, "title": "Assessing Composition in Sentence Vector Representations", "background_label": "An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models.", "method_label": "We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models.", "result_label": "We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems' capturing of sentence information. We make available for public use the datasets used for these experiments, as well as the generation system.", "abstract": "An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models. We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems' capturing of sentence information. We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems' capturing of sentence information. We make available for public use the datasets used for these experiments, as well as the generation system."}, {"paper_id": "52165443", "adju_relevance": 2, "title": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency", "background_label": "Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective.", "method_label": "Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM.", "result_label": "We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.", "abstract": "Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items."}, {"paper_id": "16562917", "adju_relevance": 2, "title": "Structural priming in artificial languages and the regularisation of unpredictable variation", "background_label": "We present a novel experimental technique using artificial language learning to investigate the relationship between structural priming during communicative interaction, and linguistic regularity. We use unpredictable variation as a test-case, because it is a well-established paradigm to study learners\u2019 biases during acquisition, transmission and interaction.", "method_label": "We trained participants on artificial languages exhibiting unpredictable variation in word order, and subsequently had them communicate using these artificial languages. We found evidence for structural priming in two different grammatical constructions and across human-human and human-computer interaction. Priming occurred regardless of behavioral convergence: communication led to shared word order use only in human-human interaction, but priming was observed in all conditions. Furthermore, interaction resulted in the reduction of unpredictable variation in all conditions, suggesting a role for communicative interaction in eliminating unpredictable variation.", "result_label": "Regularisation was strongest in human-human interaction and in a condition where participants believed they were interacting with a human but were in fact interacting with a computer. We suggest that participants recognize the counter-functional nature of unpredictable variation and thus act to eliminate this variability during communication. Furthermore, reciprocal priming occurring in human-human interaction drove some pairs of participants to converge on maximally regular, highly predictable linguistic systems. Our method offers potential benefits to both the artificial language learning and the structural priming fields, and provides a useful tool to investigate communicative processes that lead to language change and ultimately language design.", "abstract": "We present a novel experimental technique using artificial language learning to investigate the relationship between structural priming during communicative interaction, and linguistic regularity. We present a novel experimental technique using artificial language learning to investigate the relationship between structural priming during communicative interaction, and linguistic regularity. We use unpredictable variation as a test-case, because it is a well-established paradigm to study learners\u2019 biases during acquisition, transmission and interaction. We trained participants on artificial languages exhibiting unpredictable variation in word order, and subsequently had them communicate using these artificial languages. We trained participants on artificial languages exhibiting unpredictable variation in word order, and subsequently had them communicate using these artificial languages. We found evidence for structural priming in two different grammatical constructions and across human-human and human-computer interaction. We trained participants on artificial languages exhibiting unpredictable variation in word order, and subsequently had them communicate using these artificial languages. We found evidence for structural priming in two different grammatical constructions and across human-human and human-computer interaction. Priming occurred regardless of behavioral convergence: communication led to shared word order use only in human-human interaction, but priming was observed in all conditions. We trained participants on artificial languages exhibiting unpredictable variation in word order, and subsequently had them communicate using these artificial languages. We found evidence for structural priming in two different grammatical constructions and across human-human and human-computer interaction. Priming occurred regardless of behavioral convergence: communication led to shared word order use only in human-human interaction, but priming was observed in all conditions. Furthermore, interaction resulted in the reduction of unpredictable variation in all conditions, suggesting a role for communicative interaction in eliminating unpredictable variation. Regularisation was strongest in human-human interaction and in a condition where participants believed they were interacting with a human but were in fact interacting with a computer. Regularisation was strongest in human-human interaction and in a condition where participants believed they were interacting with a human but were in fact interacting with a computer. We suggest that participants recognize the counter-functional nature of unpredictable variation and thus act to eliminate this variability during communication. Regularisation was strongest in human-human interaction and in a condition where participants believed they were interacting with a human but were in fact interacting with a computer. We suggest that participants recognize the counter-functional nature of unpredictable variation and thus act to eliminate this variability during communication. Furthermore, reciprocal priming occurring in human-human interaction drove some pairs of participants to converge on maximally regular, highly predictable linguistic systems. Regularisation was strongest in human-human interaction and in a condition where participants believed they were interacting with a human but were in fact interacting with a computer. We suggest that participants recognize the counter-functional nature of unpredictable variation and thus act to eliminate this variability during communication. Furthermore, reciprocal priming occurring in human-human interaction drove some pairs of participants to converge on maximally regular, highly predictable linguistic systems. Our method offers potential benefits to both the artificial language learning and the structural priming fields, and provides a useful tool to investigate communicative processes that lead to language change and ultimately language design."}, {"paper_id": "3580012", "adju_relevance": 2, "title": "Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks", "background_label": "Syntactic rules in natural language typically need to make reference to hierarchical sentence structure. However, the simple examples that language learners receive are often equally compatible with linear rules. Children consistently ignore these linear explanations and settle instead on the correct hierarchical one. This fact has motivated the proposal that the learner's hypothesis space is constrained to include only hierarchical rules.", "method_label": "We examine this proposal using recurrent neural networks (RNNs), which are not constrained in such a way. We simulate the acquisition of question formation, a hierarchical transformation, in a fragment of English. We find that some RNN architectures tend to learn the hierarchical rule, suggesting that hierarchical cues within the language, combined with the implicit architectural biases inherent in certain RNNs, may be sufficient to induce hierarchical generalizations.", "result_label": "The likelihood of acquiring the hierarchical generalization increased when the language included an additional cue to hierarchy in the form of subject-verb agreement, underscoring the role of cues to hierarchy in the learner's input.", "abstract": "Syntactic rules in natural language typically need to make reference to hierarchical sentence structure. Syntactic rules in natural language typically need to make reference to hierarchical sentence structure. However, the simple examples that language learners receive are often equally compatible with linear rules. Syntactic rules in natural language typically need to make reference to hierarchical sentence structure. However, the simple examples that language learners receive are often equally compatible with linear rules. Children consistently ignore these linear explanations and settle instead on the correct hierarchical one. Syntactic rules in natural language typically need to make reference to hierarchical sentence structure. However, the simple examples that language learners receive are often equally compatible with linear rules. Children consistently ignore these linear explanations and settle instead on the correct hierarchical one. This fact has motivated the proposal that the learner's hypothesis space is constrained to include only hierarchical rules. We examine this proposal using recurrent neural networks (RNNs), which are not constrained in such a way. We examine this proposal using recurrent neural networks (RNNs), which are not constrained in such a way. We simulate the acquisition of question formation, a hierarchical transformation, in a fragment of English. We examine this proposal using recurrent neural networks (RNNs), which are not constrained in such a way. We simulate the acquisition of question formation, a hierarchical transformation, in a fragment of English. We find that some RNN architectures tend to learn the hierarchical rule, suggesting that hierarchical cues within the language, combined with the implicit architectural biases inherent in certain RNNs, may be sufficient to induce hierarchical generalizations. The likelihood of acquiring the hierarchical generalization increased when the language included an additional cue to hierarchy in the form of subject-verb agreement, underscoring the role of cues to hierarchy in the learner's input."}, {"paper_id": "10817864", "adju_relevance": 2, "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages", "background_label": "We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format.", "abstract": "We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format."}, {"paper_id": "24461982", "adju_relevance": 1, "title": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties", "background_label": "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. \"Downstream\"tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations.", "method_label": "We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.", "abstract": "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. \"Downstream\"tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. \"Downstream\"tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods."}, {"paper_id": "5421814", "adju_relevance": 1, "title": "A noisy-channel account of crosslinguistic word-order variation.", "background_label": "The distribution of word orders across languages is highly nonuniform, with subject-verb-object (SVO) and subject-object-verb (SOV) orders being prevalent. Recent work suggests that the SOV order may be the default in human language. Why, then, is SVO order so common?", "abstract": "The distribution of word orders across languages is highly nonuniform, with subject-verb-object (SVO) and subject-object-verb (SOV) orders being prevalent. The distribution of word orders across languages is highly nonuniform, with subject-verb-object (SVO) and subject-object-verb (SOV) orders being prevalent. Recent work suggests that the SOV order may be the default in human language. The distribution of word orders across languages is highly nonuniform, with subject-verb-object (SVO) and subject-object-verb (SOV) orders being prevalent. Recent work suggests that the SOV order may be the default in human language. Why, then, is SVO order so common?"}, {"paper_id": "52191945", "adju_relevance": 1, "title": "Can LSTM Learn to Capture Agreement? The Case of Basque", "background_label": "Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire?", "abstract": "Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire?"}, {"paper_id": "7821379", "adju_relevance": 1, "title": "Exploring the Syntactic Abilities of RNNs with Multi-task Learning", "background_label": "Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016).", "abstract": "Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016)."}, {"paper_id": "3502245", "adju_relevance": 1, "title": "From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings", "background_label": "A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS). Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS.", "method_label": "We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging). We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task.", "result_label": "For instance, although Norwegian Bokm{\\aa}l and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task. We are also able to predict typological features in WALS with high accuracies, even for unseen language families.", "abstract": "A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS). A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS). Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS. We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging). We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging). We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task. For instance, although Norwegian Bokm{\\aa}l and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task. For instance, although Norwegian Bokm{\\aa}l and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task. We are also able to predict typological features in WALS with high accuracies, even for unseen language families."}, {"paper_id": "52090220", "adju_relevance": 1, "title": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information", "background_label": "How do neural language models keep track of number agreement between subject and verb? We show that `diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented.", "method_label": "Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences.", "result_label": "Results from such an intervention reveal a large increase in the language model's accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.", "abstract": "How do neural language models keep track of number agreement between subject and verb? How do neural language models keep track of number agreement between subject and verb? We show that `diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors. Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention reveal a large increase in the language model's accuracy. Results from such an intervention reveal a large increase in the language model's accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance."}, {"paper_id": "6352511", "adju_relevance": 1, "title": "Learning biases predict a word order universal.", "background_label": "How recurrent typological patterns, or universals, emerge from the extensive diversity found across the world's languages constitutes a central question for linguistics and cognitive science. Recent challenges to a fundamental assumption of generative linguistics-that universal properties of the human language acquisition faculty constrain the types of grammatical systems which can occur-suggest the need for new types of empirical evidence connecting typology to biases of learners.", "method_label": "Using an artificial language learning paradigm in which adult subjects are exposed to a mix of grammatical systems (similar to a period of linguistic change), we show that learners' biases mirror a word-order universal, first proposed by Joseph Greenberg, which constrains typological patterns of adjective, numeral, and noun ordering.", "result_label": "We briefly summarize the results of a probabilistic model of the hypothesized biases and their effect on learning, and discuss the broader implications of the results for current theories of the origins of cross-linguistic word-order preferences.", "abstract": "How recurrent typological patterns, or universals, emerge from the extensive diversity found across the world's languages constitutes a central question for linguistics and cognitive science. How recurrent typological patterns, or universals, emerge from the extensive diversity found across the world's languages constitutes a central question for linguistics and cognitive science. Recent challenges to a fundamental assumption of generative linguistics-that universal properties of the human language acquisition faculty constrain the types of grammatical systems which can occur-suggest the need for new types of empirical evidence connecting typology to biases of learners. Using an artificial language learning paradigm in which adult subjects are exposed to a mix of grammatical systems (similar to a period of linguistic change), we show that learners' biases mirror a word-order universal, first proposed by Joseph Greenberg, which constrains typological patterns of adjective, numeral, and noun ordering. We briefly summarize the results of a probabilistic model of the hypothesized biases and their effect on learning, and discuss the broader implications of the results for current theories of the origins of cross-linguistic word-order preferences."}, {"paper_id": "145055053", "adju_relevance": 1, "title": "Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks", "background_label": "In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language.", "method_label": "The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English.", "result_label": "The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. This is further supported by a study on paraphrase generation that we also include at the end of the paper.", "abstract": "In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language. The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. This is further supported by a study on paraphrase generation that we also include at the end of the paper."}, {"paper_id": "2078255", "adju_relevance": 1, "title": "From Characters to Words to in Between: Do We Capture Morphology?", "background_label": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies.", "method_label": "On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled.", "result_label": "Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "abstract": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data."}, {"paper_id": "17499628", "adju_relevance": 1, "title": "Can CANISO activate CASINO? Transposed-letter similarity effects with nonadjacent letter positions q", "background_label": "Nonwords created by transposing two adjacent letters (i.e., transposed-letter (TL) nonwords like jugde) are very effective at activating the lexical representation of their base words. This fact poses problems for most computational models of word recognition (e.g., the interactive-activation model and its extensions), which assume that exact letter positions are rapidly coded during the word recognition process.", "method_label": "To examine the scope of TL similarity effects further, we asked whether TL similarity effects occur for nonwords created by exchanging two nonadjacent letters (e.g., canisoCASINO) in three masked form priming experiments using the lexical decision task. The two nonadjacent transposed letters were consonants in Experiment 1 (e.g., caniso-CASINO), vowels in Experiment 2 (anamil-ANIMAL) and both consonants and vowels in Experiment 3.", "result_label": "Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants. In a final experiment we examined latencies for nonwords created by nonadjacent transpositions of consonants versus vowels in a lexical decision task. Both types of nonwords produced longer latencies than matched controls, with consonant TL nonwords being more difficult than vowel TL nonwords. The implications of these findings for models having \u2018\u2018position-specific\u2019\u2019 coding schemes as well as for models proposing alternative coding schemes are discussed.", "abstract": "Nonwords created by transposing two adjacent letters (i.e., transposed-letter (TL) nonwords like jugde) are very effective at activating the lexical representation of their base words. Nonwords created by transposing two adjacent letters (i.e., transposed-letter (TL) nonwords like jugde) are very effective at activating the lexical representation of their base words. This fact poses problems for most computational models of word recognition (e.g., the interactive-activation model and its extensions), which assume that exact letter positions are rapidly coded during the word recognition process. To examine the scope of TL similarity effects further, we asked whether TL similarity effects occur for nonwords created by exchanging two nonadjacent letters (e.g., canisoCASINO) in three masked form priming experiments using the lexical decision task. To examine the scope of TL similarity effects further, we asked whether TL similarity effects occur for nonwords created by exchanging two nonadjacent letters (e.g., canisoCASINO) in three masked form priming experiments using the lexical decision task. The two nonadjacent transposed letters were consonants in Experiment 1 (e.g., caniso-CASINO), vowels in Experiment 2 (anamil-ANIMAL) and both consonants and vowels in Experiment 3. Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants. Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants. In a final experiment we examined latencies for nonwords created by nonadjacent transpositions of consonants versus vowels in a lexical decision task. Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants. In a final experiment we examined latencies for nonwords created by nonadjacent transpositions of consonants versus vowels in a lexical decision task. Both types of nonwords produced longer latencies than matched controls, with consonant TL nonwords being more difficult than vowel TL nonwords. Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants. In a final experiment we examined latencies for nonwords created by nonadjacent transpositions of consonants versus vowels in a lexical decision task. Both types of nonwords produced longer latencies than matched controls, with consonant TL nonwords being more difficult than vowel TL nonwords. The implications of these findings for models having \u2018\u2018position-specific\u2019\u2019 coding schemes as well as for models proposing alternative coding schemes are discussed."}, {"paper_id": "13860931", "adju_relevance": 1, "title": "Processing relative clauses in Chinese", "background_label": "This paper reports results from a self-paced reading study in Chinese that demonstrates that object-extracted relative clause structures are less complex than corresponding subject-extracted structures. These results contrast with results from processing other Subject-Verb-Object languages like English, in which object-extracted structures are more complex than subject-extracted structures. A key word-order difference between Chinese and other Subject-Verb-Object languages is that Chinese relative clauses precede their head nouns.", "method_label": "Because of this word order difference, the results follow from a resource-based theory of sentence complexity, according to which there is a storage cost associated with predicting syntactic heads in order to form a grammatical sentence.", "result_label": "The results are also consistent with a theory according to which people have less difficulty processing embedded clauses whose word order matches the word order in main clauses. Some corpus analyses of Chinese texts provide results that constrain the classes of possible frequency-based theories. Critically, these results demonstrate that there is nothing intrinsically easy about extracting from subject position: depending on the word order in the main clause and in a relative clause, extraction from object position can be easier to process in some circumstances.", "abstract": "This paper reports results from a self-paced reading study in Chinese that demonstrates that object-extracted relative clause structures are less complex than corresponding subject-extracted structures. This paper reports results from a self-paced reading study in Chinese that demonstrates that object-extracted relative clause structures are less complex than corresponding subject-extracted structures. These results contrast with results from processing other Subject-Verb-Object languages like English, in which object-extracted structures are more complex than subject-extracted structures. This paper reports results from a self-paced reading study in Chinese that demonstrates that object-extracted relative clause structures are less complex than corresponding subject-extracted structures. These results contrast with results from processing other Subject-Verb-Object languages like English, in which object-extracted structures are more complex than subject-extracted structures. A key word-order difference between Chinese and other Subject-Verb-Object languages is that Chinese relative clauses precede their head nouns. Because of this word order difference, the results follow from a resource-based theory of sentence complexity, according to which there is a storage cost associated with predicting syntactic heads in order to form a grammatical sentence. The results are also consistent with a theory according to which people have less difficulty processing embedded clauses whose word order matches the word order in main clauses. The results are also consistent with a theory according to which people have less difficulty processing embedded clauses whose word order matches the word order in main clauses. Some corpus analyses of Chinese texts provide results that constrain the classes of possible frequency-based theories. The results are also consistent with a theory according to which people have less difficulty processing embedded clauses whose word order matches the word order in main clauses. Some corpus analyses of Chinese texts provide results that constrain the classes of possible frequency-based theories. Critically, these results demonstrate that there is nothing intrinsically easy about extracting from subject position: depending on the word order in the main clause and in a relative clause, extraction from object position can be easier to process in some circumstances."}, {"paper_id": "9426092", "adju_relevance": 1, "title": "Language learners privilege structured meaning over surface frequency.", "background_label": "Although it is widely agreed that learning the syntax of natural languages involves acquiring structure-dependent rules, recent work on acquisition has nevertheless attempted to characterize the outcome of learning primarily in terms of statistical generalizations about surface distributional information.", "abstract": "Although it is widely agreed that learning the syntax of natural languages involves acquiring structure-dependent rules, recent work on acquisition has nevertheless attempted to characterize the outcome of learning primarily in terms of statistical generalizations about surface distributional information."}, {"paper_id": "17822109", "adju_relevance": 0, "title": "The case of the nonce loan in Tamil", "background_label": "Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals. With established loanwords, however, they share the characteristics of morphological and syntactic integration into the host language and consist of single content words or compounds. Furthermore, both types of loanwords differ fro m intrasentential code-switching \u2014alternate sentence fragments in the two languages, each of which is grammatical by monolingual standards from the standpoints of appropriate function words, morphology, and syntax. In a large corpus of Tamil-English bilingual speech, many words of English origin are found in objects governed by Tamil verbs and vice versa. The equivalence constraint on intrasentential code-switching predicts that no code-switch should occur between verb and object in an SOV/SVO bilingual situation, and hence that objects whose language differs from that of the verb must be borrowed, if only for the nonce.", "result_label": "To verify this prediction, we compare quantitatively the distribution across various syntactic contexts of both native Tamil and English-origin complements of Tamil verbs, and find them to be parallel. But the strongest evidence in favor of the nonce borrowing hypothesis comes from an analysis of variable accusative and dative case marking in these complements, in which the English-origin material is shown, morphologically and syntactically, to be virtually indistinguishable from Tamil (nonpronominal) nouns. In addition, we present supporting evidence from the genitive, locative, and other cases and from nonce borrowings from Tamil into these speakers' English.", "abstract": "Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals. Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals. With established loanwords, however, they share the characteristics of morphological and syntactic integration into the host language and consist of single content words or compounds. Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals. With established loanwords, however, they share the characteristics of morphological and syntactic integration into the host language and consist of single content words or compounds. Furthermore, both types of loanwords differ fro m intrasentential code-switching \u2014alternate sentence fragments in the two languages, each of which is grammatical by monolingual standards from the standpoints of appropriate function words, morphology, and syntax. Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals. With established loanwords, however, they share the characteristics of morphological and syntactic integration into the host language and consist of single content words or compounds. Furthermore, both types of loanwords differ fro m intrasentential code-switching \u2014alternate sentence fragments in the two languages, each of which is grammatical by monolingual standards from the standpoints of appropriate function words, morphology, and syntax. In a large corpus of Tamil-English bilingual speech, many words of English origin are found in objects governed by Tamil verbs and vice versa. Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals. With established loanwords, however, they share the characteristics of morphological and syntactic integration into the host language and consist of single content words or compounds. Furthermore, both types of loanwords differ fro m intrasentential code-switching \u2014alternate sentence fragments in the two languages, each of which is grammatical by monolingual standards from the standpoints of appropriate function words, morphology, and syntax. In a large corpus of Tamil-English bilingual speech, many words of English origin are found in objects governed by Tamil verbs and vice versa. The equivalence constraint on intrasentential code-switching predicts that no code-switch should occur between verb and object in an SOV/SVO bilingual situation, and hence that objects whose language differs from that of the verb must be borrowed, if only for the nonce. To verify this prediction, we compare quantitatively the distribution across various syntactic contexts of both native Tamil and English-origin complements of Tamil verbs, and find them to be parallel. To verify this prediction, we compare quantitatively the distribution across various syntactic contexts of both native Tamil and English-origin complements of Tamil verbs, and find them to be parallel. But the strongest evidence in favor of the nonce borrowing hypothesis comes from an analysis of variable accusative and dative case marking in these complements, in which the English-origin material is shown, morphologically and syntactically, to be virtually indistinguishable from Tamil (nonpronominal) nouns. To verify this prediction, we compare quantitatively the distribution across various syntactic contexts of both native Tamil and English-origin complements of Tamil verbs, and find them to be parallel. But the strongest evidence in favor of the nonce borrowing hypothesis comes from an analysis of variable accusative and dative case marking in these complements, in which the English-origin material is shown, morphologically and syntactically, to be virtually indistinguishable from Tamil (nonpronominal) nouns. In addition, we present supporting evidence from the genitive, locative, and other cases and from nonce borrowings from Tamil into these speakers' English."}, {"paper_id": "52144417", "adju_relevance": 0, "title": "Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items", "method_label": "We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions. We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.", "result_label": "With this research, we hope to pave the way for other studies linking formal linguistics to deep learning.", "abstract": " We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions. We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions. We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence. With this research, we hope to pave the way for other studies linking formal linguistics to deep learning."}, {"paper_id": "15991124", "adju_relevance": 0, "title": "Chinese Word Ordering Errors Detection and Correction for Non-Native Chinese Language Learners", "background_label": "AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners. Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. That makes WOEs detection and correction more challenging.", "abstract": "AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners. AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners. Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences. AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners. Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners. Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. That makes WOEs detection and correction more challenging."}, {"paper_id": "3829825", "adju_relevance": 0, "title": "Co-occurrence of the Benford-like and Zipf Laws Arising from the Texts Representing Human and Artificial Languages", "background_label": "We demonstrate that large texts, representing human (English, Russian, Ukrainian) and artificial (C++, Java) languages, display quantitative patterns characterized by the Benford-like and Zipf laws. The frequency of a word following the Zipf law is inversely proportional to its rank, whereas the total numbers of a certain word appearing in the text generate the uneven Benford-like distribution of leading numbers.", "method_label": "Excluding the most popular words essentially improves the correlation of actual textual data with the Zipfian distribution, whereas the Benford distribution of leading numbers (arising from the overall amount of a certain word) is insensitive to the same elimination procedure.", "result_label": "The calculated values of the moduli of slopes of double logarithmical plots for artificial languages (C++, Java) are markedly larger than those for human ones.", "abstract": "We demonstrate that large texts, representing human (English, Russian, Ukrainian) and artificial (C++, Java) languages, display quantitative patterns characterized by the Benford-like and Zipf laws. We demonstrate that large texts, representing human (English, Russian, Ukrainian) and artificial (C++, Java) languages, display quantitative patterns characterized by the Benford-like and Zipf laws. The frequency of a word following the Zipf law is inversely proportional to its rank, whereas the total numbers of a certain word appearing in the text generate the uneven Benford-like distribution of leading numbers. Excluding the most popular words essentially improves the correlation of actual textual data with the Zipfian distribution, whereas the Benford distribution of leading numbers (arising from the overall amount of a certain word) is insensitive to the same elimination procedure. The calculated values of the moduli of slopes of double logarithmical plots for artificial languages (C++, Java) are markedly larger than those for human ones."}, {"paper_id": "3249779", "adju_relevance": 0, "title": "A Survey of Inductive Biases for Factorial Representation-Learning", "background_label": "With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to.", "method_label": "A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.", "result_label": "This article surveys various constraints that encourage a learning algorithm to discover factorial representations. This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.", "abstract": "With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning. This article surveys various constraints that encourage a learning algorithm to discover factorial representations. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to. This article surveys various constraints that encourage a learning algorithm to discover factorial representations. This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases."}, {"paper_id": "16760547", "adju_relevance": 0, "title": "A Database of Paradigmatic Semantic Relation Pairs for German Nouns, Verbs, and Adjectives", "background_label": "AbstractA new collection of semantically related word pairs in German is presented, which was compiled via human judgement experiments and comprises (i) a representative selection of target lexical units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of humangenerated semantically related word pairs based on the target units, and (iii) a subset of the generated word pairs rated for their relation strength, including positive and negative relation evidence.", "method_label": "We address the three paradigmatic relations antonymy, hypernymy and synonymy, and systematically work across the three word classes of adjectives, nouns, and verbs.", "result_label": "A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canonical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but hypernymy and synonymy are often confused. We anticipate that our new collection of semantic relation pairs will not only be of considerable use in computational areas in which semantic relations play a role, but also in studies in theoretical linguistics and psycholinguistics.", "abstract": "AbstractA new collection of semantically related word pairs in German is presented, which was compiled via human judgement experiments and comprises (i) a representative selection of target lexical units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of humangenerated semantically related word pairs based on the target units, and (iii) a subset of the generated word pairs rated for their relation strength, including positive and negative relation evidence. We address the three paradigmatic relations antonymy, hypernymy and synonymy, and systematically work across the three word classes of adjectives, nouns, and verbs. A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canonical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but hypernymy and synonymy are often confused. A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canonical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but hypernymy and synonymy are often confused. We anticipate that our new collection of semantic relation pairs will not only be of considerable use in computational areas in which semantic relations play a role, but also in studies in theoretical linguistics and psycholinguistics."}, {"paper_id": "22611976", "adju_relevance": 0, "title": "A natural approach to studying vision", "background_label": "Achieving this goal requires knowledge of both the characteristics of natural stimuli and the response properties of sensory neurons under natural stimulation. Most of our current notions of sensory processing have come from experiments using simple, parametric stimulus sets. However, a growing number of researchers have begun to question whether this approach alone is sufficient for understanding the real-life sensory tasks performed by the organism.", "result_label": "Here, focusing on the early visual pathway, we argue that the use of natural stimuli is vital for advancing our understanding of sensory processing.", "abstract": " Achieving this goal requires knowledge of both the characteristics of natural stimuli and the response properties of sensory neurons under natural stimulation. Achieving this goal requires knowledge of both the characteristics of natural stimuli and the response properties of sensory neurons under natural stimulation. Most of our current notions of sensory processing have come from experiments using simple, parametric stimulus sets. Achieving this goal requires knowledge of both the characteristics of natural stimuli and the response properties of sensory neurons under natural stimulation. Most of our current notions of sensory processing have come from experiments using simple, parametric stimulus sets. However, a growing number of researchers have begun to question whether this approach alone is sufficient for understanding the real-life sensory tasks performed by the organism. Here, focusing on the early visual pathway, we argue that the use of natural stimuli is vital for advancing our understanding of sensory processing."}, {"paper_id": "21700944", "adju_relevance": 0, "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context", "background_label": "We know very little about how neural language models (LM) use prior linguistic context.", "abstract": "We know very little about how neural language models (LM) use prior linguistic context."}, {"paper_id": "51880056", "adju_relevance": 0, "title": "An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation", "background_label": "AbstractNeural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations. This, however, limits the accuracy and the generalization capability of the models, especially for morphologically-rich languages, which usually have very sparse vocabularies containing rare inflected or derivated word forms. Some studies tried to overcome this problem by segmenting words into subword level representations and modeling translation at this level. However, recent findings have shown that if these methods interrupt the word structure during segmentation, they might cause semantic or syntactic losses and lead to generating inaccurate translations.", "method_label": "In order to investigate this phenomenon, we present an extensive evaluation of two unsupervised vocabulary reduction methods in NMT. The first is the wellknown byte-pair-encoding (BPE), a statistical subword segmentation method, whereas the second is linguistically-motivated vocabulary reduction (LMVR), a segmentation method which also considers morphological properties of subwords.", "result_label": "We compare both approaches on ten translation directions involving English and five other languages (Arabic, Czech, German, Italian and Turkish), each representing a distinct language family and morphological typology. LMVR obtains significantly better performance in most languages, showing gains proportional to the sparseness of the vocabulary and the morphological complexity of the tested language.", "abstract": "AbstractNeural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations. AbstractNeural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations. This, however, limits the accuracy and the generalization capability of the models, especially for morphologically-rich languages, which usually have very sparse vocabularies containing rare inflected or derivated word forms. AbstractNeural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations. This, however, limits the accuracy and the generalization capability of the models, especially for morphologically-rich languages, which usually have very sparse vocabularies containing rare inflected or derivated word forms. Some studies tried to overcome this problem by segmenting words into subword level representations and modeling translation at this level. AbstractNeural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations. This, however, limits the accuracy and the generalization capability of the models, especially for morphologically-rich languages, which usually have very sparse vocabularies containing rare inflected or derivated word forms. Some studies tried to overcome this problem by segmenting words into subword level representations and modeling translation at this level. However, recent findings have shown that if these methods interrupt the word structure during segmentation, they might cause semantic or syntactic losses and lead to generating inaccurate translations. In order to investigate this phenomenon, we present an extensive evaluation of two unsupervised vocabulary reduction methods in NMT. In order to investigate this phenomenon, we present an extensive evaluation of two unsupervised vocabulary reduction methods in NMT. The first is the wellknown byte-pair-encoding (BPE), a statistical subword segmentation method, whereas the second is linguistically-motivated vocabulary reduction (LMVR), a segmentation method which also considers morphological properties of subwords. We compare both approaches on ten translation directions involving English and five other languages (Arabic, Czech, German, Italian and Turkish), each representing a distinct language family and morphological typology. We compare both approaches on ten translation directions involving English and five other languages (Arabic, Czech, German, Italian and Turkish), each representing a distinct language family and morphological typology. LMVR obtains significantly better performance in most languages, showing gains proportional to the sparseness of the vocabulary and the morphological complexity of the tested language."}, {"paper_id": "9442505", "adju_relevance": 0, "title": "Distributional Cues to Word Boundaries: Context is Important", "background_label": "Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). The kinds of statistical regularities studied by Saffran et al. (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003). These facts have led to the proposal that strategies exploiting the statistical patterns found in sound sequences are a crucial first step in bootstrapping word segmentation (Thiessen and Saffran, 2003), and have provoked a great deal of research into statistical word segmentation using both human subjects and computational models. This observation has led to proposals that infants use statistics such as transitional probabilities or mutual information in order to segment words from speech. A number of models have been developed in an attempt to explain how these kinds of statistics can be used procedurally to identify words or word boundaries.", "result_label": "Most previous work on statistical word segmentation is based on the observation that transitions from one syllable or phoneme to the next tend to be less predictable at word boundaries than within words (Harris, 1955; Saffran et al., 1996a).", "abstract": "Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). The kinds of statistical regularities studied by Saffran et al. Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). The kinds of statistical regularities studied by Saffran et al. (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003). Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). The kinds of statistical regularities studied by Saffran et al. (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003). These facts have led to the proposal that strategies exploiting the statistical patterns found in sound sequences are a crucial first step in bootstrapping word segmentation (Thiessen and Saffran, 2003), and have provoked a great deal of research into statistical word segmentation using both human subjects and computational models. Most previous work on statistical word segmentation is based on the observation that transitions from one syllable or phoneme to the next tend to be less predictable at word boundaries than within words (Harris, 1955; Saffran et al., 1996a). Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). The kinds of statistical regularities studied by Saffran et al. (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003). These facts have led to the proposal that strategies exploiting the statistical patterns found in sound sequences are a crucial first step in bootstrapping word segmentation (Thiessen and Saffran, 2003), and have provoked a great deal of research into statistical word segmentation using both human subjects and computational models. This observation has led to proposals that infants use statistics such as transitional probabilities or mutual information in order to segment words from speech. Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language. A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a). The kinds of statistical regularities studied by Saffran et al. (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003). These facts have led to the proposal that strategies exploiting the statistical patterns found in sound sequences are a crucial first step in bootstrapping word segmentation (Thiessen and Saffran, 2003), and have provoked a great deal of research into statistical word segmentation using both human subjects and computational models. This observation has led to proposals that infants use statistics such as transitional probabilities or mutual information in order to segment words from speech. A number of models have been developed in an attempt to explain how these kinds of statistics can be used procedurally to identify words or word boundaries."}, {"paper_id": "8327599", "adju_relevance": 0, "title": "Eugene \u2013 A Domain Specific Language for Specifying and                     Constraining Synthetic Biological Parts, Devices, and Systems", "background_label": "BACKGROUND Synthetic biological systems are currently created by an ad-hoc, iterative process of specification, design, and assembly. These systems would greatly benefit from a more formalized and rigorous specification of the desired system components as well as constraints on their composition. Therefore, the creation of robust and efficient design flows and tools is imperative.", "method_label": "We present a human readable language (Eugene) that allows for the specification of synthetic biological designs based on biological parts, as well as provides a very expressive constraint system to drive the automatic creation of composite Parts (Devices) from a collection of individual Parts.", "result_label": "RESULTS We illustrate Eugene's capabilities in three different areas: Device specification, design space exploration, and assembly and simulation integration. These results highlight Eugene's ability to create combinatorial design spaces and prune these spaces for simulation or physical assembly. Eugene creates functional designs quickly and cost-effectively.", "abstract": "BACKGROUND Synthetic biological systems are currently created by an ad-hoc, iterative process of specification, design, and assembly. BACKGROUND Synthetic biological systems are currently created by an ad-hoc, iterative process of specification, design, and assembly. These systems would greatly benefit from a more formalized and rigorous specification of the desired system components as well as constraints on their composition. BACKGROUND Synthetic biological systems are currently created by an ad-hoc, iterative process of specification, design, and assembly. These systems would greatly benefit from a more formalized and rigorous specification of the desired system components as well as constraints on their composition. Therefore, the creation of robust and efficient design flows and tools is imperative. We present a human readable language (Eugene) that allows for the specification of synthetic biological designs based on biological parts, as well as provides a very expressive constraint system to drive the automatic creation of composite Parts (Devices) from a collection of individual Parts. RESULTS We illustrate Eugene's capabilities in three different areas: Device specification, design space exploration, and assembly and simulation integration. RESULTS We illustrate Eugene's capabilities in three different areas: Device specification, design space exploration, and assembly and simulation integration. These results highlight Eugene's ability to create combinatorial design spaces and prune these spaces for simulation or physical assembly. RESULTS We illustrate Eugene's capabilities in three different areas: Device specification, design space exploration, and assembly and simulation integration. These results highlight Eugene's ability to create combinatorial design spaces and prune these spaces for simulation or physical assembly. Eugene creates functional designs quickly and cost-effectively."}, {"paper_id": "41924426", "adju_relevance": 0, "title": "Simulation of biological evolution and machine learning. I. Selection of self-reproducing numeric patterns by data processing machines, effects of hereditary control, mutation type and crossing.", "background_label": "Abstract The effect of crossing and of different types of mutations (or genetic control) on the speed of selective adaptation has been investigated by data processing machines. The results obtained can be summarized as follows: under conditions simulating regular Mendelian (non-polygenic nor quantitative) inheritance, crossing greatly enhances the speed of selective adaptation, particularly if interaction between the expressions of different hereditary factors is avoided; under conditions simulating polygenic control of quantitative characters, crossing does not enhance the speed of selective adaptation.", "method_label": "The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. Some of the properties represented crossing, mutation or reproductive characteristics of the pattern. Some other properties represented other characteristics essential for the pattern's ability to survive and reproduce, for example, by identifying the pattern's strategy in a game of poker used as a means to select the patterns to be reproduced. Furthermore, in a period of rapid selective adaptation the ability to interbreed spreads rapidly in the population as a positively selected characteristic when conditions simulating regular Mendelian inheritance are used. On the other hand, the breeding characteristic spreads less rapidly or not at all under conditions simulating quantitative characters under polygenic control. In the simple game arrangements used in our adaptive selection experiments, the patterns were very often able to develop an optimum game strategy.", "result_label": "A measure of the speed of improvement as a function of generation number has been defined and used to compare the improvement under various conditions. However, in some instances a \u201cselective instability\u201d characterized by statistical fluctuations preventing the achievement of an optimum game strategy was developed. Selective instability, if not properly controlled, is likely to assume larger proportions in experiments of a more complicated nature, and may constitute a serious problem for the practical applications of adaptive selection methods by data processing machines.", "abstract": "Abstract The effect of crossing and of different types of mutations (or genetic control) on the speed of selective adaptation has been investigated by data processing machines. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. Some of the properties represented crossing, mutation or reproductive characteristics of the pattern. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. Some of the properties represented crossing, mutation or reproductive characteristics of the pattern. Some other properties represented other characteristics essential for the pattern's ability to survive and reproduce, for example, by identifying the pattern's strategy in a game of poker used as a means to select the patterns to be reproduced. A measure of the speed of improvement as a function of generation number has been defined and used to compare the improvement under various conditions. Abstract The effect of crossing and of different types of mutations (or genetic control) on the speed of selective adaptation has been investigated by data processing machines. The results obtained can be summarized as follows: under conditions simulating regular Mendelian (non-polygenic nor quantitative) inheritance, crossing greatly enhances the speed of selective adaptation, particularly if interaction between the expressions of different hereditary factors is avoided; under conditions simulating polygenic control of quantitative characters, crossing does not enhance the speed of selective adaptation. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. Some of the properties represented crossing, mutation or reproductive characteristics of the pattern. Some other properties represented other characteristics essential for the pattern's ability to survive and reproduce, for example, by identifying the pattern's strategy in a game of poker used as a means to select the patterns to be reproduced. Furthermore, in a period of rapid selective adaptation the ability to interbreed spreads rapidly in the population as a positively selected characteristic when conditions simulating regular Mendelian inheritance are used. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. Some of the properties represented crossing, mutation or reproductive characteristics of the pattern. Some other properties represented other characteristics essential for the pattern's ability to survive and reproduce, for example, by identifying the pattern's strategy in a game of poker used as a means to select the patterns to be reproduced. Furthermore, in a period of rapid selective adaptation the ability to interbreed spreads rapidly in the population as a positively selected characteristic when conditions simulating regular Mendelian inheritance are used. On the other hand, the breeding characteristic spreads less rapidly or not at all under conditions simulating quantitative characters under polygenic control. The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information. Each number or bit was used to identify a property of the pattern, which could be subject to selection. Some of the properties represented crossing, mutation or reproductive characteristics of the pattern. Some other properties represented other characteristics essential for the pattern's ability to survive and reproduce, for example, by identifying the pattern's strategy in a game of poker used as a means to select the patterns to be reproduced. Furthermore, in a period of rapid selective adaptation the ability to interbreed spreads rapidly in the population as a positively selected characteristic when conditions simulating regular Mendelian inheritance are used. On the other hand, the breeding characteristic spreads less rapidly or not at all under conditions simulating quantitative characters under polygenic control. In the simple game arrangements used in our adaptive selection experiments, the patterns were very often able to develop an optimum game strategy. A measure of the speed of improvement as a function of generation number has been defined and used to compare the improvement under various conditions. However, in some instances a \u201cselective instability\u201d characterized by statistical fluctuations preventing the achievement of an optimum game strategy was developed. A measure of the speed of improvement as a function of generation number has been defined and used to compare the improvement under various conditions. However, in some instances a \u201cselective instability\u201d characterized by statistical fluctuations preventing the achievement of an optimum game strategy was developed. Selective instability, if not properly controlled, is likely to assume larger proportions in experiments of a more complicated nature, and may constitute a serious problem for the practical applications of adaptive selection methods by data processing machines."}, {"paper_id": "114655108", "adju_relevance": 0, "title": "Evaluation of Off-The-Shelf CNNs for the Representation of Natural Scenes with Large Seasonal Variations", "abstract": ""}, {"paper_id": "46761158", "adju_relevance": 0, "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks", "background_label": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb\"dax,\"he or she can immediately understand the meaning of\"dax twice\"or\"sing and dax.", "method_label": "\"In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply\"mix-and-match\"strategies to solve the task.", "result_label": "However, when generalization requires systematic compositional skills (as in the\"dax\"example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.", "abstract": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb\"dax,\"he or she can immediately understand the meaning of\"dax twice\"or\"sing and dax. \"In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. \"In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. \"In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply\"mix-and-match\"strategies to solve the task. However, when generalization requires systematic compositional skills (as in the\"dax\"example above), RNNs fail spectacularly. However, when generalization requires systematic compositional skills (as in the\"dax\"example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst."}, {"paper_id": "18874161", "adju_relevance": 0, "title": "Syntactic Islands and Learning Biases: Combining Experimental Syntax and Computational Modeling to Investigate the Language Acquisition Problem", "background_label": "The induction problems facing language learners have played a central role in debates about the types of learning biases that exist in the human brain. Many linguists have argued that some of the learning biases necessary to solve these language induction problems must be both innate and language-specific (i.e., the Universal Grammar (UG) hypothesis). Though there have been several recent high-profile investigations of the necessary learning bias types for different linguistic phenomena, the UG hypothesis is still the dominant assumption for a large segment of linguists due to the lack of studies addressing central phenomena in generative linguistics.", "abstract": "The induction problems facing language learners have played a central role in debates about the types of learning biases that exist in the human brain. The induction problems facing language learners have played a central role in debates about the types of learning biases that exist in the human brain. Many linguists have argued that some of the learning biases necessary to solve these language induction problems must be both innate and language-specific (i.e., the Universal Grammar (UG) hypothesis). The induction problems facing language learners have played a central role in debates about the types of learning biases that exist in the human brain. Many linguists have argued that some of the learning biases necessary to solve these language induction problems must be both innate and language-specific (i.e., the Universal Grammar (UG) hypothesis). Though there have been several recent high-profile investigations of the necessary learning bias types for different linguistic phenomena, the UG hypothesis is still the dominant assumption for a large segment of linguists due to the lack of studies addressing central phenomena in generative linguistics."}, {"paper_id": "5531014", "adju_relevance": 0, "title": "Lemmatization and Lexicalized Statistical Parsing of Morphologically-Rich Languages: the Case of French", "background_label": "AbstractThis paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results.", "method_label": "We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used.", "result_label": "This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words.", "abstract": "AbstractThis paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results. We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used. This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words."}, {"paper_id": "2146123", "adju_relevance": 0, "title": "Learning phonology with substantive bias: An experimental and computational study of velar palatalization", "background_label": "There is an active debate within the field of phonology concerning the cognitive status of substantive phonetic factors such as ease of articulation and perceptual distinctiveness.", "abstract": "There is an active debate within the field of phonology concerning the cognitive status of substantive phonetic factors such as ease of articulation and perceptual distinctiveness."}, {"paper_id": "199661323", "adju_relevance": 0, "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "background_label": "A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources.", "method_label": "We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP.", "result_label": "In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.", "abstract": " A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge."}, {"paper_id": "115176295", "adju_relevance": 0, "title": "Order of subject, object, and verb", "background_label": "Fuel cell elements.", "abstract": " Fuel cell elements."}, {"paper_id": "3238248", "adju_relevance": 0, "title": "Combining Shallow and Linguistically Motivated Features in Native Language Identification", "background_label": "AbstractWe explore a range of features and ensembles for the task of Native Language Identification as part of the NLI Shared Task .", "method_label": "Starting with recurring word-based ngrams (Bykh and Meurers, 2012), we tested different linguistic abstractions such as partof-speech, dependencies, and syntactic trees as features for NLI. We also experimented with features encoding morphological properties, the nature of the realizations of particular lemmas, and several measures of complexity developed for proficiency and readability classification (Vajjala and Meurers, 2012) . Employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2% (rank 5) in the closed task and 83.5% (rank 1) in the open-2 task.", "result_label": "In the open-1 task, the word-based recurring ngrams outperformed the ensemble, yielding 38.5% (rank 2). Overall, across all three tasks, our best accuracy of 83.5% for the standard TOEFL11 test set came in second place.", "abstract": "AbstractWe explore a range of features and ensembles for the task of Native Language Identification as part of the NLI Shared Task . Starting with recurring word-based ngrams (Bykh and Meurers, 2012), we tested different linguistic abstractions such as partof-speech, dependencies, and syntactic trees as features for NLI. Starting with recurring word-based ngrams (Bykh and Meurers, 2012), we tested different linguistic abstractions such as partof-speech, dependencies, and syntactic trees as features for NLI. We also experimented with features encoding morphological properties, the nature of the realizations of particular lemmas, and several measures of complexity developed for proficiency and readability classification (Vajjala and Meurers, 2012) . Starting with recurring word-based ngrams (Bykh and Meurers, 2012), we tested different linguistic abstractions such as partof-speech, dependencies, and syntactic trees as features for NLI. We also experimented with features encoding morphological properties, the nature of the realizations of particular lemmas, and several measures of complexity developed for proficiency and readability classification (Vajjala and Meurers, 2012) . Employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2% (rank 5) in the closed task and 83.5% (rank 1) in the open-2 task. In the open-1 task, the word-based recurring ngrams outperformed the ensemble, yielding 38.5% (rank 2). In the open-1 task, the word-based recurring ngrams outperformed the ensemble, yielding 38.5% (rank 2). Overall, across all three tasks, our best accuracy of 83.5% for the standard TOEFL11 test set came in second place."}, {"paper_id": "1656805", "adju_relevance": 0, "title": "A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax", "background_label": "AbstractA traditional claim in linguistics is that all human languages are equally expressiveable to convey the same wide range of meanings.Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues.", "method_label": "We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech.", "result_label": "The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.", "abstract": "AbstractA traditional claim in linguistics is that all human languages are equally expressiveable to convey the same wide range of meanings.Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. AbstractA traditional claim in linguistics is that all human languages are equally expressiveable to convey the same wide range of meanings.Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation."}, {"paper_id": "14236681", "adju_relevance": 0, "title": "Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling", "background_label": "A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores.", "method_label": "In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language.", "result_label": "Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments.", "abstract": "A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores. In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language. Our analysis reveals that human judgments are strongly impacted by the judgment language. Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments."}, {"paper_id": "1356419", "adju_relevance": 0, "title": "The Role of Verbs in Document Analysis", "background_label": "We present results of two methods for assessing the event profile of news articles as a function of verb type.", "abstract": "We present results of two methods for assessing the event profile of news articles as a function of verb type."}, {"paper_id": "9530012", "adju_relevance": 0, "title": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning", "background_label": "We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction.", "abstract": "We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction."}, {"paper_id": "8364931", "adju_relevance": 0, "title": "NeuroML: A Language for Describing Data Driven Models of Neurons and Networks with a High Degree of Biological Detail", "background_label": "Biologically detailed single neuron and network models are important for understanding how ion channels, synapses and anatomical connectivity underlie the complex electrical behavior of the brain. While neuronal simulators such as NEURON, GENESIS, MOOSE, NEST, and PSICS facilitate the development of these data-driven neuronal models, the specialized languages they employ are generally not interoperable, limiting model accessibility and preventing reuse of model components and cross-simulator validation.", "method_label": "To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language). This enables these detailed models and their components to be defined in a standalone form, allowing them to be used across multiple simulators and archived in a standardized format. Here we describe the structure of NeuroML and demonstrate its scope by converting into NeuroML models of a number of different voltage- and ligand-gated conductances, models of electrical coupling, synaptic transmission and short-term plasticity, together with morphologically detailed models of individual neurons. We have also used these NeuroML-based components to develop an highly detailed cortical network model. NeuroML-based model descriptions were validated by demonstrating similar model behavior across five independently developed simulators.", "result_label": "Although our results confirm that simulations run on different simulators converge, they reveal limits to model interoperability, by showing that for some models convergence only occurs at high levels of spatial and temporal discretisation, when the computational overhead is high. Our development of NeuroML as a common description language for biophysically detailed neuronal and network models enables interoperability across multiple simulation environments, thereby improving model transparency, accessibility and reuse in computational neuroscience.", "abstract": "Biologically detailed single neuron and network models are important for understanding how ion channels, synapses and anatomical connectivity underlie the complex electrical behavior of the brain. Biologically detailed single neuron and network models are important for understanding how ion channels, synapses and anatomical connectivity underlie the complex electrical behavior of the brain. While neuronal simulators such as NEURON, GENESIS, MOOSE, NEST, and PSICS facilitate the development of these data-driven neuronal models, the specialized languages they employ are generally not interoperable, limiting model accessibility and preventing reuse of model components and cross-simulator validation. To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language). To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language). This enables these detailed models and their components to be defined in a standalone form, allowing them to be used across multiple simulators and archived in a standardized format. To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language). This enables these detailed models and their components to be defined in a standalone form, allowing them to be used across multiple simulators and archived in a standardized format. Here we describe the structure of NeuroML and demonstrate its scope by converting into NeuroML models of a number of different voltage- and ligand-gated conductances, models of electrical coupling, synaptic transmission and short-term plasticity, together with morphologically detailed models of individual neurons. To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language). This enables these detailed models and their components to be defined in a standalone form, allowing them to be used across multiple simulators and archived in a standardized format. Here we describe the structure of NeuroML and demonstrate its scope by converting into NeuroML models of a number of different voltage- and ligand-gated conductances, models of electrical coupling, synaptic transmission and short-term plasticity, together with morphologically detailed models of individual neurons. We have also used these NeuroML-based components to develop an highly detailed cortical network model. To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language). This enables these detailed models and their components to be defined in a standalone form, allowing them to be used across multiple simulators and archived in a standardized format. Here we describe the structure of NeuroML and demonstrate its scope by converting into NeuroML models of a number of different voltage- and ligand-gated conductances, models of electrical coupling, synaptic transmission and short-term plasticity, together with morphologically detailed models of individual neurons. We have also used these NeuroML-based components to develop an highly detailed cortical network model. NeuroML-based model descriptions were validated by demonstrating similar model behavior across five independently developed simulators. Although our results confirm that simulations run on different simulators converge, they reveal limits to model interoperability, by showing that for some models convergence only occurs at high levels of spatial and temporal discretisation, when the computational overhead is high. Although our results confirm that simulations run on different simulators converge, they reveal limits to model interoperability, by showing that for some models convergence only occurs at high levels of spatial and temporal discretisation, when the computational overhead is high. Our development of NeuroML as a common description language for biophysically detailed neuronal and network models enables interoperability across multiple simulation environments, thereby improving model transparency, accessibility and reuse in computational neuroscience."}, {"paper_id": "119989792", "adju_relevance": 0, "title": "Language clusters based on linguistic complex networks", "method_label": "The results show that it is possible to classify human languages by means of the following main parameters of complex networks: (a) average degree of the node, (b) cluster coefficients, (c) average path length, (d) network centralization, (e) diameter, (f) power exponent of degree distribution, and (g) the determination coefficient of power law distributions. The precision of this method is similar to the results achieved by means of modern word order typology.", "abstract": " The results show that it is possible to classify human languages by means of the following main parameters of complex networks: (a) average degree of the node, (b) cluster coefficients, (c) average path length, (d) network centralization, (e) diameter, (f) power exponent of degree distribution, and (g) the determination coefficient of power law distributions. The results show that it is possible to classify human languages by means of the following main parameters of complex networks: (a) average degree of the node, (b) cluster coefficients, (c) average path length, (d) network centralization, (e) diameter, (f) power exponent of degree distribution, and (g) the determination coefficient of power law distributions. The precision of this method is similar to the results achieved by means of modern word order typology."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "16874763", "adju_relevance": 0, "title": "Explaining the hierarchy of visual representational geometries by remixing of features from many computational vision models", "background_label": "AbstractVisual processing in cortex happens through a hierarchy of increasingly sophisticated representations. Here we explore a very wide range of model representations (29 models), testing their categorization performance (animate/inanimate) and their ability to account for the representational geometry of brain regions along the visual hierarchy (V1, V2, V3, V4, and LO). The model features are linearly remixed so as to best explain the voxel responses (as in voxel/population receptive-field modelling). This new approach of combining RSA with voxel receptive field modelling may help bridge the gap between the two methods.", "method_label": "We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model. The GWP implementations we used performed similarly with and without remixing, suggesting that the original features already approximate the representational space, obviating the need for remixing or reweighting. The lateral occipital region (LO), a higher visual representation, was best explained by the higher layers of a deep convolutional network (Krizhevsky et al., 2012) . However, this model could explain the LO representation only after appropriate remixing of its feature set. Remixed RSA takes a step in an important direction, where each computational model representation is explored more broadly by considering not only its representational geometry, but the set of all geometries within reach of a linear transform.", "result_label": "Voxel responses are predicted by linear combinations of the model features. The exploration of many models and many brain areas may lead to a better understanding of the processing stages in the visual hierarchy, from low-level image representations in V1 to visuo-semantic representations in higher-level visual areas.", "abstract": "AbstractVisual processing in cortex happens through a hierarchy of increasingly sophisticated representations. AbstractVisual processing in cortex happens through a hierarchy of increasingly sophisticated representations. Here we explore a very wide range of model representations (29 models), testing their categorization performance (animate/inanimate) and their ability to account for the representational geometry of brain regions along the visual hierarchy (V1, V2, V3, V4, and LO). We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. Voxel responses are predicted by linear combinations of the model features. AbstractVisual processing in cortex happens through a hierarchy of increasingly sophisticated representations. Here we explore a very wide range of model representations (29 models), testing their categorization performance (animate/inanimate) and their ability to account for the representational geometry of brain regions along the visual hierarchy (V1, V2, V3, V4, and LO). The model features are linearly remixed so as to best explain the voxel responses (as in voxel/population receptive-field modelling). AbstractVisual processing in cortex happens through a hierarchy of increasingly sophisticated representations. Here we explore a very wide range of model representations (29 models), testing their categorization performance (animate/inanimate) and their ability to account for the representational geometry of brain regions along the visual hierarchy (V1, V2, V3, V4, and LO). The model features are linearly remixed so as to best explain the voxel responses (as in voxel/population receptive-field modelling). This new approach of combining RSA with voxel receptive field modelling may help bridge the gap between the two methods. We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model. We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model. The GWP implementations we used performed similarly with and without remixing, suggesting that the original features already approximate the representational space, obviating the need for remixing or reweighting. We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model. The GWP implementations we used performed similarly with and without remixing, suggesting that the original features already approximate the representational space, obviating the need for remixing or reweighting. The lateral occipital region (LO), a higher visual representation, was best explained by the higher layers of a deep convolutional network (Krizhevsky et al., 2012) . We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model. The GWP implementations we used performed similarly with and without remixing, suggesting that the original features already approximate the representational space, obviating the need for remixing or reweighting. The lateral occipital region (LO), a higher visual representation, was best explained by the higher layers of a deep convolutional network (Krizhevsky et al., 2012) . However, this model could explain the LO representation only after appropriate remixing of its feature set. We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features. Reweighting and remixing was based on brain responses to an independent training set of 1750 images. We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM). In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses. We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model. The GWP implementations we used performed similarly with and without remixing, suggesting that the original features already approximate the representational space, obviating the need for remixing or reweighting. The lateral occipital region (LO), a higher visual representation, was best explained by the higher layers of a deep convolutional network (Krizhevsky et al., 2012) . However, this model could explain the LO representation only after appropriate remixing of its feature set. Remixed RSA takes a step in an important direction, where each computational model representation is explored more broadly by considering not only its representational geometry, but the set of all geometries within reach of a linear transform. Voxel responses are predicted by linear combinations of the model features. The exploration of many models and many brain areas may lead to a better understanding of the processing stages in the visual hierarchy, from low-level image representations in V1 to visuo-semantic representations in higher-level visual areas."}, {"paper_id": "198147626", "adju_relevance": 0, "title": "Learning cross-lingual phonological and orthagraphic adaptations: a case study in improving neural machine translation between low-resource languages", "background_label": "Out-of-vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for low-resource language (LRL) pairs, i.e., language pairs for which few or no parallel corpora exist.", "abstract": "Out-of-vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for low-resource language (LRL) pairs, i.e., language pairs for which few or no parallel corpora exist."}, {"paper_id": "2244960", "adju_relevance": 0, "title": "Tree-structured composition in neural networks without tree-structured architectures", "background_label": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models.", "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models."}, {"paper_id": "56015860", "adju_relevance": 0, "title": "Cross-linguistic analysis of discourse variation across registers", "background_label": "The present study deals with variation in discourse relations in different registers of English and German. Our previous analyses have been concerned with the systemic contrasts between English and German, cf. Kunz & Steiner (2013 a/b), Kunz & Lapshinova (to appear) and have addressed some cross-linguistic differences with regard to textual realizations of selected subtypes of cohesion.", "abstract": "The present study deals with variation in discourse relations in different registers of English and German. The present study deals with variation in discourse relations in different registers of English and German. Our previous analyses have been concerned with the systemic contrasts between English and German, cf. The present study deals with variation in discourse relations in different registers of English and German. Our previous analyses have been concerned with the systemic contrasts between English and German, cf. Kunz & Steiner (2013 a/b), Kunz & Lapshinova (to appear) and have addressed some cross-linguistic differences with regard to textual realizations of selected subtypes of cohesion."}, {"paper_id": "9558272", "adju_relevance": 0, "title": "Contrasting intrusion profiles for agreement and anaphora: Experimental and modeling evidence", "background_label": "We investigated the relationship between linguistic representation and memory access by comparing the processing of two linguistic dependencies that require comprehenders to check that the subject of the current clause has the correct morphological features: subject\u2013verb agreement and reflexive anaphors in English.", "method_label": "In two eye-tracking experiments we examined the impact of structurally illicit noun phrases on the computation of reflexive and subject\u2013verb agreement. Experiment 1 directly compared the two dependencies within participants.", "result_label": "Results show a clear difference in the intrusion profile associated with each dependency: agreement resolution displays clear intrusion effects in comprehension (as found by Pearlmutter, Garnsey, & Bock, 1999; Wagers, Lau, & Phillips, 2009), but reflexives show no such intrusion effect from illicit antecedents (Sturt, 2003; Xiang, Dillon, & Phillips, 2009). Experiment 2 replicated the lack of intrusion for reflexives, confirming the reliability of the pattern and examining a wider range of feature combinations. In addition, we present modeling evidence that suggests that the reflexive results are best captured by a memory retrieval mechanism that uses primarily syntactic information to guide retrievals for the anaphor\u2019s antecedent, in contrast to the mixed morphological and syntactic cues used resolve subject\u2013verb agreement dependencies. Despite the fact that agreement and reflexive dependencies are subject to a similar morphological agreement constraint, in online processing comprehenders appear to implement this constraint in distinct ways for the two dependencies.", "abstract": "We investigated the relationship between linguistic representation and memory access by comparing the processing of two linguistic dependencies that require comprehenders to check that the subject of the current clause has the correct morphological features: subject\u2013verb agreement and reflexive anaphors in English. In two eye-tracking experiments we examined the impact of structurally illicit noun phrases on the computation of reflexive and subject\u2013verb agreement. In two eye-tracking experiments we examined the impact of structurally illicit noun phrases on the computation of reflexive and subject\u2013verb agreement. Experiment 1 directly compared the two dependencies within participants. Results show a clear difference in the intrusion profile associated with each dependency: agreement resolution displays clear intrusion effects in comprehension (as found by Pearlmutter, Garnsey, & Bock, 1999; Wagers, Lau, & Phillips, 2009), but reflexives show no such intrusion effect from illicit antecedents (Sturt, 2003; Xiang, Dillon, & Phillips, 2009). Results show a clear difference in the intrusion profile associated with each dependency: agreement resolution displays clear intrusion effects in comprehension (as found by Pearlmutter, Garnsey, & Bock, 1999; Wagers, Lau, & Phillips, 2009), but reflexives show no such intrusion effect from illicit antecedents (Sturt, 2003; Xiang, Dillon, & Phillips, 2009). Experiment 2 replicated the lack of intrusion for reflexives, confirming the reliability of the pattern and examining a wider range of feature combinations. Results show a clear difference in the intrusion profile associated with each dependency: agreement resolution displays clear intrusion effects in comprehension (as found by Pearlmutter, Garnsey, & Bock, 1999; Wagers, Lau, & Phillips, 2009), but reflexives show no such intrusion effect from illicit antecedents (Sturt, 2003; Xiang, Dillon, & Phillips, 2009). Experiment 2 replicated the lack of intrusion for reflexives, confirming the reliability of the pattern and examining a wider range of feature combinations. In addition, we present modeling evidence that suggests that the reflexive results are best captured by a memory retrieval mechanism that uses primarily syntactic information to guide retrievals for the anaphor\u2019s antecedent, in contrast to the mixed morphological and syntactic cues used resolve subject\u2013verb agreement dependencies. Results show a clear difference in the intrusion profile associated with each dependency: agreement resolution displays clear intrusion effects in comprehension (as found by Pearlmutter, Garnsey, & Bock, 1999; Wagers, Lau, & Phillips, 2009), but reflexives show no such intrusion effect from illicit antecedents (Sturt, 2003; Xiang, Dillon, & Phillips, 2009). Experiment 2 replicated the lack of intrusion for reflexives, confirming the reliability of the pattern and examining a wider range of feature combinations. In addition, we present modeling evidence that suggests that the reflexive results are best captured by a memory retrieval mechanism that uses primarily syntactic information to guide retrievals for the anaphor\u2019s antecedent, in contrast to the mixed morphological and syntactic cues used resolve subject\u2013verb agreement dependencies. Despite the fact that agreement and reflexive dependencies are subject to a similar morphological agreement constraint, in online processing comprehenders appear to implement this constraint in distinct ways for the two dependencies."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "17983355", "adju_relevance": 0, "title": "Gradient Grammar: An Effect of Animacy on the Syntax of give in New Zealand and American English", "background_label": "Abstract Bresnan et al. (2007) show that a statistical model can predict United States (US) English speakers\u2019 syntactic choices with \u2018give\u2019-type verbs extremely accurately. They argue that these results are consistent with probabilistic models of grammar, which assume that grammar is quantitive, and learned from exposure to other speakers. Such a model would also predict syntactic differences across time and space which are reflected not only in the use of clear dialectal features or clear-cut changes in progress, but also in subtle factors such as the relative importance of conditioning factors, and changes over time in speakers\u2019 preferences between equally well-formed variants.", "abstract": "Abstract Bresnan et al. Abstract Bresnan et al. (2007) show that a statistical model can predict United States (US) English speakers\u2019 syntactic choices with \u2018give\u2019-type verbs extremely accurately. Abstract Bresnan et al. (2007) show that a statistical model can predict United States (US) English speakers\u2019 syntactic choices with \u2018give\u2019-type verbs extremely accurately. They argue that these results are consistent with probabilistic models of grammar, which assume that grammar is quantitive, and learned from exposure to other speakers. Abstract Bresnan et al. (2007) show that a statistical model can predict United States (US) English speakers\u2019 syntactic choices with \u2018give\u2019-type verbs extremely accurately. They argue that these results are consistent with probabilistic models of grammar, which assume that grammar is quantitive, and learned from exposure to other speakers. Such a model would also predict syntactic differences across time and space which are reflected not only in the use of clear dialectal features or clear-cut changes in progress, but also in subtle factors such as the relative importance of conditioning factors, and changes over time in speakers\u2019 preferences between equally well-formed variants."}, {"paper_id": "122766670", "adju_relevance": 0, "title": "The distributional structure of grammatical categories in speech to young children", "background_label": "We present a series of three analyses of young children\u2019s linguistic input to determine the distributional information it could plausibly offer to the process of grammatical category learning. Each analysis was conducted on four separate corpora from the CHILDES database (MacWhinney, 2000) of speech directed to children under 2;5.", "method_label": "We show that, in accord with other findings, a distributional analysis which categorizes words based on their co-occurrence patterns with surrounding words successfully categorizes the majority of nouns and verbs. In Analyses 2 and 3, we attempt to make our analyses more closely relevant to natural language acquisition by adopting more realistic assumptions about how young children represent their input. In Analysis 2, we limit the distributional context by imposing phrase structure boundaries, and find that categorization improves even beyond that obtained from less limited contexts.", "result_label": "In Analysis 3, we reduce the representation of input elements which young children might not fully process and we find that categorization is not adversely affected: Although noun categorization is worse than in Analyses 1 and 2, it is still good; and verb categorization actually improves. Overall, successful categorization of nouns and verbs is maintained across all analyses. These results provide promising support for theories of grammatical category formation involving distributional analysis, as long as these analyses are combined with appropriate assumptions about the child learner\u2019s computational biases and capabilities.", "abstract": "We present a series of three analyses of young children\u2019s linguistic input to determine the distributional information it could plausibly offer to the process of grammatical category learning. We present a series of three analyses of young children\u2019s linguistic input to determine the distributional information it could plausibly offer to the process of grammatical category learning. Each analysis was conducted on four separate corpora from the CHILDES database (MacWhinney, 2000) of speech directed to children under 2;5. We show that, in accord with other findings, a distributional analysis which categorizes words based on their co-occurrence patterns with surrounding words successfully categorizes the majority of nouns and verbs. We show that, in accord with other findings, a distributional analysis which categorizes words based on their co-occurrence patterns with surrounding words successfully categorizes the majority of nouns and verbs. In Analyses 2 and 3, we attempt to make our analyses more closely relevant to natural language acquisition by adopting more realistic assumptions about how young children represent their input. We show that, in accord with other findings, a distributional analysis which categorizes words based on their co-occurrence patterns with surrounding words successfully categorizes the majority of nouns and verbs. In Analyses 2 and 3, we attempt to make our analyses more closely relevant to natural language acquisition by adopting more realistic assumptions about how young children represent their input. In Analysis 2, we limit the distributional context by imposing phrase structure boundaries, and find that categorization improves even beyond that obtained from less limited contexts. In Analysis 3, we reduce the representation of input elements which young children might not fully process and we find that categorization is not adversely affected: Although noun categorization is worse than in Analyses 1 and 2, it is still good; and verb categorization actually improves. In Analysis 3, we reduce the representation of input elements which young children might not fully process and we find that categorization is not adversely affected: Although noun categorization is worse than in Analyses 1 and 2, it is still good; and verb categorization actually improves. Overall, successful categorization of nouns and verbs is maintained across all analyses. In Analysis 3, we reduce the representation of input elements which young children might not fully process and we find that categorization is not adversely affected: Although noun categorization is worse than in Analyses 1 and 2, it is still good; and verb categorization actually improves. Overall, successful categorization of nouns and verbs is maintained across all analyses. These results provide promising support for theories of grammatical category formation involving distributional analysis, as long as these analyses are combined with appropriate assumptions about the child learner\u2019s computational biases and capabilities."}, {"paper_id": "13926782", "adju_relevance": 0, "title": "Morphological features help POS tagging of unknown words across language varieties", "background_label": "AbstractPart-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties. We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRCHong Kong, and Taiwan). An analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging.", "method_label": "Unknown words in English tend to be proper nouns. By contrast, we found that Mandarin unknown words were mostly common nouns and verbs. Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German. Our features were implemented in a maximum entropy Markov model.", "result_label": "We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English. Our system achieves state-of-the-art performance in Mandarin tagging, including improving unknown-word tagging performance on unseen varieties in Chinese Treebank 5.0 from 61% to 80% correct.", "abstract": "AbstractPart-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties. AbstractPart-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties. We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRCHong Kong, and Taiwan). AbstractPart-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties. We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRCHong Kong, and Taiwan). An analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging. Unknown words in English tend to be proper nouns. Unknown words in English tend to be proper nouns. By contrast, we found that Mandarin unknown words were mostly common nouns and verbs. We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English. Unknown words in English tend to be proper nouns. By contrast, we found that Mandarin unknown words were mostly common nouns and verbs. Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German. Unknown words in English tend to be proper nouns. By contrast, we found that Mandarin unknown words were mostly common nouns and verbs. Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German. Our features were implemented in a maximum entropy Markov model. We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English. Our system achieves state-of-the-art performance in Mandarin tagging, including improving unknown-word tagging performance on unseen varieties in Chinese Treebank 5.0 from 61% to 80% correct."}, {"paper_id": "3638905", "adju_relevance": 0, "title": "Learning Inductive Biases with Simple Neural Networks", "background_label": "People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as\"inductive biases\"- pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning. However, these networks use unrealistically large quantities of training data, and the conditions required for these biases to develop are not well understood. Moreover, it is unclear how the learning dynamics of these networks relate to developmental processes in childhood.", "method_label": "We investigate the development and influence of the shape bias in neural networks using controlled datasets of abstract patterns and synthetic images, allowing us to systematically vary the quantity and form of the experience provided to the learning algorithms.", "result_label": "We find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories. The development of these biases predicts the onset of vocabulary acceleration in our networks, consistent with the developmental process in children.", "abstract": "People use rich prior knowledge about the world in order to efficiently learn new concepts. People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as\"inductive biases\"- pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as\"inductive biases\"- pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning. People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as\"inductive biases\"- pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning. However, these networks use unrealistically large quantities of training data, and the conditions required for these biases to develop are not well understood. People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as\"inductive biases\"- pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning. However, these networks use unrealistically large quantities of training data, and the conditions required for these biases to develop are not well understood. Moreover, it is unclear how the learning dynamics of these networks relate to developmental processes in childhood. We investigate the development and influence of the shape bias in neural networks using controlled datasets of abstract patterns and synthetic images, allowing us to systematically vary the quantity and form of the experience provided to the learning algorithms. We find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories. We find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories. The development of these biases predicts the onset of vocabulary acceleration in our networks, consistent with the developmental process in children."}, {"paper_id": "86492460", "adju_relevance": 0, "title": "On Inductive Biases in Deep Reinforcement Learning", "background_label": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively.", "abstract": "Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively."}, {"paper_id": "52213711", "adju_relevance": 0, "title": "Thai EFL Students' Writing Errors in Different Text Types: The Interference of the First Language", "method_label": "120 English paragraphs written by 40 second year English major students were analyzed by using Error Analysis (EA).The results revealed that the first language interference errors fell into 16 categories: verb tense, word choice, sentence structure, article, preposition, modal/auxiliary, singular/plural form, fragment, verb form, pronoun, run-on sentence, infinitive/gerund, transition, subject-verb agreement, parallel structure, and comparison structure, respectively, and the number of frequent errors made in each type of written tasks was apparently different.", "result_label": "In narration, the five most frequent errors found were verb tense, word choice, sentence structure, preposition, and modal/auxiliary, respectively, while the five most frequent errors in description and comparison/contrast were article, sentence structure, word choice, singular/plural form, and subject-verb agreement, respectively. Interestingly, in the narrative and descriptive paragraphs, comparison structure was found to be the least frequent error, whereas it became the 10th frequent error in comparison/contrast writing. It was apparent that a genre did affect writing errors as different text types required different structural features. It could be concluded that to enhance students\u2019 grammatical and lexical accuracy, a second language (L2) writing teacher should take into consideration L1 interference categories in different genres.", "abstract": " 120 English paragraphs written by 40 second year English major students were analyzed by using Error Analysis (EA).The results revealed that the first language interference errors fell into 16 categories: verb tense, word choice, sentence structure, article, preposition, modal/auxiliary, singular/plural form, fragment, verb form, pronoun, run-on sentence, infinitive/gerund, transition, subject-verb agreement, parallel structure, and comparison structure, respectively, and the number of frequent errors made in each type of written tasks was apparently different. In narration, the five most frequent errors found were verb tense, word choice, sentence structure, preposition, and modal/auxiliary, respectively, while the five most frequent errors in description and comparison/contrast were article, sentence structure, word choice, singular/plural form, and subject-verb agreement, respectively. In narration, the five most frequent errors found were verb tense, word choice, sentence structure, preposition, and modal/auxiliary, respectively, while the five most frequent errors in description and comparison/contrast were article, sentence structure, word choice, singular/plural form, and subject-verb agreement, respectively. Interestingly, in the narrative and descriptive paragraphs, comparison structure was found to be the least frequent error, whereas it became the 10th frequent error in comparison/contrast writing. In narration, the five most frequent errors found were verb tense, word choice, sentence structure, preposition, and modal/auxiliary, respectively, while the five most frequent errors in description and comparison/contrast were article, sentence structure, word choice, singular/plural form, and subject-verb agreement, respectively. Interestingly, in the narrative and descriptive paragraphs, comparison structure was found to be the least frequent error, whereas it became the 10th frequent error in comparison/contrast writing. It was apparent that a genre did affect writing errors as different text types required different structural features. In narration, the five most frequent errors found were verb tense, word choice, sentence structure, preposition, and modal/auxiliary, respectively, while the five most frequent errors in description and comparison/contrast were article, sentence structure, word choice, singular/plural form, and subject-verb agreement, respectively. Interestingly, in the narrative and descriptive paragraphs, comparison structure was found to be the least frequent error, whereas it became the 10th frequent error in comparison/contrast writing. It was apparent that a genre did affect writing errors as different text types required different structural features. It could be concluded that to enhance students\u2019 grammatical and lexical accuracy, a second language (L2) writing teacher should take into consideration L1 interference categories in different genres."}, {"paper_id": "52909053", "adju_relevance": 0, "title": "An Exploration of Dropout with RNNs for Natural Language Inference", "background_label": "Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models.", "abstract": "Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models."}, {"paper_id": "15625279", "adju_relevance": 0, "title": "Agreement attraction in comprehension: Representations and processes", "background_label": "Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45\u201393]), in which a verb erroneously agrees with an intervening noun. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Two main findings provide evidence against the first class of theories.", "result_label": "Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject\u2013verb agreement violations when these \u2018attractor\u2019 nouns intervene. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb.", "method_label": "First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., \u2018The drivers who the runner wave to each morning\u2019). Second, we observe a \u2018grammatical asymmetry\u2019: attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error.", "abstract": "Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. (1991). Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. (1991). Broken agreement. Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45\u201393]), in which a verb erroneously agrees with an intervening noun. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject\u2013verb agreement violations when these \u2018attractor\u2019 nouns intervene. Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45\u201393]), in which a verb erroneously agrees with an intervening noun. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Much work has demonstrated so-called attraction errors in the production of subject\u2013verb agreement (e.g., \u2018The key to the cabinets are on the table\u2019, [Bock, J. K., & Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45\u201393]), in which a verb erroneously agrees with an intervening noun. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Two main findings provide evidence against the first class of theories. First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., \u2018The drivers who the runner wave to each morning\u2019). First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., \u2018The drivers who the runner wave to each morning\u2019). Second, we observe a \u2018grammatical asymmetry\u2019: attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject\u2013verb agreement violations when these \u2018attractor\u2019 nouns intervene. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject\u2013verb agreement violations when these \u2018attractor\u2019 nouns intervene. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb."}, {"paper_id": "202539170", "adju_relevance": 0, "title": "Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction", "background_label": "Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl\\'aramos. Note the lexeme itself, hablar, is relatively common.", "abstract": "Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl\\'aramos. Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl\\'aramos. Note the lexeme itself, hablar, is relatively common."}, {"paper_id": "189927918", "adju_relevance": 0, "title": "On the Computational Power of RNNs", "background_label": "Recent neural network architectures such as the basic recurrent neural network (RNN) and Gated Recurrent Unit (GRU) have gained prominence as end-to-end learning architectures for natural language processing tasks. But what is the computational power of such systems?", "method_label": "We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata. If we also allow infinite precision, infinite edge weights, and nonlinear output activation functions, we prove that GRUs are at least as computationally powerful as pushdown automata.", "result_label": "All results are shown constructively.", "abstract": "Recent neural network architectures such as the basic recurrent neural network (RNN) and Gated Recurrent Unit (GRU) have gained prominence as end-to-end learning architectures for natural language processing tasks. Recent neural network architectures such as the basic recurrent neural network (RNN) and Gated Recurrent Unit (GRU) have gained prominence as end-to-end learning architectures for natural language processing tasks. But what is the computational power of such systems? We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata. We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata. If we also allow infinite precision, infinite edge weights, and nonlinear output activation functions, we prove that GRUs are at least as computationally powerful as pushdown automata. All results are shown constructively."}, {"paper_id": "44119185", "adju_relevance": 0, "title": "Using Morphological Knowledge in Open-Vocabulary Neural Language Models", "background_label": "AbstractLanguages with productive morphology pose problems for language models that generate words from a fixed vocabulary. Although character-based models allow any possible word type to be generated, they are linguistically na\u00efve: they must discover that words exist and are delimited by spaces-basic linguistic facts that are built in to the structure of word-based models.", "method_label": "We introduce an openvocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer.", "result_label": "Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures. Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.", "abstract": "AbstractLanguages with productive morphology pose problems for language models that generate words from a fixed vocabulary. AbstractLanguages with productive morphology pose problems for language models that generate words from a fixed vocabulary. Although character-based models allow any possible word type to be generated, they are linguistically na\u00efve: they must discover that words exist and are delimited by spaces-basic linguistic facts that are built in to the structure of word-based models. We introduce an openvocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer. Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures. Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures. Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation."}, {"paper_id": "2255575", "adju_relevance": 0, "title": "A data-based classification of Slavic languages: Indices of qualitative variation applied to grapheme frequencies", "background_label": "The Ord's graph is a simple graphical method for displaying frequency distributions of data or theoretical distributions in the two-dimensional plane. Its coordinates are proportions of the first three moments, either empirical or theoretical ones.", "method_label": "A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies. Data were taken from parallel corpora, i.e., we work with grapheme frequencies from a Russian novel and its translations to ten other Slavic languages. Then, cluster analysis is applied to the graph coordinates.", "result_label": "While the original graph yields results which are not linguistically interpretable, the modification reveals meaningful relations among the languages.", "abstract": "The Ord's graph is a simple graphical method for displaying frequency distributions of data or theoretical distributions in the two-dimensional plane. The Ord's graph is a simple graphical method for displaying frequency distributions of data or theoretical distributions in the two-dimensional plane. Its coordinates are proportions of the first three moments, either empirical or theoretical ones. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies. Data were taken from parallel corpora, i.e., we work with grapheme frequencies from a Russian novel and its translations to ten other Slavic languages. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies. Data were taken from parallel corpora, i.e., we work with grapheme frequencies from a Russian novel and its translations to ten other Slavic languages. Then, cluster analysis is applied to the graph coordinates. While the original graph yields results which are not linguistically interpretable, the modification reveals meaningful relations among the languages."}, {"paper_id": "7677690", "adju_relevance": 0, "title": "Malagasy Dialects and the Peopling of Madagascar", "background_label": "The origin of Malagasy DNA is half African and half Indonesian, nevertheless the Malagasy language, spoken by the entire population, belongs to the Austronesian family. The language most closely related to Malagasy is Maanyan (Greater Barito East group of the Austronesian family), but related languages are also in Sulawesi, Malaysia and Sumatra. For this reason, and because Maanyan is spoken by a population which lives along the Barito river in Kalimantan and which does not possess the necessary skill for long maritime navigation, the ethnic composition of the Indonesian colonizers is still unclear. There is a general consensus that Indonesian sailors reached Madagascar by a maritime trek, but the time, the path and the landing area of the first colonization are all disputed.", "abstract": "The origin of Malagasy DNA is half African and half Indonesian, nevertheless the Malagasy language, spoken by the entire population, belongs to the Austronesian family. The origin of Malagasy DNA is half African and half Indonesian, nevertheless the Malagasy language, spoken by the entire population, belongs to the Austronesian family. The language most closely related to Malagasy is Maanyan (Greater Barito East group of the Austronesian family), but related languages are also in Sulawesi, Malaysia and Sumatra. The origin of Malagasy DNA is half African and half Indonesian, nevertheless the Malagasy language, spoken by the entire population, belongs to the Austronesian family. The language most closely related to Malagasy is Maanyan (Greater Barito East group of the Austronesian family), but related languages are also in Sulawesi, Malaysia and Sumatra. For this reason, and because Maanyan is spoken by a population which lives along the Barito river in Kalimantan and which does not possess the necessary skill for long maritime navigation, the ethnic composition of the Indonesian colonizers is still unclear. The origin of Malagasy DNA is half African and half Indonesian, nevertheless the Malagasy language, spoken by the entire population, belongs to the Austronesian family. The language most closely related to Malagasy is Maanyan (Greater Barito East group of the Austronesian family), but related languages are also in Sulawesi, Malaysia and Sumatra. For this reason, and because Maanyan is spoken by a population which lives along the Barito river in Kalimantan and which does not possess the necessary skill for long maritime navigation, the ethnic composition of the Indonesian colonizers is still unclear. There is a general consensus that Indonesian sailors reached Madagascar by a maritime trek, but the time, the path and the landing area of the first colonization are all disputed."}, {"paper_id": "145704027", "adju_relevance": 0, "title": "The phonetics and phonology of Tashlhiyt Berber syllabic consonants", "background_label": "Tashlhiyt Berber has entered the folklore of phonological theory as an unusual language in which any consonant can be syllabic, many words consisting entirely of consonants. I shall argue for an alternative analysis, according to which the epenthetic vowels which frequently accompany syllabic consonants are the phonetic realisations of syllable nuclei. Where no vowel occurs, it can be regarded as hidden by the following consonant, according to a gestural overlap model.", "result_label": "On this view, Tashlhiyt syllable structure is a quite unmarked CV(C(C)), and syllabification is unproblematic.", "abstract": "Tashlhiyt Berber has entered the folklore of phonological theory as an unusual language in which any consonant can be syllabic, many words consisting entirely of consonants. Tashlhiyt Berber has entered the folklore of phonological theory as an unusual language in which any consonant can be syllabic, many words consisting entirely of consonants. I shall argue for an alternative analysis, according to which the epenthetic vowels which frequently accompany syllabic consonants are the phonetic realisations of syllable nuclei. Tashlhiyt Berber has entered the folklore of phonological theory as an unusual language in which any consonant can be syllabic, many words consisting entirely of consonants. I shall argue for an alternative analysis, according to which the epenthetic vowels which frequently accompany syllabic consonants are the phonetic realisations of syllable nuclei. Where no vowel occurs, it can be regarded as hidden by the following consonant, according to a gestural overlap model. On this view, Tashlhiyt syllable structure is a quite unmarked CV(C(C)), and syllabification is unproblematic."}, {"paper_id": "184486914", "adju_relevance": 0, "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "background_label": "Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages.", "abstract": "Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages."}, {"paper_id": "202558708", "adju_relevance": 0, "title": "From English to Code-Switching: Transfer Learning with Strong Morphological Clues", "background_label": "Code-switching is still an understudied phenomenon in natural language processing mainly because of two related challenges: it lacks annotated data, and it combines a vast diversity of low-resource languages. Despite the language diversity, many code-switching scenarios occur in language pairs, and English is often a common factor among them.", "method_label": "In the first part of this paper, we use transfer learning from English to English-paired code-switched languages for the language identification (LID) task by applying two simple yet effective techniques: 1) a hierarchical attention mechanism that enhances morphological clues from character n-grams, and 2) a secondary loss that forces the model to learn n-gram representations that are particular to the languages involved. We use the bottom layers of the ELMo architecture to learn these morphological clues by essentially recognizing what is and what is not English. Our approach outperforms the previous state of the art on Nepali-English, Spanish-English, and Hindi-English datasets. In the second part of the paper, we use our best LID models for the tasks of Spanish-English named entity recognition and Hindi-English part-of-speech tagging by replacing their inference layers and retraining them.", "result_label": "We show that our retrained models are capable of using the code-switching information on both tasks to outperform models that do not have such knowledge.", "abstract": "Code-switching is still an understudied phenomenon in natural language processing mainly because of two related challenges: it lacks annotated data, and it combines a vast diversity of low-resource languages. Code-switching is still an understudied phenomenon in natural language processing mainly because of two related challenges: it lacks annotated data, and it combines a vast diversity of low-resource languages. Despite the language diversity, many code-switching scenarios occur in language pairs, and English is often a common factor among them. In the first part of this paper, we use transfer learning from English to English-paired code-switched languages for the language identification (LID) task by applying two simple yet effective techniques: 1) a hierarchical attention mechanism that enhances morphological clues from character n-grams, and 2) a secondary loss that forces the model to learn n-gram representations that are particular to the languages involved. In the first part of this paper, we use transfer learning from English to English-paired code-switched languages for the language identification (LID) task by applying two simple yet effective techniques: 1) a hierarchical attention mechanism that enhances morphological clues from character n-grams, and 2) a secondary loss that forces the model to learn n-gram representations that are particular to the languages involved. We use the bottom layers of the ELMo architecture to learn these morphological clues by essentially recognizing what is and what is not English. In the first part of this paper, we use transfer learning from English to English-paired code-switched languages for the language identification (LID) task by applying two simple yet effective techniques: 1) a hierarchical attention mechanism that enhances morphological clues from character n-grams, and 2) a secondary loss that forces the model to learn n-gram representations that are particular to the languages involved. We use the bottom layers of the ELMo architecture to learn these morphological clues by essentially recognizing what is and what is not English. Our approach outperforms the previous state of the art on Nepali-English, Spanish-English, and Hindi-English datasets. In the first part of this paper, we use transfer learning from English to English-paired code-switched languages for the language identification (LID) task by applying two simple yet effective techniques: 1) a hierarchical attention mechanism that enhances morphological clues from character n-grams, and 2) a secondary loss that forces the model to learn n-gram representations that are particular to the languages involved. We use the bottom layers of the ELMo architecture to learn these morphological clues by essentially recognizing what is and what is not English. Our approach outperforms the previous state of the art on Nepali-English, Spanish-English, and Hindi-English datasets. In the second part of the paper, we use our best LID models for the tasks of Spanish-English named entity recognition and Hindi-English part-of-speech tagging by replacing their inference layers and retraining them. We show that our retrained models are capable of using the code-switching information on both tasks to outperform models that do not have such knowledge."}, {"paper_id": "1070525", "adju_relevance": 0, "title": "A Multilingual Paradigm For Automatic Verb Classification", "background_label": "We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages. Our research incorporates two interrelated threads.", "method_label": "In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%). Our second strand of research exploits the differences across languages in the syntactic expression of semantic properties, to show that complementary information about English verbs can be extracted from their translations in a second language (Chinese).", "result_label": "The use of multilingual features improves classification performance of the English verbs, achieving an accuracy of 83.5% (baseline 33.3%).", "abstract": "We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages. We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages. Our research incorporates two interrelated threads. In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%). In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%). Our second strand of research exploits the differences across languages in the syntactic expression of semantic properties, to show that complementary information about English verbs can be extracted from their translations in a second language (Chinese). The use of multilingual features improves classification performance of the English verbs, achieving an accuracy of 83.5% (baseline 33.3%)."}, {"paper_id": "192058044", "adju_relevance": 0, "title": "The alchemy of English : the spread, functions, and models of non-native Englishes", "background_label": "\"What emerges from Kachru's fine work is the potential demarcation of an entire field, rather than merely the fruitful exploration of a topic. .", "abstract": "\"What emerges from Kachru's fine work is the potential demarcation of an entire field, rather than merely the fruitful exploration of a topic. \"What emerges from Kachru's fine work is the potential demarcation of an entire field, rather than merely the fruitful exploration of a topic. ."}, {"paper_id": "8247293", "adju_relevance": 0, "title": "Towards a model of formal and informal address in English", "background_label": "AbstractInformal and formal (\"T/V\") address in dialogue is not distinguished overtly in modern English, e.g. by pronoun choice like in many other languages such as French (\"tu\"/\"vous\").", "abstract": "AbstractInformal and formal (\"T/V\") address in dialogue is not distinguished overtly in modern English, e.g. AbstractInformal and formal (\"T/V\") address in dialogue is not distinguished overtly in modern English, e.g. by pronoun choice like in many other languages such as French (\"tu\"/\"vous\")."}, {"paper_id": "3883482", "adju_relevance": 0, "title": "Generating Bilingual Pragmatic Color References", "background_label": "Contextual influences on language often exhibit substantial cross-lingual regularities; for example, we are more verbose in situations that require finer distinctions. However, these regularities are sometimes obscured by semantic and syntactic differences.", "method_label": "Using a newly-collected dataset of color reference games in Mandarin Chinese (which we release to the public), we confirm that a variety of constructions display the same sensitivity to contextual difficulty in Chinese and English. We then show that a neural speaker agent trained on bilingual data with a simple multitask learning approach displays more human-like patterns of context dependence and is more pragmatically informative than its monolingual Chinese counterpart.", "result_label": "Moreover, this is not at the expense of language-specific semantic understanding: the resulting speaker model learns the different basic color term systems of English and Chinese (with noteworthy cross-lingual influences), and it can identify synonyms between the two languages using vector analogy operations on its output layer, despite having no exposure to parallel data.", "abstract": "Contextual influences on language often exhibit substantial cross-lingual regularities; for example, we are more verbose in situations that require finer distinctions. Contextual influences on language often exhibit substantial cross-lingual regularities; for example, we are more verbose in situations that require finer distinctions. However, these regularities are sometimes obscured by semantic and syntactic differences. Using a newly-collected dataset of color reference games in Mandarin Chinese (which we release to the public), we confirm that a variety of constructions display the same sensitivity to contextual difficulty in Chinese and English. Using a newly-collected dataset of color reference games in Mandarin Chinese (which we release to the public), we confirm that a variety of constructions display the same sensitivity to contextual difficulty in Chinese and English. We then show that a neural speaker agent trained on bilingual data with a simple multitask learning approach displays more human-like patterns of context dependence and is more pragmatically informative than its monolingual Chinese counterpart. Moreover, this is not at the expense of language-specific semantic understanding: the resulting speaker model learns the different basic color term systems of English and Chinese (with noteworthy cross-lingual influences), and it can identify synonyms between the two languages using vector analogy operations on its output layer, despite having no exposure to parallel data."}, {"paper_id": "1642440", "adju_relevance": 0, "title": "Reconstructing Native Language Typology from Foreign Language Usage", "background_label": "Linguists and psychologists have long been studying cross-linguistic transfer, the influence of native language properties on linguistic performance in a foreign language.", "abstract": "Linguists and psychologists have long been studying cross-linguistic transfer, the influence of native language properties on linguistic performance in a foreign language."}, {"paper_id": "13848633", "adju_relevance": 0, "title": "Are All Languages Equally Hard to Language-Model?", "background_label": "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles?", "method_label": "In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information.", "result_label": "We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both $n$-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.", "abstract": "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both $n$-gram and LSTM language models. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both $n$-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages."}, {"paper_id": "2675113", "adju_relevance": 0, "title": "Some Universals of Grammar with Particular Reference to the Order of Meaningful Elements", "background_label": "A log slasher includes an elongated log receiving bunk and a saw assembly movable longitudinally along the bunk to saw logs supported in the bunk into shortened lengths. The saw is pivotally supported at one end on a traveling trolley for movement with the trolley along the bunk and in a vertical plane toward and away from the bunk.", "method_label": "A guide moves with the saw to hold the saw against whip as the saw is moving into, and away from, a cut. A lift crane lifts logs into and out of the bunk and includes clam fingers having opposite angled surfaces adjacent the free ends thereof for engaging logs of different diameter.", "result_label": "The trolley drive and that for moving the saw toward and away from the bunk are carried on the trolley and an articulated arm assembly, extendible and contractible in response to trolley movement, supports the power supply conduits in a compact arrangement throughout the range of movement of the trolley and saw.", "abstract": "A log slasher includes an elongated log receiving bunk and a saw assembly movable longitudinally along the bunk to saw logs supported in the bunk into shortened lengths. A log slasher includes an elongated log receiving bunk and a saw assembly movable longitudinally along the bunk to saw logs supported in the bunk into shortened lengths. The saw is pivotally supported at one end on a traveling trolley for movement with the trolley along the bunk and in a vertical plane toward and away from the bunk. A guide moves with the saw to hold the saw against whip as the saw is moving into, and away from, a cut. A guide moves with the saw to hold the saw against whip as the saw is moving into, and away from, a cut. A lift crane lifts logs into and out of the bunk and includes clam fingers having opposite angled surfaces adjacent the free ends thereof for engaging logs of different diameter. The trolley drive and that for moving the saw toward and away from the bunk are carried on the trolley and an articulated arm assembly, extendible and contractible in response to trolley movement, supports the power supply conduits in a compact arrangement throughout the range of movement of the trolley and saw."}, {"paper_id": "17111881", "adju_relevance": 0, "title": "AnCora-Verb: A Lexical Resource for the Semantic Annotation of Corpora", "background_label": "AbstractIn this paper we present two large-scale verbal lexicons, AnCora-Verb-Ca for Catalan and AnCora-Verb-Es for Spanish, which are the basis for the semantic annotation with arguments and thematic roles of AnCora corpora. The big amount of linguistic information contained in both resources should be of great interest for computational applications and linguistic studies.", "method_label": "In AnCora-Verb lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate. Each verbal predicate is related to one or more semantic classes basically differentiated according to the four event classes \u2500accomplishments, achievements, states and activities\u2500, and on the diatheses alternations in which a verb can occur. AnCora-Verb-Es contains a total of 1965 different verbs corresponding to 3671 senses and AnCora-Verb-Ca contains 2151 verbs and 4513 senses.", "result_label": "These figures correspond to the total of 500,000 words contained in each corpus, AnCora-Ca and AnCora-Es. The lexicons and the annotated corpora constitute the richest linguistic resources of this kind freely available for Spanish and Catalan.", "abstract": "AbstractIn this paper we present two large-scale verbal lexicons, AnCora-Verb-Ca for Catalan and AnCora-Verb-Es for Spanish, which are the basis for the semantic annotation with arguments and thematic roles of AnCora corpora. In AnCora-Verb lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate. In AnCora-Verb lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate. Each verbal predicate is related to one or more semantic classes basically differentiated according to the four event classes \u2500accomplishments, achievements, states and activities\u2500, and on the diatheses alternations in which a verb can occur. In AnCora-Verb lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate. Each verbal predicate is related to one or more semantic classes basically differentiated according to the four event classes \u2500accomplishments, achievements, states and activities\u2500, and on the diatheses alternations in which a verb can occur. AnCora-Verb-Es contains a total of 1965 different verbs corresponding to 3671 senses and AnCora-Verb-Ca contains 2151 verbs and 4513 senses. These figures correspond to the total of 500,000 words contained in each corpus, AnCora-Ca and AnCora-Es. These figures correspond to the total of 500,000 words contained in each corpus, AnCora-Ca and AnCora-Es. The lexicons and the annotated corpora constitute the richest linguistic resources of this kind freely available for Spanish and Catalan. AbstractIn this paper we present two large-scale verbal lexicons, AnCora-Verb-Ca for Catalan and AnCora-Verb-Es for Spanish, which are the basis for the semantic annotation with arguments and thematic roles of AnCora corpora. The big amount of linguistic information contained in both resources should be of great interest for computational applications and linguistic studies."}, {"paper_id": "49564714", "adju_relevance": 0, "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "background_label": "A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources.", "method_label": "We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-employment of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP.", "result_label": "In particular, we suggest that such approach could be facilitated by recent developments in data-driven induction of typological knowledge.", "abstract": " A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-employment of the typological features included in them. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-employment of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such approach could be facilitated by recent developments in data-driven induction of typological knowledge."}, {"paper_id": "8665208", "adju_relevance": 0, "title": "A Class-Based Agreement Model for Generating Accurately Inflected Translations", "background_label": "AbstractWhen automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors.", "abstract": "AbstractWhen automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors."}, {"paper_id": "7003847", "adju_relevance": 0, "title": "Co-Occurrences Of Antonymous Adjectives And Their Contexts", "background_label": "Antonymic AssociationMuch current research in linguistics is concerned with textual or discourse bases for linguistic structure; within lexical semantics, such research is directed at particular lexical relations and at correlations between syntax and semantics. This paper addresses the textual underpinnings of antonymy between predicative adjectives, following up research reported by Charles and Miller (1989) .Antonymy is a special lexical association between word pairs. That it is lexical and not simply semantic follows from the fact that different words for the same concept can have different antonyms; for example, big-little and large-small are good antonym pairs, but large-little is not.", "method_label": "The classic work on associations among adjectives, and between antonymous adjectives in particular, is by Deese (1964 Deese ( , 1965 , analyzing the results of stimulus-response word-association tests. 1 Charles and Miller (1989) argue, contrary to more complex psycholinguistic theories, that the primary source of these associations is a tendency they hypothesize for antonyms to co-occur within the same sentences in discourse.", "result_label": "This paper supports and extends their hypothesis.Deese's work on word association for adjectives was based on a list of the 278 most frequent adjectives in the Thorndike-Lorge (1944) count, those having a frequency of 50 per million or more. Thirty-four pairs of these words had a reciprocal property: each member of the pair was the most frequent response to the other on word-association\u2022 Natural", "abstract": "Antonymic AssociationMuch current research in linguistics is concerned with textual or discourse bases for linguistic structure; within lexical semantics, such research is directed at particular lexical relations and at correlations between syntax and semantics. Antonymic AssociationMuch current research in linguistics is concerned with textual or discourse bases for linguistic structure; within lexical semantics, such research is directed at particular lexical relations and at correlations between syntax and semantics. This paper addresses the textual underpinnings of antonymy between predicative adjectives, following up research reported by Charles and Miller (1989) .Antonymy is a special lexical association between word pairs. Antonymic AssociationMuch current research in linguistics is concerned with textual or discourse bases for linguistic structure; within lexical semantics, such research is directed at particular lexical relations and at correlations between syntax and semantics. This paper addresses the textual underpinnings of antonymy between predicative adjectives, following up research reported by Charles and Miller (1989) .Antonymy is a special lexical association between word pairs. That it is lexical and not simply semantic follows from the fact that different words for the same concept can have different antonyms; for example, big-little and large-small are good antonym pairs, but large-little is not. The classic work on associations among adjectives, and between antonymous adjectives in particular, is by Deese (1964 Deese ( , 1965 , analyzing the results of stimulus-response word-association tests. The classic work on associations among adjectives, and between antonymous adjectives in particular, is by Deese (1964 Deese ( , 1965 , analyzing the results of stimulus-response word-association tests. 1 Charles and Miller (1989) argue, contrary to more complex psycholinguistic theories, that the primary source of these associations is a tendency they hypothesize for antonyms to co-occur within the same sentences in discourse. This paper supports and extends their hypothesis.Deese's work on word association for adjectives was based on a list of the 278 most frequent adjectives in the Thorndike-Lorge (1944) count, those having a frequency of 50 per million or more. This paper supports and extends their hypothesis.Deese's work on word association for adjectives was based on a list of the 278 most frequent adjectives in the Thorndike-Lorge (1944) count, those having a frequency of 50 per million or more. Thirty-four pairs of these words had a reciprocal property: each member of the pair was the most frequent response to the other on word-association\u2022 Natural"}, {"paper_id": "46935302", "adju_relevance": 0, "title": "Relational inductive biases, deep learning, and graph networks", "background_label": "Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between\"hand-engineering\"and\"end-to-end\"learning, and instead advocate for an approach which benefits from their complementary strengths.", "result_label": "The following is part position paper, part review, and part unification. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.", "method_label": "We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning.", "abstract": "Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between\"hand-engineering\"and\"end-to-end\"learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. The following is part position paper, part review, and part unification. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice."}, {"paper_id": "56358815", "adju_relevance": 0, "title": "Statistical self-similarity of spatial variations of snow cover: verification of the hypothesis and application in the snowmelt runoff generation models", "background_label": "An analysis of snow cover measurement data in a number of physiographic regions and landscapes has shown that fields of snow cover characteristics can be considered as random fields with homogeneous increments and that these fields exhibit statistical self-similarity.", "method_label": "A physically based distributed model of snowmelt runoff generation developed for the Upper Kolyma River basin (the catchment area is about 100 000 km2) has been used to estimate the sensitivity of snowmelt dynamics over the basin and flood hydrographs to the parameterization of subgrid effects based on the hypothesis of statistical self-similarity of the maximum snow water equivalent fields. Such parameterization of subgrid effects enables us to improve the description of snowmelt dynamics both within subgrid areas and over the entire river basin.", "result_label": "The snowmelt flood hydrographs appear less sensitive to the self-similarity of snow cover over subgrid areas than to the dynamics of snowmelt because of a too large catchment area of the basin under consideration. However, for certain hydrometeorological conditions and for small river basins this effect may lead to significant changes of the calculated hydrographs.", "abstract": "An analysis of snow cover measurement data in a number of physiographic regions and landscapes has shown that fields of snow cover characteristics can be considered as random fields with homogeneous increments and that these fields exhibit statistical self-similarity. A physically based distributed model of snowmelt runoff generation developed for the Upper Kolyma River basin (the catchment area is about 100 000 km2) has been used to estimate the sensitivity of snowmelt dynamics over the basin and flood hydrographs to the parameterization of subgrid effects based on the hypothesis of statistical self-similarity of the maximum snow water equivalent fields. A physically based distributed model of snowmelt runoff generation developed for the Upper Kolyma River basin (the catchment area is about 100 000 km2) has been used to estimate the sensitivity of snowmelt dynamics over the basin and flood hydrographs to the parameterization of subgrid effects based on the hypothesis of statistical self-similarity of the maximum snow water equivalent fields. Such parameterization of subgrid effects enables us to improve the description of snowmelt dynamics both within subgrid areas and over the entire river basin. The snowmelt flood hydrographs appear less sensitive to the self-similarity of snow cover over subgrid areas than to the dynamics of snowmelt because of a too large catchment area of the basin under consideration. The snowmelt flood hydrographs appear less sensitive to the self-similarity of snow cover over subgrid areas than to the dynamics of snowmelt because of a too large catchment area of the basin under consideration. However, for certain hydrometeorological conditions and for small river basins this effect may lead to significant changes of the calculated hydrographs."}, {"paper_id": "67855753", "adju_relevance": 0, "title": "Structural Supervision Improves Learning of Non-Local Grammatical Dependencies", "background_label": "State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success.", "abstract": "State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success."}, {"paper_id": "61829139", "adju_relevance": 0, "title": "RE-THINKING SIGN LANGUAGE VERB CLASSES: THE BODY AS SUBJECT", "background_label": "This paper offers a new look at the traditional analysis of verb classes in sign languages. According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs. These classes differ from each other with respect to the properties of the arguments which they encode. Agreement verbs, verbs denoting transfer, encode the syntactic role of the arguments, as well as their person and number features, by the direction of the movement of the hands and the facing of the palms. The above analysis focuses on the role of the hands in encoding the relevant grammatical features. The hands are the active articulator in sign languages, and they carry most of the informational load of the sign.", "method_label": "In spatial verbs, the class of verbs denoting motion and location in space, the direction of movement encodes the locations of locative arguments, the source and the goal.", "result_label": "The shape of the path movement the hands are tracing often depicts the shape of the path that an object traverses in space. Plain verbs, which constitute the default semantic class, do not encode any grammatical features of their arguments.", "abstract": "This paper offers a new look at the traditional analysis of verb classes in sign languages. This paper offers a new look at the traditional analysis of verb classes in sign languages. According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs. This paper offers a new look at the traditional analysis of verb classes in sign languages. According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs. These classes differ from each other with respect to the properties of the arguments which they encode. This paper offers a new look at the traditional analysis of verb classes in sign languages. According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs. These classes differ from each other with respect to the properties of the arguments which they encode. Agreement verbs, verbs denoting transfer, encode the syntactic role of the arguments, as well as their person and number features, by the direction of the movement of the hands and the facing of the palms. In spatial verbs, the class of verbs denoting motion and location in space, the direction of movement encodes the locations of locative arguments, the source and the goal. The shape of the path movement the hands are tracing often depicts the shape of the path that an object traverses in space. The shape of the path movement the hands are tracing often depicts the shape of the path that an object traverses in space. Plain verbs, which constitute the default semantic class, do not encode any grammatical features of their arguments. This paper offers a new look at the traditional analysis of verb classes in sign languages. According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs. These classes differ from each other with respect to the properties of the arguments which they encode. Agreement verbs, verbs denoting transfer, encode the syntactic role of the arguments, as well as their person and number features, by the direction of the movement of the hands and the facing of the palms. The above analysis focuses on the role of the hands in encoding the relevant grammatical features. This paper offers a new look at the traditional analysis of verb classes in sign languages. According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs. These classes differ from each other with respect to the properties of the arguments which they encode. Agreement verbs, verbs denoting transfer, encode the syntactic role of the arguments, as well as their person and number features, by the direction of the movement of the hands and the facing of the palms. The above analysis focuses on the role of the hands in encoding the relevant grammatical features. The hands are the active articulator in sign languages, and they carry most of the informational load of the sign."}, {"paper_id": "193493017", "adju_relevance": 0, "title": "Germans, Queenslanders and Londoners: The Semantics of Demonyms", "background_label": "Demonyms are best defined as words that refer to people of a place, although very little research has actually touched upon the different types of demonyms (by types I do not mean endonyms or exonyms, but the subgrouping within the demonyms).", "method_label": "This paper will fill the gap by exploring the semantics of demonyms, as they are used in the English language, It focuses on three terms Germans, Queenslanders and Londoners and shows how the semantic molecule \u2018country\u2019 is embedded in these concepts and is essential to understanding their meanings. It also shows how the use of semantic template within the NSM framework can help to identify the different types of demonyms and their relationship. The analysis will draw from the British National Corpus, Corpus of Contemporary American English, and Collins Wordbanks Online and demonstrate that the different types of demonyms can be identified, at least to some extent, by observing their behaviour in natural language.", "result_label": "It will show the terms used refer to people from countries ( Germans, Australians , and Danes ) do not occur alongside terms that refer to people from cities ( Londoners, Melbournians , Parisians ), that each type of demonym seems to have its own set of restrictions.", "abstract": "Demonyms are best defined as words that refer to people of a place, although very little research has actually touched upon the different types of demonyms (by types I do not mean endonyms or exonyms, but the subgrouping within the demonyms). This paper will fill the gap by exploring the semantics of demonyms, as they are used in the English language, It focuses on three terms Germans, Queenslanders and Londoners and shows how the semantic molecule \u2018country\u2019 is embedded in these concepts and is essential to understanding their meanings. This paper will fill the gap by exploring the semantics of demonyms, as they are used in the English language, It focuses on three terms Germans, Queenslanders and Londoners and shows how the semantic molecule \u2018country\u2019 is embedded in these concepts and is essential to understanding their meanings. It also shows how the use of semantic template within the NSM framework can help to identify the different types of demonyms and their relationship. This paper will fill the gap by exploring the semantics of demonyms, as they are used in the English language, It focuses on three terms Germans, Queenslanders and Londoners and shows how the semantic molecule \u2018country\u2019 is embedded in these concepts and is essential to understanding their meanings. It also shows how the use of semantic template within the NSM framework can help to identify the different types of demonyms and their relationship. The analysis will draw from the British National Corpus, Corpus of Contemporary American English, and Collins Wordbanks Online and demonstrate that the different types of demonyms can be identified, at least to some extent, by observing their behaviour in natural language. It will show the terms used refer to people from countries ( Germans, Australians , and Danes ) do not occur alongside terms that refer to people from cities ( Londoners, Melbournians , Parisians ), that each type of demonym seems to have its own set of restrictions."}, {"paper_id": "14091946", "adju_relevance": 0, "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "background_label": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?", "method_label": "We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting.", "result_label": "We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "122349887", "adju_relevance": 0, "title": "Computational Consequences of Agreement and Ambiguity in Natural Language", "background_label": "The computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information processing tasks. Here we show that a simple, theory-neutral linguistic model of syntactic agreement and ambiguity demonstrates that natural language parsing may be computationally intractable. Significantly, we show that it may be syntactic features rather than rules that can cause this difficulty.", "abstract": "The computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information processing tasks. The computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information processing tasks. Here we show that a simple, theory-neutral linguistic model of syntactic agreement and ambiguity demonstrates that natural language parsing may be computationally intractable. The computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information processing tasks. Here we show that a simple, theory-neutral linguistic model of syntactic agreement and ambiguity demonstrates that natural language parsing may be computationally intractable. Significantly, we show that it may be syntactic features rather than rules that can cause this difficulty."}, {"paper_id": "5243955", "adju_relevance": 0, "title": "Melbourne at SemEval 2016 Task 11: Classifying Type-level Word Complexity using Random Forests with Corpus and Word List Features", "background_label": "SemEval 2016 task 11 involved determining whether words in a sentence were complex or simple for a cohort of people with English as a second language. Training data consisted of 200 annotated sentences, representing the combined judgements of 20 human annota- tors, such that if any annotator of the group labelled a word as complex, then it was con- sidered to be complex. Testing was based on single annotator judgements.", "method_label": "Our system used a random forest classifier with a variety of features, the most important of which were term frequency statistics garnered from four large corpora, and style lexicons built on two large corpora. Minor features in the final system include the presence or absence of words in various readability word lists; many other features we tried were not successful.", "result_label": "Our rank- ing amongst submitted systems did not reflect the strength of our system, due to submitting a far from optimal weighting between complex and simple, but we show that when a more ap- propriate weighting is used, our system ranks amongst the best submitted systems.", "abstract": "SemEval 2016 task 11 involved determining whether words in a sentence were complex or simple for a cohort of people with English as a second language. SemEval 2016 task 11 involved determining whether words in a sentence were complex or simple for a cohort of people with English as a second language. Training data consisted of 200 annotated sentences, representing the combined judgements of 20 human annota- tors, such that if any annotator of the group labelled a word as complex, then it was con- sidered to be complex. SemEval 2016 task 11 involved determining whether words in a sentence were complex or simple for a cohort of people with English as a second language. Training data consisted of 200 annotated sentences, representing the combined judgements of 20 human annota- tors, such that if any annotator of the group labelled a word as complex, then it was con- sidered to be complex. Testing was based on single annotator judgements. Our system used a random forest classifier with a variety of features, the most important of which were term frequency statistics garnered from four large corpora, and style lexicons built on two large corpora. Our system used a random forest classifier with a variety of features, the most important of which were term frequency statistics garnered from four large corpora, and style lexicons built on two large corpora. Minor features in the final system include the presence or absence of words in various readability word lists; many other features we tried were not successful. Our rank- ing amongst submitted systems did not reflect the strength of our system, due to submitting a far from optimal weighting between complex and simple, but we show that when a more ap- propriate weighting is used, our system ranks amongst the best submitted systems."}, {"paper_id": "22350021", "adju_relevance": 0, "title": "Learning to express motion events in English and Korean: The influence of language-specific lexicalization patterns", "background_label": "English and Korean differ in how they lexicalize the components of motion events. English characteristically conflates Motion with Manner, Cause, or Deixis, and expresses Path separately. Korean, in contrast, conflates Motion with Path and elements of Figure and Ground in transitive clauses for caused Motion, but conflates motion with Deixis and spells out Path and Manner separately in intransitive clauses for spontaneous motion. Children learning English and Korean show sensitivity to language-specific patterns in the way they talk about motion from as early as 17-20 months.", "method_label": "For example, learners of English quickly generalize their earliest spatial words--Path particles like up, down, and in--to both spontaneous and caused changes of location and, for up and down, to posture changes, while learners of Korean keep words for spontaneous and caused motion strictly separate and use different words for vertical changes of location and posture changes.", "result_label": "These findings challenge the widespread view that children initially map spatial words directly to nonlinguistic spatial concepts, and suggest that they are influenced by the semantic organization of their language virtually from the beginning. We discuss how input and cognition may interact in the early phases of learning to talk about space.", "abstract": "English and Korean differ in how they lexicalize the components of motion events. English and Korean differ in how they lexicalize the components of motion events. English characteristically conflates Motion with Manner, Cause, or Deixis, and expresses Path separately. English and Korean differ in how they lexicalize the components of motion events. English characteristically conflates Motion with Manner, Cause, or Deixis, and expresses Path separately. Korean, in contrast, conflates Motion with Path and elements of Figure and Ground in transitive clauses for caused Motion, but conflates motion with Deixis and spells out Path and Manner separately in intransitive clauses for spontaneous motion. English and Korean differ in how they lexicalize the components of motion events. English characteristically conflates Motion with Manner, Cause, or Deixis, and expresses Path separately. Korean, in contrast, conflates Motion with Path and elements of Figure and Ground in transitive clauses for caused Motion, but conflates motion with Deixis and spells out Path and Manner separately in intransitive clauses for spontaneous motion. Children learning English and Korean show sensitivity to language-specific patterns in the way they talk about motion from as early as 17-20 months. For example, learners of English quickly generalize their earliest spatial words--Path particles like up, down, and in--to both spontaneous and caused changes of location and, for up and down, to posture changes, while learners of Korean keep words for spontaneous and caused motion strictly separate and use different words for vertical changes of location and posture changes. These findings challenge the widespread view that children initially map spatial words directly to nonlinguistic spatial concepts, and suggest that they are influenced by the semantic organization of their language virtually from the beginning. These findings challenge the widespread view that children initially map spatial words directly to nonlinguistic spatial concepts, and suggest that they are influenced by the semantic organization of their language virtually from the beginning. We discuss how input and cognition may interact in the early phases of learning to talk about space."}, {"paper_id": "13297601", "adju_relevance": 0, "title": "Making syntax of sense: number agreement in sentence production.", "background_label": "Grammatical agreement flags the parts of sentences that belong together regardless of whether the parts appear together. In English, the major agreement controller is the sentence subject, the major agreement targets are verbs and pronouns, and the major agreement category is number.", "method_label": "The authors expand an account of number agreement whose tenets are that pronouns acquire number lexically, whereas verbs acquire it syntactically but with similar contributions from number meaning and from the number morphology of agreement controllers. These tenets were instantiated in a model using existing verb agreement data. The model was then fit to a new, more extensive set of verb data and tested with a parallel set of pronoun data.", "result_label": "The theory was supported by the model's outcomes. The results have implications for the integration of words and structures, for the workings of agreement categories, and for the nature of the transition from thought to language.", "abstract": "Grammatical agreement flags the parts of sentences that belong together regardless of whether the parts appear together. Grammatical agreement flags the parts of sentences that belong together regardless of whether the parts appear together. In English, the major agreement controller is the sentence subject, the major agreement targets are verbs and pronouns, and the major agreement category is number. The authors expand an account of number agreement whose tenets are that pronouns acquire number lexically, whereas verbs acquire it syntactically but with similar contributions from number meaning and from the number morphology of agreement controllers. The authors expand an account of number agreement whose tenets are that pronouns acquire number lexically, whereas verbs acquire it syntactically but with similar contributions from number meaning and from the number morphology of agreement controllers. These tenets were instantiated in a model using existing verb agreement data. The authors expand an account of number agreement whose tenets are that pronouns acquire number lexically, whereas verbs acquire it syntactically but with similar contributions from number meaning and from the number morphology of agreement controllers. These tenets were instantiated in a model using existing verb agreement data. The model was then fit to a new, more extensive set of verb data and tested with a parallel set of pronoun data. The theory was supported by the model's outcomes. The theory was supported by the model's outcomes. The results have implications for the integration of words and structures, for the workings of agreement categories, and for the nature of the transition from thought to language."}, {"paper_id": "18198045", "adju_relevance": 0, "title": "Harnessing the CRF Complexity with Domain-Specific Constraints. The Case of Morphosyntactic Tagging of a Highly Inflected Language", "background_label": "ABSTRACTWe describe a domain-specific method of adapting conditional random fields (CRFs) to morphosyntactic tagging of highly-inflectional languages.", "method_label": "The solution involves extending CRFs with additional, position-wise restrictions on the output domain, which are used to impose consistency between the modeled label sequences and morphosyntactic analysis results both at the level of decoding and, more importantly, in parameters estimation process. We decompose the problem of morphosyntactic disambiguation into two consecutive stages of the context-sensitive morphosyntactic guessing and the disambiguation proper. The division helps in designing well-adjusted, CRF-based methods for both tasks, which in combination constitute Concraft, a highly accurate tagging system for the Polish language available under the 2-clause BSD license.", "result_label": "Evaluation on the National Corpus of Polish shows that our solution significantly outperforms other state-of-the-art taggers for Polish -Pantera, WMBT and WCRFT -especially in terms of the accuracy measured with respect to unknown words.", "abstract": "ABSTRACTWe describe a domain-specific method of adapting conditional random fields (CRFs) to morphosyntactic tagging of highly-inflectional languages. The solution involves extending CRFs with additional, position-wise restrictions on the output domain, which are used to impose consistency between the modeled label sequences and morphosyntactic analysis results both at the level of decoding and, more importantly, in parameters estimation process. The solution involves extending CRFs with additional, position-wise restrictions on the output domain, which are used to impose consistency between the modeled label sequences and morphosyntactic analysis results both at the level of decoding and, more importantly, in parameters estimation process. We decompose the problem of morphosyntactic disambiguation into two consecutive stages of the context-sensitive morphosyntactic guessing and the disambiguation proper. The solution involves extending CRFs with additional, position-wise restrictions on the output domain, which are used to impose consistency between the modeled label sequences and morphosyntactic analysis results both at the level of decoding and, more importantly, in parameters estimation process. We decompose the problem of morphosyntactic disambiguation into two consecutive stages of the context-sensitive morphosyntactic guessing and the disambiguation proper. The division helps in designing well-adjusted, CRF-based methods for both tasks, which in combination constitute Concraft, a highly accurate tagging system for the Polish language available under the 2-clause BSD license. Evaluation on the National Corpus of Polish shows that our solution significantly outperforms other state-of-the-art taggers for Polish -Pantera, WMBT and WCRFT -especially in terms of the accuracy measured with respect to unknown words."}, {"paper_id": "11620784", "adju_relevance": 0, "title": "Measuring Gradience in Speakers' Grammaticality Judgements", "background_label": "The question of whether grammaticality is a binary categorical or a gradient property has been the subject of ongoing debate in linguistics and psychology for many years. Linguists have tended to use constructed examples to test speakers\u2019 judgements on specific sorts of constraint violation.", "method_label": "We applied machine translation to randomly selected subsets of the British National Corpus (BNC) to generate a large test set which contains well-formed English source sentences, and sentences that exhibit a wide variety of grammatical infelicities. We tested a large number of speakers through (filtered) crowd sourcing, with three distinct modes of classification, one binary and two ordered scales. We found a high degree of correlation in mean judgements for sentences across the three classification tasks. We also did two visual image classification tasks to obtain benchmarks for binary and gradient judgement patterns, respectively.", "result_label": "Finally, we did a second crowd source experiment on 100 randomly selected linguistic textbook example sentences. The sentence judgement distributions for individual speakers strongly resemble the gradience benchmark pattern. This evidence suggests that speakers represent grammatical well-formedness as a gradient property.", "abstract": "The question of whether grammaticality is a binary categorical or a gradient property has been the subject of ongoing debate in linguistics and psychology for many years. The question of whether grammaticality is a binary categorical or a gradient property has been the subject of ongoing debate in linguistics and psychology for many years. Linguists have tended to use constructed examples to test speakers\u2019 judgements on specific sorts of constraint violation. We applied machine translation to randomly selected subsets of the British National Corpus (BNC) to generate a large test set which contains well-formed English source sentences, and sentences that exhibit a wide variety of grammatical infelicities. We applied machine translation to randomly selected subsets of the British National Corpus (BNC) to generate a large test set which contains well-formed English source sentences, and sentences that exhibit a wide variety of grammatical infelicities. We tested a large number of speakers through (filtered) crowd sourcing, with three distinct modes of classification, one binary and two ordered scales. We applied machine translation to randomly selected subsets of the British National Corpus (BNC) to generate a large test set which contains well-formed English source sentences, and sentences that exhibit a wide variety of grammatical infelicities. We tested a large number of speakers through (filtered) crowd sourcing, with three distinct modes of classification, one binary and two ordered scales. We found a high degree of correlation in mean judgements for sentences across the three classification tasks. We applied machine translation to randomly selected subsets of the British National Corpus (BNC) to generate a large test set which contains well-formed English source sentences, and sentences that exhibit a wide variety of grammatical infelicities. We tested a large number of speakers through (filtered) crowd sourcing, with three distinct modes of classification, one binary and two ordered scales. We found a high degree of correlation in mean judgements for sentences across the three classification tasks. We also did two visual image classification tasks to obtain benchmarks for binary and gradient judgement patterns, respectively. Finally, we did a second crowd source experiment on 100 randomly selected linguistic textbook example sentences. Finally, we did a second crowd source experiment on 100 randomly selected linguistic textbook example sentences. The sentence judgement distributions for individual speakers strongly resemble the gradience benchmark pattern. Finally, we did a second crowd source experiment on 100 randomly selected linguistic textbook example sentences. The sentence judgement distributions for individual speakers strongly resemble the gradience benchmark pattern. This evidence suggests that speakers represent grammatical well-formedness as a gradient property."}, {"paper_id": "3161350", "adju_relevance": 0, "title": "Experience and grammatical agreement: Statistical learning shapes number agreement production", "background_label": "A robust result in research on the production of grammatical agreement is that speakers are more likely to produce an erroneous verb with phrases such as the key to the cabinets, with a singular noun followed by a plural one, than with phrases such as the keys to the cabinet, where a plural noun is followed by a singular. These asymmetries are thought to reflect core language production processes. Previous accounts have attributed error patterns to a syntactic number feature present on plurals but not singulars.", "method_label": "An alternative approach is presented in which a process similar to structural priming contributes to the error asymmetry via speakers' past experiences with related agreement constructions. A corpus analysis and two agreement production studies test this account.", "result_label": "The results suggest that agreement production is shaped by statistical learning from past language experience. Implications for accounts of agreement are discussed.", "abstract": "A robust result in research on the production of grammatical agreement is that speakers are more likely to produce an erroneous verb with phrases such as the key to the cabinets, with a singular noun followed by a plural one, than with phrases such as the keys to the cabinet, where a plural noun is followed by a singular. A robust result in research on the production of grammatical agreement is that speakers are more likely to produce an erroneous verb with phrases such as the key to the cabinets, with a singular noun followed by a plural one, than with phrases such as the keys to the cabinet, where a plural noun is followed by a singular. These asymmetries are thought to reflect core language production processes. A robust result in research on the production of grammatical agreement is that speakers are more likely to produce an erroneous verb with phrases such as the key to the cabinets, with a singular noun followed by a plural one, than with phrases such as the keys to the cabinet, where a plural noun is followed by a singular. These asymmetries are thought to reflect core language production processes. Previous accounts have attributed error patterns to a syntactic number feature present on plurals but not singulars. An alternative approach is presented in which a process similar to structural priming contributes to the error asymmetry via speakers' past experiences with related agreement constructions. An alternative approach is presented in which a process similar to structural priming contributes to the error asymmetry via speakers' past experiences with related agreement constructions. A corpus analysis and two agreement production studies test this account. The results suggest that agreement production is shaped by statistical learning from past language experience. The results suggest that agreement production is shaped by statistical learning from past language experience. Implications for accounts of agreement are discussed."}, {"paper_id": "53219915", "adju_relevance": 0, "title": "Do RNNs learn human-like abstract word order preferences?", "background_label": "RNN language models have achieved state-of-the-art results on various tasks, but what exactly they are representing about syntax is as yet unclear.", "abstract": "RNN language models have achieved state-of-the-art results on various tasks, but what exactly they are representing about syntax is as yet unclear."}, {"paper_id": "52010508", "adju_relevance": 0, "title": "RNN Simulations of Grammaticality Judgments on Long-distance Dependencies", "background_label": "AbstractThe paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism.", "method_label": "The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality, at least in the domain we tested.", "result_label": "Title and Abstract in ItalianRNN Simulazioni di giudizi di grammaticit\u00e0 sulle dipendenze a distanza L'articolo studia la capacit\u00e0 delle reti neurali LSTM addestrate su un compito di modellazione linguistica di rilevare strutture linguistiche che sono agrammaticali a causa di violazioni nella estrazione di argomenti (dovute alla presenza di argomenti di troppo, o alla presenza di isole del soggetto e delle frasi relative), esplorando le implicazioni per il dibattito sull'innatismo linguistico. I risultati mostrano che l'attuale modello RNN pu\u00f2 classificare correttamente frasi grammaticali, in certe condizioni, ma\u00e8 eccessivamente sensibile a fattori di elaborazione linguistica e probabilmente non in grado di indurre una nozione pi\u00f9 astratta di grammaticalit\u00e0, almeno nel dominio da noi testato.", "abstract": "AbstractThe paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism. The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality, at least in the domain we tested. Title and Abstract in ItalianRNN Simulazioni di giudizi di grammaticit\u00e0 sulle dipendenze a distanza L'articolo studia la capacit\u00e0 delle reti neurali LSTM addestrate su un compito di modellazione linguistica di rilevare strutture linguistiche che sono agrammaticali a causa di violazioni nella estrazione di argomenti (dovute alla presenza di argomenti di troppo, o alla presenza di isole del soggetto e delle frasi relative), esplorando le implicazioni per il dibattito sull'innatismo linguistico. Title and Abstract in ItalianRNN Simulazioni di giudizi di grammaticit\u00e0 sulle dipendenze a distanza L'articolo studia la capacit\u00e0 delle reti neurali LSTM addestrate su un compito di modellazione linguistica di rilevare strutture linguistiche che sono agrammaticali a causa di violazioni nella estrazione di argomenti (dovute alla presenza di argomenti di troppo, o alla presenza di isole del soggetto e delle frasi relative), esplorando le implicazioni per il dibattito sull'innatismo linguistico. I risultati mostrano che l'attuale modello RNN pu\u00f2 classificare correttamente frasi grammaticali, in certe condizioni, ma\u00e8 eccessivamente sensibile a fattori di elaborazione linguistica e probabilmente non in grado di indurre una nozione pi\u00f9 astratta di grammaticalit\u00e0, almeno nel dominio da noi testato."}, {"paper_id": "21855669", "adju_relevance": 0, "title": "Quantifying complex patterns of bioacoustic variation: use of a neural network to compare killer whale (Orcinus orca) dialects.", "background_label": "A quantitative measure of acoustic similarity is crucial to any study comparing vocalizations of different species, social groups, or individuals.", "abstract": "A quantitative measure of acoustic similarity is crucial to any study comparing vocalizations of different species, social groups, or individuals."}, {"paper_id": "24041868", "adju_relevance": 0, "title": "The Controlled Natural Language of Randall Munroe's Thing Explainer", "background_label": "It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year. Randall Munroe's Thing Explainer uses only the 1'000 most often used words of the English language together with drawn pictures to explain complicated things such as nuclear reactors, jet engines, the solar system, and dishwashers. This restricted language is a very interesting new case for the CNL community.", "result_label": "I describe here its place in the context of existing approaches on Controlled Natural Languages, and I provide a first analysis from a scientific perspective, covering the word production rules and word distributions.", "abstract": "It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year. It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year. Randall Munroe's Thing Explainer uses only the 1'000 most often used words of the English language together with drawn pictures to explain complicated things such as nuclear reactors, jet engines, the solar system, and dishwashers. It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year. Randall Munroe's Thing Explainer uses only the 1'000 most often used words of the English language together with drawn pictures to explain complicated things such as nuclear reactors, jet engines, the solar system, and dishwashers. This restricted language is a very interesting new case for the CNL community. I describe here its place in the context of existing approaches on Controlled Natural Languages, and I provide a first analysis from a scientific perspective, covering the word production rules and word distributions."}, {"paper_id": "16959835", "adju_relevance": 0, "title": "Revisiting the syntactic abilities of non-human animals: natural vocalizations and artificial grammar learning", "background_label": "The domain of syntax is seen as the core of the language faculty and as the most critical difference between animal vocalizations and language. We review evidence from spontaneously produced vocalizations as well as from perceptual experiments using artificial grammars to analyse animal syntactic abilities, i.e.", "abstract": "The domain of syntax is seen as the core of the language faculty and as the most critical difference between animal vocalizations and language. The domain of syntax is seen as the core of the language faculty and as the most critical difference between animal vocalizations and language. We review evidence from spontaneously produced vocalizations as well as from perceptual experiments using artificial grammars to analyse animal syntactic abilities, i.e."}, {"paper_id": "57825754", "adju_relevance": 0, "title": "Linguistic Analysis of Pretrained Sentence Encoders with Acceptability Judgments", "background_label": "Recent work on evaluating grammatical knowledge in pretrained sentence encoders gives a fine-grained view of a small number of phenomena. We introduce a new analysis dataset that also has broad coverage of linguistic phenomena.", "method_label": "We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al. We find that these models have a strong command of complex or non-canonical argument structures like ditransitives (Sue gave Dan a book) and passives (The book was read). Sentences with long distance dependencies like questions (What do you think I ate?)", "result_label": "challenge all models, but for these, BERT and GPT have a distinct advantage over the baseline. We conclude that recent sentence encoders, despite showing near-human performance on acceptability classification overall, still fail to make fine-grained grammaticality distinctions for many complex syntactic structures.", "abstract": "Recent work on evaluating grammatical knowledge in pretrained sentence encoders gives a fine-grained view of a small number of phenomena. Recent work on evaluating grammatical knowledge in pretrained sentence encoders gives a fine-grained view of a small number of phenomena. We introduce a new analysis dataset that also has broad coverage of linguistic phenomena. We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al. We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al. We find that these models have a strong command of complex or non-canonical argument structures like ditransitives (Sue gave Dan a book) and passives (The book was read). We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al. We find that these models have a strong command of complex or non-canonical argument structures like ditransitives (Sue gave Dan a book) and passives (The book was read). Sentences with long distance dependencies like questions (What do you think I ate?) challenge all models, but for these, BERT and GPT have a distinct advantage over the baseline. challenge all models, but for these, BERT and GPT have a distinct advantage over the baseline. We conclude that recent sentence encoders, despite showing near-human performance on acceptability classification overall, still fail to make fine-grained grammaticality distinctions for many complex syntactic structures."}, {"paper_id": "25446416", "adju_relevance": 0, "title": "GEOMETRIC MORPHOMETRICS OF DEVELOPMENTAL INSTABILITY: ANALYZING PATTERNS OF FLUCTUATING ASYMMETRY WITH PROCRUSTES METHODS.", "background_label": "Although fluctuating asymmetry has become popular as a measure of developmental instability, few studies have examined its developmental basis.", "abstract": "Although fluctuating asymmetry has become popular as a measure of developmental instability, few studies have examined its developmental basis."}, {"paper_id": "10729723", "adju_relevance": 0, "title": "Neural network processing of natural language: II. Towards a unified model of corticostriatal function in learning sentence comprehension and non-linguistic sequencing.", "background_label": "A central issue in cognitive neuroscience today concerns how distributed neural networks in the brain that are used in language learning and processing can be involved in non-linguistic cognitive sequence learning. This issue is informed by a wealth of functional neurophysiology studies of sentence comprehension, along with a number of recent studies that examined the brain processes involved in learning non-linguistic sequences, or artificial grammar learning (AGL).", "abstract": "A central issue in cognitive neuroscience today concerns how distributed neural networks in the brain that are used in language learning and processing can be involved in non-linguistic cognitive sequence learning. A central issue in cognitive neuroscience today concerns how distributed neural networks in the brain that are used in language learning and processing can be involved in non-linguistic cognitive sequence learning. This issue is informed by a wealth of functional neurophysiology studies of sentence comprehension, along with a number of recent studies that examined the brain processes involved in learning non-linguistic sequences, or artificial grammar learning (AGL)."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "60232836", "adju_relevance": 0, "title": "The ICNALE and Sophisticated Contrastive Interlanguage Analysis of Asian Learners of English", "background_label": "The International Corpus Network of Asian Learners of English (ICNALE) is a new learner corpus designed for a reliable contrastive interlanguage analysis of varied English learners in Asia. The ICNALE, in which writing conditions are controlled more strictly compared with other major learner corpora, allows researchers to examine the differences between writer groups in greater detail.", "abstract": "The International Corpus Network of Asian Learners of English (ICNALE) is a new learner corpus designed for a reliable contrastive interlanguage analysis of varied English learners in Asia. The International Corpus Network of Asian Learners of English (ICNALE) is a new learner corpus designed for a reliable contrastive interlanguage analysis of varied English learners in Asia. The ICNALE, in which writing conditions are controlled more strictly compared with other major learner corpora, allows researchers to examine the differences between writer groups in greater detail."}]