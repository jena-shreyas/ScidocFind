[{"paper_id": "174799296", "title": "From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions", "method_label": "We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences.", "result_label": "In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.", "abstract": "We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall."}, {"paper_id": "106402715", "adju_relevance": 2, "title": "A Structural Probe for Finding Syntax in Word Representations", "background_label": "AbstractRecent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety.", "abstract": "AbstractRecent work has improved our ability to detect linguistic knowledge in word representations. AbstractRecent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety."}, {"paper_id": "21663989", "adju_relevance": 2, "title": "Deep RNNs Encode Soft Hierarchical Syntax", "background_label": "We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision.", "method_label": "We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling.", "result_label": "In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.", "abstract": "We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision."}, {"paper_id": "53083604", "adju_relevance": 2, "title": "Phrase-level Self-Attention Networks for Universal Sentence Encoding", "background_label": "AbstractUniversal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time. However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected.", "method_label": "To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word's representation hierarchically with longer-term context dependencies captured in a larger phrase.", "result_label": "As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. At the same time, syntactic information can be easily integrated in the model. Experiment results show that PSAN can achieve the state-ofthe-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence textual similarity.", "abstract": "AbstractUniversal sentence encoding is a hot topic in recent NLP research. AbstractUniversal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence. AbstractUniversal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time. AbstractUniversal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time. However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected. To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word's representation hierarchically with longer-term context dependencies captured in a larger phrase. As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. At the same time, syntactic information can be easily integrated in the model. As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. At the same time, syntactic information can be easily integrated in the model. Experiment results show that PSAN can achieve the state-ofthe-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence textual similarity."}, {"paper_id": "14091946", "adju_relevance": 2, "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "background_label": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?", "method_label": "We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting.", "result_label": "We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured."}, {"paper_id": "184486755", "adju_relevance": 2, "title": "Analyzing the Structure of Attention in a Transformer Language Model", "background_label": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks.", "abstract": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks."}, {"paper_id": "52938959", "adju_relevance": 1, "title": "Phrase-Based Attentions", "background_label": "Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation.", "abstract": "Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation."}, {"paper_id": "19206893", "adju_relevance": 1, "title": "Constituency Parsing with a Self-Attentive Encoder", "background_label": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser.", "method_label": "The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation.", "result_label": "Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.", "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset."}, {"paper_id": "202539179", "adju_relevance": 1, "title": "Multi-Granularity Self-Attention for Neural Machine Translation", "background_label": "Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases.", "abstract": "Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases."}, {"paper_id": "13756489", "adju_relevance": 1, "title": "Attention Is All You Need", "background_label": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism.", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism."}, {"paper_id": "7197724", "adju_relevance": 1, "title": "Does String-Based Neural MT Learn Source Syntax?", "background_label": "AbstractWe investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training.", "method_label": "We propose two methods to detect whether the encoder has learned local and global source syntax.", "result_label": "A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing.", "abstract": "AbstractWe investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing."}, {"paper_id": "2265207", "adju_relevance": 1, "title": "Structural Embedding of Syntactic Trees for Machine Comprehension", "background_label": "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees.", "abstract": "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees."}, {"paper_id": "52845092", "adju_relevance": 1, "title": "Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis", "background_label": "Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn.", "method_label": "With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information. We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the LSTM architecture. We find that representations from language models consistently perform best on our syntactic auxiliary prediction tasks, even when trained on relatively small amounts of data.", "result_label": "These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information. We also find that the representations from randomly-initialized, frozen LSTMs perform strikingly well on our syntactic auxiliary tasks, but this effect disappears when the amount of training data for the auxiliary tasks is reduced.", "abstract": "Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models. Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information. With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information. We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the LSTM architecture. With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information. We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the LSTM architecture. We find that representations from language models consistently perform best on our syntactic auxiliary prediction tasks, even when trained on relatively small amounts of data. These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information. These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information. We also find that the representations from randomly-initialized, frozen LSTMs perform strikingly well on our syntactic auxiliary tasks, but this effect disappears when the amount of training data for the auxiliary tasks is reduced."}, {"paper_id": "201694322", "adju_relevance": 1, "title": "Incorporating Source Syntax into Transformer-Based Neural Machine Translation", "background_label": "AbstractTransformer-based neural machine translation (NMT) has recently achieved state-ofthe-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases.", "method_label": "In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it.We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences.", "result_label": "We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low-and medium-resource NMT but degenerates high-resource English\u2192German translation.", "abstract": "AbstractTransformer-based neural machine translation (NMT) has recently achieved state-ofthe-art performance on many machine translation tasks. AbstractTransformer-based neural machine translation (NMT) has recently achieved state-ofthe-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it.We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low-and medium-resource NMT but degenerates high-resource English\u2192German translation."}, {"paper_id": "52100282", "adju_relevance": 1, "title": "Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures", "background_label": "Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth.", "method_label": "We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required).", "result_label": "Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.", "abstract": "Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation."}, {"paper_id": "18193214", "adju_relevance": 1, "title": "Supervised Attentions for Neural Machine Translation", "background_label": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs.", "method_label": "We simply compute the distance between the machine attentions and the\"true\"alignments, and minimize this cost in the training procedure.", "result_label": "Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "abstract": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the\"true\"alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system."}, {"paper_id": "13292366", "adju_relevance": 1, "title": "Neural Machine Translation with Supervised Attention", "background_label": "The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy.", "abstract": "The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy."}, {"paper_id": "12851711", "adju_relevance": 1, "title": "Tree-to-Sequence Attentional Neural Machine Translation", "background_label": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information.", "abstract": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information."}, {"paper_id": "174799480", "adju_relevance": 1, "title": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future", "background_label": "Common language models typically predict the next word given the context.", "method_label": "In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets.", "result_label": "We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.", "abstract": "Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation."}, {"paper_id": "202540407", "adju_relevance": 1, "title": "Self-Attention with Structural Position Representations", "background_label": "Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018).", "abstract": "Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018)."}, {"paper_id": "85990031", "adju_relevance": 0, "title": "Castles built on sand : dysfunctionality in plankton models and the inadequacy of dialogue between biologists and modellers", "background_label": "Although lip service is often paid to the involvement of modellers in the design of biological experiments and to a lesser degree to a role for biologists in construction of dynamic models, on closer examination the ultimate communication failings and associated waste of effort are all too obvious. Biologists need to work with modellers to ensure that data collected are more amenable to modelling (notably C-N-P biomass, rather than just Chl, or organism numbers), to measure the fate of non- or lesser-limiting nutrients, and the release/production of particulate and dissolved organics from organisms.", "method_label": "Modellers should not omit representations of biological behaviour unless it is demonstrated (empirically and/or mathematically) that it is safe to do so; the performance of each part of an ecosystem model should be demonstrated as being fit for purpose and not dysfunctional.", "result_label": "Modelling should be accepted as a research tool within biology and ecology with just as much emphasis as enjoyed by statistical and molecular methods.", "abstract": "Although lip service is often paid to the involvement of modellers in the design of biological experiments and to a lesser degree to a role for biologists in construction of dynamic models, on closer examination the ultimate communication failings and associated waste of effort are all too obvious. Although lip service is often paid to the involvement of modellers in the design of biological experiments and to a lesser degree to a role for biologists in construction of dynamic models, on closer examination the ultimate communication failings and associated waste of effort are all too obvious. Biologists need to work with modellers to ensure that data collected are more amenable to modelling (notably C-N-P biomass, rather than just Chl, or organism numbers), to measure the fate of non- or lesser-limiting nutrients, and the release/production of particulate and dissolved organics from organisms. Modellers should not omit representations of biological behaviour unless it is demonstrated (empirically and/or mathematically) that it is safe to do so; the performance of each part of an ecosystem model should be demonstrated as being fit for purpose and not dysfunctional. Modelling should be accepted as a research tool within biology and ecology with just as much emphasis as enjoyed by statistical and molecular methods."}, {"paper_id": "7113872", "adju_relevance": 0, "title": "Classify or Select: Neural Architectures for Extractive Document Summarization", "background_label": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary.", "method_label": "The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy.", "result_label": "We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.", "abstract": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence."}, {"paper_id": "6692380", "adju_relevance": 0, "title": "Identifying missing dictionary entries with frequency-conserving context models", "method_label": "A number of these methods for organization fall into a family defined by word ordering. Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study. To do so, we employ our previously developed framework for generating word-conserving phrase-frequency data. Upon training our model with the Wiktionary---an extensive, online, collaborative, and open-source dictionary that contains over 100,000 phrasal-definitions---we develop highly effective filters for the identification of meaningful, missing phrase-entries.", "background_label": "Unlike demographic or spatial partitions of data, these collocation models are of special importance for their universal applicability. While we are interested here in text and have framed our treatment appropriately, our work is potentially applicable to other areas of research (e.g., speech, genomics, and mobility patterns) where one has ordered categorical data, (e.g., sounds, genes, and locations).", "result_label": "With our predictions we then engage the editorial community of the Wiktionary and propose short lists of potential missing entries for definition, developing a breakthrough, lexical extraction technique, and expanding our knowledge of the defined English lexicon of phrases.", "abstract": " A number of these methods for organization fall into a family defined by word ordering. Unlike demographic or spatial partitions of data, these collocation models are of special importance for their universal applicability. Unlike demographic or spatial partitions of data, these collocation models are of special importance for their universal applicability. While we are interested here in text and have framed our treatment appropriately, our work is potentially applicable to other areas of research (e.g., speech, genomics, and mobility patterns) where one has ordered categorical data, (e.g., sounds, genes, and locations). A number of these methods for organization fall into a family defined by word ordering. Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study. A number of these methods for organization fall into a family defined by word ordering. Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study. To do so, we employ our previously developed framework for generating word-conserving phrase-frequency data. A number of these methods for organization fall into a family defined by word ordering. Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study. To do so, we employ our previously developed framework for generating word-conserving phrase-frequency data. Upon training our model with the Wiktionary---an extensive, online, collaborative, and open-source dictionary that contains over 100,000 phrasal-definitions---we develop highly effective filters for the identification of meaningful, missing phrase-entries. With our predictions we then engage the editorial community of the Wiktionary and propose short lists of potential missing entries for definition, developing a breakthrough, lexical extraction technique, and expanding our knowledge of the defined English lexicon of phrases."}, {"paper_id": "4790050", "adju_relevance": 0, "title": "ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters", "background_label": "We analyze 26.2 million comments published in Arabic language on Twitter, from July 2014 to January 2015, when ISIS' strength reached its peak and the group was prominently expanding the territorial area under its control.", "method_label": "By doing that, we are able to measure the share of support and aversion toward the Islamic State within the online Arab communities. We then investigate two specific topics. First, by exploiting the time-granularity of the tweets, we link the opinions with daily events to understand the main determinants of the changing trend in support toward ISIS.", "result_label": "Second, by taking advantage of the geographical locations of tweets, we explore the relationship between online opinions across countries and the number of foreign fighters joining ISIS.", "abstract": "We analyze 26.2 million comments published in Arabic language on Twitter, from July 2014 to January 2015, when ISIS' strength reached its peak and the group was prominently expanding the territorial area under its control. By doing that, we are able to measure the share of support and aversion toward the Islamic State within the online Arab communities. By doing that, we are able to measure the share of support and aversion toward the Islamic State within the online Arab communities. We then investigate two specific topics. By doing that, we are able to measure the share of support and aversion toward the Islamic State within the online Arab communities. We then investigate two specific topics. First, by exploiting the time-granularity of the tweets, we link the opinions with daily events to understand the main determinants of the changing trend in support toward ISIS. Second, by taking advantage of the geographical locations of tweets, we explore the relationship between online opinions across countries and the number of foreign fighters joining ISIS."}, {"paper_id": "13666623", "adju_relevance": 0, "title": "Obligation and Prohibition Extraction Using Hierarchical RNNs", "background_label": "We consider the task of detecting contractual obligations and prohibitions.", "method_label": "We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence.", "result_label": "Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.", "abstract": "We consider the task of detecting contractual obligations and prohibitions. We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view."}, {"paper_id": "6219199", "adju_relevance": 0, "title": "Syntactic Annotation of a German Newspaper Corpus", "background_label": "We report on the syntactic annotation of a German newspaper corpus. The annotations consist of context-free structures, additionally allowing crossing branches, with labeled nodes (phrases) and edges (grammatical functions).", "method_label": "Furthermore, we present a new, interactive semi-automatic annotation process that allows efficient and reliable annotations. The annotation process is sped up by incrementally presenting structures and by automatically highlighting unreliable assignments.", "abstract": "We report on the syntactic annotation of a German newspaper corpus. We report on the syntactic annotation of a German newspaper corpus. The annotations consist of context-free structures, additionally allowing crossing branches, with labeled nodes (phrases) and edges (grammatical functions). Furthermore, we present a new, interactive semi-automatic annotation process that allows efficient and reliable annotations. Furthermore, we present a new, interactive semi-automatic annotation process that allows efficient and reliable annotations. The annotation process is sped up by incrementally presenting structures and by automatically highlighting unreliable assignments."}, {"paper_id": "144498228", "adju_relevance": 0, "title": "Totalitarian Language: Orwell's Newspeak and Its Nazi and Communist Antecedents", "background_label": "In this analysis of the language of totalitarianism, John Wesley Young examines the manipulation of language by Nazi and Communist regimes. Relating the language of totalitarian regimes to the language Newspeak in George Orwell's satirical novel \"1984\", Young addresses the similarities and differences between the real and fictional languages, demonstrates the accuracy of Newspeak, and explores the degree of control that language can exert over the thought and behaviour of a people. Based on Orwell's own perceptions of political, literary and linguistic practices, Newspeak is used in the novel by a totalitarian dictatorship to create a distorted view of reality and to control the thought and behaviour of its subjects. It is a language in which heretical ideas cannot be expressed or even concieved.", "method_label": "Young's analysis of the speeches and writings of actual dictators, particularly in Nazi Germany and the Soviet Union, reveals a similar propagandistic structure in the use, or misuse, of language, the purpose of which was to enslave and control the populace.", "result_label": "Young's analysis of the effects of these languages on the subjects of totalitarian dictatorships reveals that although Orwell's Newspeak was a potent instrument of control over the thought and behaviour of the citizens, the language of actual totalitarian regimes was not nearly so successful. When official rhetoric and reality did not agree, the skepticism and cynicism of the people gave rise to the phenomenon of counterlanguages that attempted to reinscribe the reality of life under dictorial rule. Young evaluates the extent to which thought control succeeded under the Nazi and Soviet regimes and considers the implications that these findings offer for linguistic determinism.", "abstract": "In this analysis of the language of totalitarianism, John Wesley Young examines the manipulation of language by Nazi and Communist regimes. In this analysis of the language of totalitarianism, John Wesley Young examines the manipulation of language by Nazi and Communist regimes. Relating the language of totalitarian regimes to the language Newspeak in George Orwell's satirical novel \"1984\", Young addresses the similarities and differences between the real and fictional languages, demonstrates the accuracy of Newspeak, and explores the degree of control that language can exert over the thought and behaviour of a people. In this analysis of the language of totalitarianism, John Wesley Young examines the manipulation of language by Nazi and Communist regimes. Relating the language of totalitarian regimes to the language Newspeak in George Orwell's satirical novel \"1984\", Young addresses the similarities and differences between the real and fictional languages, demonstrates the accuracy of Newspeak, and explores the degree of control that language can exert over the thought and behaviour of a people. Based on Orwell's own perceptions of political, literary and linguistic practices, Newspeak is used in the novel by a totalitarian dictatorship to create a distorted view of reality and to control the thought and behaviour of its subjects. In this analysis of the language of totalitarianism, John Wesley Young examines the manipulation of language by Nazi and Communist regimes. Relating the language of totalitarian regimes to the language Newspeak in George Orwell's satirical novel \"1984\", Young addresses the similarities and differences between the real and fictional languages, demonstrates the accuracy of Newspeak, and explores the degree of control that language can exert over the thought and behaviour of a people. Based on Orwell's own perceptions of political, literary and linguistic practices, Newspeak is used in the novel by a totalitarian dictatorship to create a distorted view of reality and to control the thought and behaviour of its subjects. It is a language in which heretical ideas cannot be expressed or even concieved. Young's analysis of the speeches and writings of actual dictators, particularly in Nazi Germany and the Soviet Union, reveals a similar propagandistic structure in the use, or misuse, of language, the purpose of which was to enslave and control the populace. Young's analysis of the effects of these languages on the subjects of totalitarian dictatorships reveals that although Orwell's Newspeak was a potent instrument of control over the thought and behaviour of the citizens, the language of actual totalitarian regimes was not nearly so successful. Young's analysis of the effects of these languages on the subjects of totalitarian dictatorships reveals that although Orwell's Newspeak was a potent instrument of control over the thought and behaviour of the citizens, the language of actual totalitarian regimes was not nearly so successful. When official rhetoric and reality did not agree, the skepticism and cynicism of the people gave rise to the phenomenon of counterlanguages that attempted to reinscribe the reality of life under dictorial rule. Young's analysis of the effects of these languages on the subjects of totalitarian dictatorships reveals that although Orwell's Newspeak was a potent instrument of control over the thought and behaviour of the citizens, the language of actual totalitarian regimes was not nearly so successful. When official rhetoric and reality did not agree, the skepticism and cynicism of the people gave rise to the phenomenon of counterlanguages that attempted to reinscribe the reality of life under dictorial rule. Young evaluates the extent to which thought control succeeded under the Nazi and Soviet regimes and considers the implications that these findings offer for linguistic determinism."}, {"paper_id": "67856251", "adju_relevance": 0, "title": "Bridging the Gap: Attending to Discontinuity in Identification of Multiword Expressions", "method_label": "We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined model that integrates complementary information from both through a gating mechanism.", "result_label": "The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score.", "abstract": "We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined model that integrates complementary information from both through a gating mechanism. The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score."}, {"paper_id": "2783229", "adju_relevance": 0, "title": "\"Dahntahn\" Pittsburgh: Monophthongal /aw/ and Representations of Localness in Southwestern Pennsylvania", "background_label": "\ufffdIn this paper we report on an exploratory study of the history of the monophthongization of the diphthong /aw/ in Pittsburgh, Pennsylvania. We suggest that the persistence of this feature may be linked to the dominant role it plays in print representations of local-sounding speech.", "abstract": "\ufffdIn this paper we report on an exploratory study of the history of the monophthongization of the diphthong /aw/ in Pittsburgh, Pennsylvania. \ufffdIn this paper we report on an exploratory study of the history of the monophthongization of the diphthong /aw/ in Pittsburgh, Pennsylvania. We suggest that the persistence of this feature may be linked to the dominant role it plays in print representations of local-sounding speech."}, {"paper_id": "17947758", "adju_relevance": 0, "title": "A co-located interface for narration to support reconciliation in a conflict: initial results from Jewish and Palestinian youth", "background_label": "So called intractable conflicts may benefit from more modest and socially oriented approaches than those based on classical conflict resolution techniques. This paper is inspired by theories on small group intervention in a conflict. The general claim is that participants may achieve a greater understanding of and appreciation for the other's viewpoint under conditions that support partaking in a tangible joint task and creating a shared narration.", "abstract": "So called intractable conflicts may benefit from more modest and socially oriented approaches than those based on classical conflict resolution techniques. So called intractable conflicts may benefit from more modest and socially oriented approaches than those based on classical conflict resolution techniques. This paper is inspired by theories on small group intervention in a conflict. So called intractable conflicts may benefit from more modest and socially oriented approaches than those based on classical conflict resolution techniques. This paper is inspired by theories on small group intervention in a conflict. The general claim is that participants may achieve a greater understanding of and appreciation for the other's viewpoint under conditions that support partaking in a tangible joint task and creating a shared narration."}, {"paper_id": "44117283", "adju_relevance": 0, "title": "Sluice Resolution without Hand-Crafted Features over Brittle Syntax Trees", "background_label": "AbstractSluice resolution in English is the problem of finding antecedents of wh-fronted ellipses. Previous work has relied on handcrafted features over syntax trees that scale poorly to other languages and domains; in particular, to dialogue, which is one of the most interesting applications of sluice resolution.", "method_label": "Syntactic information is arguably important for sluice resolution, but we show that multi-task learning with partial parsing as auxiliary tasks effectively closes the gap and buys us an additional 9% error reduction over previous work.", "result_label": "Since we are not directly relying on features from partial parsers, our system is more robust to domain shifts, giving a 26% error reduction on embedded sluices in dialogue.", "abstract": "AbstractSluice resolution in English is the problem of finding antecedents of wh-fronted ellipses. AbstractSluice resolution in English is the problem of finding antecedents of wh-fronted ellipses. Previous work has relied on handcrafted features over syntax trees that scale poorly to other languages and domains; in particular, to dialogue, which is one of the most interesting applications of sluice resolution. Syntactic information is arguably important for sluice resolution, but we show that multi-task learning with partial parsing as auxiliary tasks effectively closes the gap and buys us an additional 9% error reduction over previous work. Since we are not directly relying on features from partial parsers, our system is more robust to domain shifts, giving a 26% error reduction on embedded sluices in dialogue."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "60771008", "adju_relevance": 0, "title": "Verbnet: a broad-coverage, comprehensive verb lexicon", "background_label": "Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. To evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank. VerbNet syntactic frames account for over 84% exact matches to the frames found in PropBank. VerbNet provides mappings between its verbs and WordNet senses and between its verbs and FameNet II frames, and mappings between the syntactic frames and Xtag tree families. All these resources are complementary and can be used as extensions of each other.", "method_label": "In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. One of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment. The original set of classes described by Levin has been refined and extended in many ways through systematic efforts: the coverage experiment against PropBank corpus instances proposed a large set of new syntactic frames and a better treatment of prepositions; new classes from Korhonen and Briscoe's resource were integrated into the lexicon; and new members from the LCS database were added. Taking advantage of VerbNet's class-based approach automatic acquisition methods were investigated. Additional verbs derived from Kingsbury's clustering experiments and from Loper's VerbNet-WordNet correlation experiment were integrated into the lexicon.", "result_label": "In order to support the animation of the actions, PARs have to make explicit many details that are often underspecified in the language. These experiments show that it is possible to semi-automatically supplement and tune VerbNet with novel information from corpus data. These approaches reduce the manual classification and enable easy adaptation of the lexicon to specific tasks and applications.", "abstract": "Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. One of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment. In order to support the animation of the actions, PARs have to make explicit many details that are often underspecified in the language. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. To evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. To evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank. VerbNet syntactic frames account for over 84% exact matches to the frames found in PropBank. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. To evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank. VerbNet syntactic frames account for over 84% exact matches to the frames found in PropBank. VerbNet provides mappings between its verbs and WordNet senses and between its verbs and FameNet II frames, and mappings between the syntactic frames and Xtag tree families. Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications. James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs. WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions. Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora. This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua. To evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank. VerbNet syntactic frames account for over 84% exact matches to the frames found in PropBank. VerbNet provides mappings between its verbs and WordNet senses and between its verbs and FameNet II frames, and mappings between the syntactic frames and Xtag tree families. All these resources are complementary and can be used as extensions of each other. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. One of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment. The original set of classes described by Levin has been refined and extended in many ways through systematic efforts: the coverage experiment against PropBank corpus instances proposed a large set of new syntactic frames and a better treatment of prepositions; new classes from Korhonen and Briscoe's resource were integrated into the lexicon; and new members from the LCS database were added. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. One of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment. The original set of classes described by Levin has been refined and extended in many ways through systematic efforts: the coverage experiment against PropBank corpus instances proposed a large set of new syntactic frames and a better treatment of prepositions; new classes from Korhonen and Briscoe's resource were integrated into the lexicon; and new members from the LCS database were added. Taking advantage of VerbNet's class-based approach automatic acquisition methods were investigated. In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries. Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties. Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments. One of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment. The original set of classes described by Levin has been refined and extended in many ways through systematic efforts: the coverage experiment against PropBank corpus instances proposed a large set of new syntactic frames and a better treatment of prepositions; new classes from Korhonen and Briscoe's resource were integrated into the lexicon; and new members from the LCS database were added. Taking advantage of VerbNet's class-based approach automatic acquisition methods were investigated. Additional verbs derived from Kingsbury's clustering experiments and from Loper's VerbNet-WordNet correlation experiment were integrated into the lexicon. In order to support the animation of the actions, PARs have to make explicit many details that are often underspecified in the language. These experiments show that it is possible to semi-automatically supplement and tune VerbNet with novel information from corpus data. In order to support the animation of the actions, PARs have to make explicit many details that are often underspecified in the language. These experiments show that it is possible to semi-automatically supplement and tune VerbNet with novel information from corpus data. These approaches reduce the manual classification and enable easy adaptation of the lexicon to specific tasks and applications."}, {"paper_id": "202539273", "adju_relevance": 0, "title": "Improving Neural Machine Translation with Parent-Scaled Self-Attention", "background_label": "Most neural machine translation (NMT) models operate on source and target sentences, treating them as sequences of words and neglecting their syntactic structure. Recent studies have shown that embedding the syntax information of a source sentence in recurrent neural networks can improve their translation accuracy, especially for low-resource language pairs. However, state-of-the-art NMT models are based on self-attention networks (e.g., Transformer), in which it is still not clear how to best embed syntactic information.", "abstract": "Most neural machine translation (NMT) models operate on source and target sentences, treating them as sequences of words and neglecting their syntactic structure. Most neural machine translation (NMT) models operate on source and target sentences, treating them as sequences of words and neglecting their syntactic structure. Recent studies have shown that embedding the syntax information of a source sentence in recurrent neural networks can improve their translation accuracy, especially for low-resource language pairs. Most neural machine translation (NMT) models operate on source and target sentences, treating them as sequences of words and neglecting their syntactic structure. Recent studies have shown that embedding the syntax information of a source sentence in recurrent neural networks can improve their translation accuracy, especially for low-resource language pairs. However, state-of-the-art NMT models are based on self-attention networks (e.g., Transformer), in which it is still not clear how to best embed syntactic information."}, {"paper_id": "91183938", "adju_relevance": 0, "title": "A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations", "background_label": "We propose a generative model for a sentence that uses two latent variables, with one intended to represent the syntax of the sentence and the other to represent its semantics.", "method_label": "We show we can achieve better disentanglement between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphrastic sentences and word-order information. We also investigate the effect of moving from bag-of-words to recurrent neural network modules.", "result_label": "We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks. Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations.", "abstract": "We propose a generative model for a sentence that uses two latent variables, with one intended to represent the syntax of the sentence and the other to represent its semantics. We show we can achieve better disentanglement between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphrastic sentences and word-order information. We show we can achieve better disentanglement between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphrastic sentences and word-order information. We also investigate the effect of moving from bag-of-words to recurrent neural network modules. We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks. We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks. Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations."}, {"paper_id": "52074926", "adju_relevance": 0, "title": "Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model", "background_label": "Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency graph or constituent trees.", "method_label": "In this paper, we first propose to use the \\textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features. We further employ a graph-to-sequence model to encode the syntactic graph and decode a logical form.", "result_label": "Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.", "abstract": "Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency graph or constituent trees. In this paper, we first propose to use the \\textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features. In this paper, we first propose to use the \\textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features. We further employ a graph-to-sequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information."}, {"paper_id": "106868491", "adju_relevance": 0, "title": "Learning to Eat Soup with a Knife: Counterinsurgency Lessons from Malaya and Vietnam", "background_label": "Invariably, armies are accused of preparing to fight the previous war. In \"Learning to Eat Soup with a Knife\", Lieutenant Colonel John A. Nagl - a veteran of both Operation Desert Storm and the current conflict in Iraq - considers the now-crucial question of how armies adapt to changing circumstances during the course of conflicts for which they are initially unprepared.", "method_label": "Through the use of archival sources and interviews with participants in both engagements, Nagl compares the development of counterinsurgency doctrine and practice in the Malayan Emergency from 1948 to 1960 with what developed in the Vietnam War from 1950 to 1975. In examining these two events, Nagl - the subject of a recent New York Times Magazine cover story by Peter Maass - argues that organizational culture is key to the ability to learn from unanticipated conditions, a variable which explains why the British army successfully conducted counterinsurgency in Malaya but why the American army failed to do so in Vietnam, treating the war instead as a conventional conflict. Nagl concludes that the British army, because of its role as a colonial police force and the organizational characteristics created by its history and national culture, was better able to quickly learn and apply the lessons of counterinsurgency during the course of the Malayan Emergency.", "result_label": "With a new preface reflecting on the author's combat experience in Iraq, \"Learning to Eat Soup with a Knife\" is a timely examination of the lessons of previous counterinsurgency campaigns that will be hailed by both military leaders and interested civilians.", "abstract": "Invariably, armies are accused of preparing to fight the previous war. Invariably, armies are accused of preparing to fight the previous war. In \"Learning to Eat Soup with a Knife\", Lieutenant Colonel John A. Nagl - a veteran of both Operation Desert Storm and the current conflict in Iraq - considers the now-crucial question of how armies adapt to changing circumstances during the course of conflicts for which they are initially unprepared. Through the use of archival sources and interviews with participants in both engagements, Nagl compares the development of counterinsurgency doctrine and practice in the Malayan Emergency from 1948 to 1960 with what developed in the Vietnam War from 1950 to 1975. Through the use of archival sources and interviews with participants in both engagements, Nagl compares the development of counterinsurgency doctrine and practice in the Malayan Emergency from 1948 to 1960 with what developed in the Vietnam War from 1950 to 1975. In examining these two events, Nagl - the subject of a recent New York Times Magazine cover story by Peter Maass - argues that organizational culture is key to the ability to learn from unanticipated conditions, a variable which explains why the British army successfully conducted counterinsurgency in Malaya but why the American army failed to do so in Vietnam, treating the war instead as a conventional conflict. Through the use of archival sources and interviews with participants in both engagements, Nagl compares the development of counterinsurgency doctrine and practice in the Malayan Emergency from 1948 to 1960 with what developed in the Vietnam War from 1950 to 1975. In examining these two events, Nagl - the subject of a recent New York Times Magazine cover story by Peter Maass - argues that organizational culture is key to the ability to learn from unanticipated conditions, a variable which explains why the British army successfully conducted counterinsurgency in Malaya but why the American army failed to do so in Vietnam, treating the war instead as a conventional conflict. Nagl concludes that the British army, because of its role as a colonial police force and the organizational characteristics created by its history and national culture, was better able to quickly learn and apply the lessons of counterinsurgency during the course of the Malayan Emergency. With a new preface reflecting on the author's combat experience in Iraq, \"Learning to Eat Soup with a Knife\" is a timely examination of the lessons of previous counterinsurgency campaigns that will be hailed by both military leaders and interested civilians."}, {"paper_id": "38407095", "adju_relevance": 0, "title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "background_label": "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web1. This corpus has found widespread use in the NLP community.", "abstract": "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web1. We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web1. This corpus has found widespread use in the NLP community."}, {"paper_id": "62172538", "adju_relevance": 0, "title": "A Fully-Lexicalized Probabilistic Model for Japanese Syntactic and Case Structure Analysis", "background_label": "We present an integrated probabilistic model for Japanese syntactic and case structure analysis.", "method_label": "Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure.", "result_label": "In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers.", "abstract": "We present an integrated probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers."}, {"paper_id": "53629257", "adju_relevance": 0, "title": "Investigating NP-Chunking with Universal Dependencies for English", "background_label": "AbstractChunking is a pre-processing task generally dedicated to improving constituency parsing.", "abstract": "AbstractChunking is a pre-processing task generally dedicated to improving constituency parsing."}, {"paper_id": "202558670", "adju_relevance": 0, "title": "Dependency-Aware Named Entity Recognition with Relative and Global Attentions", "background_label": "Named entity recognition is one of the core tasks in NLP. Although many improvements have been made on this task during the last years, the state-of-the-art systems do not explicitly take into account the recursive nature of language.", "method_label": "Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees. Then, we enhance these features by applying relative and global attention mechanisms. On the one hand, the relative attention detects the most informative words in the sentence with respect to the word being evaluated. On the other hand, the global attention spots the most relevant words in the sequence. Lastly, we linearly project the weighted vectors into the tagging space so that a conditional random field classifier predicts the entity labels.", "result_label": "Our findings show that the model detects words that disclose the entity types based on their syntactic roles in a sentence (e.g., verbs such as speak and write are attended when the entity type is PERSON, whereas meet and travel strongly relate to LOCATION). We confirm our findings and establish a new state of the art on two datasets.", "abstract": "Named entity recognition is one of the core tasks in NLP. Named entity recognition is one of the core tasks in NLP. Although many improvements have been made on this task during the last years, the state-of-the-art systems do not explicitly take into account the recursive nature of language. Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees. Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees. Then, we enhance these features by applying relative and global attention mechanisms. Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees. Then, we enhance these features by applying relative and global attention mechanisms. On the one hand, the relative attention detects the most informative words in the sentence with respect to the word being evaluated. Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees. Then, we enhance these features by applying relative and global attention mechanisms. On the one hand, the relative attention detects the most informative words in the sentence with respect to the word being evaluated. On the other hand, the global attention spots the most relevant words in the sequence. Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures. Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees. Then, we enhance these features by applying relative and global attention mechanisms. On the one hand, the relative attention detects the most informative words in the sentence with respect to the word being evaluated. On the other hand, the global attention spots the most relevant words in the sequence. Lastly, we linearly project the weighted vectors into the tagging space so that a conditional random field classifier predicts the entity labels. Our findings show that the model detects words that disclose the entity types based on their syntactic roles in a sentence (e.g., verbs such as speak and write are attended when the entity type is PERSON, whereas meet and travel strongly relate to LOCATION). Our findings show that the model detects words that disclose the entity types based on their syntactic roles in a sentence (e.g., verbs such as speak and write are attended when the entity type is PERSON, whereas meet and travel strongly relate to LOCATION). We confirm our findings and establish a new state of the art on two datasets."}, {"paper_id": "14068874", "adju_relevance": 0, "title": "The Stanford CoreNLP Natural Language Processing Toolkit", "background_label": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology.", "result_label": "We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.", "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."}, {"paper_id": "10015691", "adju_relevance": 0, "title": "Morphological and Syntactic Case in Statistical Dependency Parsing", "background_label": "Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb. The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological (case) information prior to automatic syntactic analysis. Parsing experiments on Czech, German, and Hungarian show that this approach is susceptible to propagating morphological annotation errors when parsing languages displaying syncretism in their morphological case paradigms.", "method_label": "We develop a different architecture where we use case as a possibly underspecified filtering device restricting the options for syntactic analysis. Carefully designed morpho-syntactic constraints can delimit the search space of a statistical dependency parser and exclude solutions that would violate the restrictions overtly marked in the morphology of the words in a given sentence.", "result_label": "The constrained system outperforms a state-of-the-art data-driven pipeline architecture, as we show experimentally, and, in addition, the parser output comes with guarantees about local and global morpho-syntactic wellformedness, which can be useful for downstream applications.", "abstract": "Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb. Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb. The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological (case) information prior to automatic syntactic analysis. Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb. The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological (case) information prior to automatic syntactic analysis. Parsing experiments on Czech, German, and Hungarian show that this approach is susceptible to propagating morphological annotation errors when parsing languages displaying syncretism in their morphological case paradigms. We develop a different architecture where we use case as a possibly underspecified filtering device restricting the options for syntactic analysis. We develop a different architecture where we use case as a possibly underspecified filtering device restricting the options for syntactic analysis. Carefully designed morpho-syntactic constraints can delimit the search space of a statistical dependency parser and exclude solutions that would violate the restrictions overtly marked in the morphology of the words in a given sentence. The constrained system outperforms a state-of-the-art data-driven pipeline architecture, as we show experimentally, and, in addition, the parser output comes with guarantees about local and global morpho-syntactic wellformedness, which can be useful for downstream applications."}, {"paper_id": "37555979", "adju_relevance": 0, "title": "Schemas of Marital Change: From Arranged Marriages to Eloping for Love.", "background_label": "In recent decades, arranged marriages have become less common in many parts of Asia.", "abstract": "In recent decades, arranged marriages have become less common in many parts of Asia."}, {"paper_id": "5929878", "adju_relevance": 0, "title": "From Lawvere to Brandenburger-Keisler: interactive forms of diagonalization and self-reference", "background_label": "We analyze the Brandenburger-Keisler paradox in epistemic game theory, which is a `two-person version of Russell's paradox'.", "abstract": "We analyze the Brandenburger-Keisler paradox in epistemic game theory, which is a `two-person version of Russell's paradox'."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "52072951", "adju_relevance": 0, "title": "Weakly-supervised Neural Semantic Parsing with a Generative Ranker", "background_label": "Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent. The task is challenging due to the large search space and spuriousness of logical forms.", "abstract": "Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent. Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent. The task is challenging due to the large search space and spuriousness of logical forms."}, {"paper_id": "162097304", "adju_relevance": 0, "title": "Louis Armstrong and the Syntax of Scat", "background_label": "618 Early versions of this paper were presented first inMay 2000 at \u201cRhythm-a-ning: A Symposium on Jazz Culture\u201d and then in December 2000 as a lecture at the Center for Jazz Studies, both at Columbia University. I am grateful to Robert G. O\u2019Meally for those invitations and for discussing the project with me on a number of occasions. The essay has also benefited from the suggestions and comments of two other audiences: the Performance of Culture seminar at the Center for the Critical Analysis of ContemporaryCulture at Rutgers University and the department of English at Johns HopkinsUniversity.", "method_label": "I would also like to thankMichael Cogswell at the Armstrong Archives at Queens College, Phoebe Jacobs at the Louis Armstrong Foundation, and the staff in theMusic Division at the Library of Congress.", "result_label": "I thank Krin Gabbard and RobertWalser for their detailed suggestions.", "abstract": "618 Early versions of this paper were presented first inMay 2000 at \u201cRhythm-a-ning: A Symposium on Jazz Culture\u201d and then in December 2000 as a lecture at the Center for Jazz Studies, both at Columbia University. 618 Early versions of this paper were presented first inMay 2000 at \u201cRhythm-a-ning: A Symposium on Jazz Culture\u201d and then in December 2000 as a lecture at the Center for Jazz Studies, both at Columbia University. I am grateful to Robert G. O\u2019Meally for those invitations and for discussing the project with me on a number of occasions. 618 Early versions of this paper were presented first inMay 2000 at \u201cRhythm-a-ning: A Symposium on Jazz Culture\u201d and then in December 2000 as a lecture at the Center for Jazz Studies, both at Columbia University. I am grateful to Robert G. O\u2019Meally for those invitations and for discussing the project with me on a number of occasions. The essay has also benefited from the suggestions and comments of two other audiences: the Performance of Culture seminar at the Center for the Critical Analysis of ContemporaryCulture at Rutgers University and the department of English at Johns HopkinsUniversity. I would also like to thankMichael Cogswell at the Armstrong Archives at Queens College, Phoebe Jacobs at the Louis Armstrong Foundation, and the staff in theMusic Division at the Library of Congress. I thank Krin Gabbard and RobertWalser for their detailed suggestions."}, {"paper_id": "147528281", "adju_relevance": 0, "title": "From EPIC to EPTIC \u2014 Exploring simplification in interpreting and translation from an intermodal perspective", "background_label": "This article introduces EPTIC (the European Parliament Translation and Interpreting Corpus), a new bidirectional (English Italian) corpus of interpreted and translated EU Parliament proceedings. Built as an extension of the English Italian subsection of EPIC (the European Parliament Interpreting Corpus), EPTIC is an intermodal corpus featuring the pseudo-parallel outputs of interpreting and translation processes, aligned to each other and to the corresponding source texts (speeches by MEPs and their written up versions).", "method_label": "As a first attempt at unearthing the potential of EPTIC, we investigate lexical simplification replicating the methodology proposed by Laviosa ( 1998a ; 1998b ), but extending it to encompass both a monolingual comparable and an intermodal perspective.", "result_label": "Our results indicate that the mediation process reduces complexity in both modes of language production and both language directions, with interpreters simplifying the input more than translators, and evidence of simplification being more lexical in English and more lexico-syntactic in Italian.", "abstract": "This article introduces EPTIC (the European Parliament Translation and Interpreting Corpus), a new bidirectional (English Italian) corpus of interpreted and translated EU Parliament proceedings. This article introduces EPTIC (the European Parliament Translation and Interpreting Corpus), a new bidirectional (English Italian) corpus of interpreted and translated EU Parliament proceedings. Built as an extension of the English Italian subsection of EPIC (the European Parliament Interpreting Corpus), EPTIC is an intermodal corpus featuring the pseudo-parallel outputs of interpreting and translation processes, aligned to each other and to the corresponding source texts (speeches by MEPs and their written up versions). As a first attempt at unearthing the potential of EPTIC, we investigate lexical simplification replicating the methodology proposed by Laviosa ( 1998a ; 1998b ), but extending it to encompass both a monolingual comparable and an intermodal perspective. Our results indicate that the mediation process reduces complexity in both modes of language production and both language directions, with interpreters simplifying the input more than translators, and evidence of simplification being more lexical in English and more lexico-syntactic in Italian."}, {"paper_id": "161535030", "adju_relevance": 0, "title": "The mathemagician and pied puzzler : a collection in tribute to Martin Gardner", "background_label": "This volume comprises an imaginative collection of pieces created in tribute to Martin Gardner. Perhaps best known for writing Scientific American's \"Mathematical Games\" column for years, Gardner used his personal exuberance and fascination with puzzles and magic to entice a wide range of readers into a world of mathematical discovery. This tribute therefore contains pieces as widely varied as Gardner's own interests, ranging from limericks to lengthy treatises, from mathematical journal articles to personal stories.", "method_label": "This book makes a charming and unusual addition to any personal library.", "result_label": "Selected papers: - The Odyssey of the Figure Eight Puzzle by Stewart Coffin - Block-Packing Jambalaya by Bill Cutler - O'Beirne's Hexiamond by Richard K. Guy - Biblical Ladders by Donald E. Knuth - Three Limericks: On Space, Time and Speed by Tim Rowett.", "abstract": "This volume comprises an imaginative collection of pieces created in tribute to Martin Gardner. This volume comprises an imaginative collection of pieces created in tribute to Martin Gardner. Perhaps best known for writing Scientific American's \"Mathematical Games\" column for years, Gardner used his personal exuberance and fascination with puzzles and magic to entice a wide range of readers into a world of mathematical discovery. This volume comprises an imaginative collection of pieces created in tribute to Martin Gardner. Perhaps best known for writing Scientific American's \"Mathematical Games\" column for years, Gardner used his personal exuberance and fascination with puzzles and magic to entice a wide range of readers into a world of mathematical discovery. This tribute therefore contains pieces as widely varied as Gardner's own interests, ranging from limericks to lengthy treatises, from mathematical journal articles to personal stories. This book makes a charming and unusual addition to any personal library. Selected papers: - The Odyssey of the Figure Eight Puzzle by Stewart Coffin - Block-Packing Jambalaya by Bill Cutler - O'Beirne's Hexiamond by Richard K. Guy - Biblical Ladders by Donald E. Knuth - Three Limericks: On Space, Time and Speed by Tim Rowett."}, {"paper_id": "46613837", "adju_relevance": 0, "title": "Extracting spatial relations from document for geographic information retrieval", "background_label": "Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. For example, searching documents on \u201cdebris flow took place in Hunan last year\u201d, the documents selected in this way may only contain the words \u201cdebris flow\u201d and \u201cHunan\u201d rather than refer to \u201cdebris flow actually occurred in Hunan\u201d. Lack of spatial relations between thematic activates (debris flow) and geographic entities (Hunan) is the key reason for this problem. First, we analyze the characters of spatial relation expressions in natural language and there are two types of spatial relations: topology and direction.", "method_label": "In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations. We observe that the features required to find a relationship between two spatial named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency tree. Therefore, we construct a shortest path dependency kernel for SVM to complete the task.", "result_label": "The experiment results show that our dependency tree kernel achieves significant improvement than previous method.", "abstract": "Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. For example, searching documents on \u201cdebris flow took place in Hunan last year\u201d, the documents selected in this way may only contain the words \u201cdebris flow\u201d and \u201cHunan\u201d rather than refer to \u201cdebris flow actually occurred in Hunan\u201d. Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. For example, searching documents on \u201cdebris flow took place in Hunan last year\u201d, the documents selected in this way may only contain the words \u201cdebris flow\u201d and \u201cHunan\u201d rather than refer to \u201cdebris flow actually occurred in Hunan\u201d. Lack of spatial relations between thematic activates (debris flow) and geographic entities (Hunan) is the key reason for this problem. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. For example, searching documents on \u201cdebris flow took place in Hunan last year\u201d, the documents selected in this way may only contain the words \u201cdebris flow\u201d and \u201cHunan\u201d rather than refer to \u201cdebris flow actually occurred in Hunan\u201d. Lack of spatial relations between thematic activates (debris flow) and geographic entities (Hunan) is the key reason for this problem. First, we analyze the characters of spatial relation expressions in natural language and there are two types of spatial relations: topology and direction. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations. We observe that the features required to find a relationship between two spatial named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency tree. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations. We observe that the features required to find a relationship between two spatial named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency tree. Therefore, we construct a shortest path dependency kernel for SVM to complete the task. The experiment results show that our dependency tree kernel achieves significant improvement than previous method."}, {"paper_id": "16977542", "adju_relevance": 0, "title": "Head-Driven PCFGs With Latent-Head Statistics", "background_label": "Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition.", "abstract": "Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank. Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition."}, {"paper_id": "9648838", "adju_relevance": 0, "title": "CODRA: A Novel Discriminative Framework for Rhetorical Analysis", "background_label": "Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence structure.", "method_label": "In this article, we present CODRA\u2014 a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. We present two approaches to combine these two stages of parsing effectively.", "result_label": "By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further.", "abstract": "Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence structure. In this article, we present CODRA\u2014 a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.CODRA comprises a discourse segmenter and a discourse parser. In this article, we present CODRA\u2014 a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. In this article, we present CODRA\u2014 a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. In this article, we present CODRA\u2014 a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. We present two approaches to combine these two stages of parsing effectively. By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin. By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further."}, {"paper_id": "3353110", "adju_relevance": 0, "title": "Image Transformer", "background_label": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences.", "method_label": "In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks.", "result_label": "While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art."}, {"paper_id": "15149661", "adju_relevance": 0, "title": "Building a Treebank for French", "background_label": "We present a treebank project for French. We have annotated a newspaper corpus of 1 Million words with part of speech, inflection, compounds, lemmas and constituency.", "method_label": "We describe the tagging and parsing phases of the project, and for each, the automatic tools, the guidelines and the validation process. We then present some uses of the corpus as well as some directions for future work.", "abstract": "We present a treebank project for French. We present a treebank project for French. We have annotated a newspaper corpus of 1 Million words with part of speech, inflection, compounds, lemmas and constituency. We describe the tagging and parsing phases of the project, and for each, the automatic tools, the guidelines and the validation process. We describe the tagging and parsing phases of the project, and for each, the automatic tools, the guidelines and the validation process. We then present some uses of the corpus as well as some directions for future work."}, {"paper_id": "1448113", "adju_relevance": 0, "title": "A Bayesian Model for Unsupervised Semantic Parsing", "method_label": "Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model.", "result_label": "The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.", "abstract": " Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain."}, {"paper_id": "7014930", "adju_relevance": 0, "title": "The Role of Context in Neural Morphological Disambiguation", "background_label": "AbstractLanguages with rich morphology often introduce sparsity in language processing tasks. While morphological analyzers can reduce this sparsity by providing morpheme-level analyses for words, they will often introduce ambiguity by returning multiple analyses for the same surface form. The problem of disambiguating between these morphological parses is further complicated by the fact that a correct parse for a word is not only dependent on the surface form but also on other words in its context.", "abstract": "AbstractLanguages with rich morphology often introduce sparsity in language processing tasks. AbstractLanguages with rich morphology often introduce sparsity in language processing tasks. While morphological analyzers can reduce this sparsity by providing morpheme-level analyses for words, they will often introduce ambiguity by returning multiple analyses for the same surface form. AbstractLanguages with rich morphology often introduce sparsity in language processing tasks. While morphological analyzers can reduce this sparsity by providing morpheme-level analyses for words, they will often introduce ambiguity by returning multiple analyses for the same surface form. The problem of disambiguating between these morphological parses is further complicated by the fact that a correct parse for a word is not only dependent on the surface form but also on other words in its context."}, {"paper_id": "9931139", "adju_relevance": 0, "title": "Gamed-based iSTART Practice: From MiBoard to Self-Explanation Showdown", "background_label": "MiBoard (Multiplayer Interactive Board Game) is an online, turnbased board game that was developed to assess the integration of game characteristics (point rewards, game-like interaction, and peer feedback) and how that might affect student engagement and learning efficacy. This online board game was designed to fit within the Extended Practice module of iSTART (Interactive Strategy Training for Active Reading and Thinking).", "method_label": "Unfortunately, preliminary research shows that MiBoard actually reduces engagement and does not benefit the quality of student self-explanations when compared to the original Extended Practice module. Consequently the MiBoard framework has been revamped to create Self-Explanation Showdown, a faster-paced, less analytically oriented game that adds competition to the creation of self-explanations.", "result_label": "Students are evaluated on the quality of their self-explanations using the same assessment algorithms from iSTART Extended Practice module (this includes both word-based and LSA-based assessments). The technical issues involved in development of MiBoard and Self- Explanation Showdown are described. The lessons learned from the MiBoard experience are also discussed in this paper.", "abstract": "MiBoard (Multiplayer Interactive Board Game) is an online, turnbased board game that was developed to assess the integration of game characteristics (point rewards, game-like interaction, and peer feedback) and how that might affect student engagement and learning efficacy. MiBoard (Multiplayer Interactive Board Game) is an online, turnbased board game that was developed to assess the integration of game characteristics (point rewards, game-like interaction, and peer feedback) and how that might affect student engagement and learning efficacy. This online board game was designed to fit within the Extended Practice module of iSTART (Interactive Strategy Training for Active Reading and Thinking). Unfortunately, preliminary research shows that MiBoard actually reduces engagement and does not benefit the quality of student self-explanations when compared to the original Extended Practice module. Unfortunately, preliminary research shows that MiBoard actually reduces engagement and does not benefit the quality of student self-explanations when compared to the original Extended Practice module. Consequently the MiBoard framework has been revamped to create Self-Explanation Showdown, a faster-paced, less analytically oriented game that adds competition to the creation of self-explanations. Students are evaluated on the quality of their self-explanations using the same assessment algorithms from iSTART Extended Practice module (this includes both word-based and LSA-based assessments). Students are evaluated on the quality of their self-explanations using the same assessment algorithms from iSTART Extended Practice module (this includes both word-based and LSA-based assessments). The technical issues involved in development of MiBoard and Self- Explanation Showdown are described. Students are evaluated on the quality of their self-explanations using the same assessment algorithms from iSTART Extended Practice module (this includes both word-based and LSA-based assessments). The technical issues involved in development of MiBoard and Self- Explanation Showdown are described. The lessons learned from the MiBoard experience are also discussed in this paper."}, {"paper_id": "1361675", "adju_relevance": 0, "title": "From data to analysis: linking NWChem and Avogadro with the syntax and semantics of Chemical Markup Language", "background_label": "BACKGROUND Multidisciplinary integrated research requires the ability to couple the diverse sets of data obtained from a range of complex experiments and computer simulations. Integrating data requires semantically rich information. Draft dictionary entries and a format for molecular orbitals within CML CompChem were developed.", "method_label": "In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization. The Avogadro application was extended to read in CML data, and display molecular geometry and electronic structure in the GUI allowing for an end-to-end solution where Avogadro can create input structures, generate input files, NWChem can run the calculation and Avogadro can then read in and analyse the CML output produced. The developments outlined in this paper will be made available in future releases of NWChem, FoX, and Avogadro. CONCLUSIONS The production of CML compliant XML files for computational chemistry software such as NWChem can be accomplished relatively easily using the FoX library. The CML data can be read in by a newly developed reader in Avogadro and analysed or visualized in various ways.", "result_label": "RESULTS The NWChem computational chemistry software has been modified and coupled to the FoX library to write CML compliant XML data files. The FoX library was expanded to represent the lexical input files and molecular orbitals used by the computational chemistry software. A community-based effort is needed to further develop the CML CompChem convention and dictionary. This will enable the long-term goal of allowing a researcher to run simple \"Google-style\" searches of chemistry and physics and have the results of computational calculations returned in a comprehensible form alongside articles from the published literature.", "abstract": "BACKGROUND Multidisciplinary integrated research requires the ability to couple the diverse sets of data obtained from a range of complex experiments and computer simulations. BACKGROUND Multidisciplinary integrated research requires the ability to couple the diverse sets of data obtained from a range of complex experiments and computer simulations. Integrating data requires semantically rich information. In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization. RESULTS The NWChem computational chemistry software has been modified and coupled to the FoX library to write CML compliant XML data files. RESULTS The NWChem computational chemistry software has been modified and coupled to the FoX library to write CML compliant XML data files. The FoX library was expanded to represent the lexical input files and molecular orbitals used by the computational chemistry software. BACKGROUND Multidisciplinary integrated research requires the ability to couple the diverse sets of data obtained from a range of complex experiments and computer simulations. Integrating data requires semantically rich information. Draft dictionary entries and a format for molecular orbitals within CML CompChem were developed. In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization. The Avogadro application was extended to read in CML data, and display molecular geometry and electronic structure in the GUI allowing for an end-to-end solution where Avogadro can create input structures, generate input files, NWChem can run the calculation and Avogadro can then read in and analyse the CML output produced. In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization. The Avogadro application was extended to read in CML data, and display molecular geometry and electronic structure in the GUI allowing for an end-to-end solution where Avogadro can create input structures, generate input files, NWChem can run the calculation and Avogadro can then read in and analyse the CML output produced. The developments outlined in this paper will be made available in future releases of NWChem, FoX, and Avogadro. In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization. The Avogadro application was extended to read in CML data, and display molecular geometry and electronic structure in the GUI allowing for an end-to-end solution where Avogadro can create input structures, generate input files, NWChem can run the calculation and Avogadro can then read in and analyse the CML output produced. The developments outlined in this paper will be made available in future releases of NWChem, FoX, and Avogadro. CONCLUSIONS The production of CML compliant XML files for computational chemistry software such as NWChem can be accomplished relatively easily using the FoX library. In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework. Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization. The Avogadro application was extended to read in CML data, and display molecular geometry and electronic structure in the GUI allowing for an end-to-end solution where Avogadro can create input structures, generate input files, NWChem can run the calculation and Avogadro can then read in and analyse the CML output produced. The developments outlined in this paper will be made available in future releases of NWChem, FoX, and Avogadro. CONCLUSIONS The production of CML compliant XML files for computational chemistry software such as NWChem can be accomplished relatively easily using the FoX library. The CML data can be read in by a newly developed reader in Avogadro and analysed or visualized in various ways. RESULTS The NWChem computational chemistry software has been modified and coupled to the FoX library to write CML compliant XML data files. The FoX library was expanded to represent the lexical input files and molecular orbitals used by the computational chemistry software. A community-based effort is needed to further develop the CML CompChem convention and dictionary. RESULTS The NWChem computational chemistry software has been modified and coupled to the FoX library to write CML compliant XML data files. The FoX library was expanded to represent the lexical input files and molecular orbitals used by the computational chemistry software. A community-based effort is needed to further develop the CML CompChem convention and dictionary. This will enable the long-term goal of allowing a researcher to run simple \"Google-style\" searches of chemistry and physics and have the results of computational calculations returned in a comprehensible form alongside articles from the published literature."}, {"paper_id": "7272384", "adju_relevance": 0, "title": "Dynamic shift in isolating referents: From social to self-generated input", "background_label": "Infants as young as 6 months of age start comprehending some familiar words, yet there is little understanding of how young infants utilize information presented in their social environment in order to make sense of the world.", "method_label": "As an initial step, we used recent technology that allowed us to narrow in on the point of view of the infant to explore how infants' visual input is dynamically synchronized with their own participation, as well as from social input in the context of parent-child word-learning play.", "result_label": "The results reveal dynamic changes in the relation between infants' view and their level of participation.", "abstract": "Infants as young as 6 months of age start comprehending some familiar words, yet there is little understanding of how young infants utilize information presented in their social environment in order to make sense of the world. As an initial step, we used recent technology that allowed us to narrow in on the point of view of the infant to explore how infants' visual input is dynamically synchronized with their own participation, as well as from social input in the context of parent-child word-learning play. The results reveal dynamic changes in the relation between infants' view and their level of participation."}, {"paper_id": "5279459", "adju_relevance": 0, "title": "Unsupervised discovery and extraction of semi-structured regions in text via self-information", "background_label": "We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language. Examples include tables, key-value listings, or repeated enumerations of properties. Because of their generally non-sentential nature, these regions can present problems for standard information extraction algorithms.", "abstract": "We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language. We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language. Examples include tables, key-value listings, or repeated enumerations of properties. We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language. Examples include tables, key-value listings, or repeated enumerations of properties. Because of their generally non-sentential nature, these regions can present problems for standard information extraction algorithms."}, {"paper_id": "194866932", "adju_relevance": 0, "title": "North and South as Jane Austen Fanfiction: How Gaskell\u2019s Use of Austen\u2019s Characters and Structure Strengthen Her Social Protest Novel", "background_label": "The genre of fanfiction has, arguably, existed for centuries, with many well-known pieces of literature matching the definition of \u201cfanfiction\u201d. While countless classics meet the requirements of a \u201cfanfiction\u201d text by retelling the stories of classic figures such as King Arthur or Julius Caesar, others offer more subtle examples of early fanfiction, using characters and storylines from earlier works. Elizabeth Gaskell\u2019s North and South, which largely parallels Jane Austen\u2019s Pride and Prejudice is, arguably, an example of fanfiction writing prior to the official recognition of the genre in the early 20th century.", "abstract": "The genre of fanfiction has, arguably, existed for centuries, with many well-known pieces of literature matching the definition of \u201cfanfiction\u201d. The genre of fanfiction has, arguably, existed for centuries, with many well-known pieces of literature matching the definition of \u201cfanfiction\u201d. While countless classics meet the requirements of a \u201cfanfiction\u201d text by retelling the stories of classic figures such as King Arthur or Julius Caesar, others offer more subtle examples of early fanfiction, using characters and storylines from earlier works. The genre of fanfiction has, arguably, existed for centuries, with many well-known pieces of literature matching the definition of \u201cfanfiction\u201d. While countless classics meet the requirements of a \u201cfanfiction\u201d text by retelling the stories of classic figures such as King Arthur or Julius Caesar, others offer more subtle examples of early fanfiction, using characters and storylines from earlier works. Elizabeth Gaskell\u2019s North and South, which largely parallels Jane Austen\u2019s Pride and Prejudice is, arguably, an example of fanfiction writing prior to the official recognition of the genre in the early 20th century."}, {"paper_id": "13867055", "adju_relevance": 0, "title": "Neural Monkey: The Current State and Beyond", "background_label": "AbstractNeural Monkey is an open-source toolkit for sequence-to-sequence learning.", "abstract": "AbstractNeural Monkey is an open-source toolkit for sequence-to-sequence learning."}, {"paper_id": "11728957", "adju_relevance": 0, "title": "Efficient sentence segmentation using syntactic features", "background_label": "To enable downstream language processing,automatic speech recognition output must be segmented into its individual sentences. Previous sentence segmentation systems have typically been very local,using low-level prosodic and lexical features to independently decide whether or not to segment at each word boundary position. In this work,we leverage global syntactic information from a syntactic parser, which is better able to capture long distance dependencies.", "method_label": "While some previous work has included syntactic features, ours is the first to do so in a tractable, lattice-based way, which is crucial for scaling up to long-sentence contexts. Specifically, an initial hypothesis lattice is constructed using local features. Candidate sentences are then assigned syntactic language model scores. These global syntactic scores are combined with local low-level scores in a log-linear model.", "result_label": "The resulting system significantly outperforms the most popular long-span model for sentence segmentation (the hidden event language model) on both reference text and automatic speech recognizer output from news broadcasts.", "abstract": "To enable downstream language processing,automatic speech recognition output must be segmented into its individual sentences. To enable downstream language processing,automatic speech recognition output must be segmented into its individual sentences. Previous sentence segmentation systems have typically been very local,using low-level prosodic and lexical features to independently decide whether or not to segment at each word boundary position. To enable downstream language processing,automatic speech recognition output must be segmented into its individual sentences. Previous sentence segmentation systems have typically been very local,using low-level prosodic and lexical features to independently decide whether or not to segment at each word boundary position. In this work,we leverage global syntactic information from a syntactic parser, which is better able to capture long distance dependencies. While some previous work has included syntactic features, ours is the first to do so in a tractable, lattice-based way, which is crucial for scaling up to long-sentence contexts. While some previous work has included syntactic features, ours is the first to do so in a tractable, lattice-based way, which is crucial for scaling up to long-sentence contexts. Specifically, an initial hypothesis lattice is constructed using local features. While some previous work has included syntactic features, ours is the first to do so in a tractable, lattice-based way, which is crucial for scaling up to long-sentence contexts. Specifically, an initial hypothesis lattice is constructed using local features. Candidate sentences are then assigned syntactic language model scores. While some previous work has included syntactic features, ours is the first to do so in a tractable, lattice-based way, which is crucial for scaling up to long-sentence contexts. Specifically, an initial hypothesis lattice is constructed using local features. Candidate sentences are then assigned syntactic language model scores. These global syntactic scores are combined with local low-level scores in a log-linear model. The resulting system significantly outperforms the most popular long-span model for sentence segmentation (the hidden event language model) on both reference text and automatic speech recognizer output from news broadcasts."}, {"paper_id": "52967399", "adju_relevance": 0, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "background_label": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "method_label": "As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful.", "result_label": "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, {"paper_id": "171076149", "adju_relevance": 0, "title": "Neuroarthistory: From Aristotle and Pliny to Baxandall and Zeki", "background_label": "This provocative book offers a fascinating account of neuroarthistory, one of the newest and most exciting fields in the human sciences. In recent decades there has been a dramatic increase in our knowledge of the visual brain. Knowledge of phenomena such as neural plasticity and neural mirroring is making it possible to answer with a new level of precision some of the most challenging questions about both the creative process and the response to art.", "result_label": "Exploring the writings of major thinkers (among them Montesquieu, Burke, Kant, Marx and Freud), and leading art historians (including Pliny, Winckelmann, Ruskin, Pater, Gombrich and Baxandall), as well as artists such as Alberti and Leonardo and scientists from Aristotle to Zeki, John Onians shows how an understanding of the neural basis of the mind contributes to an understanding of all human behaviors--including art.", "abstract": "This provocative book offers a fascinating account of neuroarthistory, one of the newest and most exciting fields in the human sciences. This provocative book offers a fascinating account of neuroarthistory, one of the newest and most exciting fields in the human sciences. In recent decades there has been a dramatic increase in our knowledge of the visual brain. This provocative book offers a fascinating account of neuroarthistory, one of the newest and most exciting fields in the human sciences. In recent decades there has been a dramatic increase in our knowledge of the visual brain. Knowledge of phenomena such as neural plasticity and neural mirroring is making it possible to answer with a new level of precision some of the most challenging questions about both the creative process and the response to art. Exploring the writings of major thinkers (among them Montesquieu, Burke, Kant, Marx and Freud), and leading art historians (including Pliny, Winckelmann, Ruskin, Pater, Gombrich and Baxandall), as well as artists such as Alberti and Leonardo and scientists from Aristotle to Zeki, John Onians shows how an understanding of the neural basis of the mind contributes to an understanding of all human behaviors--including art."}, {"paper_id": "9745687", "adju_relevance": 0, "title": "Recursive Autoencoders for ITG-Based Translation", "background_label": "AbstractWhile inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge.", "abstract": "AbstractWhile inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge."}, {"paper_id": "157238803", "adju_relevance": 0, "title": "From Isolation to Radicalization: Anti-Muslim Hostility and Support for ISIS in the West", "background_label": "What explains online radicalization and support for ISIS in Europe? In the past few years, thousands in the West have become radicalized by consuming extremist content online, and many have left for Syria to join the Islamic State.", "abstract": "What explains online radicalization and support for ISIS in Europe? What explains online radicalization and support for ISIS in Europe? In the past few years, thousands in the West have become radicalized by consuming extremist content online, and many have left for Syria to join the Islamic State."}, {"paper_id": "32106959", "adju_relevance": 0, "title": "Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations", "background_label": "We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. The representations change significantly when translation and part-of-speech decoders are added.", "method_label": "The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure.", "result_label": "Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way.", "abstract": "We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. The representations change significantly when translation and part-of-speech decoders are added. The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure. Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way."}, {"paper_id": "2735247", "adju_relevance": 0, "title": "Improving NLP through Marginalization of Hidden Syntactic Structure", "background_label": "AbstractMany NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable.", "abstract": "AbstractMany NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable."}, {"paper_id": "174802532", "adju_relevance": 0, "title": "Syntax-Infused Variational Autoencoder for Text Generation", "background_label": "We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees.", "abstract": "We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees."}, {"paper_id": "29304770", "adju_relevance": 0, "title": "Syllabic Consonants and Syllabification in Imdlawn Tashlhiyt Berber", "background_label": "Improved means for releasably locking a typewriter ball-type print head to a driving head shaft are disclosed.", "method_label": "The print head has a downwardly-projecting hub which is slidably received on the head shaft and is axially locked thereto by means of a spring-loaded locking pin which snaps out from its retracted position within the head shaft to rest on the top wall of the print head, the print head being axially releasable by operating a cam lever located on the print head top wall. A key pin extending from the head shaft engages a key slot at the lower edge of the hub when the print head is seated on the drive shaft to angularly lock the print head to the head shaft.", "abstract": "Improved means for releasably locking a typewriter ball-type print head to a driving head shaft are disclosed. The print head has a downwardly-projecting hub which is slidably received on the head shaft and is axially locked thereto by means of a spring-loaded locking pin which snaps out from its retracted position within the head shaft to rest on the top wall of the print head, the print head being axially releasable by operating a cam lever located on the print head top wall. The print head has a downwardly-projecting hub which is slidably received on the head shaft and is axially locked thereto by means of a spring-loaded locking pin which snaps out from its retracted position within the head shaft to rest on the top wall of the print head, the print head being axially releasable by operating a cam lever located on the print head top wall. A key pin extending from the head shaft engages a key slot at the lower edge of the hub when the print head is seated on the drive shaft to angularly lock the print head to the head shaft."}, {"paper_id": "10118836", "adju_relevance": 0, "title": "Word Representations, Tree Models and Syntactic Functions", "background_label": "Word representations induced from models with discrete latent variables (e.g.\\ HMMs) have been shown to be beneficial in many NLP applications.", "method_label": "In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification.", "result_label": "We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident.", "abstract": "Word representations induced from models with discrete latent variables (e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification. We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident."}, {"paper_id": "11432578", "adju_relevance": 0, "title": "Improved Neural Machine Translation with Source Syntax", "background_label": "AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e. phrase structure), sometimes they can be in long distance. Phrase structures are not the best way to explicitly model long distance dependencies.", "method_label": "In this paper we propose a simple but effective method to incorporate source-side long distance dependencies into NMT. Our method based on dependency trees enriches each source state with global dependency structures, which can better capture the inherent syntactic structure of source sentences.", "result_label": "Experiments on Chinese-English and English-Japanese translation tasks show that our proposed method outperforms state-of-the-art SMT and NMT baselines.", "abstract": "AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e. AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e. phrase structure), sometimes they can be in long distance. AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e. phrase structure), sometimes they can be in long distance. Phrase structures are not the best way to explicitly model long distance dependencies. In this paper we propose a simple but effective method to incorporate source-side long distance dependencies into NMT. In this paper we propose a simple but effective method to incorporate source-side long distance dependencies into NMT. Our method based on dependency trees enriches each source state with global dependency structures, which can better capture the inherent syntactic structure of source sentences. Experiments on Chinese-English and English-Japanese translation tasks show that our proposed method outperforms state-of-the-art SMT and NMT baselines."}, {"paper_id": "57613254", "adju_relevance": 0, "title": "From Knowledge Representation to Argumentation in AI, Law and Policy Making. a Festscrift in Honour of Trevor Bench-Capon on the Occasion of His 60th", "background_label": "Trevor Bench-Capon is well recognised as an outstanding figure in Artificial Intelligence and Law, having published extensively over the long course of his career on legal knowledge representation, engineering methods for knowledge-based systems, theoretical and applied argumentation, case-based reasoning, policy-making, reasoning about evidence, and many other related topics. He has deeply influenced many of his colleagues, particularly with his earlier work on principled methods for legal knowledge representation and the engineering of knowledge-based systems, and later with the introduction of values in the study of argumentation.", "method_label": "This Festschrift is in honour of Bench-Capon's work and its seminal influence. The articles contained here by his colleagues extensively review, comment on, and extend Bench-Capon's work.", "result_label": "As a whole, the volume is a substantive introduction to the main topics and issues to which Bench-Capon has contributed so much.", "abstract": "Trevor Bench-Capon is well recognised as an outstanding figure in Artificial Intelligence and Law, having published extensively over the long course of his career on legal knowledge representation, engineering methods for knowledge-based systems, theoretical and applied argumentation, case-based reasoning, policy-making, reasoning about evidence, and many other related topics. Trevor Bench-Capon is well recognised as an outstanding figure in Artificial Intelligence and Law, having published extensively over the long course of his career on legal knowledge representation, engineering methods for knowledge-based systems, theoretical and applied argumentation, case-based reasoning, policy-making, reasoning about evidence, and many other related topics. He has deeply influenced many of his colleagues, particularly with his earlier work on principled methods for legal knowledge representation and the engineering of knowledge-based systems, and later with the introduction of values in the study of argumentation. This Festschrift is in honour of Bench-Capon's work and its seminal influence. This Festschrift is in honour of Bench-Capon's work and its seminal influence. The articles contained here by his colleagues extensively review, comment on, and extend Bench-Capon's work. As a whole, the volume is a substantive introduction to the main topics and issues to which Bench-Capon has contributed so much."}, {"paper_id": "14857072", "adju_relevance": 0, "title": "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing", "background_label": "AbstractMorphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.", "abstract": "AbstractMorphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. AbstractMorphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance."}, {"paper_id": "6099034", "adju_relevance": 0, "title": "Spatial Transformer Networks", "background_label": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.", "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner."}, {"paper_id": "118046049", "adju_relevance": 0, "title": "The syntax of Chichewa", "abstract": ""}, {"paper_id": "797950", "adju_relevance": 0, "title": "Clustering Words by Syntactic Similarity improves Dependency Parsing of Predicate-argument Structures", "background_label": "We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts.", "method_label": "We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy.", "abstract": "We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy."}, {"paper_id": "14941047", "adju_relevance": 0, "title": "Discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair", "background_label": "Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. This is primarily because of the limitation of their search techniques.", "method_label": "In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English-Japanese). We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER).", "result_label": "When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus.", "abstract": "Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. This is primarily because of the limitation of their search techniques. In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English-Japanese). In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English-Japanese). We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER). When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus."}, {"paper_id": "84186736", "adju_relevance": 0, "title": "Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences for Fact-Checking", "background_label": "Automatic fact-checking systems detect misinformation, such as fake news, by (i) selecting check-worthy sentences for fact-checking, (ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences. Most prior research on (i) uses hand-crafted features to select check-worthy sentences, and does not explicitly account for the recent finding that the top weighted terms in both check-worthy and non-check-worthy sentences are actually overlapping [15].", "method_label": "Motivated by this, we present a neural check-worthiness sentence ranking model that represents each word in a sentence by \\textit{both} its embedding (aiming to capture its semantics) and its syntactic dependencies (aiming to capture its role in modifying the semantics of other terms in the sentence). Our model is an end-to-end trainable neural network for check-worthiness ranking, which is trained on large amounts of unlabelled data through weak supervision.", "result_label": "Thorough experimental evaluation against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP and +28% at various Precision cut-offs from the best baseline with statistical significance). Empirical analysis of the use of weak supervision, word embedding pretraining on domain-specific data, and the use of syntactic dependencies of our model reveals that check-worthy sentences contain notably more identical syntactic dependencies than non-check-worthy sentences.", "abstract": "Automatic fact-checking systems detect misinformation, such as fake news, by (i) selecting check-worthy sentences for fact-checking, (ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences. Automatic fact-checking systems detect misinformation, such as fake news, by (i) selecting check-worthy sentences for fact-checking, (ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences. Most prior research on (i) uses hand-crafted features to select check-worthy sentences, and does not explicitly account for the recent finding that the top weighted terms in both check-worthy and non-check-worthy sentences are actually overlapping [15]. Motivated by this, we present a neural check-worthiness sentence ranking model that represents each word in a sentence by \\textit{both} its embedding (aiming to capture its semantics) and its syntactic dependencies (aiming to capture its role in modifying the semantics of other terms in the sentence). Motivated by this, we present a neural check-worthiness sentence ranking model that represents each word in a sentence by \\textit{both} its embedding (aiming to capture its semantics) and its syntactic dependencies (aiming to capture its role in modifying the semantics of other terms in the sentence). Our model is an end-to-end trainable neural network for check-worthiness ranking, which is trained on large amounts of unlabelled data through weak supervision. Thorough experimental evaluation against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP and +28% at various Precision cut-offs from the best baseline with statistical significance). Thorough experimental evaluation against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP and +28% at various Precision cut-offs from the best baseline with statistical significance). Empirical analysis of the use of weak supervision, word embedding pretraining on domain-specific data, and the use of syntactic dependencies of our model reveals that check-worthy sentences contain notably more identical syntactic dependencies than non-check-worthy sentences."}, {"paper_id": "63432627", "adju_relevance": 0, "title": "Clause Analysis: Using Syntactic Information to Automatically Extract Source, Subject, and Predicate from Texts with an Application to the 2008\u20132009 Gaza War", "background_label": "This article presents a new method and open source R package that uses syntactic information to automatically extract source\u2013subject\u2013predicate clauses .", "method_label": "This improves on frequency-based text analysis methods by dividing text into predicates with an identified subject and optional source, extracting the statements and actions of (political) actors as mentioned in the text. The content of these predicates can be analyzed using existing frequency-based methods, allowing for the analysis of actions, issue positions and framing by different actors within a single text. We show that a small set of syntactic patterns can extract clauses and identify quotes with good accuracy, significantly outperforming a baseline system based on word order.", "result_label": "Taking the 2008\u20132009 Gaza war as an example, we further show how corpus comparison and semantic network analysis applied to the results of the clause analysis can show differences in citation and framing patterns between U.S. and English-language Chinese coverage of this war.", "abstract": "This article presents a new method and open source R package that uses syntactic information to automatically extract source\u2013subject\u2013predicate clauses . This improves on frequency-based text analysis methods by dividing text into predicates with an identified subject and optional source, extracting the statements and actions of (political) actors as mentioned in the text. This improves on frequency-based text analysis methods by dividing text into predicates with an identified subject and optional source, extracting the statements and actions of (political) actors as mentioned in the text. The content of these predicates can be analyzed using existing frequency-based methods, allowing for the analysis of actions, issue positions and framing by different actors within a single text. This improves on frequency-based text analysis methods by dividing text into predicates with an identified subject and optional source, extracting the statements and actions of (political) actors as mentioned in the text. The content of these predicates can be analyzed using existing frequency-based methods, allowing for the analysis of actions, issue positions and framing by different actors within a single text. We show that a small set of syntactic patterns can extract clauses and identify quotes with good accuracy, significantly outperforming a baseline system based on word order. Taking the 2008\u20132009 Gaza war as an example, we further show how corpus comparison and semantic network analysis applied to the results of the clause analysis can show differences in citation and framing patterns between U.S. and English-language Chinese coverage of this war."}, {"paper_id": "11117517", "adju_relevance": 0, "title": "Structured Attentions for Visual Question Answering", "background_label": "Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models. Although the questions may involve complex relations among multiple regions, few attention models can effectively encode such cross-region relations.", "abstract": "Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models. Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models. Although the questions may involve complex relations among multiple regions, few attention models can effectively encode such cross-region relations."}, {"paper_id": "14351497", "adju_relevance": 0, "title": "Abstract syntax from concrete syntax", "background_label": "ABSTRACTModem Software Engineering practice advocates the development of domain-specific specification languages to characterize formally the idioms of discourse and jargon of specific problem domains. With poorly-understood domains it is best to construct an abstract syntax to characterize the domain concepts and abstractions before developing a concrete syntax. Often, however, a good concrete syntax exists a priori: sometimes in sophisticated formal languages characterizing (often mathematical) domains but more often in miniature, legacy-code languages, sorely in need of reverse engineering. In such cases, it is necessary to derive an appropriate abstract syntax -or its first cousin, an object-oriented model -from the concrete syntax.", "method_label": "This report describes a transformation process that produces a good abstract representation from a low-level concrete syntax specification.", "abstract": "ABSTRACTModem Software Engineering practice advocates the development of domain-specific specification languages to characterize formally the idioms of discourse and jargon of specific problem domains. ABSTRACTModem Software Engineering practice advocates the development of domain-specific specification languages to characterize formally the idioms of discourse and jargon of specific problem domains. With poorly-understood domains it is best to construct an abstract syntax to characterize the domain concepts and abstractions before developing a concrete syntax. ABSTRACTModem Software Engineering practice advocates the development of domain-specific specification languages to characterize formally the idioms of discourse and jargon of specific problem domains. With poorly-understood domains it is best to construct an abstract syntax to characterize the domain concepts and abstractions before developing a concrete syntax. Often, however, a good concrete syntax exists a priori: sometimes in sophisticated formal languages characterizing (often mathematical) domains but more often in miniature, legacy-code languages, sorely in need of reverse engineering. ABSTRACTModem Software Engineering practice advocates the development of domain-specific specification languages to characterize formally the idioms of discourse and jargon of specific problem domains. With poorly-understood domains it is best to construct an abstract syntax to characterize the domain concepts and abstractions before developing a concrete syntax. Often, however, a good concrete syntax exists a priori: sometimes in sophisticated formal languages characterizing (often mathematical) domains but more often in miniature, legacy-code languages, sorely in need of reverse engineering. In such cases, it is necessary to derive an appropriate abstract syntax -or its first cousin, an object-oriented model -from the concrete syntax. This report describes a transformation process that produces a good abstract representation from a low-level concrete syntax specification."}, {"paper_id": "6771196", "adju_relevance": 0, "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "background_label": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture.", "method_label": "Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning.", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture."}, {"paper_id": "2180008", "adju_relevance": 0, "title": "Bi-directional Attention with Agreement for Dependency Parsing", "background_label": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions.", "method_label": "The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity.", "result_label": "We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.", "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages."}, {"paper_id": "53733849", "adju_relevance": 0, "title": "Implanting Rational Knowledge into Distributed Representation at Morpheme Level", "background_label": "Previously, researchers paid no attention to the creation of unambiguous morpheme embeddings independent from the corpus, while such information plays an important role in expressing the exact meanings of words for parataxis languages like Chinese.", "abstract": "Previously, researchers paid no attention to the creation of unambiguous morpheme embeddings independent from the corpus, while such information plays an important role in expressing the exact meanings of words for parataxis languages like Chinese."}, {"paper_id": "462553", "adju_relevance": 0, "title": "Translating Phrases in Neural Machine Translation", "background_label": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word.", "method_label": "In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does.", "result_label": "Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.", "abstract": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets."}, {"paper_id": "84841767", "adju_relevance": 0, "title": "Linguistic Knowledge and Transferability of Contextual Representations", "background_label": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks.", "method_label": "We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks.", "result_label": "For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.", "abstract": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results."}, {"paper_id": "28681432", "adju_relevance": 0, "title": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences", "background_label": "In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'.", "abstract": "In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'."}, {"paper_id": "96440785", "adju_relevance": 0, "title": "The market for paintings in the Venetian Republic from Renaissance to Rococ\u00f2", "background_label": "We study the art market in the Venetian Republic from 1550 to 1750 analyzing the determinants of the prices (adjusted for the cost of living measured by the price of wheat) of figurative paintings. Reputation of the painters, size of the paintings and other quantifiable factors affect prices as expected.", "method_label": "Other relevant factors include the placement of the paintings (on altars, ceilings or walls), whose impact reflects differences in demand elasticities.", "result_label": "We find evidence of the law of one price confirming price equalization between high and low demand geographical destinations and between different subjects. Finally, in a Schumpeterian perspective, we relate the temporal trend of the price of a representative painting with waves of artistic innovations, whose peacks were in the 1500s and in the 1700s with a dark Baroque age in the intermediate century.", "abstract": "We study the art market in the Venetian Republic from 1550 to 1750 analyzing the determinants of the prices (adjusted for the cost of living measured by the price of wheat) of figurative paintings. We study the art market in the Venetian Republic from 1550 to 1750 analyzing the determinants of the prices (adjusted for the cost of living measured by the price of wheat) of figurative paintings. Reputation of the painters, size of the paintings and other quantifiable factors affect prices as expected. Other relevant factors include the placement of the paintings (on altars, ceilings or walls), whose impact reflects differences in demand elasticities. We find evidence of the law of one price confirming price equalization between high and low demand geographical destinations and between different subjects. We find evidence of the law of one price confirming price equalization between high and low demand geographical destinations and between different subjects. Finally, in a Schumpeterian perspective, we relate the temporal trend of the price of a representative painting with waves of artistic innovations, whose peacks were in the 1500s and in the 1700s with a dark Baroque age in the intermediate century."}, {"paper_id": "5698842", "adju_relevance": 0, "title": "Apertium: a free/open-source platform for rule-based machine translation", "background_label": "Apertium is a free/open-source platform for rule-based machine translation. It is being widely used to build machine translation systems for a variety of language pairs, especially in those cases (mainly with related-language pairs) where shallow transfer suffices to produce good quality translations, although it has also proven useful in assimilation scenarios with more distant pairs involved.", "method_label": "This article summarises the Apertium platform: the translation engine, the encoding of linguistic data, and the tools developed around the platform. The present limitations of the platform and the challenges posed for the coming years are also discussed.", "result_label": "Finally, evaluation results for some of the most active language pairs are presented. An appendix describes Apertium as a free/open-source project.", "abstract": "Apertium is a free/open-source platform for rule-based machine translation. Apertium is a free/open-source platform for rule-based machine translation. It is being widely used to build machine translation systems for a variety of language pairs, especially in those cases (mainly with related-language pairs) where shallow transfer suffices to produce good quality translations, although it has also proven useful in assimilation scenarios with more distant pairs involved. This article summarises the Apertium platform: the translation engine, the encoding of linguistic data, and the tools developed around the platform. This article summarises the Apertium platform: the translation engine, the encoding of linguistic data, and the tools developed around the platform. The present limitations of the platform and the challenges posed for the coming years are also discussed. Finally, evaluation results for some of the most active language pairs are presented. Finally, evaluation results for some of the most active language pairs are presented. An appendix describes Apertium as a free/open-source project."}, {"paper_id": "2382276", "adju_relevance": 0, "title": "Sentence Simplification for Semantic Role Labeling", "background_label": "AbstractParse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems. These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression.", "method_label": "In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. We apply our simplification system to semantic role labeling (SRL). As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable. By extracting and labeling simplified sentences, this combined simplification/SRL system better generalizes across syntactic variation.", "result_label": "It achieves a statistically significant 1.2% F1 measure increase over a strong baseline on the Conll-2005 SRL task, attaining near-state-of-the-art performance.", "abstract": "AbstractParse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems. AbstractParse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems. These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. We apply our simplification system to semantic role labeling (SRL). In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. We apply our simplification system to semantic role labeling (SRL). As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule \"depassivizes\" a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. We apply our simplification system to semantic role labeling (SRL). As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable. By extracting and labeling simplified sentences, this combined simplification/SRL system better generalizes across syntactic variation. It achieves a statistically significant 1.2% F1 measure increase over a strong baseline on the Conll-2005 SRL task, attaining near-state-of-the-art performance."}, {"paper_id": "174776087", "adju_relevance": 0, "title": "On the Compositionality Prediction of Noun Phrases using Poincar\\'e Embeddings", "background_label": "The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models.", "abstract": "The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models."}, {"paper_id": "914127", "adju_relevance": 0, "title": "Where to Look Next? Combining Static and Dynamic Proto-objects in a TVA-based Model of Visual Attention", "background_label": "To decide \u201cWhere to look next ?\u201d is a central function of the attention system of humans, animals and robots. Control of attention depends on three factors, that is, low-level static and dynamic visual features of the environment (bottom-up), medium-level visual features of proto-objects and the task (top-down).", "method_label": "We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis. Proto-objects serve as input to the TVA process that combines top-down and bottom-up information for computing attentional priorities so that relatively complex search tasks can be implemented. To this end, separately computed static and dynamic proto-objects are filtered and subsequently merged into one combined map of proto-objects. For each proto-object, attentional priorities in the form of attentional weights are computed according to TVA. The target of the next saccade is the center of gravity of the proto-object with the highest weight according to the task.", "result_label": "We illustrate the approach by applying it to several real world image sequences and show that it is robust to parameter variations.", "abstract": "To decide \u201cWhere to look next ?\u201d is a central function of the attention system of humans, animals and robots. To decide \u201cWhere to look next ?\u201d is a central function of the attention system of humans, animals and robots. Control of attention depends on three factors, that is, low-level static and dynamic visual features of the environment (bottom-up), medium-level visual features of proto-objects and the task (top-down). We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis. We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis. Proto-objects serve as input to the TVA process that combines top-down and bottom-up information for computing attentional priorities so that relatively complex search tasks can be implemented. We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis. Proto-objects serve as input to the TVA process that combines top-down and bottom-up information for computing attentional priorities so that relatively complex search tasks can be implemented. To this end, separately computed static and dynamic proto-objects are filtered and subsequently merged into one combined map of proto-objects. We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis. Proto-objects serve as input to the TVA process that combines top-down and bottom-up information for computing attentional priorities so that relatively complex search tasks can be implemented. To this end, separately computed static and dynamic proto-objects are filtered and subsequently merged into one combined map of proto-objects. For each proto-object, attentional priorities in the form of attentional weights are computed according to TVA. We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system. The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the \u201cTheory of Visual Attention\u201d (TVA, [7]). Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis. Proto-objects serve as input to the TVA process that combines top-down and bottom-up information for computing attentional priorities so that relatively complex search tasks can be implemented. To this end, separately computed static and dynamic proto-objects are filtered and subsequently merged into one combined map of proto-objects. For each proto-object, attentional priorities in the form of attentional weights are computed according to TVA. The target of the next saccade is the center of gravity of the proto-object with the highest weight according to the task. We illustrate the approach by applying it to several real world image sequences and show that it is robust to parameter variations."}, {"paper_id": "1114678", "adju_relevance": 0, "title": "Neural Machine Translation of Rare Words with Subword Units", "background_label": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary.", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary."}, {"paper_id": "18061968", "adju_relevance": 0, "title": "Supervised Learning of Syntactic Contexts for Uncovering Definitions and Extracting Hypernym Relations in Text Databases", "background_label": "In this paper we address the problem of automatically constructing structured knowledge from plain texts.", "method_label": "In particular, we present a supervised learning technique to first identify definitions in text data, while then finding hypernym relations within them making use of extracted syntactic structures. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a method which only uses syntactic dependencies between terms extracted with a syntactic parser. Our assumption is that syntax is more robust than patterns when coping with the length and the complexity of the texts. Then, we transform the syntactic contexts of each noun in a coarse-grained textual representation, that is later fed into hyponym/hypernym-centered Support Vector Machine classifiers.", "result_label": "The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking the current state of the art.", "abstract": " In this paper we address the problem of automatically constructing structured knowledge from plain texts. In particular, we present a supervised learning technique to first identify definitions in text data, while then finding hypernym relations within them making use of extracted syntactic structures. In particular, we present a supervised learning technique to first identify definitions in text data, while then finding hypernym relations within them making use of extracted syntactic structures. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a method which only uses syntactic dependencies between terms extracted with a syntactic parser. In particular, we present a supervised learning technique to first identify definitions in text data, while then finding hypernym relations within them making use of extracted syntactic structures. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a method which only uses syntactic dependencies between terms extracted with a syntactic parser. Our assumption is that syntax is more robust than patterns when coping with the length and the complexity of the texts. In particular, we present a supervised learning technique to first identify definitions in text data, while then finding hypernym relations within them making use of extracted syntactic structures. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a method which only uses syntactic dependencies between terms extracted with a syntactic parser. Our assumption is that syntax is more robust than patterns when coping with the length and the complexity of the texts. Then, we transform the syntactic contexts of each noun in a coarse-grained textual representation, that is later fed into hyponym/hypernym-centered Support Vector Machine classifiers. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking the current state of the art."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "56657817", "adju_relevance": 0, "title": "Analysis Methods in Neural Language Processing: A Survey", "background_label": "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways.", "method_label": "In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.", "abstract": "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work."}, {"paper_id": "76666182", "adju_relevance": 0, "title": "Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition", "background_label": "Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition. Existing attention-based approaches localize and amplify significant parts to learn fine-grained details, which often suffer from a limited number of parts and heavy computational cost.", "abstract": "Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition. Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition. Existing attention-based approaches localize and amplify significant parts to learn fine-grained details, which often suffer from a limited number of parts and heavy computational cost."}]