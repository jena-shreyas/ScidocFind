[{"paper_id": "1791179", "title": "A Sequential Model for Multi-Class Classification", "background_label": "Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion.", "method_label": "We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set.", "result_label": "Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging.", "abstract": "Many classification problems require decisions among a large number of competing classes. Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging."}, {"paper_id": "15781707", "adju_relevance": 2, "title": "A Novel Approach to Distributed Multi-Class SVM", "background_label": "With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored.", "abstract": "With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored."}, {"paper_id": "49191384", "adju_relevance": 2, "title": "SGM: Sequence Generation Model for Multi-label Classification", "background_label": "Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models.", "method_label": "In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it.", "result_label": "Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.", "abstract": "Multi-label classification is an important yet challenging task in natural language processing. Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels."}, {"paper_id": "3117752", "adju_relevance": 2, "title": "Deep Decision Network for Multi-class Image Classification", "background_label": "In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network.", "method_label": "During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications.", "result_label": "We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.", "abstract": "In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems."}, {"paper_id": "12156882", "adju_relevance": 2, "title": "A review on the combination of binary classifiers in multiclass problems", "background_label": "Several real problems involve the classification of data into categories or classes. Given a data set containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predict the class of new data from the same domain, performing the desired discrimination. Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary classification problems. However, many problems require the discrimination of examples into more than two categories or classes.", "abstract": "Several real problems involve the classification of data into categories or classes. Several real problems involve the classification of data into categories or classes. Given a data set containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predict the class of new data from the same domain, performing the desired discrimination. Several real problems involve the classification of data into categories or classes. Given a data set containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predict the class of new data from the same domain, performing the desired discrimination. Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary classification problems. Several real problems involve the classification of data into categories or classes. Given a data set containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predict the class of new data from the same domain, performing the desired discrimination. Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary classification problems. However, many problems require the discrimination of examples into more than two categories or classes."}, {"paper_id": "53281152", "adju_relevance": 1, "title": "Adversarial Learning of Label Dependency: A Novel Framework for Multi-class Classification", "background_label": "Recent work has shown that exploiting relations between labels improves the performance of multi-label classification.", "abstract": "Recent work has shown that exploiting relations between labels improves the performance of multi-label classification."}, {"paper_id": "11729594", "adju_relevance": 1, "title": "Model-shared subspace boosting for multi-label classification", "background_label": "Typical approaches to the multi-label classification problem require learning an independent classifier for every label from all the examples and features. This can become a computational bottleneck for sizeable datasets with a large label space.", "abstract": "Typical approaches to the multi-label classification problem require learning an independent classifier for every label from all the examples and features. Typical approaches to the multi-label classification problem require learning an independent classifier for every label from all the examples and features. This can become a computational bottleneck for sizeable datasets with a large label space."}, {"paper_id": "8896798", "adju_relevance": 1, "title": "Semi-Supervised Boosting for Multi-Class Classification", "background_label": "Most semi-supervised learning algorithms have been designed for binary classification, and are extended to multi-class classification by approaches such as one-against-the-rest. The main shortcoming of these approaches is that they are unable to exploit the fact that each example is only assigned to one class. Additional problems with extending semisupervised binary classifiers to multi-class problems include imbalanced classification and different output scales of different binary classifiers.", "method_label": "We propose a semi-supervised boosting framework, termed Multi-Class Semi-Supervised Boosting (MCSSB), that directly solves the semisupervised multi-class learning problem. Compared to the existing semisupervised boosting methods, the proposed framework is advantageous in that it exploits both classification confidence and similarities among examples when deciding the pseudo-labels for unlabeled examples.", "result_label": "Empirical study with a number of UCI datasets shows that the proposed MCSSB algorithm performs better than the state-of-the-art boosting algorithms for semi-supervised learning.", "abstract": " Most semi-supervised learning algorithms have been designed for binary classification, and are extended to multi-class classification by approaches such as one-against-the-rest. Most semi-supervised learning algorithms have been designed for binary classification, and are extended to multi-class classification by approaches such as one-against-the-rest. The main shortcoming of these approaches is that they are unable to exploit the fact that each example is only assigned to one class. Most semi-supervised learning algorithms have been designed for binary classification, and are extended to multi-class classification by approaches such as one-against-the-rest. The main shortcoming of these approaches is that they are unable to exploit the fact that each example is only assigned to one class. Additional problems with extending semisupervised binary classifiers to multi-class problems include imbalanced classification and different output scales of different binary classifiers. We propose a semi-supervised boosting framework, termed Multi-Class Semi-Supervised Boosting (MCSSB), that directly solves the semisupervised multi-class learning problem. We propose a semi-supervised boosting framework, termed Multi-Class Semi-Supervised Boosting (MCSSB), that directly solves the semisupervised multi-class learning problem. Compared to the existing semisupervised boosting methods, the proposed framework is advantageous in that it exploits both classification confidence and similarities among examples when deciding the pseudo-labels for unlabeled examples. Empirical study with a number of UCI datasets shows that the proposed MCSSB algorithm performs better than the state-of-the-art boosting algorithms for semi-supervised learning."}, {"paper_id": "67770291", "adju_relevance": 1, "title": "N-ary decomposition for multi-class classification", "background_label": "A common way of solving a multi-class classification problem is to decompose it into a collection of simpler two-class problems. One major disadvantage is that with such a binary decomposition scheme it may be difficult to represent subtle between-class differences in many-class classification problems due to limited choices of binary-value partitions.", "method_label": "To overcome this challenge, we propose a new decomposition method called N-ary decomposition that decomposes the original multi-class problem into a set of simpler multi-class subproblems. We theoretically show that the proposed N-ary decomposition could be unified into the framework of error correcting output codes and give the generalization error bound of an N-ary decomposition for multi-class classification.", "result_label": "Extensive experimental results demonstrate the state-of-the-art performance of our approach.", "abstract": "A common way of solving a multi-class classification problem is to decompose it into a collection of simpler two-class problems. A common way of solving a multi-class classification problem is to decompose it into a collection of simpler two-class problems. One major disadvantage is that with such a binary decomposition scheme it may be difficult to represent subtle between-class differences in many-class classification problems due to limited choices of binary-value partitions. To overcome this challenge, we propose a new decomposition method called N-ary decomposition that decomposes the original multi-class problem into a set of simpler multi-class subproblems. To overcome this challenge, we propose a new decomposition method called N-ary decomposition that decomposes the original multi-class problem into a set of simpler multi-class subproblems. We theoretically show that the proposed N-ary decomposition could be unified into the framework of error correcting output codes and give the generalization error bound of an N-ary decomposition for multi-class classification. Extensive experimental results demonstrate the state-of-the-art performance of our approach."}, {"paper_id": "11917361", "adju_relevance": 1, "title": "Implementing Multi-class Classifiers by One-class Classification Methods", "method_label": "One-class classifiers are first trained for each class and then a decision function is formulated based on minimum distance rules. Two kinds of one-class classifiers are explored: the support vector domain description and a kernel principle component analysis based method. Both of the two methods can work in the feature space and deal with nonlinear classification problems.", "result_label": "Experiments on some benchmark datasets show that the proposed methods with carefully tuned parameters have comparable generalization ability with support vector machines while having some other advantages.", "abstract": " One-class classifiers are first trained for each class and then a decision function is formulated based on minimum distance rules. One-class classifiers are first trained for each class and then a decision function is formulated based on minimum distance rules. Two kinds of one-class classifiers are explored: the support vector domain description and a kernel principle component analysis based method. One-class classifiers are first trained for each class and then a decision function is formulated based on minimum distance rules. Two kinds of one-class classifiers are explored: the support vector domain description and a kernel principle component analysis based method. Both of the two methods can work in the feature space and deal with nonlinear classification problems. Experiments on some benchmark datasets show that the proposed methods with carefully tuned parameters have comparable generalization ability with support vector machines while having some other advantages."}, {"paper_id": "15526621", "adju_relevance": 1, "title": "A Generalized Mixture Framework for Multi-label Classification.", "background_label": "We develop a novel probabilistic ensemble framework for multi-label classification that is based on the mixtures-of-experts architecture.", "method_label": "In this framework, we combine multi-label classification models in the classifier chains family that decompose the class posterior distribution P(Y1, \u2026, Yd |X) using a product of posterior distributions over components of the output space. Our approach captures different input-output and output-output relations that tend to change across data. As a result, we can recover a rich set of dependency relations among inputs and outputs that a single multi-label classification model cannot capture due to its modeling simplifications. We develop and present algorithms for learning the mixtures-of-experts models from data and for performing multi-label predictions on unseen data instances.", "result_label": "Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.", "abstract": "We develop a novel probabilistic ensemble framework for multi-label classification that is based on the mixtures-of-experts architecture. In this framework, we combine multi-label classification models in the classifier chains family that decompose the class posterior distribution P(Y1, \u2026, Yd |X) using a product of posterior distributions over components of the output space. In this framework, we combine multi-label classification models in the classifier chains family that decompose the class posterior distribution P(Y1, \u2026, Yd |X) using a product of posterior distributions over components of the output space. Our approach captures different input-output and output-output relations that tend to change across data. In this framework, we combine multi-label classification models in the classifier chains family that decompose the class posterior distribution P(Y1, \u2026, Yd |X) using a product of posterior distributions over components of the output space. Our approach captures different input-output and output-output relations that tend to change across data. As a result, we can recover a rich set of dependency relations among inputs and outputs that a single multi-label classification model cannot capture due to its modeling simplifications. In this framework, we combine multi-label classification models in the classifier chains family that decompose the class posterior distribution P(Y1, \u2026, Yd |X) using a product of posterior distributions over components of the output space. Our approach captures different input-output and output-output relations that tend to change across data. As a result, we can recover a rich set of dependency relations among inputs and outputs that a single multi-label classification model cannot capture due to its modeling simplifications. We develop and present algorithms for learning the mixtures-of-experts models from data and for performing multi-label predictions on unseen data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods."}, {"paper_id": "9790719", "adju_relevance": 1, "title": "Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "background_label": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm.", "method_label": "The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner.", "result_label": "Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.", "abstract": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms."}, {"paper_id": "5782250", "adju_relevance": 1, "title": "A generalized flow for multi-class and binary classification tasks: An Azure ML approach", "background_label": "The constant growth in the present day real-world databases pose computational challenges for a single computer. Cloud-based platforms, on the other hand, are capable of handling large volumes of information manipulation tasks, thereby necessitating their use for large real-world data set computations.", "abstract": "The constant growth in the present day real-world databases pose computational challenges for a single computer. The constant growth in the present day real-world databases pose computational challenges for a single computer. Cloud-based platforms, on the other hand, are capable of handling large volumes of information manipulation tasks, thereby necessitating their use for large real-world data set computations."}, {"paper_id": "10782967", "adju_relevance": 1, "title": "A simple technique for improving multi-class classification with neural networks", "method_label": "Our method consists of a standard one-against-all (OAA) classification, followed by another network layer classifying the resulting class scores, possibly augmented by the original raw input vector. This allows the network to disambiguate hard-to-separate classes as the distribution of class scores carries considerable information as well, and is in fact often used for assessing the confidence of a decision.", "result_label": "We show that by this approach we are able to significantly boost our results, overall as well as for particular difficult cases, on the hard 10-class gesture classification task.", "abstract": " Our method consists of a standard one-against-all (OAA) classification, followed by another network layer classifying the resulting class scores, possibly augmented by the original raw input vector. Our method consists of a standard one-against-all (OAA) classification, followed by another network layer classifying the resulting class scores, possibly augmented by the original raw input vector. This allows the network to disambiguate hard-to-separate classes as the distribution of class scores carries considerable information as well, and is in fact often used for assessing the confidence of a decision. We show that by this approach we are able to significantly boost our results, overall as well as for particular difficult cases, on the hard 10-class gesture classification task."}, {"paper_id": "8441645", "adju_relevance": 1, "title": "Multi-Class Support Vector Machine via Maximizing Multi-Class Margins", "background_label": "AbstractSupport Vector Machine (SVM) is originally proposed as a binary classification model with achieving great success in many applications. In reality, it is more often to solve a problem which has more than two classes. So, it is natural to extend SVM to a multi-class classifier. There have been many works proposed to construct a multi-class classifier based on binary SVM, such as one versus rest strategy (OvsR), one versus one strategy (OvsO) and Weston's multi-class SVM.", "method_label": "The first two split the multi-class problem to multiple binary classification subproblems, and we need to train multiple binary classifiers. Weston's multi-class SVM is formed by ensuring risk constraints and imposing a specific regularization, like Frobenius norm. It is not derived by maximizing the margin between hyperplane and training data which is the motivation in SVM. In this paper, we propose a multiclass SVM model from the perspective of maximizing margin between training points and hyperplane, and analyze the relation between our model and other related methods.", "result_label": "In the experiment, it shows that our model can get better or compared results when comparing with other related methods.", "abstract": "AbstractSupport Vector Machine (SVM) is originally proposed as a binary classification model with achieving great success in many applications. AbstractSupport Vector Machine (SVM) is originally proposed as a binary classification model with achieving great success in many applications. In reality, it is more often to solve a problem which has more than two classes. AbstractSupport Vector Machine (SVM) is originally proposed as a binary classification model with achieving great success in many applications. In reality, it is more often to solve a problem which has more than two classes. So, it is natural to extend SVM to a multi-class classifier. AbstractSupport Vector Machine (SVM) is originally proposed as a binary classification model with achieving great success in many applications. In reality, it is more often to solve a problem which has more than two classes. So, it is natural to extend SVM to a multi-class classifier. There have been many works proposed to construct a multi-class classifier based on binary SVM, such as one versus rest strategy (OvsR), one versus one strategy (OvsO) and Weston's multi-class SVM. The first two split the multi-class problem to multiple binary classification subproblems, and we need to train multiple binary classifiers. The first two split the multi-class problem to multiple binary classification subproblems, and we need to train multiple binary classifiers. Weston's multi-class SVM is formed by ensuring risk constraints and imposing a specific regularization, like Frobenius norm. The first two split the multi-class problem to multiple binary classification subproblems, and we need to train multiple binary classifiers. Weston's multi-class SVM is formed by ensuring risk constraints and imposing a specific regularization, like Frobenius norm. It is not derived by maximizing the margin between hyperplane and training data which is the motivation in SVM. The first two split the multi-class problem to multiple binary classification subproblems, and we need to train multiple binary classifiers. Weston's multi-class SVM is formed by ensuring risk constraints and imposing a specific regularization, like Frobenius norm. It is not derived by maximizing the margin between hyperplane and training data which is the motivation in SVM. In this paper, we propose a multiclass SVM model from the perspective of maximizing margin between training points and hyperplane, and analyze the relation between our model and other related methods. In the experiment, it shows that our model can get better or compared results when comparing with other related methods."}, {"paper_id": "19155038", "adju_relevance": 1, "title": "Sequential Multi-Class Labeling in Crowdsourcing", "background_label": "We consider a crowdsourcing platform where workers' responses to questions posed by a crowdsourcer are used to determine the hidden state of a multi-class labeling problem.", "abstract": "We consider a crowdsourcing platform where workers' responses to questions posed by a crowdsourcer are used to determine the hidden state of a multi-class labeling problem."}, {"paper_id": "12188377", "adju_relevance": 1, "title": "Adaptive Base Class Boost for Multi-class Classification", "background_label": "We develop the concept of ABC-Boost (Adaptive Base Class Boost) for multi-class classification and present ABC-MART, a concrete implementation of ABC-Boost.", "method_label": "The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications. For binary classification, ABC-MART recovers MART.", "result_label": "For multi-class classification, ABC-MART considerably improves MART, as evaluated on several public data sets.", "abstract": "We develop the concept of ABC-Boost (Adaptive Base Class Boost) for multi-class classification and present ABC-MART, a concrete implementation of ABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications. The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications. For binary classification, ABC-MART recovers MART. For multi-class classification, ABC-MART considerably improves MART, as evaluated on several public data sets."}, {"paper_id": "6859542", "adju_relevance": 1, "title": "Multi-Class Multi-Scale Series Contextual Model for Image Segmentation", "background_label": "Contextual information has been widely used as a rich source of information to segment multiple objects in an image. A contextual model uses the relationships between the objects in a scene to facilitate object detection and segmentation. Using contextual information from different objects in an effective way for object segmentation, however, remains a difficult problem.", "method_label": "In this paper, we introduce a novel framework, called multiclass multiscale (MCMS) series contextual model, which uses contextual information from multiple objects and at different scales for learning discriminative models in a supervised setting. The MCMS model incorporates cross-object and inter-object information into one probabilistic framework and thus is able to capture geometrical relationships and dependencies among multiple objects in addition to local information from each single object present in an image.", "result_label": "We demonstrate that our MCMS model improves object segmentation performance in electron microscopy images and provides a coherent segmentation of multiple objects. Through speeding up the segmentation process, the proposed method will allow neurobiologists to move beyond individual specimens and analyze populations paving the way for understanding neurodegenerative diseases at the microscopic level.", "abstract": "Contextual information has been widely used as a rich source of information to segment multiple objects in an image. Contextual information has been widely used as a rich source of information to segment multiple objects in an image. A contextual model uses the relationships between the objects in a scene to facilitate object detection and segmentation. Contextual information has been widely used as a rich source of information to segment multiple objects in an image. A contextual model uses the relationships between the objects in a scene to facilitate object detection and segmentation. Using contextual information from different objects in an effective way for object segmentation, however, remains a difficult problem. In this paper, we introduce a novel framework, called multiclass multiscale (MCMS) series contextual model, which uses contextual information from multiple objects and at different scales for learning discriminative models in a supervised setting. In this paper, we introduce a novel framework, called multiclass multiscale (MCMS) series contextual model, which uses contextual information from multiple objects and at different scales for learning discriminative models in a supervised setting. The MCMS model incorporates cross-object and inter-object information into one probabilistic framework and thus is able to capture geometrical relationships and dependencies among multiple objects in addition to local information from each single object present in an image. We demonstrate that our MCMS model improves object segmentation performance in electron microscopy images and provides a coherent segmentation of multiple objects. We demonstrate that our MCMS model improves object segmentation performance in electron microscopy images and provides a coherent segmentation of multiple objects. Through speeding up the segmentation process, the proposed method will allow neurobiologists to move beyond individual specimens and analyze populations paving the way for understanding neurodegenerative diseases at the microscopic level."}, {"paper_id": "1785", "adju_relevance": 1, "title": "Learning to Resolve Natural Language Ambiguities: A Unified Approach", "background_label": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be re-cast as learning linear separators in the feature space. Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis.", "method_label": "Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes. In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging.", "result_label": "In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.", "abstract": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be re-cast as learning linear separators in the feature space. We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be re-cast as learning linear separators in the feature space. Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes. In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."}, {"paper_id": "6683105", "adju_relevance": 1, "title": "Sequential Feature Selection for Classification", "background_label": "In most real-world information processing problems, data is not a free resource; its acquisition is rather time-consuming and/or expensive.", "abstract": " In most real-world information processing problems, data is not a free resource; its acquisition is rather time-consuming and/or expensive."}, {"paper_id": "22480768", "adju_relevance": 1, "title": "A Class-Incremental Learning Method for Multi-Class Support Vector Machines in Text Classification", "method_label": "CIL consists of two phases: incremental feature selection and incremental training, for updating the knowledge of old SVM classifiers in text classification when new classes are added to the system. CIL reuses the old models of the classifier and learns only one binary sub-classifier with an additional phase of feature selection when a new class comes. In the testing phase, current classifier is applied to the vectors' projections on the sub-spaces concerned. CIL can serve as a flexible approach for all binary classification algorithms in text classification.", "result_label": "Our experiment shows that the CIL-based SVM was not only substantially faster in training time than the popular batch SVM learning methods such as 1-against-rest, 1-against-1 and divide-by-2 but also almost competed to the best performances in effectiveness of them", "abstract": " CIL consists of two phases: incremental feature selection and incremental training, for updating the knowledge of old SVM classifiers in text classification when new classes are added to the system. CIL consists of two phases: incremental feature selection and incremental training, for updating the knowledge of old SVM classifiers in text classification when new classes are added to the system. CIL reuses the old models of the classifier and learns only one binary sub-classifier with an additional phase of feature selection when a new class comes. CIL consists of two phases: incremental feature selection and incremental training, for updating the knowledge of old SVM classifiers in text classification when new classes are added to the system. CIL reuses the old models of the classifier and learns only one binary sub-classifier with an additional phase of feature selection when a new class comes. In the testing phase, current classifier is applied to the vectors' projections on the sub-spaces concerned. CIL consists of two phases: incremental feature selection and incremental training, for updating the knowledge of old SVM classifiers in text classification when new classes are added to the system. CIL reuses the old models of the classifier and learns only one binary sub-classifier with an additional phase of feature selection when a new class comes. In the testing phase, current classifier is applied to the vectors' projections on the sub-spaces concerned. CIL can serve as a flexible approach for all binary classification algorithms in text classification. Our experiment shows that the CIL-based SVM was not only substantially faster in training time than the popular batch SVM learning methods such as 1-against-rest, 1-against-1 and divide-by-2 but also almost competed to the best performances in effectiveness of them"}, {"paper_id": "16373719", "adju_relevance": 1, "title": "Dendogram based SVM for multi-class classification", "method_label": "First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. Finally, for classifying a pattern query, we present it to the \"root\" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the \"leaf\" nodes. Therefore, the classification procedure is done in a descendant way in the taxonomy from the root through the end level which represents the classes. The pattern is thus associated to one of last SVMs associated class. AHC decomposition uses distance measures to investigate the class grouping in binary form at each level in the hierarchy. SVM method requires little tuning and yields both high accuracy levels and good generalization for binary classification.", "result_label": "Therefore, DSVM method gives good results for multi class problems by both, training an optimal number of SVMs and rapidly classifying patterns in a descendant way by selecting an optimal set of SVMs which participate to the final decision. The proposed method is compared to other multi-class SVM methods over several complex problems", "abstract": " First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. Finally, for classifying a pattern query, we present it to the \"root\" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the \"leaf\" nodes. First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. Finally, for classifying a pattern query, we present it to the \"root\" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the \"leaf\" nodes. Therefore, the classification procedure is done in a descendant way in the taxonomy from the root through the end level which represents the classes. First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. Finally, for classifying a pattern query, we present it to the \"root\" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the \"leaf\" nodes. Therefore, the classification procedure is done in a descendant way in the taxonomy from the root through the end level which represents the classes. The pattern is thus associated to one of last SVMs associated class. First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. Finally, for classifying a pattern query, we present it to the \"root\" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the \"leaf\" nodes. Therefore, the classification procedure is done in a descendant way in the taxonomy from the root through the end level which represents the classes. The pattern is thus associated to one of last SVMs associated class. AHC decomposition uses distance measures to investigate the class grouping in binary form at each level in the hierarchy. First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC). Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node. Finally, for classifying a pattern query, we present it to the \"root\" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the \"leaf\" nodes. Therefore, the classification procedure is done in a descendant way in the taxonomy from the root through the end level which represents the classes. The pattern is thus associated to one of last SVMs associated class. AHC decomposition uses distance measures to investigate the class grouping in binary form at each level in the hierarchy. SVM method requires little tuning and yields both high accuracy levels and good generalization for binary classification. Therefore, DSVM method gives good results for multi class problems by both, training an optimal number of SVMs and rapidly classifying patterns in a descendant way by selecting an optimal set of SVMs which participate to the final decision. Therefore, DSVM method gives good results for multi class problems by both, training an optimal number of SVMs and rapidly classifying patterns in a descendant way by selecting an optimal set of SVMs which participate to the final decision. The proposed method is compared to other multi-class SVM methods over several complex problems"}, {"paper_id": "57375742", "adju_relevance": 1, "title": "Multi-class Classification without Multi-class Labels", "method_label": "The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings.", "result_label": "Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.", "abstract": " The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option."}, {"paper_id": "125136660", "adju_relevance": 1, "title": "Rigorous and compliant approaches to one-class classification", "background_label": "Abstract A wide number of real problems requiring qualitative answers should be addressed by one-class classification (OCC), as in the case of authentication studies, verification of particular claims and quality control. The key feature of OCC is that models are developed using only samples from the target class, so that a representative sampling is not strictly required for non-target classes. On the contrary, in the discriminant analysis (DA) approach, all of the classes considered (at least two) have a non-negligible influence in the definition of the delimiter. It follows that faults in the definition of the classes involved and in representative sampling for each of them may determine a bias in the classification rules.", "abstract": "Abstract A wide number of real problems requiring qualitative answers should be addressed by one-class classification (OCC), as in the case of authentication studies, verification of particular claims and quality control. Abstract A wide number of real problems requiring qualitative answers should be addressed by one-class classification (OCC), as in the case of authentication studies, verification of particular claims and quality control. The key feature of OCC is that models are developed using only samples from the target class, so that a representative sampling is not strictly required for non-target classes. Abstract A wide number of real problems requiring qualitative answers should be addressed by one-class classification (OCC), as in the case of authentication studies, verification of particular claims and quality control. The key feature of OCC is that models are developed using only samples from the target class, so that a representative sampling is not strictly required for non-target classes. On the contrary, in the discriminant analysis (DA) approach, all of the classes considered (at least two) have a non-negligible influence in the definition of the delimiter. Abstract A wide number of real problems requiring qualitative answers should be addressed by one-class classification (OCC), as in the case of authentication studies, verification of particular claims and quality control. The key feature of OCC is that models are developed using only samples from the target class, so that a representative sampling is not strictly required for non-target classes. On the contrary, in the discriminant analysis (DA) approach, all of the classes considered (at least two) have a non-negligible influence in the definition of the delimiter. It follows that faults in the definition of the classes involved and in representative sampling for each of them may determine a bias in the classification rules."}, {"paper_id": "52986657", "adju_relevance": 1, "title": "Incremental Few-Shot Learning with Attention Attractor Networks", "background_label": "Machine learning classifiers are often trained to recognize a set of pre-defined classes. However, in many real applications, it is often desirable to have the flexibility of learning additional concepts, without re-training on the full training set.", "method_label": "This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes. To this end, we propose a meta-learning model, the Attention Attractor Network, which regularizes the learning of novel classes. In each episode, we train a set of new weights to recognize novel classes until they converge, and we show that the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of the attractor network regularizer.", "result_label": "We demonstrate that the learned attractor network can recognize novel classes while remembering old classes without the need to review the original training set, outperforming baselines that do not rely on an iterative optimization process.", "abstract": "Machine learning classifiers are often trained to recognize a set of pre-defined classes. Machine learning classifiers are often trained to recognize a set of pre-defined classes. However, in many real applications, it is often desirable to have the flexibility of learning additional concepts, without re-training on the full training set. This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes. This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes. To this end, we propose a meta-learning model, the Attention Attractor Network, which regularizes the learning of novel classes. This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes. To this end, we propose a meta-learning model, the Attention Attractor Network, which regularizes the learning of novel classes. In each episode, we train a set of new weights to recognize novel classes until they converge, and we show that the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of the attractor network regularizer. We demonstrate that the learned attractor network can recognize novel classes while remembering old classes without the need to review the original training set, outperforming baselines that do not rely on an iterative optimization process."}, {"paper_id": "4189528", "adju_relevance": 1, "title": "Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification", "abstract": ""}, {"paper_id": "12510650", "adju_relevance": 1, "title": "A Novel Progressive Learning Technique for Multi-class Classification", "background_label": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes.", "method_label": "Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique.", "result_label": "A comparative study shows that the developed technique is superior.", "abstract": "In this paper, a progressive learning technique for multi-class classification is proposed. In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior."}, {"paper_id": "674508", "adju_relevance": 1, "title": "A pragmatic approach to multi-class classification", "background_label": "We present a novel hierarchical approach to multi-class classification which is generic in that it can be applied to different classification models (e.g., support vector machines, perceptrons), and makes no explicit assumptions about the probabilistic structure of the problem as it is usually done in multi-class classification.", "method_label": "By adding a cascade of additional classifiers, each of which receives the previous classifier's output in addition to regular input data, the approach harnesses unused information that manifests itself in the form of, e.g., correlations between predicted classes.", "result_label": "Using multilayer perceptrons as a classification model, we demonstrate the validity of this approach by testing it on a complex ten-class 3D gesture recognition task.", "abstract": "We present a novel hierarchical approach to multi-class classification which is generic in that it can be applied to different classification models (e.g., support vector machines, perceptrons), and makes no explicit assumptions about the probabilistic structure of the problem as it is usually done in multi-class classification. By adding a cascade of additional classifiers, each of which receives the previous classifier's output in addition to regular input data, the approach harnesses unused information that manifests itself in the form of, e.g., correlations between predicted classes. Using multilayer perceptrons as a classification model, we demonstrate the validity of this approach by testing it on a complex ten-class 3D gesture recognition task."}, {"paper_id": "2876102", "adju_relevance": 1, "title": "MMAC: A New Multi-Class, Multi-Label Associative Classification Approach", "background_label": "Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence that integrating classification and association rule mining together can produce more efficient and accurate classifiers than traditional classification techniques.", "abstract": "Building fast and accurate classifiers for large-scale databases is an important task in data mining. Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence that integrating classification and association rule mining together can produce more efficient and accurate classifiers than traditional classification techniques."}, {"paper_id": "34254969", "adju_relevance": 0, "title": "Self-Similar Layered Hidden Markov Models", "background_label": "Hidden Markov Models (HMM) have proven to be useful in a variety of real world applications where considerations for uncertainty are crucial. Such an advantage can be more leveraged if HMM can be scaled up to deal with complex problems.", "abstract": " Hidden Markov Models (HMM) have proven to be useful in a variety of real world applications where considerations for uncertainty are crucial. Hidden Markov Models (HMM) have proven to be useful in a variety of real world applications where considerations for uncertainty are crucial. Such an advantage can be more leveraged if HMM can be scaled up to deal with complex problems."}, {"paper_id": "1177419", "adju_relevance": 0, "title": "Part of Speech Tagging Using a Network of Linear Separators", "background_label": "We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space.", "method_label": "In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word. The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded.", "result_label": "This has significance in terms of efficiency, as well as quick adaptation to new contexts.We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS.", "abstract": "We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space. In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word. In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word. The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded. This has significance in terms of efficiency, as well as quick adaptation to new contexts.We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS."}, {"paper_id": "6748437", "adju_relevance": 0, "title": "Estimating Markov model structures", "background_label": "The author investigates the derivation of Markov model structures from text corpora. The structure of a Markov model is its number of states plus the set of outputs and transitions with non-zero probability. The domain of the investigated models is part-of-speech tagging.", "method_label": "The investigations concern two methods to derive Markov models and their structures. Both are able to form categories and allow words to belong to more than one of them. The first method is model merging, which starts with a large and corpus-specific model and successively merges states to generate smaller and more general models. The second method is model splitting, which is the inverse procedure and starts with a small and general model. States are successively split to generate larger and more specific models.", "result_label": "In an experiment, the author shows that the combination of these techniques yields tagging accuracies that are at least equivalent to those of standard approaches.", "abstract": "The author investigates the derivation of Markov model structures from text corpora. The author investigates the derivation of Markov model structures from text corpora. The structure of a Markov model is its number of states plus the set of outputs and transitions with non-zero probability. The author investigates the derivation of Markov model structures from text corpora. The structure of a Markov model is its number of states plus the set of outputs and transitions with non-zero probability. The domain of the investigated models is part-of-speech tagging. The investigations concern two methods to derive Markov models and their structures. The investigations concern two methods to derive Markov models and their structures. Both are able to form categories and allow words to belong to more than one of them. The investigations concern two methods to derive Markov models and their structures. Both are able to form categories and allow words to belong to more than one of them. The first method is model merging, which starts with a large and corpus-specific model and successively merges states to generate smaller and more general models. The investigations concern two methods to derive Markov models and their structures. Both are able to form categories and allow words to belong to more than one of them. The first method is model merging, which starts with a large and corpus-specific model and successively merges states to generate smaller and more general models. The second method is model splitting, which is the inverse procedure and starts with a small and general model. The investigations concern two methods to derive Markov models and their structures. Both are able to form categories and allow words to belong to more than one of them. The first method is model merging, which starts with a large and corpus-specific model and successively merges states to generate smaller and more general models. The second method is model splitting, which is the inverse procedure and starts with a small and general model. States are successively split to generate larger and more specific models. In an experiment, the author shows that the combination of these techniques yields tagging accuracies that are at least equivalent to those of standard approaches."}, {"paper_id": "1742928", "adju_relevance": 0, "title": "Resolving PP attachment Ambiguities with Memory-Based Learning", "background_label": "AbstractIn this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation.", "method_label": "We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features.", "result_label": "We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space.", "abstract": "AbstractIn this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation. We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features. We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space."}, {"paper_id": "43792", "adju_relevance": 0, "title": "Training for Fast Sequential Prediction Using Dynamic Feature Selection", "background_label": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components.", "method_label": "This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence.", "result_label": "We present experiments in left-to-right part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy above 97% with over a five-fold reduction in run-time.", "abstract": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. We present experiments in left-to-right part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy above 97% with over a five-fold reduction in run-time."}, {"paper_id": "28482216", "adju_relevance": 0, "title": "A Generative Model For Zero Shot Learning Using Conditional Variational Autoencoders", "background_label": "Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training.", "method_label": "While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes.", "result_label": "By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.", "abstract": "Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training. While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes. By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes."}, {"paper_id": "938434", "adju_relevance": 0, "title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes", "background_label": "This paper presents a novel framework of error-correcting output coding (ECOC) addressing the problem of multi-class classification.", "method_label": "By weighting the output space of each base classifier which is trained independently, the distance function of decoding is adapted so that the samples are more discriminative. A criterion generated over the Extended Pair Samples (EPS) is proposed to train the weights of output space. Some properties still hold in the new framework: any classifier, as well as distance function, is still applicable. We first conduct empirical studies on UCI datasets to verify the presented framework with four frequently used coding matrixes and then apply it in RoboCup domain to enhance the performance of agent control.", "result_label": "Experimental results show that our supervised learned decoding scheme improves the accuracy of classification significantly and betters the ball control of agents in a soccer game after learning from experience.", "abstract": "This paper presents a novel framework of error-correcting output coding (ECOC) addressing the problem of multi-class classification. By weighting the output space of each base classifier which is trained independently, the distance function of decoding is adapted so that the samples are more discriminative. By weighting the output space of each base classifier which is trained independently, the distance function of decoding is adapted so that the samples are more discriminative. A criterion generated over the Extended Pair Samples (EPS) is proposed to train the weights of output space. By weighting the output space of each base classifier which is trained independently, the distance function of decoding is adapted so that the samples are more discriminative. A criterion generated over the Extended Pair Samples (EPS) is proposed to train the weights of output space. Some properties still hold in the new framework: any classifier, as well as distance function, is still applicable. By weighting the output space of each base classifier which is trained independently, the distance function of decoding is adapted so that the samples are more discriminative. A criterion generated over the Extended Pair Samples (EPS) is proposed to train the weights of output space. Some properties still hold in the new framework: any classifier, as well as distance function, is still applicable. We first conduct empirical studies on UCI datasets to verify the presented framework with four frequently used coding matrixes and then apply it in RoboCup domain to enhance the performance of agent control. Experimental results show that our supervised learned decoding scheme improves the accuracy of classification significantly and betters the ball control of agents in a soccer game after learning from experience."}, {"paper_id": "4502442", "adju_relevance": 0, "title": "A framework for kernel-based multi-category classification", "background_label": "AbstractA geometric framework for understanding multi-category classification is introduced, through which many existing 'all-together' algorithms can be understood. The structure enables parsimonious optimisation, through a direct extension of the binary methodology.", "method_label": "The focus is on Support Vector Classification, with parallels drawn to related methods.The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency. Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds.It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms. An initial example of how this might be achieved is developed in the formulation of a straightforward multi-category Sequential Minimal Optimisation algorithm.", "result_label": "Proof-of-concept experimental results have shown that this, combined with the mapping of pairwise results, is comparable with benchmark optimisation speeds.", "abstract": "AbstractA geometric framework for understanding multi-category classification is introduced, through which many existing 'all-together' algorithms can be understood. AbstractA geometric framework for understanding multi-category classification is introduced, through which many existing 'all-together' algorithms can be understood. The structure enables parsimonious optimisation, through a direct extension of the binary methodology. The focus is on Support Vector Classification, with parallels drawn to related methods.The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency. The focus is on Support Vector Classification, with parallels drawn to related methods.The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency. Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds.It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms. The focus is on Support Vector Classification, with parallels drawn to related methods.The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency. Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds.It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms. An initial example of how this might be achieved is developed in the formulation of a straightforward multi-category Sequential Minimal Optimisation algorithm. Proof-of-concept experimental results have shown that this, combined with the mapping of pairwise results, is comparable with benchmark optimisation speeds."}, {"paper_id": "261545", "adju_relevance": 0, "title": "A sequential dual method for large scale multi-class linear svms", "background_label": "Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features.", "abstract": "Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features."}, {"paper_id": "14919201", "adju_relevance": 0, "title": "Single-class SVM for dynamic scene modeling", "background_label": "Scene modeling is the starting point and thus the most crucial stage for many vision-based systems involving tracking or recognition. Most of the existing approaches attempt at solving this problem by making some simplifying assumptions such as that of a stationary background. However, this might not always be the case, as swaying trees or ripples in the water often violate these assumptions.", "method_label": "In this paper, we present a novel method for modeling background of a dynamic scene, i.e., scenes that contain \u201cnon-stationary\u201d background motions, such as periodic motions (e.g., pendulums or escalators) or dynamic textures (e.g., water fountain in the background, swaying trees, or water ripples, etc.). The paper proposes single-class support vector machine (SVM), and we show why it is preferable to other scene modeling techniques currently in use for this particular problem. Using a rectangular region around a pixel, spatial and appearance-based features are extracted from limited amount of training data, used for learning the SVMs. These features are unique, easy to compute and immune to rotation, and changes in scale and illumination.", "result_label": "We experiment on a diverse set of dynamic scenes and present both qualitative and quantitative results, indicating the practicality and the effectiveness of the proposed method.", "abstract": "Scene modeling is the starting point and thus the most crucial stage for many vision-based systems involving tracking or recognition. Scene modeling is the starting point and thus the most crucial stage for many vision-based systems involving tracking or recognition. Most of the existing approaches attempt at solving this problem by making some simplifying assumptions such as that of a stationary background. Scene modeling is the starting point and thus the most crucial stage for many vision-based systems involving tracking or recognition. Most of the existing approaches attempt at solving this problem by making some simplifying assumptions such as that of a stationary background. However, this might not always be the case, as swaying trees or ripples in the water often violate these assumptions. In this paper, we present a novel method for modeling background of a dynamic scene, i.e., scenes that contain \u201cnon-stationary\u201d background motions, such as periodic motions (e.g., pendulums or escalators) or dynamic textures (e.g., water fountain in the background, swaying trees, or water ripples, etc.). In this paper, we present a novel method for modeling background of a dynamic scene, i.e., scenes that contain \u201cnon-stationary\u201d background motions, such as periodic motions (e.g., pendulums or escalators) or dynamic textures (e.g., water fountain in the background, swaying trees, or water ripples, etc.). The paper proposes single-class support vector machine (SVM), and we show why it is preferable to other scene modeling techniques currently in use for this particular problem. In this paper, we present a novel method for modeling background of a dynamic scene, i.e., scenes that contain \u201cnon-stationary\u201d background motions, such as periodic motions (e.g., pendulums or escalators) or dynamic textures (e.g., water fountain in the background, swaying trees, or water ripples, etc.). The paper proposes single-class support vector machine (SVM), and we show why it is preferable to other scene modeling techniques currently in use for this particular problem. Using a rectangular region around a pixel, spatial and appearance-based features are extracted from limited amount of training data, used for learning the SVMs. In this paper, we present a novel method for modeling background of a dynamic scene, i.e., scenes that contain \u201cnon-stationary\u201d background motions, such as periodic motions (e.g., pendulums or escalators) or dynamic textures (e.g., water fountain in the background, swaying trees, or water ripples, etc.). The paper proposes single-class support vector machine (SVM), and we show why it is preferable to other scene modeling techniques currently in use for this particular problem. Using a rectangular region around a pixel, spatial and appearance-based features are extracted from limited amount of training data, used for learning the SVMs. These features are unique, easy to compute and immune to rotation, and changes in scale and illumination. We experiment on a diverse set of dynamic scenes and present both qualitative and quantitative results, indicating the practicality and the effectiveness of the proposed method."}, {"paper_id": "59843641", "adju_relevance": 0, "title": "Structured Gradient Boosting", "background_label": "The goal of many machine learning problems can be formalized as the creation of a function that can properly classify an input vector, given a set of examples of that function. While this formalism has produced a number of success stories, there are notable situations in which it fails. One such situation arises when the class labels are composed of multiple variables, each of which may be correlated with all or part of the input or output vectors. Such problems, known as structured prediction problems, are common in the fields of information retrieval, computational linguistics, and computer vision, among others.", "method_label": "In this dissertation, I will discuss structured prediction problems and some of the previous approaches to solving them. I will then present a new algorithm, structured gradient boosting, that combines strong points of previous approaches while retaining their generality. More specifically, the algorithm will combine some of the notions of margin maximization present in support vector methods with the speed and flexibility of the structured perceptron algorithm.", "result_label": "Finally, I will show a number of novel ways in which this algorithm can be applied effectively, highlighting applications in learning by demonstration and music information retrieval.", "abstract": "The goal of many machine learning problems can be formalized as the creation of a function that can properly classify an input vector, given a set of examples of that function. The goal of many machine learning problems can be formalized as the creation of a function that can properly classify an input vector, given a set of examples of that function. While this formalism has produced a number of success stories, there are notable situations in which it fails. The goal of many machine learning problems can be formalized as the creation of a function that can properly classify an input vector, given a set of examples of that function. While this formalism has produced a number of success stories, there are notable situations in which it fails. One such situation arises when the class labels are composed of multiple variables, each of which may be correlated with all or part of the input or output vectors. The goal of many machine learning problems can be formalized as the creation of a function that can properly classify an input vector, given a set of examples of that function. While this formalism has produced a number of success stories, there are notable situations in which it fails. One such situation arises when the class labels are composed of multiple variables, each of which may be correlated with all or part of the input or output vectors. Such problems, known as structured prediction problems, are common in the fields of information retrieval, computational linguistics, and computer vision, among others. In this dissertation, I will discuss structured prediction problems and some of the previous approaches to solving them. In this dissertation, I will discuss structured prediction problems and some of the previous approaches to solving them. I will then present a new algorithm, structured gradient boosting, that combines strong points of previous approaches while retaining their generality. In this dissertation, I will discuss structured prediction problems and some of the previous approaches to solving them. I will then present a new algorithm, structured gradient boosting, that combines strong points of previous approaches while retaining their generality. More specifically, the algorithm will combine some of the notions of margin maximization present in support vector methods with the speed and flexibility of the structured perceptron algorithm. Finally, I will show a number of novel ways in which this algorithm can be applied effectively, highlighting applications in learning by demonstration and music information retrieval."}, {"paper_id": "18011791", "adju_relevance": 0, "title": "Techniques for learning and tuning fuzzy rule-based systems for linguistic modeling and their application", "background_label": "Nowadays, Linguistic Modeling is considered to be one of the most important areas of application for Fuzzy Logic. Linguistic Mamdani-type Fuzzy Rule-Based Systems (FRBSs), the ones used to perform this task, provide a human-readable description of the model in the form of linguistic rules, which is a desirable characteristic in many problems.", "method_label": "In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect. Thereafter, we are going to review several learning (when it sets the Rule Base and sometimes the Data Base as well) and tuning (when it only sets the Data Base) methods. These methods are inspired in the three most well known approaches: ad hoc data covering, neural networks, and genetic algorithms. We shall introduce a brief description of these techniques and their synergy with FRBSs. The accuracy of the reviewed methods will be compared when solving two real-world applications.", "result_label": "Some interesting conclusions will be obtained about the behavior of the methods, approaches, and techniques.", "abstract": "Nowadays, Linguistic Modeling is considered to be one of the most important areas of application for Fuzzy Logic. Nowadays, Linguistic Modeling is considered to be one of the most important areas of application for Fuzzy Logic. Linguistic Mamdani-type Fuzzy Rule-Based Systems (FRBSs), the ones used to perform this task, provide a human-readable description of the model in the form of linguistic rules, which is a desirable characteristic in many problems. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect. Thereafter, we are going to review several learning (when it sets the Rule Base and sometimes the Data Base as well) and tuning (when it only sets the Data Base) methods. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect. Thereafter, we are going to review several learning (when it sets the Rule Base and sometimes the Data Base as well) and tuning (when it only sets the Data Base) methods. These methods are inspired in the three most well known approaches: ad hoc data covering, neural networks, and genetic algorithms. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect. Thereafter, we are going to review several learning (when it sets the Rule Base and sometimes the Data Base as well) and tuning (when it only sets the Data Base) methods. These methods are inspired in the three most well known approaches: ad hoc data covering, neural networks, and genetic algorithms. We shall introduce a brief description of these techniques and their synergy with FRBSs. In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS. The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions. Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect. Thereafter, we are going to review several learning (when it sets the Rule Base and sometimes the Data Base as well) and tuning (when it only sets the Data Base) methods. These methods are inspired in the three most well known approaches: ad hoc data covering, neural networks, and genetic algorithms. We shall introduce a brief description of these techniques and their synergy with FRBSs. The accuracy of the reviewed methods will be compared when solving two real-world applications. Some interesting conclusions will be obtained about the behavior of the methods, approaches, and techniques."}, {"paper_id": "3872220", "adju_relevance": 0, "title": "An Online Universal Classifier for Binary, Multi-class and Multi-label Classification", "background_label": "Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification.", "method_label": "In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems.", "result_label": "The results obtained are compared with state-of-the-art techniques from each of the classification types.", "abstract": "Classification involves the learning of the mapping function that associates input samples to corresponding target label. Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types."}, {"paper_id": "52184207", "adju_relevance": 0, "title": "Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders", "background_label": "Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model.", "abstract": "Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model."}, {"paper_id": "173187918", "adju_relevance": 0, "title": "Large Scale Incremental Learning", "background_label": "Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes.", "method_label": "Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model.", "result_label": "With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1% and 13.2% respectively.", "abstract": "Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1% and 13.2% respectively."}, {"paper_id": "120221455", "adju_relevance": 0, "title": "A New Class of Market Share Models", "background_label": "Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance. Three types of choice models---Tversky's Elimination-By-Aspects model, Tree Models, and Generalized PROBIT---have been offered as solutions to the problems identified with the Luce model, but they each suffer from limitations which have prevented their widespread application in marketing contexts. Tversky's elegant EBA model has not been widely used because it requires a large number of parameters and no special-purpose parameter estimation software has yet emerged. This paper proposes a new class of market share models. Rather than model the choice process explicitly, the new models simply scale the effects competing products have on each other's market share.", "method_label": "Tree models have been offered as more parsimonious special cases of EBA, but they are more restrictive in that they presume: 1 that products, and the process of choosing from among them, can be characterized in terms of hierarchical, attribute-based trees; and 2 that the aspects governing choice are well-known. These competitive effects are scaled in the context of a class of market-share models which: 1 do not assume a tree-like structure for the competing products; 2 do not presume any a priori knowledge about the attributes governing choice; 3 are characterized in terms of parameters that can be estimated using ordinary least-squares; and 4 provide clear managerial insight into the sources of competition. The paper begins with a brief review of previous work. Following the review, the paper offers a theorem and proof which guarantees the existence of the new class of models.", "result_label": "Generalized PROBIT can paramorphically handle the problems with the Luce model, but parameter estimation software has proved problematic because it cannot guarantee a globally optimum solution. The empirical validity and strategic utility of the models are demonstrated using two separate sets of data.", "abstract": "Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance. Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance. Three types of choice models---Tversky's Elimination-By-Aspects model, Tree Models, and Generalized PROBIT---have been offered as solutions to the problems identified with the Luce model, but they each suffer from limitations which have prevented their widespread application in marketing contexts. Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance. Three types of choice models---Tversky's Elimination-By-Aspects model, Tree Models, and Generalized PROBIT---have been offered as solutions to the problems identified with the Luce model, but they each suffer from limitations which have prevented their widespread application in marketing contexts. Tversky's elegant EBA model has not been widely used because it requires a large number of parameters and no special-purpose parameter estimation software has yet emerged. Tree models have been offered as more parsimonious special cases of EBA, but they are more restrictive in that they presume: 1 that products, and the process of choosing from among them, can be characterized in terms of hierarchical, attribute-based trees; and 2 that the aspects governing choice are well-known. Generalized PROBIT can paramorphically handle the problems with the Luce model, but parameter estimation software has proved problematic because it cannot guarantee a globally optimum solution. Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance. Three types of choice models---Tversky's Elimination-By-Aspects model, Tree Models, and Generalized PROBIT---have been offered as solutions to the problems identified with the Luce model, but they each suffer from limitations which have prevented their widespread application in marketing contexts. Tversky's elegant EBA model has not been widely used because it requires a large number of parameters and no special-purpose parameter estimation software has yet emerged. This paper proposes a new class of market share models. Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance. Three types of choice models---Tversky's Elimination-By-Aspects model, Tree Models, and Generalized PROBIT---have been offered as solutions to the problems identified with the Luce model, but they each suffer from limitations which have prevented their widespread application in marketing contexts. Tversky's elegant EBA model has not been widely used because it requires a large number of parameters and no special-purpose parameter estimation software has yet emerged. This paper proposes a new class of market share models. Rather than model the choice process explicitly, the new models simply scale the effects competing products have on each other's market share. Tree models have been offered as more parsimonious special cases of EBA, but they are more restrictive in that they presume: 1 that products, and the process of choosing from among them, can be characterized in terms of hierarchical, attribute-based trees; and 2 that the aspects governing choice are well-known. These competitive effects are scaled in the context of a class of market-share models which: 1 do not assume a tree-like structure for the competing products; 2 do not presume any a priori knowledge about the attributes governing choice; 3 are characterized in terms of parameters that can be estimated using ordinary least-squares; and 4 provide clear managerial insight into the sources of competition. Tree models have been offered as more parsimonious special cases of EBA, but they are more restrictive in that they presume: 1 that products, and the process of choosing from among them, can be characterized in terms of hierarchical, attribute-based trees; and 2 that the aspects governing choice are well-known. These competitive effects are scaled in the context of a class of market-share models which: 1 do not assume a tree-like structure for the competing products; 2 do not presume any a priori knowledge about the attributes governing choice; 3 are characterized in terms of parameters that can be estimated using ordinary least-squares; and 4 provide clear managerial insight into the sources of competition. The paper begins with a brief review of previous work. Tree models have been offered as more parsimonious special cases of EBA, but they are more restrictive in that they presume: 1 that products, and the process of choosing from among them, can be characterized in terms of hierarchical, attribute-based trees; and 2 that the aspects governing choice are well-known. These competitive effects are scaled in the context of a class of market-share models which: 1 do not assume a tree-like structure for the competing products; 2 do not presume any a priori knowledge about the attributes governing choice; 3 are characterized in terms of parameters that can be estimated using ordinary least-squares; and 4 provide clear managerial insight into the sources of competition. The paper begins with a brief review of previous work. Following the review, the paper offers a theorem and proof which guarantees the existence of the new class of models. Generalized PROBIT can paramorphically handle the problems with the Luce model, but parameter estimation software has proved problematic because it cannot guarantee a globally optimum solution. The empirical validity and strategic utility of the models are demonstrated using two separate sets of data."}, {"paper_id": "10201889", "adju_relevance": 0, "title": "Deep Learning for Multi-label Classification", "background_label": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy.", "method_label": "In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature", "abstract": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature"}, {"paper_id": "17567112", "adju_relevance": 0, "title": "A method for disambiguating word senses in a large corpus", "background_label": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources.", "abstract": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources."}, {"paper_id": "47081191", "adju_relevance": 0, "title": "Continuous Speech Recognition From Phonetic Transcription", "background_label": "Previous research by the authors has been directed toward phonetic transcription of fluent speech.", "method_label": "We have applied our techniques to speech recognition on the DARPA Resource Management Task. In order to perform speech recognition, however, the phonetic transcription must be interpreted as a sequence of words. A central component of this process is lexical access for which a novel method is proposed.", "abstract": "Previous research by the authors has been directed toward phonetic transcription of fluent speech. We have applied our techniques to speech recognition on the DARPA Resource Management Task. We have applied our techniques to speech recognition on the DARPA Resource Management Task. In order to perform speech recognition, however, the phonetic transcription must be interpreted as a sequence of words. We have applied our techniques to speech recognition on the DARPA Resource Management Task. In order to perform speech recognition, however, the phonetic transcription must be interpreted as a sequence of words. A central component of this process is lexical access for which a novel method is proposed."}, {"paper_id": "5241746", "adju_relevance": 0, "title": "Collective multi-label classification", "background_label": "Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent. However, in many domains labels are highly interdependent.", "abstract": "Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent. Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent. However, in many domains labels are highly interdependent."}, {"paper_id": "16456996", "adju_relevance": 0, "title": "Learning Compact Class Codes for Fast Inference in Large Multi Class Classification", "background_label": "We describe a new approach for classification with a very large number of classes where we assume some class similarity information is available, e.g.", "abstract": " We describe a new approach for classification with a very large number of classes where we assume some class similarity information is available, e.g."}, {"paper_id": "34085479", "adju_relevance": 0, "title": "Classification with many classes: challenges and pluses", "background_label": "While this problem arises in many practical applications and many techniques have been recently developed for its solution, to the best of our knowledge nobody provided a rigorous theoretical analysis of this important setup.", "abstract": " While this problem arises in many practical applications and many techniques have been recently developed for its solution, to the best of our knowledge nobody provided a rigorous theoretical analysis of this important setup."}, {"paper_id": "2894303", "adju_relevance": 0, "title": "Modelling human preferences for ranking and collaborative filtering: a probabilistic ordered partition approach", "background_label": "Learning preference models from human generated data is an important task in modern information processing systems. Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query. Since ratings are often specified within a small range, several objects may have the same ratings, thus creating ties among objects for a given query. Dealing with this phenomena presents a general problem of modelling preferences in the presence of ties and being query-specific. Our approach exploits discrete choice theory, imposing generative process such that the finite set of objects is partitioned into subsets in a stagewise procedure, and thus reducing the state-space at each stage significantly.", "method_label": "To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them. Efficient Markov chain Monte Carlo algorithms are then presented for the proposed models. We demonstrate that the model can potentially be trained in a large-scale setting of hundreds of thousands objects using an ordinary computer. In fact, in some special cases with appropriate model specification, our models can be learned in linear time.", "result_label": "Learning and inference in such a large state-space are challenging, and yet we present in this paper efficient algorithms to perform these tasks.", "abstract": "Learning preference models from human generated data is an important task in modern information processing systems. Learning preference models from human generated data is an important task in modern information processing systems. Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query. Learning preference models from human generated data is an important task in modern information processing systems. Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query. Since ratings are often specified within a small range, several objects may have the same ratings, thus creating ties among objects for a given query. Learning preference models from human generated data is an important task in modern information processing systems. Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query. Since ratings are often specified within a small range, several objects may have the same ratings, thus creating ties among objects for a given query. Dealing with this phenomena presents a general problem of modelling preferences in the presence of ties and being query-specific. To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them. Learning and inference in such a large state-space are challenging, and yet we present in this paper efficient algorithms to perform these tasks. Learning preference models from human generated data is an important task in modern information processing systems. Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query. Since ratings are often specified within a small range, several objects may have the same ratings, thus creating ties among objects for a given query. Dealing with this phenomena presents a general problem of modelling preferences in the presence of ties and being query-specific. Our approach exploits discrete choice theory, imposing generative process such that the finite set of objects is partitioned into subsets in a stagewise procedure, and thus reducing the state-space at each stage significantly. To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them. Efficient Markov chain Monte Carlo algorithms are then presented for the proposed models. To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them. Efficient Markov chain Monte Carlo algorithms are then presented for the proposed models. We demonstrate that the model can potentially be trained in a large-scale setting of hundreds of thousands objects using an ordinary computer. To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them. Efficient Markov chain Monte Carlo algorithms are then presented for the proposed models. We demonstrate that the model can potentially be trained in a large-scale setting of hundreds of thousands objects using an ordinary computer. In fact, in some special cases with appropriate model specification, our models can be learned in linear time."}, {"paper_id": "17468577", "adju_relevance": 0, "title": "Semisupervised Multitask Learning", "background_label": "Context plays an important role when performing classification, and in this paper we examine context from two perspectives. First, the classification of items within a single task is placed within the context of distinct concurrent or previous classification tasks (multiple distinct data collections).", "method_label": "This is referred to as multi-task learning (MTL), and is implemented here in a statistical manner, using a simplified form of the Dirichlet process. In addition, when performing many classification tasks one has simultaneous access to all unlabeled data that must be classified, and therefore there is an opportunity to place the classification of any one feature vector within the context of all unlabeled feature vectors; this is referred to as semi-supervised learning. In this paper we integrate MTL and semi-supervised learning into a single framework, thereby exploiting two forms of contextual information.", "result_label": "Example results are presented on a \"toy\" example, to demonstrate the concept, and the algorithm is also applied to three real data sets.", "abstract": "Context plays an important role when performing classification, and in this paper we examine context from two perspectives. Context plays an important role when performing classification, and in this paper we examine context from two perspectives. First, the classification of items within a single task is placed within the context of distinct concurrent or previous classification tasks (multiple distinct data collections). This is referred to as multi-task learning (MTL), and is implemented here in a statistical manner, using a simplified form of the Dirichlet process. This is referred to as multi-task learning (MTL), and is implemented here in a statistical manner, using a simplified form of the Dirichlet process. In addition, when performing many classification tasks one has simultaneous access to all unlabeled data that must be classified, and therefore there is an opportunity to place the classification of any one feature vector within the context of all unlabeled feature vectors; this is referred to as semi-supervised learning. This is referred to as multi-task learning (MTL), and is implemented here in a statistical manner, using a simplified form of the Dirichlet process. In addition, when performing many classification tasks one has simultaneous access to all unlabeled data that must be classified, and therefore there is an opportunity to place the classification of any one feature vector within the context of all unlabeled feature vectors; this is referred to as semi-supervised learning. In this paper we integrate MTL and semi-supervised learning into a single framework, thereby exploiting two forms of contextual information. Example results are presented on a \"toy\" example, to demonstrate the concept, and the algorithm is also applied to three real data sets."}, {"paper_id": "2284272", "adju_relevance": 0, "title": "Multi-class Open Set Recognition Using Probability of Inclusion", "background_label": "Abstract.The perceived success of recent visual recognition approaches has largely been derived from their performance on classification tasks, where all possible classes are known at training time. But what about open set problems, where unknown classes appear at test time? Intuitively, if we could accurately model just the positive data for any known class without overfitting, we could reject the large set of unknown classes even under an assumption of incomplete class knowledge.", "method_label": "In this paper, we formulate the problem as one of modeling positive training data at the decision boundary, where we can invoke the statistical extreme value theory. A new algorithm called the PI -SVM is introduced for estimating the unnormalized posterior probability of class inclusion.", "abstract": "Abstract.The perceived success of recent visual recognition approaches has largely been derived from their performance on classification tasks, where all possible classes are known at training time. Abstract.The perceived success of recent visual recognition approaches has largely been derived from their performance on classification tasks, where all possible classes are known at training time. But what about open set problems, where unknown classes appear at test time? Abstract.The perceived success of recent visual recognition approaches has largely been derived from their performance on classification tasks, where all possible classes are known at training time. But what about open set problems, where unknown classes appear at test time? Intuitively, if we could accurately model just the positive data for any known class without overfitting, we could reject the large set of unknown classes even under an assumption of incomplete class knowledge. In this paper, we formulate the problem as one of modeling positive training data at the decision boundary, where we can invoke the statistical extreme value theory. In this paper, we formulate the problem as one of modeling positive training data at the decision boundary, where we can invoke the statistical extreme value theory. A new algorithm called the PI -SVM is introduced for estimating the unnormalized posterior probability of class inclusion."}, {"paper_id": "23387485", "adju_relevance": 0, "title": "Bayesian regularization of neural networks.", "background_label": "Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation. Bayesian regularization is a mathematical process that converts a nonlinear regression into a \"well-posed\" statistical problem in the manner of a ridge regression. The advantage of BRANNs is that the models are robust and the validation process, which scales as O(N2) in normal regression methods, such as back propagation, is unnecessary. They are also difficult to overfit, because the BRANN calculates and trains on a number of effective network parameters or weights, effectively turning off those that are not relevant. This effective number is usually considerably smaller than the number of weights in a standard fully connected back-propagation neural net.", "method_label": "These networks provide solutions to a number of problems that arise in QSAR modeling, such as choice of model, robustness of model, choice of validation set, size of validation effort, and optimization of network architecture. Automatic relevance determination (ARD) of the input variables can be used with BRANNs, and this allows the network to \"estimate\" the importance of each input. The ARD method ensures that irrelevant or highly correlated indices used in the modeling are neglected as well as showing which are the most important variables for modeling the activity data. This chapter outlines the equations that define the BRANN method plus a flowchart for producing a BRANN-QSAR model.", "result_label": "They are difficult to overtrain, since evidence procedures provide an objective Bayesian criterion for stopping training. Some results of the use of BRANNs on a number of data sets are illustrated and compared with other linear and nonlinear models.", "abstract": "Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation. Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation. Bayesian regularization is a mathematical process that converts a nonlinear regression into a \"well-posed\" statistical problem in the manner of a ridge regression. Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation. Bayesian regularization is a mathematical process that converts a nonlinear regression into a \"well-posed\" statistical problem in the manner of a ridge regression. The advantage of BRANNs is that the models are robust and the validation process, which scales as O(N2) in normal regression methods, such as back propagation, is unnecessary. These networks provide solutions to a number of problems that arise in QSAR modeling, such as choice of model, robustness of model, choice of validation set, size of validation effort, and optimization of network architecture. They are difficult to overtrain, since evidence procedures provide an objective Bayesian criterion for stopping training. Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation. Bayesian regularization is a mathematical process that converts a nonlinear regression into a \"well-posed\" statistical problem in the manner of a ridge regression. The advantage of BRANNs is that the models are robust and the validation process, which scales as O(N2) in normal regression methods, such as back propagation, is unnecessary. They are also difficult to overfit, because the BRANN calculates and trains on a number of effective network parameters or weights, effectively turning off those that are not relevant. Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation. Bayesian regularization is a mathematical process that converts a nonlinear regression into a \"well-posed\" statistical problem in the manner of a ridge regression. The advantage of BRANNs is that the models are robust and the validation process, which scales as O(N2) in normal regression methods, such as back propagation, is unnecessary. They are also difficult to overfit, because the BRANN calculates and trains on a number of effective network parameters or weights, effectively turning off those that are not relevant. This effective number is usually considerably smaller than the number of weights in a standard fully connected back-propagation neural net. These networks provide solutions to a number of problems that arise in QSAR modeling, such as choice of model, robustness of model, choice of validation set, size of validation effort, and optimization of network architecture. Automatic relevance determination (ARD) of the input variables can be used with BRANNs, and this allows the network to \"estimate\" the importance of each input. These networks provide solutions to a number of problems that arise in QSAR modeling, such as choice of model, robustness of model, choice of validation set, size of validation effort, and optimization of network architecture. Automatic relevance determination (ARD) of the input variables can be used with BRANNs, and this allows the network to \"estimate\" the importance of each input. The ARD method ensures that irrelevant or highly correlated indices used in the modeling are neglected as well as showing which are the most important variables for modeling the activity data. These networks provide solutions to a number of problems that arise in QSAR modeling, such as choice of model, robustness of model, choice of validation set, size of validation effort, and optimization of network architecture. Automatic relevance determination (ARD) of the input variables can be used with BRANNs, and this allows the network to \"estimate\" the importance of each input. The ARD method ensures that irrelevant or highly correlated indices used in the modeling are neglected as well as showing which are the most important variables for modeling the activity data. This chapter outlines the equations that define the BRANN method plus a flowchart for producing a BRANN-QSAR model. They are difficult to overtrain, since evidence procedures provide an objective Bayesian criterion for stopping training. Some results of the use of BRANNs on a number of data sets are illustrated and compared with other linear and nonlinear models."}, {"paper_id": "1890316", "adju_relevance": 0, "title": "Joint Binary Classifier Learning for ECOC-Based Multi-Class Classification", "background_label": "Error-correcting output coding (ECOC) is one of the most widely used strategies for dealing with multi-class problems by decomposing the original multi-class problem into a series of binary sub-problems. In traditional ECOC-based methods, binary classifiers corresponding to those sub-problems are usually trained separately without considering the relationships among these classifiers. However, as these classifiers are established on the same training data, there may be some inherent relationships among them.", "method_label": "Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms. In this paper, we explore to mine and utilize such relationship through a joint classifier learning method, by integrating the training of binary classifiers and the learning of the relationship among them into a unified objective function. We also develop an efficient alternating optimization algorithm to solve the objective function. To evaluate the proposed method, we perform a series of experiments on eleven datasets from the UCI machine learning repository as well as two datasets from real-world image recognition tasks.", "result_label": "The experimental results demonstrate the efficacy of the proposed method, compared with state-of-the-art methods for ECOC-based multi-class classification.", "abstract": "Error-correcting output coding (ECOC) is one of the most widely used strategies for dealing with multi-class problems by decomposing the original multi-class problem into a series of binary sub-problems. Error-correcting output coding (ECOC) is one of the most widely used strategies for dealing with multi-class problems by decomposing the original multi-class problem into a series of binary sub-problems. In traditional ECOC-based methods, binary classifiers corresponding to those sub-problems are usually trained separately without considering the relationships among these classifiers. Error-correcting output coding (ECOC) is one of the most widely used strategies for dealing with multi-class problems by decomposing the original multi-class problem into a series of binary sub-problems. In traditional ECOC-based methods, binary classifiers corresponding to those sub-problems are usually trained separately without considering the relationships among these classifiers. However, as these classifiers are established on the same training data, there may be some inherent relationships among them. Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms. Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms. In this paper, we explore to mine and utilize such relationship through a joint classifier learning method, by integrating the training of binary classifiers and the learning of the relationship among them into a unified objective function. Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms. In this paper, we explore to mine and utilize such relationship through a joint classifier learning method, by integrating the training of binary classifiers and the learning of the relationship among them into a unified objective function. We also develop an efficient alternating optimization algorithm to solve the objective function. Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms. In this paper, we explore to mine and utilize such relationship through a joint classifier learning method, by integrating the training of binary classifiers and the learning of the relationship among them into a unified objective function. We also develop an efficient alternating optimization algorithm to solve the objective function. To evaluate the proposed method, we perform a series of experiments on eleven datasets from the UCI machine learning repository as well as two datasets from real-world image recognition tasks. The experimental results demonstrate the efficacy of the proposed method, compared with state-of-the-art methods for ECOC-based multi-class classification."}, {"paper_id": "15368138", "adju_relevance": 0, "title": "Handwritten digit recognition with a novel vision model that extracts linearly separable features", "background_label": "We use well-established results in biological vision to construct a novel vision model for handwritten digit recognition.", "method_label": "We show empirically that the features extracted by our model are linearly separable over a large training set (MNIST).", "result_label": "Using only a linear classifier on these features, our model is relatively simple yet outperforms other models on the same data set.", "abstract": "We use well-established results in biological vision to construct a novel vision model for handwritten digit recognition. We show empirically that the features extracted by our model are linearly separable over a large training set (MNIST). Using only a linear classifier on these features, our model is relatively simple yet outperforms other models on the same data set."}, {"paper_id": "2250085", "adju_relevance": 0, "title": "A consistency-based model selection for one-class classification", "background_label": "Model selection in unsupervised learning is a hard problem.", "abstract": "Model selection in unsupervised learning is a hard problem."}, {"paper_id": "489951", "adju_relevance": 0, "title": "Multi-class cosegmentation", "background_label": "Bottom-up, fully unsupervised segmentation remains a daunting challenge for computer vision. In the cosegmentation context, on the other hand, the availability of multiple images assumed to contain instances of the same object classes provides a weak form of supervision that can be exploited by discriminative approaches. Unfortunately, most existing algorithms are limited to a very small number of images and/or object classes (typically two of each).", "abstract": "Bottom-up, fully unsupervised segmentation remains a daunting challenge for computer vision. Bottom-up, fully unsupervised segmentation remains a daunting challenge for computer vision. In the cosegmentation context, on the other hand, the availability of multiple images assumed to contain instances of the same object classes provides a weak form of supervision that can be exploited by discriminative approaches. Bottom-up, fully unsupervised segmentation remains a daunting challenge for computer vision. In the cosegmentation context, on the other hand, the availability of multiple images assumed to contain instances of the same object classes provides a weak form of supervision that can be exploited by discriminative approaches. Unfortunately, most existing algorithms are limited to a very small number of images and/or object classes (typically two of each)."}, {"paper_id": "117059123", "adju_relevance": 0, "title": "Generativity and Systematicity in Neural Network Combinatorial Learning", "background_label": "This thesis addresses a set of problems faced by connectionist learning that have originated from the observation that connectionist cognitive models lack two fundamental properties of the mind: Generativity, stemming from the boundless cognitive competence one can exhibit, and systematicity, due to the existence of symmetries within them. Such properties have seldom been seen in neural networks models, which have typically suffered from problems of inadequate generalization, as examplified both by small number of generalizations relative to training set sizes and heavy interference between newly learned items and previously learned information. Symbolic theories, arguing that mental representations have syntactic and semantic structure built from structured combinations of symbolic constituents, can in principle account for these properties (both arise from the sensitivity of structured semantic content with a generative and systematic syntax).", "abstract": "This thesis addresses a set of problems faced by connectionist learning that have originated from the observation that connectionist cognitive models lack two fundamental properties of the mind: Generativity, stemming from the boundless cognitive competence one can exhibit, and systematicity, due to the existence of symmetries within them. This thesis addresses a set of problems faced by connectionist learning that have originated from the observation that connectionist cognitive models lack two fundamental properties of the mind: Generativity, stemming from the boundless cognitive competence one can exhibit, and systematicity, due to the existence of symmetries within them. Such properties have seldom been seen in neural networks models, which have typically suffered from problems of inadequate generalization, as examplified both by small number of generalizations relative to training set sizes and heavy interference between newly learned items and previously learned information. This thesis addresses a set of problems faced by connectionist learning that have originated from the observation that connectionist cognitive models lack two fundamental properties of the mind: Generativity, stemming from the boundless cognitive competence one can exhibit, and systematicity, due to the existence of symmetries within them. Such properties have seldom been seen in neural networks models, which have typically suffered from problems of inadequate generalization, as examplified both by small number of generalizations relative to training set sizes and heavy interference between newly learned items and previously learned information. Symbolic theories, arguing that mental representations have syntactic and semantic structure built from structured combinations of symbolic constituents, can in principle account for these properties (both arise from the sensitivity of structured semantic content with a generative and systematic syntax)."}, {"paper_id": "3204825", "adju_relevance": 0, "title": "A Bayesian hybrid method for context-sensitive spelling correction", "background_label": "Two classes of methods have been shown to be useful for resolving lexical ambiguity. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.", "method_label": "The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. ), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method.", "result_label": "However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.", "abstract": "Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. Two classes of methods have been shown to be useful for resolving lexical ambiguity. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. ), while the latter captures local syntax. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. ), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. ), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. ), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. ), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated."}, {"paper_id": "2370", "adju_relevance": 0, "title": "ATNoSFERES revisited", "background_label": "ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph. The original ATNoSFERES, using a bitstring to represent the language tokens, has been favorably compared in previous work to several Michigan style LCSs architectures in the context of Non Markov problems.", "method_label": "Several modifications of ATNoSFERES are proposed here: the most important one conceptually being a representational change: each token is now represented by an integer, hence the genotype is a string of integers; several other modifications of the underlying grammar language are also proposed.", "result_label": "The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature. The reasons for these improvement are carefully analyzed, and some assumptions are proposed on the underlying mechanisms in order to explain these good results.", "abstract": "ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph. ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph. The original ATNoSFERES, using a bitstring to represent the language tokens, has been favorably compared in previous work to several Michigan style LCSs architectures in the context of Non Markov problems. Several modifications of ATNoSFERES are proposed here: the most important one conceptually being a representational change: each token is now represented by an integer, hence the genotype is a string of integers; several other modifications of the underlying grammar language are also proposed. The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature. The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature. The reasons for these improvement are carefully analyzed, and some assumptions are proposed on the underlying mechanisms in order to explain these good results."}, {"paper_id": "63643710", "adju_relevance": 0, "title": "Multiclass Classification Through Multidimensional Clustering", "background_label": "Classification is one of the most important machine learning tasks in science and engineering. However, it can be a difficult task, in particular when a high number of classes is involved. Genetic Programming, despite its recognized successfulness in so many different domains, is one of the machine learning methods that typically struggles, and often fails, to provide accurate solutions for multiclass classification problems.", "method_label": "We present a novel algorithm for tree based GP that incorporates some ideas on the representation of the solution space in higher dimensions, and can be generalized to other types of GP.", "result_label": "We test three variants of this new approach on a large set of benchmark problems from several different sources, and observe their competitiveness against the most successful state-of-the-art classifiers like Random Forests, Random Subspaces and Multilayer Perceptron.", "abstract": "Classification is one of the most important machine learning tasks in science and engineering. Classification is one of the most important machine learning tasks in science and engineering. However, it can be a difficult task, in particular when a high number of classes is involved. Classification is one of the most important machine learning tasks in science and engineering. However, it can be a difficult task, in particular when a high number of classes is involved. Genetic Programming, despite its recognized successfulness in so many different domains, is one of the machine learning methods that typically struggles, and often fails, to provide accurate solutions for multiclass classification problems. We present a novel algorithm for tree based GP that incorporates some ideas on the representation of the solution space in higher dimensions, and can be generalized to other types of GP. We test three variants of this new approach on a large set of benchmark problems from several different sources, and observe their competitiveness against the most successful state-of-the-art classifiers like Random Forests, Random Subspaces and Multilayer Perceptron."}, {"paper_id": "8847651", "adju_relevance": 0, "title": "Classification of Sets using Restricted Boltzmann Machines", "background_label": "We consider the problem of classification when inputs correspond to sets of vectors. This setting occurs in many problems such as the classification of pieces of mail containing several pages, of web sites with several sections or of images that have been pre-segmented into smaller regions.", "method_label": "We propose generalizations of the restricted Boltzmann machine (RBM) that are appropriate in this context and explore how to incorporate different assumptions about the relationship between the input sets and the target class within the RBM.", "result_label": "In experiments on standard multiple-instance learning datasets, we demonstrate the competitiveness of approaches based on RBMs and apply the proposed variants to the problem of incoming mail classification.", "abstract": "We consider the problem of classification when inputs correspond to sets of vectors. We consider the problem of classification when inputs correspond to sets of vectors. This setting occurs in many problems such as the classification of pieces of mail containing several pages, of web sites with several sections or of images that have been pre-segmented into smaller regions. We propose generalizations of the restricted Boltzmann machine (RBM) that are appropriate in this context and explore how to incorporate different assumptions about the relationship between the input sets and the target class within the RBM. In experiments on standard multiple-instance learning datasets, we demonstrate the competitiveness of approaches based on RBMs and apply the proposed variants to the problem of incoming mail classification."}, {"paper_id": "117710480", "adju_relevance": 0, "title": "Correlation models for paired comparison data", "background_label": "Binary paired comparison data are binary data that record which of two objects being compared is preferred. Applications arise in many contexts including biology, acoustics, genetics, consumer behaviour and sports tournaments. Most of traditional models, such as the Bradley-Terry model, are based on unrealistic independence assumptions. However, in many instances it is sensible to believe that the results of two paired comparisons involving a common object will be correlated.", "abstract": "Binary paired comparison data are binary data that record which of two objects being compared is preferred. Binary paired comparison data are binary data that record which of two objects being compared is preferred. Applications arise in many contexts including biology, acoustics, genetics, consumer behaviour and sports tournaments. Binary paired comparison data are binary data that record which of two objects being compared is preferred. Applications arise in many contexts including biology, acoustics, genetics, consumer behaviour and sports tournaments. Most of traditional models, such as the Bradley-Terry model, are based on unrealistic independence assumptions. Binary paired comparison data are binary data that record which of two objects being compared is preferred. Applications arise in many contexts including biology, acoustics, genetics, consumer behaviour and sports tournaments. Most of traditional models, such as the Bradley-Terry model, are based on unrealistic independence assumptions. However, in many instances it is sensible to believe that the results of two paired comparisons involving a common object will be correlated."}, {"paper_id": "14509422", "adju_relevance": 0, "title": "The Use of Classifiers in Sequential Inference", "background_label": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints.", "abstract": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints."}, {"paper_id": "7408595", "adju_relevance": 0, "title": "Decontamination of Training Samples for Supervised Pattern Recognition Methods", "background_label": "The present work discusses what have been called 'imperfectly supervised situations': pattern recognition applications where the assumption of label correctness does not hold for all the elements of the training sample.", "abstract": "The present work discusses what have been called 'imperfectly supervised situations': pattern recognition applications where the assumption of label correctness does not hold for all the elements of the training sample."}, {"paper_id": "118156376", "adju_relevance": 0, "title": "Partially Identified Econometric Models", "background_label": "This paper studies a class of models where full identification is not necessarily assumed. We term such models partially identified. It is argued that partially identified systems are of practical importance since empirical investigators frequently proceed under conditions that are best described as apparent identification.", "abstract": "This paper studies a class of models where full identification is not necessarily assumed. This paper studies a class of models where full identification is not necessarily assumed. We term such models partially identified. This paper studies a class of models where full identification is not necessarily assumed. We term such models partially identified. It is argued that partially identified systems are of practical importance since empirical investigators frequently proceed under conditions that are best described as apparent identification."}, {"paper_id": "53051208", "adju_relevance": 0, "title": "Closing Brackets with Recurrent Neural Networks", "background_label": "AbstractMany natural and formal languages contain words or symbols that require a matching counterpart for making an expression wellformed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such rules is important for language modeling. Currently, recurrent neural networks (RNNs) are extensively used for this task.", "method_label": "We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of brackets.", "result_label": "We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the theoretical baseline in most cases.", "abstract": "AbstractMany natural and formal languages contain words or symbols that require a matching counterpart for making an expression wellformed. AbstractMany natural and formal languages contain words or symbols that require a matching counterpart for making an expression wellformed. The combination of opening and closing brackets is a typical example of such a construction. AbstractMany natural and formal languages contain words or symbols that require a matching counterpart for making an expression wellformed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such rules is important for language modeling. AbstractMany natural and formal languages contain words or symbols that require a matching counterpart for making an expression wellformed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such rules is important for language modeling. Currently, recurrent neural networks (RNNs) are extensively used for this task. We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of brackets. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the theoretical baseline in most cases."}, {"paper_id": "8749298", "adju_relevance": 0, "title": "Online Open World Recognition", "background_label": "As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes.", "method_label": "In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. For all these scenarios our proposed methods outperform their non-online counterparts.", "result_label": "Experimentally we validate our approach on two large-scale datasets in different learning scenarios. We conclude that local and online learning is important to capture the full dynamics of open world recognition.", "abstract": "As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. For all these scenarios our proposed methods outperform their non-online counterparts. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. We conclude that local and online learning is important to capture the full dynamics of open world recognition."}, {"paper_id": "58036507", "adju_relevance": 0, "title": "Syntactic Neural Networks", "background_label": "We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms. Thus, learning is seen as a process of grammatical inference and recognition as a process of parsing. The resulting grammars are called \u2018strictly-hierarchical\u2019 and map straightforwardly onto a temporal connectionist parser (TCP) using a relatively small number of neurons. The new paradigm is applicable to a variety of pattern-processing tasks such as speech recognition and character recognition. We concentrate here on hand-written character recognition; performance in other problem domains will be reported in future publications.", "method_label": "Naturally, the possible realizations of this theme are diverse; in this paper we present some initial explorations of the case where the pattern grammar is context-free, inferred (from examples) by a separate procedure, and then mapped onto a connectionist parser. Unlike most neural networks for which structure is pre-defined, the resulting network has as many levels as are necessary and arbitrary connections between levels. Furthermore, by the addition of a delay element, the network becomes capable of dealing with time-varying patterns in a simple and efficient manner. Results are presented to illustrate the performance of the system with respect to a number of parameters, namely, the inherent variability of the data, the nature of the learning (supervised or unsupervised) and the details of the clustering procedure used to limit the number of non-terminals inferred.", "result_label": "Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred. This dramatically reduces complexity. In each of these cases (eight in total), we contrast the performance of a stochastic and a non-stochastic TCP. The stochastic TCP does have greater powers of discrimination, but in many cases the results were very similar. If this result holds in practical situations it is important, because the non-stochastic version has a straightforward implementation in silicon.", "abstract": "We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms. We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms. Thus, learning is seen as a process of grammatical inference and recognition as a process of parsing. Naturally, the possible realizations of this theme are diverse; in this paper we present some initial explorations of the case where the pattern grammar is context-free, inferred (from examples) by a separate procedure, and then mapped onto a connectionist parser. Naturally, the possible realizations of this theme are diverse; in this paper we present some initial explorations of the case where the pattern grammar is context-free, inferred (from examples) by a separate procedure, and then mapped onto a connectionist parser. Unlike most neural networks for which structure is pre-defined, the resulting network has as many levels as are necessary and arbitrary connections between levels. Naturally, the possible realizations of this theme are diverse; in this paper we present some initial explorations of the case where the pattern grammar is context-free, inferred (from examples) by a separate procedure, and then mapped onto a connectionist parser. Unlike most neural networks for which structure is pre-defined, the resulting network has as many levels as are necessary and arbitrary connections between levels. Furthermore, by the addition of a delay element, the network becomes capable of dealing with time-varying patterns in a simple and efficient manner. Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred. Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred. This dramatically reduces complexity. We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms. Thus, learning is seen as a process of grammatical inference and recognition as a process of parsing. The resulting grammars are called \u2018strictly-hierarchical\u2019 and map straightforwardly onto a temporal connectionist parser (TCP) using a relatively small number of neurons. We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms. Thus, learning is seen as a process of grammatical inference and recognition as a process of parsing. The resulting grammars are called \u2018strictly-hierarchical\u2019 and map straightforwardly onto a temporal connectionist parser (TCP) using a relatively small number of neurons. The new paradigm is applicable to a variety of pattern-processing tasks such as speech recognition and character recognition. We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms. Thus, learning is seen as a process of grammatical inference and recognition as a process of parsing. The resulting grammars are called \u2018strictly-hierarchical\u2019 and map straightforwardly onto a temporal connectionist parser (TCP) using a relatively small number of neurons. The new paradigm is applicable to a variety of pattern-processing tasks such as speech recognition and character recognition. We concentrate here on hand-written character recognition; performance in other problem domains will be reported in future publications. Naturally, the possible realizations of this theme are diverse; in this paper we present some initial explorations of the case where the pattern grammar is context-free, inferred (from examples) by a separate procedure, and then mapped onto a connectionist parser. Unlike most neural networks for which structure is pre-defined, the resulting network has as many levels as are necessary and arbitrary connections between levels. Furthermore, by the addition of a delay element, the network becomes capable of dealing with time-varying patterns in a simple and efficient manner. Results are presented to illustrate the performance of the system with respect to a number of parameters, namely, the inherent variability of the data, the nature of the learning (supervised or unsupervised) and the details of the clustering procedure used to limit the number of non-terminals inferred. Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred. This dramatically reduces complexity. In each of these cases (eight in total), we contrast the performance of a stochastic and a non-stochastic TCP. Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred. This dramatically reduces complexity. In each of these cases (eight in total), we contrast the performance of a stochastic and a non-stochastic TCP. The stochastic TCP does have greater powers of discrimination, but in many cases the results were very similar. Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred. This dramatically reduces complexity. In each of these cases (eight in total), we contrast the performance of a stochastic and a non-stochastic TCP. The stochastic TCP does have greater powers of discrimination, but in many cases the results were very similar. If this result holds in practical situations it is important, because the non-stochastic version has a straightforward implementation in silicon."}, {"paper_id": "153690277", "adju_relevance": 0, "title": "MACROECONOMICS AND REALITY", "background_label": "Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them. In this lecture I intend to discuss some aspects of this situation, attempting both to offer some explanations and to suggest some means for improvement. I will argue that the style in which their builders construct claims for a connection between these models and reality-the style in which \"identification\" is achieved for these models-is inappropriate, to the point at which claims for identification in these models cannot be taken seriously. This is a venerable assertion; and there are some good old reasons for believing it;2 but there are also some reasons which have been more recently put forth.", "result_label": "It is still rare for empirical research in macroeconomics to be planned and executed within the framework of one of the large models.", "abstract": "Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them. It is still rare for empirical research in macroeconomics to be planned and executed within the framework of one of the large models. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them. In this lecture I intend to discuss some aspects of this situation, attempting both to offer some explanations and to suggest some means for improvement. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them. In this lecture I intend to discuss some aspects of this situation, attempting both to offer some explanations and to suggest some means for improvement. I will argue that the style in which their builders construct claims for a connection between these models and reality-the style in which \"identification\" is achieved for these models-is inappropriate, to the point at which claims for identification in these models cannot be taken seriously. Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them. In this lecture I intend to discuss some aspects of this situation, attempting both to offer some explanations and to suggest some means for improvement. I will argue that the style in which their builders construct claims for a connection between these models and reality-the style in which \"identification\" is achieved for these models-is inappropriate, to the point at which claims for identification in these models cannot be taken seriously. This is a venerable assertion; and there are some good old reasons for believing it;2 but there are also some reasons which have been more recently put forth."}, {"paper_id": "53776855", "adju_relevance": 0, "title": "Learning without Memorizing", "background_label": "Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory.", "abstract": "Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory."}, {"paper_id": "6475282", "adju_relevance": 0, "title": "Distributed Optimization of Multi-Class SVMs", "background_label": "Training of one-vs.-rest SVMs can be parallelized over the number of classes in a straight forward way. Given enough computational resources, one-vs.-rest SVMs can thus be trained on data involving a large number of classes. The same cannot be stated, however, for the so-called all-in-one SVMs, which require solving a quadratic program of size quadratically in the number of classes.", "abstract": "Training of one-vs.-rest SVMs can be parallelized over the number of classes in a straight forward way. Training of one-vs.-rest SVMs can be parallelized over the number of classes in a straight forward way. Given enough computational resources, one-vs.-rest SVMs can thus be trained on data involving a large number of classes. Training of one-vs.-rest SVMs can be parallelized over the number of classes in a straight forward way. Given enough computational resources, one-vs.-rest SVMs can thus be trained on data involving a large number of classes. The same cannot be stated, however, for the so-called all-in-one SVMs, which require solving a quadratic program of size quadratically in the number of classes."}, {"paper_id": "17814659", "adju_relevance": 0, "title": "Multi-Class and Single-Class Classification Approaches to Vehicle Model Recognition from Images", "background_label": "This paper investigates the use of machine learning classification techniques applied to the task of recognising the make and model of vehicles. Although a number of vehicle classification systems already exist, most of them seek only to distinguish between vehicle categories, e.g. identifying whether a vehicle is a bus, truck or car.", "method_label": "The system presented here demonstrates that a set of features extracted from the frontal view of a vehicle may be used to determine the vehicle type (make and model) with high accuracy.", "result_label": "The performance of some standard multi-class classification algorithms is compared for this problem. A one-class k-Nearest Neighbour classification algorithm is also implemented and tested.", "abstract": "This paper investigates the use of machine learning classification techniques applied to the task of recognising the make and model of vehicles. This paper investigates the use of machine learning classification techniques applied to the task of recognising the make and model of vehicles. Although a number of vehicle classification systems already exist, most of them seek only to distinguish between vehicle categories, e.g. This paper investigates the use of machine learning classification techniques applied to the task of recognising the make and model of vehicles. Although a number of vehicle classification systems already exist, most of them seek only to distinguish between vehicle categories, e.g. identifying whether a vehicle is a bus, truck or car. The system presented here demonstrates that a set of features extracted from the frontal view of a vehicle may be used to determine the vehicle type (make and model) with high accuracy. The performance of some standard multi-class classification algorithms is compared for this problem. The performance of some standard multi-class classification algorithms is compared for this problem. A one-class k-Nearest Neighbour classification algorithm is also implemented and tested."}, {"paper_id": "15648012", "adju_relevance": 0, "title": "Training a Restricted Boltzmann Machine for Classification by Labeling Model Samples", "method_label": "Using the MNIST set of handwritten digits and Restricted Boltzmann Machines, it is possible to reach a classification performance competitive to semi-supervised learning if we first train a model in an unsupervised fashion on unlabeled data only, and then manually add labels to model samples instead of training data samples with the help of a GUI. This approach can benefit from the fact that model samples can be presented to the human labeler in a video-like fashion, resulting in a higher number of labeled examples.", "result_label": "Also, after some initial training, hard-to-classify examples can be distinguished from easy ones automatically, saving manual work.", "abstract": " Using the MNIST set of handwritten digits and Restricted Boltzmann Machines, it is possible to reach a classification performance competitive to semi-supervised learning if we first train a model in an unsupervised fashion on unlabeled data only, and then manually add labels to model samples instead of training data samples with the help of a GUI. Using the MNIST set of handwritten digits and Restricted Boltzmann Machines, it is possible to reach a classification performance competitive to semi-supervised learning if we first train a model in an unsupervised fashion on unlabeled data only, and then manually add labels to model samples instead of training data samples with the help of a GUI. This approach can benefit from the fact that model samples can be presented to the human labeler in a video-like fashion, resulting in a higher number of labeled examples. Also, after some initial training, hard-to-classify examples can be distinguished from easy ones automatically, saving manual work."}, {"paper_id": "42296784", "adju_relevance": 0, "title": "Binary Stochastic Representations for Large Multi-class Classification", "background_label": "Classification with a large number of classes is a key problem in machine learning and corresponds to many real-world applications like tagging of images or textual documents in social networks. If one-vs-all methods usually reach top performance in this context, these approaches suffer from a high inference complexity, linear w.r.t the number of categories. Different models based on the notion of binary codes have been proposed to overcome this limitation, achieving in a sublinear inference complexity. But they a priori need to decide which binary code to associate to which category before learning using more or less complex heuristics.", "method_label": "We propose a new end-to-end model which aims at simultaneously learning to associate binary codes with categories, but also learning to map inputs to binary codes. This approach called Deep Stochastic Neural Codes (DSNC) keeps the sublinear inference complexity but do not need any a priori tuning.", "result_label": "Experimental results on different datasets show the effectiveness of the approach w.r.t baseline methods.", "abstract": "Classification with a large number of classes is a key problem in machine learning and corresponds to many real-world applications like tagging of images or textual documents in social networks. Classification with a large number of classes is a key problem in machine learning and corresponds to many real-world applications like tagging of images or textual documents in social networks. If one-vs-all methods usually reach top performance in this context, these approaches suffer from a high inference complexity, linear w.r.t the number of categories. Classification with a large number of classes is a key problem in machine learning and corresponds to many real-world applications like tagging of images or textual documents in social networks. If one-vs-all methods usually reach top performance in this context, these approaches suffer from a high inference complexity, linear w.r.t the number of categories. Different models based on the notion of binary codes have been proposed to overcome this limitation, achieving in a sublinear inference complexity. Classification with a large number of classes is a key problem in machine learning and corresponds to many real-world applications like tagging of images or textual documents in social networks. If one-vs-all methods usually reach top performance in this context, these approaches suffer from a high inference complexity, linear w.r.t the number of categories. Different models based on the notion of binary codes have been proposed to overcome this limitation, achieving in a sublinear inference complexity. But they a priori need to decide which binary code to associate to which category before learning using more or less complex heuristics. We propose a new end-to-end model which aims at simultaneously learning to associate binary codes with categories, but also learning to map inputs to binary codes. We propose a new end-to-end model which aims at simultaneously learning to associate binary codes with categories, but also learning to map inputs to binary codes. This approach called Deep Stochastic Neural Codes (DSNC) keeps the sublinear inference complexity but do not need any a priori tuning. Experimental results on different datasets show the effectiveness of the approach w.r.t baseline methods."}, {"paper_id": "59578351", "adju_relevance": 0, "title": "Phraseology in the language, in the dictionary, and in the computer", "background_label": "Two main families of phrasemes (= non-free phrases) are distinguish ed: lexical phrasemes and semantic-lexical phrasemes; the phrasemes of the first family are constrained only in their form (their meaning being free), those of the second family are constrained both in their meaning and in their form.", "method_label": "Two basic concepts are introduced: compositionality of complex linguistic signs and the pivot of a meaning. Three major classes of phrasemes are presented: non- compositional idioms and compositional collocations and cliches . A new type of general dictionary is proposed, and the lexicographic presentation of the three classes of phrasemes is illustrated.", "result_label": "To show how the proposed approach to phraseology can be used in Automatic Language Processing, three fully-fledged examples are examined in detail.", "abstract": "Two main families of phrasemes (= non-free phrases) are distinguish ed: lexical phrasemes and semantic-lexical phrasemes; the phrasemes of the first family are constrained only in their form (their meaning being free), those of the second family are constrained both in their meaning and in their form. Two basic concepts are introduced: compositionality of complex linguistic signs and the pivot of a meaning. Two basic concepts are introduced: compositionality of complex linguistic signs and the pivot of a meaning. Three major classes of phrasemes are presented: non- compositional idioms and compositional collocations and cliches . Two basic concepts are introduced: compositionality of complex linguistic signs and the pivot of a meaning. Three major classes of phrasemes are presented: non- compositional idioms and compositional collocations and cliches . A new type of general dictionary is proposed, and the lexicographic presentation of the three classes of phrasemes is illustrated. To show how the proposed approach to phraseology can be used in Automatic Language Processing, three fully-fledged examples are examined in detail."}, {"paper_id": "12998593", "adju_relevance": 0, "title": "Inference of complex biological networks: distinguishability issues and optimization-based solutions", "background_label": "BACKGROUND The inference of biological networks from high-throughput data has received huge attention during the last decade and can be considered an important problem class in systems biology. However, it has been recognized that reliable network inference remains an unsolved problem. Most authors have identified lack of data and deficiencies in the inference algorithms as the main reasons for this situation.", "method_label": "RESULTS We claim that another major difficulty for solving these inference problems is the frequent lack of uniqueness of many of these networks, especially when prior assumptions have not been taken properly into account.", "abstract": "BACKGROUND The inference of biological networks from high-throughput data has received huge attention during the last decade and can be considered an important problem class in systems biology. BACKGROUND The inference of biological networks from high-throughput data has received huge attention during the last decade and can be considered an important problem class in systems biology. However, it has been recognized that reliable network inference remains an unsolved problem. BACKGROUND The inference of biological networks from high-throughput data has received huge attention during the last decade and can be considered an important problem class in systems biology. However, it has been recognized that reliable network inference remains an unsolved problem. Most authors have identified lack of data and deficiencies in the inference algorithms as the main reasons for this situation. RESULTS We claim that another major difficulty for solving these inference problems is the frequent lack of uniqueness of many of these networks, especially when prior assumptions have not been taken properly into account."}, {"paper_id": "5170310", "adju_relevance": 0, "title": "A PAC-style model for learning from labeled and unlabeled data", "background_label": "There has been growing interest in practice in using unlabeled data together with labeled data in machine learning, and a number of different approaches have been developed. However, the assumptions these methods are based on are often quite distinct and not captured by standard theoretical models.", "method_label": "In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on. Our model can be viewed as an extension of the standard PAC model, where in addition to a concept class C, one also proposes a type of compatibility that one believes the target concept should have with the underlying distribution. In this view, unlabeled data can be helpful because it allows one to estimate compatibility over the space of hypotheses, and reduce the size of the search space to those that, according to one's assumptions, are a-priori reasonable with respect to the distribution. We discuss a number of technical issues that arise in this context, and provide sample-complexity bounds both for uniform convergence and e-cover based algorithms.", "result_label": "We also consider algorithmic issues, and give an efficient algorithm for a special case of co-training.", "abstract": "There has been growing interest in practice in using unlabeled data together with labeled data in machine learning, and a number of different approaches have been developed. There has been growing interest in practice in using unlabeled data together with labeled data in machine learning, and a number of different approaches have been developed. However, the assumptions these methods are based on are often quite distinct and not captured by standard theoretical models. In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on. In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on. Our model can be viewed as an extension of the standard PAC model, where in addition to a concept class C, one also proposes a type of compatibility that one believes the target concept should have with the underlying distribution. In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on. Our model can be viewed as an extension of the standard PAC model, where in addition to a concept class C, one also proposes a type of compatibility that one believes the target concept should have with the underlying distribution. In this view, unlabeled data can be helpful because it allows one to estimate compatibility over the space of hypotheses, and reduce the size of the search space to those that, according to one's assumptions, are a-priori reasonable with respect to the distribution. In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on. Our model can be viewed as an extension of the standard PAC model, where in addition to a concept class C, one also proposes a type of compatibility that one believes the target concept should have with the underlying distribution. In this view, unlabeled data can be helpful because it allows one to estimate compatibility over the space of hypotheses, and reduce the size of the search space to those that, according to one's assumptions, are a-priori reasonable with respect to the distribution. We discuss a number of technical issues that arise in this context, and provide sample-complexity bounds both for uniform convergence and e-cover based algorithms. We also consider algorithmic issues, and give an efficient algorithm for a special case of co-training."}, {"paper_id": "10997380", "adju_relevance": 0, "title": "A New Data Selection Approach for Semi-Supervised Acoustic Modeling", "background_label": "Current approaches to semi-supervised incremental learning prefer to select unlabeled examples predicted with high confidence for model re-training. However, this strategy can degrade the classification performance rather than improve it.", "method_label": "We present an analysis for the reasons of this phenomenon, showing that only relying on high confidence for data selection can lead to an erroneous estimate to the true distribution when the confidence annotator is highly correlated with the classifier in the information they use. We propose a new data selection approach to address this problem and apply it to a variety of applications, including machine learning and speech recognition.", "result_label": "Encouraging improvements in recognition accuracy are observed in our experiments", "abstract": "Current approaches to semi-supervised incremental learning prefer to select unlabeled examples predicted with high confidence for model re-training. Current approaches to semi-supervised incremental learning prefer to select unlabeled examples predicted with high confidence for model re-training. However, this strategy can degrade the classification performance rather than improve it. We present an analysis for the reasons of this phenomenon, showing that only relying on high confidence for data selection can lead to an erroneous estimate to the true distribution when the confidence annotator is highly correlated with the classifier in the information they use. We present an analysis for the reasons of this phenomenon, showing that only relying on high confidence for data selection can lead to an erroneous estimate to the true distribution when the confidence annotator is highly correlated with the classifier in the information they use. We propose a new data selection approach to address this problem and apply it to a variety of applications, including machine learning and speech recognition. Encouraging improvements in recognition accuracy are observed in our experiments"}, {"paper_id": "6764656", "adju_relevance": 0, "title": "Multi-Class Active Learning for Image Classification", "background_label": "One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. Especially for images and video, providing training data is very expensive in terms of human time and effort.", "abstract": "One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. Especially for images and video, providing training data is very expensive in terms of human time and effort."}, {"paper_id": "62500203", "adju_relevance": 0, "title": "Multi-class active learning for image classification", "background_label": "One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. Especially for images and video, providing training data is very expensive in terms of human time and effort.", "abstract": "One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required. Especially for images and video, providing training data is very expensive in terms of human time and effort."}, {"paper_id": "14406743", "adju_relevance": 0, "title": "Learning from Imbalanced Data Sets: A Comparison of Various Strategies", "background_label": "Although the majority of concept-learning systems previously designed usually assume that their training sets are well-balanced, this assumption is not necessarily correct. Indeed, there exists many domains for which one class is represented by a large number of examples while the other is represented by only a few.", "abstract": "Although the majority of concept-learning systems previously designed usually assume that their training sets are well-balanced, this assumption is not necessarily correct. Although the majority of concept-learning systems previously designed usually assume that their training sets are well-balanced, this assumption is not necessarily correct. Indeed, there exists many domains for which one class is represented by a large number of examples while the other is represented by only a few."}, {"paper_id": "3503170", "adju_relevance": 0, "title": "A Reevaluation and Benchmark of Hidden Markov Models", "background_label": "Hidden Markov models are frequently used in handwriting-recognition applications. While a large number of methodological variants have been developed to accommodate different use cases, the core concepts have not been changed much.", "abstract": "Hidden Markov models are frequently used in handwriting-recognition applications. Hidden Markov models are frequently used in handwriting-recognition applications. While a large number of methodological variants have been developed to accommodate different use cases, the core concepts have not been changed much."}, {"paper_id": "23824762", "adju_relevance": 0, "title": "MMG: a probabilistic tool to identify submodules of metabolic pathways.", "background_label": "MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.) which are differentially expressed in two different conditions. Popular approaches involve using t-test statistics, based on modelling the data as arising from a mixture distribution. A common assumption of these approaches is that the data are independent and identically distributed; however, biological quantities are usually related through a complex (weighted) network of interactions, and often the more pertinent question is which subnetworks are differentially expressed, rather than which genes. RESULTS We introduce Mixture Model on Graphs (MMG), a novel probabilistic model to identify differentially expressed submodules of biological networks and pathways.", "result_label": "Furthermore, in many interesting cases (such as high-throughput proteomics and metabolomics), only very partial observations are available, resulting in the need for efficient imputation techniques. Analysis of our model results on quantitative high-throughput proteomic data leads to the identification of biologically significant subnetworks, as well as the prediction of the expression level of a number of enzymes, some of which are then verified experimentally.", "method_label": "The method can easily incorporate information about weights in the network, is robust against missing data and can be easily generalized to directed networks. We propose an efficient sampling strategy to infer posterior probabilities of differential expression, as well as posterior probabilities over the model parameters. We assess our method on artificial data demonstrating significant improvements over standard mixture model clustering.", "abstract": "MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.) MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.) which are differentially expressed in two different conditions. MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.) which are differentially expressed in two different conditions. Popular approaches involve using t-test statistics, based on modelling the data as arising from a mixture distribution. MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.) which are differentially expressed in two different conditions. Popular approaches involve using t-test statistics, based on modelling the data as arising from a mixture distribution. A common assumption of these approaches is that the data are independent and identically distributed; however, biological quantities are usually related through a complex (weighted) network of interactions, and often the more pertinent question is which subnetworks are differentially expressed, rather than which genes. Furthermore, in many interesting cases (such as high-throughput proteomics and metabolomics), only very partial observations are available, resulting in the need for efficient imputation techniques. MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals. At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.) which are differentially expressed in two different conditions. Popular approaches involve using t-test statistics, based on modelling the data as arising from a mixture distribution. A common assumption of these approaches is that the data are independent and identically distributed; however, biological quantities are usually related through a complex (weighted) network of interactions, and often the more pertinent question is which subnetworks are differentially expressed, rather than which genes. RESULTS We introduce Mixture Model on Graphs (MMG), a novel probabilistic model to identify differentially expressed submodules of biological networks and pathways. The method can easily incorporate information about weights in the network, is robust against missing data and can be easily generalized to directed networks. The method can easily incorporate information about weights in the network, is robust against missing data and can be easily generalized to directed networks. We propose an efficient sampling strategy to infer posterior probabilities of differential expression, as well as posterior probabilities over the model parameters. The method can easily incorporate information about weights in the network, is robust against missing data and can be easily generalized to directed networks. We propose an efficient sampling strategy to infer posterior probabilities of differential expression, as well as posterior probabilities over the model parameters. We assess our method on artificial data demonstrating significant improvements over standard mixture model clustering. Furthermore, in many interesting cases (such as high-throughput proteomics and metabolomics), only very partial observations are available, resulting in the need for efficient imputation techniques. Analysis of our model results on quantitative high-throughput proteomic data leads to the identification of biologically significant subnetworks, as well as the prediction of the expression level of a number of enzymes, some of which are then verified experimentally."}, {"paper_id": "60446966", "adju_relevance": 0, "title": "Active Learning Via Sequential Design and Uncertainty Sampling", "background_label": "Classification is an important task in many fields including biomedical research and machine learning. Traditionally, a classification rule is constructed based a bunch of labeled data. Recently, due to technological innovation and automatic data collection schemes, we easily encounter with data sets containing large amounts of unlabeled samples. Because to label each of them is usually costly and inefficient, how to utilize these unlabeled data in a classifier construction process becomes an important problem.", "method_label": "In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure. In this paper, we study sequential learning procedures for building efficient and effective classifiers, where only the selected subjects are labeled and included in its learning stage. The proposed algorithm combines the ideas of Bayesian sequential optimal design and uncertainty sampling. Computational issues of the algorithm are discussed.", "result_label": "Numerical results using both synthesized data and real examples are reported.", "abstract": "Classification is an important task in many fields including biomedical research and machine learning. Classification is an important task in many fields including biomedical research and machine learning. Traditionally, a classification rule is constructed based a bunch of labeled data. Classification is an important task in many fields including biomedical research and machine learning. Traditionally, a classification rule is constructed based a bunch of labeled data. Recently, due to technological innovation and automatic data collection schemes, we easily encounter with data sets containing large amounts of unlabeled samples. Classification is an important task in many fields including biomedical research and machine learning. Traditionally, a classification rule is constructed based a bunch of labeled data. Recently, due to technological innovation and automatic data collection schemes, we easily encounter with data sets containing large amounts of unlabeled samples. Because to label each of them is usually costly and inefficient, how to utilize these unlabeled data in a classifier construction process becomes an important problem. In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure. In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure. In this paper, we study sequential learning procedures for building efficient and effective classifiers, where only the selected subjects are labeled and included in its learning stage. In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure. In this paper, we study sequential learning procedures for building efficient and effective classifiers, where only the selected subjects are labeled and included in its learning stage. The proposed algorithm combines the ideas of Bayesian sequential optimal design and uncertainty sampling. In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples. From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure. In this paper, we study sequential learning procedures for building efficient and effective classifiers, where only the selected subjects are labeled and included in its learning stage. The proposed algorithm combines the ideas of Bayesian sequential optimal design and uncertainty sampling. Computational issues of the algorithm are discussed. Numerical results using both synthesized data and real examples are reported."}, {"paper_id": "14285303", "adju_relevance": 0, "title": "Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments", "background_label": "The scope of the classical k-NN classification techniques is enlarged under this study to cover partially exposed environments. The modified classification system structure required for successful operation in environments, wherein all the inherent pattern classes are not exposed to the system prior to deployment, is developed and illustrated with the aid of a specific classification rule-the neighborhood census rule (NCR). Admittedly, alternative rules can be visualized to fit this modified structure.", "abstract": "The scope of the classical k-NN classification techniques is enlarged under this study to cover partially exposed environments. The scope of the classical k-NN classification techniques is enlarged under this study to cover partially exposed environments. The modified classification system structure required for successful operation in environments, wherein all the inherent pattern classes are not exposed to the system prior to deployment, is developed and illustrated with the aid of a specific classification rule-the neighborhood census rule (NCR). The scope of the classical k-NN classification techniques is enlarged under this study to cover partially exposed environments. The modified classification system structure required for successful operation in environments, wherein all the inherent pattern classes are not exposed to the system prior to deployment, is developed and illustrated with the aid of a specific classification rule-the neighborhood census rule (NCR). Admittedly, alternative rules can be visualized to fit this modified structure."}, {"paper_id": "9874135", "adju_relevance": 0, "title": "Efficient multi-label ranking for multi-class learning: Application to object recognition", "background_label": "Multi-label learning is useful in visual object recognition when several objects are present in an image. Conventional approaches implement multi-label learning as a set of binary classification problems, but they suffer from imbalanced data distributions when the number of classes is large.", "abstract": "Multi-label learning is useful in visual object recognition when several objects are present in an image. Multi-label learning is useful in visual object recognition when several objects are present in an image. Conventional approaches implement multi-label learning as a set of binary classification problems, but they suffer from imbalanced data distributions when the number of classes is large."}, {"paper_id": "14872665", "adju_relevance": 0, "title": "A Generative Probabilistic Model for Multi-label Classification", "background_label": "Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes.", "abstract": "Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes."}, {"paper_id": "14382246", "adju_relevance": 0, "title": "On Loss Functions for Deep Neural Networks in Classification", "background_label": "Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss.", "abstract": "Deep neural networks are currently among the most commonly used classifiers. Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss."}, {"paper_id": "10674718", "adju_relevance": 0, "title": "Multi-Kernel Support Vector Clustering for Multi-Class Classification", "background_label": "Support vector clustering (SVC) has been successfully applied to solve multi-class classification problems. However, it is usually hard to determine the hyper-parameters of RBF kernel functions. A multiple kernel learning (MKL) algorithm is developed to solve this problem, by which the kernel matrix weights and Lagrange multipliers can be simultaneously obtained with semidefinite programming. However, the amount of time and space required is very demanding.", "method_label": "We develop a two stage multiple kernel learning algorithm by incorporating sequential minimal optimization (SMO) with the gradient projection method.", "result_label": "Experimental results on data sets from UCI and Statlog show that the proposed approach outperforms single-kernel support vector clustering.", "abstract": "Support vector clustering (SVC) has been successfully applied to solve multi-class classification problems. Support vector clustering (SVC) has been successfully applied to solve multi-class classification problems. However, it is usually hard to determine the hyper-parameters of RBF kernel functions. Support vector clustering (SVC) has been successfully applied to solve multi-class classification problems. However, it is usually hard to determine the hyper-parameters of RBF kernel functions. A multiple kernel learning (MKL) algorithm is developed to solve this problem, by which the kernel matrix weights and Lagrange multipliers can be simultaneously obtained with semidefinite programming. Support vector clustering (SVC) has been successfully applied to solve multi-class classification problems. However, it is usually hard to determine the hyper-parameters of RBF kernel functions. A multiple kernel learning (MKL) algorithm is developed to solve this problem, by which the kernel matrix weights and Lagrange multipliers can be simultaneously obtained with semidefinite programming. However, the amount of time and space required is very demanding. We develop a two stage multiple kernel learning algorithm by incorporating sequential minimal optimization (SMO) with the gradient projection method. Experimental results on data sets from UCI and Statlog show that the proposed approach outperforms single-kernel support vector clustering."}, {"paper_id": "12897739", "adju_relevance": 0, "title": "Training set expansion: an approach to improving the reconstruction of biological networks from limited and uneven reliable interactions", "background_label": "MOTIVATION An important problem in systems biology is reconstructing complete networks of interactions between biological objects by extrapolating from a few known interactions as examples. While there are many computational techniques proposed for this network reconstruction task, their accuracy is consistently limited by the small number of high-confidence examples, and the uneven distribution of these examples across the potential interaction space, with some objects having many known interactions and others few.", "abstract": "MOTIVATION An important problem in systems biology is reconstructing complete networks of interactions between biological objects by extrapolating from a few known interactions as examples. MOTIVATION An important problem in systems biology is reconstructing complete networks of interactions between biological objects by extrapolating from a few known interactions as examples. While there are many computational techniques proposed for this network reconstruction task, their accuracy is consistently limited by the small number of high-confidence examples, and the uneven distribution of these examples across the potential interaction space, with some objects having many known interactions and others few."}]