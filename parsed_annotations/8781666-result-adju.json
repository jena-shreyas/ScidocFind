[{"paper_id": "8781666", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "background_label": "Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm.", "abstract": "Domain knowledge is crucial for effective performance in autonomous control systems. Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm."}, {"paper_id": "8781666", "adju_relevance": 3, "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "background_label": "Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm.", "abstract": "Domain knowledge is crucial for effective performance in autonomous control systems. Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm."}, {"paper_id": "52985864", "adju_relevance": 2, "title": "Forward Model Approximation for General Video Game Learning", "background_label": "This paper proposes a novel learning agent model for a General Video Game Playing agent. Our agent learns an approximation of the forward model from repeatedly playing a game and subsequently adapting its behavior to previously unseen levels.", "method_label": "To achieve this, it first learns the game mechanics through machine learning techniques and then extracts rule-based symbolic knowledge on different levels of abstraction. When being confronted with new levels of a game, the agent is able to revise its knowledge by a novel belief revision approach. Using methods such as Monte Carlo Tree Search and Breadth First Search, it searches for the best possible action using simulated game episodes. Those simulations are only possible due to reasoning about future states using the extracted rule-based knowledge from random episodes during the learning phase.", "result_label": "The developed agent outperforms previous agents by a large margin, while still being limited in its prediction capabilities. The proposed forward model approximation opens a new class of solutions in the context of General Video Game Playing, which do not try to learn a value function, but try to increase their accuracy in modelling the game.", "abstract": "This paper proposes a novel learning agent model for a General Video Game Playing agent. This paper proposes a novel learning agent model for a General Video Game Playing agent. Our agent learns an approximation of the forward model from repeatedly playing a game and subsequently adapting its behavior to previously unseen levels. To achieve this, it first learns the game mechanics through machine learning techniques and then extracts rule-based symbolic knowledge on different levels of abstraction. To achieve this, it first learns the game mechanics through machine learning techniques and then extracts rule-based symbolic knowledge on different levels of abstraction. When being confronted with new levels of a game, the agent is able to revise its knowledge by a novel belief revision approach. To achieve this, it first learns the game mechanics through machine learning techniques and then extracts rule-based symbolic knowledge on different levels of abstraction. When being confronted with new levels of a game, the agent is able to revise its knowledge by a novel belief revision approach. Using methods such as Monte Carlo Tree Search and Breadth First Search, it searches for the best possible action using simulated game episodes. To achieve this, it first learns the game mechanics through machine learning techniques and then extracts rule-based symbolic knowledge on different levels of abstraction. When being confronted with new levels of a game, the agent is able to revise its knowledge by a novel belief revision approach. Using methods such as Monte Carlo Tree Search and Breadth First Search, it searches for the best possible action using simulated game episodes. Those simulations are only possible due to reasoning about future states using the extracted rule-based knowledge from random episodes during the learning phase. The developed agent outperforms previous agents by a large margin, while still being limited in its prediction capabilities. The developed agent outperforms previous agents by a large margin, while still being limited in its prediction capabilities. The proposed forward model approximation opens a new class of solutions in the context of General Video Game Playing, which do not try to learn a value function, but try to increase their accuracy in modelling the game."}, {"paper_id": "52185870", "adju_relevance": 2, "title": "Jointly Learning to See, Ask, and GuessWhat", "background_label": "AbstractWe are interested in understanding how the ability to ground language in vision interacts with other abilities at play in dialogue, such as asking a series of questions to obtain the necessary information to perform a certain task.", "abstract": "AbstractWe are interested in understanding how the ability to ground language in vision interacts with other abilities at play in dialogue, such as asking a series of questions to obtain the necessary information to perform a certain task."}, {"paper_id": "1030020", "adju_relevance": 2, "title": "Online learning and mining human play in complex games", "background_label": "We propose a hybrid model for automatically acquiring a policy for a complex game, which combines online learning with mining knowledge from a corpus of human game play.", "method_label": "Our hypothesis is that a player that learns its policies by combining (online) exploration with biases towards human behaviour that's attested in a corpus of humans playing the game will outperform any agent that uses only one of the knowledge sources. During game play, the agent extracts similar moves made by players in the corpus in similar situations, and approximates their utility alongside other possible options by performing simulations from its current state. We implement and assess our model in an agent playing the complex win-lose board game Settlers of Catan, which lacks an implementation that would challenge a human expert.", "result_label": "The results from the preliminary set of experiments illustrate the potential of such a joint model.", "abstract": "We propose a hybrid model for automatically acquiring a policy for a complex game, which combines online learning with mining knowledge from a corpus of human game play. Our hypothesis is that a player that learns its policies by combining (online) exploration with biases towards human behaviour that's attested in a corpus of humans playing the game will outperform any agent that uses only one of the knowledge sources. Our hypothesis is that a player that learns its policies by combining (online) exploration with biases towards human behaviour that's attested in a corpus of humans playing the game will outperform any agent that uses only one of the knowledge sources. During game play, the agent extracts similar moves made by players in the corpus in similar situations, and approximates their utility alongside other possible options by performing simulations from its current state. Our hypothesis is that a player that learns its policies by combining (online) exploration with biases towards human behaviour that's attested in a corpus of humans playing the game will outperform any agent that uses only one of the knowledge sources. During game play, the agent extracts similar moves made by players in the corpus in similar situations, and approximates their utility alongside other possible options by performing simulations from its current state. We implement and assess our model in an agent playing the complex win-lose board game Settlers of Catan, which lacks an implementation that would challenge a human expert. The results from the preliminary set of experiments illustrate the potential of such a joint model."}, {"paper_id": "12189893", "adju_relevance": 2, "title": "Heuristic move pruning in Monte Carlo Tree Search for the strategic card game Lords of War", "background_label": "Move pruning is a technique used in game tree search which incorporates heuristic knowledge to reduce the number of moves under consideration from a particular game state.", "abstract": "Move pruning is a technique used in game tree search which incorporates heuristic knowledge to reduce the number of moves under consideration from a particular game state."}, {"paper_id": "238873", "adju_relevance": 2, "title": "Learning Semantic Correspondences with Less Supervision", "background_label": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.", "method_label": "To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.", "result_label": "We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.", "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps."}, {"paper_id": "2872916", "adju_relevance": 2, "title": "Text-based Adventures of the Golovin AI Agent", "background_label": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments.", "abstract": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments."}, {"paper_id": "11357932", "adju_relevance": 1, "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence", "background_label": "Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today.", "abstract": "Acquiring your first language is an incredible feat and not easily duplicated. Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today."}, {"paper_id": "10395192", "adju_relevance": 1, "title": "Learning to Follow Navigational Directions", "background_label": "AbstractWe present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route.", "method_label": "We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal.", "result_label": "We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.", "abstract": "AbstractWe present a system that learns to follow navigational natural language directions. AbstractWe present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. AbstractWe present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths."}, {"paper_id": "5902718", "adju_relevance": 1, "title": "Reading between the Lines: Learning to Map High-Level Instructions to Commands", "abstract": ""}, {"paper_id": "37390552", "adju_relevance": 1, "title": "Learning with Latent Language", "background_label": "The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies?", "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies?"}, {"paper_id": "5059949", "adju_relevance": 1, "title": "Attention Based Natural Language Grounding by Navigating Virtual Environment", "background_label": "The agent receives visual information through raw pixels and a natural language instruction telling what task needs to be achieved and is trained in an end-to-end way.", "method_label": "We develop an attention mechanism for multi-modal fusion of visual and textual modalities that allows the agent to learn to complete the task and achieve language grounding. We show that the learnt textual representations are semantically meaningful as they follow vector arithmetic in the embedding space. The effectiveness of our attention approach over the contemporary fusion mechanisms is also highlighted from the textual embeddings learnt by the different approaches.", "result_label": "Our experimental results show that our attention mechanism outperforms the existing multi-modal fusion mechanisms proposed for both 2D and 3D environments in order to solve the above-mentioned task in terms of both speed and success rate. We also show that our model generalizes effectively to unseen scenarios and exhibit zero-shot generalization capabilities both in 2D and 3D environments.", "abstract": " The agent receives visual information through raw pixels and a natural language instruction telling what task needs to be achieved and is trained in an end-to-end way. We develop an attention mechanism for multi-modal fusion of visual and textual modalities that allows the agent to learn to complete the task and achieve language grounding. Our experimental results show that our attention mechanism outperforms the existing multi-modal fusion mechanisms proposed for both 2D and 3D environments in order to solve the above-mentioned task in terms of both speed and success rate. We develop an attention mechanism for multi-modal fusion of visual and textual modalities that allows the agent to learn to complete the task and achieve language grounding. We show that the learnt textual representations are semantically meaningful as they follow vector arithmetic in the embedding space. We develop an attention mechanism for multi-modal fusion of visual and textual modalities that allows the agent to learn to complete the task and achieve language grounding. We show that the learnt textual representations are semantically meaningful as they follow vector arithmetic in the embedding space. The effectiveness of our attention approach over the contemporary fusion mechanisms is also highlighted from the textual embeddings learnt by the different approaches. Our experimental results show that our attention mechanism outperforms the existing multi-modal fusion mechanisms proposed for both 2D and 3D environments in order to solve the above-mentioned task in terms of both speed and success rate. We also show that our model generalizes effectively to unseen scenarios and exhibit zero-shot generalization capabilities both in 2D and 3D environments."}, {"paper_id": "146808452", "adju_relevance": 1, "title": "Comprehensible Context-driven Text Game Playing", "background_label": "In order to train a computer agent to play a text-based computer game, we must represent each hidden state of the game. A Long Short-Term Memory (LSTM) model running over observed texts is a common choice for state construction. However, a normal Deep Q-learning Network (DQN) for such an agent requires millions of steps of training or more to converge. As such, an LSTM-based DQN can take tens of days to finish the training process.", "method_label": "Though we can use a Convolutional Neural Network (CNN) as a text-encoder to construct states much faster than the LSTM, doing so without an understanding of the syntactic context of the words being analyzed can slow convergence. In this paper, we use a fast CNN to encode position- and syntax-oriented structures extracted from observed texts as states. We additionally augment the reward signal in a universal and practical manner.", "result_label": "Together, we show that our improvements can not only speed up the process by one order of magnitude but also learn a superior agent.", "abstract": "In order to train a computer agent to play a text-based computer game, we must represent each hidden state of the game. In order to train a computer agent to play a text-based computer game, we must represent each hidden state of the game. A Long Short-Term Memory (LSTM) model running over observed texts is a common choice for state construction. In order to train a computer agent to play a text-based computer game, we must represent each hidden state of the game. A Long Short-Term Memory (LSTM) model running over observed texts is a common choice for state construction. However, a normal Deep Q-learning Network (DQN) for such an agent requires millions of steps of training or more to converge. In order to train a computer agent to play a text-based computer game, we must represent each hidden state of the game. A Long Short-Term Memory (LSTM) model running over observed texts is a common choice for state construction. However, a normal Deep Q-learning Network (DQN) for such an agent requires millions of steps of training or more to converge. As such, an LSTM-based DQN can take tens of days to finish the training process. Though we can use a Convolutional Neural Network (CNN) as a text-encoder to construct states much faster than the LSTM, doing so without an understanding of the syntactic context of the words being analyzed can slow convergence. Though we can use a Convolutional Neural Network (CNN) as a text-encoder to construct states much faster than the LSTM, doing so without an understanding of the syntactic context of the words being analyzed can slow convergence. In this paper, we use a fast CNN to encode position- and syntax-oriented structures extracted from observed texts as states. Though we can use a Convolutional Neural Network (CNN) as a text-encoder to construct states much faster than the LSTM, doing so without an understanding of the syntactic context of the words being analyzed can slow convergence. In this paper, we use a fast CNN to encode position- and syntax-oriented structures extracted from observed texts as states. We additionally augment the reward signal in a universal and practical manner. Together, we show that our improvements can not only speed up the process by one order of magnitude but also learn a superior agent."}, {"paper_id": "174802906", "adju_relevance": 1, "title": "Towards Interpretable Reinforcement Learning Using Attention Augmented Agents", "background_label": "Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain.", "method_label": "This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (`where' vs. `what').", "result_label": "We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.", "abstract": "Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (`where' vs. `what'). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable."}, {"paper_id": "3827692", "adju_relevance": 1, "title": "Learning to Play General Video-Games via an Object Embedding Network", "background_label": "Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems.", "method_label": "More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition.", "result_label": "Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study.", "abstract": "Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition. Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study."}, {"paper_id": "201070814", "adju_relevance": 1, "title": "Transfer in Deep Reinforcement Learning using Knowledge Graphs", "background_label": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy transfer.", "abstract": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy transfer."}, {"paper_id": "182952502", "adju_relevance": 1, "title": "A Survey of Reinforcement Learning Informed by Natural Language", "background_label": "To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems.", "abstract": "To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems."}, {"paper_id": "6026836", "adju_relevance": 1, "title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics", "background_label": "We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects.", "method_label": "We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels.", "result_label": "Our experiments show that this joint modeling approach outperforms alternative methods.", "abstract": "We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods."}, {"paper_id": "2047201", "adju_relevance": 1, "title": "Deep Reinforcement Learning with an Unbounded Action Space", "background_label": "For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which best fits to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is difficult to pre-define the action set.", "method_label": "To address this challenge, the DRRN extracts separate high-level embedding vectors from the texts that describe states and actions, respectively, using a general interaction function, exploring inner product, bilinear, and DNN interaction, between these embedding vectors to approximate the Q-function.", "result_label": "We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures.", "abstract": " For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which best fits to the current situation (modeled as a state in the DRRN), also described by text. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which best fits to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which best fits to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is difficult to pre-define the action set. To address this challenge, the DRRN extracts separate high-level embedding vectors from the texts that describe states and actions, respectively, using a general interaction function, exploring inner product, bilinear, and DNN interaction, between these embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures."}, {"paper_id": "8395799", "adju_relevance": 1, "title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning", "background_label": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players.", "method_label": "We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states.", "result_label": "We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.", "abstract": "In this paper, we consider the task of learning control policies for text-based games. In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations."}, {"paper_id": "2488088", "adju_relevance": 1, "title": "Learning to sportscast: a test of grounded language acquisition", "background_label": "We present a novel commentator system that learns language from sportscasts of simulated soccer games.", "method_label": "The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on.", "result_label": "Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries.", "abstract": "We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries."}, {"paper_id": "166227837", "adju_relevance": 1, "title": "Learning Policies from Human Data for Skat", "background_label": "Decision-making in large imperfect information games is difficult. Thanks to recent success in Poker, Counterfactual Regret Minimization (CFR) methods have been at the forefront of research in these games. However, most of the success in large games comes with the use of a forward model and powerful state abstractions. In trick-taking card games like Bridge or Skat, large information sets and an inability to advance the simulation without fully determinizing the state make forward search problematic. Furthermore, state abstractions can be especially difficult to construct because the precise holdings of each player directly impact move values.", "abstract": "Decision-making in large imperfect information games is difficult. Decision-making in large imperfect information games is difficult. Thanks to recent success in Poker, Counterfactual Regret Minimization (CFR) methods have been at the forefront of research in these games. Decision-making in large imperfect information games is difficult. Thanks to recent success in Poker, Counterfactual Regret Minimization (CFR) methods have been at the forefront of research in these games. However, most of the success in large games comes with the use of a forward model and powerful state abstractions. Decision-making in large imperfect information games is difficult. Thanks to recent success in Poker, Counterfactual Regret Minimization (CFR) methods have been at the forefront of research in these games. However, most of the success in large games comes with the use of a forward model and powerful state abstractions. In trick-taking card games like Bridge or Skat, large information sets and an inability to advance the simulation without fully determinizing the state make forward search problematic. Decision-making in large imperfect information games is difficult. Thanks to recent success in Poker, Counterfactual Regret Minimization (CFR) methods have been at the forefront of research in these games. However, most of the success in large games comes with the use of a forward model and powerful state abstractions. In trick-taking card games like Bridge or Skat, large information sets and an inability to advance the simulation without fully determinizing the state make forward search problematic. Furthermore, state abstractions can be especially difficult to construct because the precise holdings of each player directly impact move values."}, {"paper_id": "9772678", "adju_relevance": 0, "title": "Applications of Genetic Algorithm on Optimal Sequence for Parrondo Games", "background_label": "Abstract:Parrondo game, which introduction is inspired by the flashing Brownian ratchet, presents an apparently paradoxical situation where there are ways to combine two losing games into a winning one. The original Parrondo game consists of two individual games, game A and game B. Game A is a slightly losing coin-tossing game. Game B is also a losing game if played alone.", "method_label": "Game B has two coins, with an integer parameter M. If the current cumulative capital (in discrete unit) is a multiple of M, an unfavorable coin p b is used, otherwise a favorable p g coin is used. Paradoxically, combination of game A and game B could lead to a winning game, either through random mixture, or deterministic switching. In deterministic switching, one plays according to a sequence such as ABABB. Exhaustive search and backward induction have been applied to the search for optimal finite game sequence. In this paper, we apply genetic algorithm (GA) to search for optimal game sequences with a given length N for large N. Based on results obtained through a problem-independent GA, we adapt the point mutation operator and one-point crossover operator to exploit the structure of the optimal game sequences.", "result_label": "We show by numerical results the adapted problem-dependent GA has great improvement in performance.", "abstract": "Abstract:Parrondo game, which introduction is inspired by the flashing Brownian ratchet, presents an apparently paradoxical situation where there are ways to combine two losing games into a winning one. Abstract:Parrondo game, which introduction is inspired by the flashing Brownian ratchet, presents an apparently paradoxical situation where there are ways to combine two losing games into a winning one. The original Parrondo game consists of two individual games, game A and game B. Abstract:Parrondo game, which introduction is inspired by the flashing Brownian ratchet, presents an apparently paradoxical situation where there are ways to combine two losing games into a winning one. The original Parrondo game consists of two individual games, game A and game B. Game A is a slightly losing coin-tossing game. Game B has two coins, with an integer parameter M. If the current cumulative capital (in discrete unit) is a multiple of M, an unfavorable coin p b is used, otherwise a favorable p g coin is used. Abstract:Parrondo game, which introduction is inspired by the flashing Brownian ratchet, presents an apparently paradoxical situation where there are ways to combine two losing games into a winning one. The original Parrondo game consists of two individual games, game A and game B. Game A is a slightly losing coin-tossing game. Game B is also a losing game if played alone. Game B has two coins, with an integer parameter M. If the current cumulative capital (in discrete unit) is a multiple of M, an unfavorable coin p b is used, otherwise a favorable p g coin is used. Paradoxically, combination of game A and game B could lead to a winning game, either through random mixture, or deterministic switching. Game B has two coins, with an integer parameter M. If the current cumulative capital (in discrete unit) is a multiple of M, an unfavorable coin p b is used, otherwise a favorable p g coin is used. Paradoxically, combination of game A and game B could lead to a winning game, either through random mixture, or deterministic switching. In deterministic switching, one plays according to a sequence such as ABABB. Game B has two coins, with an integer parameter M. If the current cumulative capital (in discrete unit) is a multiple of M, an unfavorable coin p b is used, otherwise a favorable p g coin is used. Paradoxically, combination of game A and game B could lead to a winning game, either through random mixture, or deterministic switching. In deterministic switching, one plays according to a sequence such as ABABB. Exhaustive search and backward induction have been applied to the search for optimal finite game sequence. Game B has two coins, with an integer parameter M. If the current cumulative capital (in discrete unit) is a multiple of M, an unfavorable coin p b is used, otherwise a favorable p g coin is used. Paradoxically, combination of game A and game B could lead to a winning game, either through random mixture, or deterministic switching. In deterministic switching, one plays according to a sequence such as ABABB. Exhaustive search and backward induction have been applied to the search for optimal finite game sequence. In this paper, we apply genetic algorithm (GA) to search for optimal game sequences with a given length N for large N. Based on results obtained through a problem-independent GA, we adapt the point mutation operator and one-point crossover operator to exploit the structure of the optimal game sequences. We show by numerical results the adapted problem-dependent GA has great improvement in performance."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "62672787", "adju_relevance": 0, "title": "Transport appraisal and Monte Carlo simulation by use of the CBA-DK model", "background_label": "This paper presents the Danish CBA-DK software model for assessment of transport infrastructure projects.", "method_label": "The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. Special emphasis has been placed on the separation between inherent randomness in the modeling system and lack of knowledge. These two concepts have been defined in terms of variability (ontological uncertainty) and uncertainty (epistemic uncertainty). After a short introduction to deterministic calculation resulting in some evaluation criteria a more comprehensive evaluation of the stochastic calculation is made. Especially, the risk analysis part of CBA-DK, with considerations about which probability distributions should be used, is explained. Furthermore, comprehensive assessments based on the set of distributions are made and implemented by use of a Danish case example.", "result_label": "Finally, conclusions and a perspective are presented.", "abstract": "This paper presents the Danish CBA-DK software model for assessment of transport infrastructure projects. The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. Special emphasis has been placed on the separation between inherent randomness in the modeling system and lack of knowledge. The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. Special emphasis has been placed on the separation between inherent randomness in the modeling system and lack of knowledge. These two concepts have been defined in terms of variability (ontological uncertainty) and uncertainty (epistemic uncertainty). The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. Special emphasis has been placed on the separation between inherent randomness in the modeling system and lack of knowledge. These two concepts have been defined in terms of variability (ontological uncertainty) and uncertainty (epistemic uncertainty). After a short introduction to deterministic calculation resulting in some evaluation criteria a more comprehensive evaluation of the stochastic calculation is made. The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. Special emphasis has been placed on the separation between inherent randomness in the modeling system and lack of knowledge. These two concepts have been defined in terms of variability (ontological uncertainty) and uncertainty (epistemic uncertainty). After a short introduction to deterministic calculation resulting in some evaluation criteria a more comprehensive evaluation of the stochastic calculation is made. Especially, the risk analysis part of CBA-DK, with considerations about which probability distributions should be used, is explained. The assessment model is based on both a deterministic calculation following the cost-benefit analysis (CBA) methodology in a Danish manual from the Ministry of Transport and on a stochastic calculation, where risk analysis is carried out using Monte Carlo simulation. Special emphasis has been placed on the separation between inherent randomness in the modeling system and lack of knowledge. These two concepts have been defined in terms of variability (ontological uncertainty) and uncertainty (epistemic uncertainty). After a short introduction to deterministic calculation resulting in some evaluation criteria a more comprehensive evaluation of the stochastic calculation is made. Especially, the risk analysis part of CBA-DK, with considerations about which probability distributions should be used, is explained. Furthermore, comprehensive assessments based on the set of distributions are made and implemented by use of a Danish case example. Finally, conclusions and a perspective are presented."}, {"paper_id": "14377964", "adju_relevance": 0, "title": "Natural Language Generation as Incremental Planning Under Uncertainty: Adaptive Information Presentation for Statistical Dialogue Systems", "background_label": "We present and evaluate a novel approach to natural language generation (NLG) in statistical spoken dialogue systems (SDS) using a data-driven statistical optimization framework for incremental information presentation (IP), where there is a trade-off to be solved between presenting \u201cenough\" information to the user while keeping the utterances short and understandable. The trained IP model is adaptive to variation from the current generation context (e.g.", "method_label": "a user and a non-deterministic sentence planner), and it incrementally adapts the IP policy at the turn level. Reinforcement learning is used to automatically optimize the IP policy with respect to a data-driven objective function. In a case study on presenting restaurant information, we show that an optimized IP strategy trained on Wizard-of-Oz data outperforms a baseline mimicking the wizard behavior in terms of total reward gained. The policy is then also tested with real users, and improves on a conventional hand-coded IP strategy used in a deployed SDS in terms of overall task success.", "result_label": "The evaluation found that the trained IP strategy significantly improves dialogue task completion for real users, with up to a 8.2% increase in task success. This methodology also provides new insights into the nature of the IP problem, which has previously been treated as a module following dialogue management with no access to lower-level context features (e.g. from a surface realizer and/or speech synthesizer).", "abstract": "We present and evaluate a novel approach to natural language generation (NLG) in statistical spoken dialogue systems (SDS) using a data-driven statistical optimization framework for incremental information presentation (IP), where there is a trade-off to be solved between presenting \u201cenough\" information to the user while keeping the utterances short and understandable. We present and evaluate a novel approach to natural language generation (NLG) in statistical spoken dialogue systems (SDS) using a data-driven statistical optimization framework for incremental information presentation (IP), where there is a trade-off to be solved between presenting \u201cenough\" information to the user while keeping the utterances short and understandable. The trained IP model is adaptive to variation from the current generation context (e.g. a user and a non-deterministic sentence planner), and it incrementally adapts the IP policy at the turn level. a user and a non-deterministic sentence planner), and it incrementally adapts the IP policy at the turn level. Reinforcement learning is used to automatically optimize the IP policy with respect to a data-driven objective function. a user and a non-deterministic sentence planner), and it incrementally adapts the IP policy at the turn level. Reinforcement learning is used to automatically optimize the IP policy with respect to a data-driven objective function. In a case study on presenting restaurant information, we show that an optimized IP strategy trained on Wizard-of-Oz data outperforms a baseline mimicking the wizard behavior in terms of total reward gained. a user and a non-deterministic sentence planner), and it incrementally adapts the IP policy at the turn level. Reinforcement learning is used to automatically optimize the IP policy with respect to a data-driven objective function. In a case study on presenting restaurant information, we show that an optimized IP strategy trained on Wizard-of-Oz data outperforms a baseline mimicking the wizard behavior in terms of total reward gained. The policy is then also tested with real users, and improves on a conventional hand-coded IP strategy used in a deployed SDS in terms of overall task success. The evaluation found that the trained IP strategy significantly improves dialogue task completion for real users, with up to a 8.2% increase in task success. The evaluation found that the trained IP strategy significantly improves dialogue task completion for real users, with up to a 8.2% increase in task success. This methodology also provides new insights into the nature of the IP problem, which has previously been treated as a module following dialogue management with no access to lower-level context features (e.g. The evaluation found that the trained IP strategy significantly improves dialogue task completion for real users, with up to a 8.2% increase in task success. This methodology also provides new insights into the nature of the IP problem, which has previously been treated as a module following dialogue management with no access to lower-level context features (e.g. from a surface realizer and/or speech synthesizer)."}, {"paper_id": "1174836", "adju_relevance": 0, "title": "Reading to Learn: Constructing Features from Semantic Abstracts", "background_label": "Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation.", "abstract": "Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation."}, {"paper_id": "52074264", "adju_relevance": 0, "title": "Playing 20 Question Game with Policy-Based Reinforcement Learning", "background_label": "The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment.", "method_label": "Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects.", "result_label": "Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.", "abstract": "The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment."}, {"paper_id": "2634828", "adju_relevance": 0, "title": "Applying Monte-Carlo Tree Search to collaboratively controlling of a Ghost Team in Ms Pac-Man", "background_label": "We present an application of Monte-Carlo Tree Search (MCTS) to controlling ghosts in the game of Ms Pac-Man.", "method_label": "We approach the problem by performing MCTS on each ghost's tree that represents the game state from the ghost's perspective.", "abstract": "We present an application of Monte-Carlo Tree Search (MCTS) to controlling ghosts in the game of Ms Pac-Man. We approach the problem by performing MCTS on each ghost's tree that represents the game state from the ghost's perspective."}, {"paper_id": "31693386", "adju_relevance": 0, "title": "Quantum monte carlo.", "background_label": "An outline of a random walk computational method for solving the Schr\u00f6dinger equation for many interacting particles is given, together with a survey of results achieved so far and of applications that remain to be explored.", "method_label": "Monte Carlo simulations can be used to calculate accurately the bulk properties of the light elements hydrogen, helium, and lithium as well as the properties of the isolated atoms and of molecules made up from these elements. It is now possible to make reliable predictions of the behavior of these substances under experimentally difficult conditions, such as high pressure, and of properties that are difficult to measure experimentally, such as the momentum distribution in superfluid helium.", "result_label": "For chemical systems, the stochastic method has a number of advantages over the widely used variational approach to determine ground-state properties, namely fast convergence to the exact result within objectively established error bounds.", "abstract": "An outline of a random walk computational method for solving the Schr\u00f6dinger equation for many interacting particles is given, together with a survey of results achieved so far and of applications that remain to be explored. Monte Carlo simulations can be used to calculate accurately the bulk properties of the light elements hydrogen, helium, and lithium as well as the properties of the isolated atoms and of molecules made up from these elements. Monte Carlo simulations can be used to calculate accurately the bulk properties of the light elements hydrogen, helium, and lithium as well as the properties of the isolated atoms and of molecules made up from these elements. It is now possible to make reliable predictions of the behavior of these substances under experimentally difficult conditions, such as high pressure, and of properties that are difficult to measure experimentally, such as the momentum distribution in superfluid helium. For chemical systems, the stochastic method has a number of advantages over the widely used variational approach to determine ground-state properties, namely fast convergence to the exact result within objectively established error bounds."}, {"paper_id": "52840590", "adju_relevance": 0, "title": "Learning to dress: synthesizing human dressing motion via deep reinforcement learning", "background_label": "Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. While deepRL has demonstrated several successes in learning complex motor skills, the data-demanding nature of the learning algorithms is at odds with the computationally costly cloth simulation required by the dressing task.", "abstract": "Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. While deepRL has demonstrated several successes in learning complex motor skills, the data-demanding nature of the learning algorithms is at odds with the computationally costly cloth simulation required by the dressing task."}, {"paper_id": "1950452", "adju_relevance": 0, "title": "Learning Context-Dependent Mappings from Sentences to Logical Form", "background_label": "We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations.", "method_label": "We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis.", "result_label": "Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy.", "abstract": "We consider the problem of learning context-dependent mappings from sentences to logical form. We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy."}, {"paper_id": "57189370", "adju_relevance": 0, "title": "StarAlgo: A Squad Movement Planning Library for StarCraft using Monte Carlo Tree Search and Negamax", "background_label": "Real-Time Strategy (RTS) games have recently become a popular testbed for artificial intelligence research. They represent a complex adversarial domain providing a number of interesting AI challenges. There exists a wide variety of research-supporting software tools, libraries and frameworks for one RTS game in particular -- StarCraft: Brood War.", "abstract": "Real-Time Strategy (RTS) games have recently become a popular testbed for artificial intelligence research. Real-Time Strategy (RTS) games have recently become a popular testbed for artificial intelligence research. They represent a complex adversarial domain providing a number of interesting AI challenges. Real-Time Strategy (RTS) games have recently become a popular testbed for artificial intelligence research. They represent a complex adversarial domain providing a number of interesting AI challenges. There exists a wide variety of research-supporting software tools, libraries and frameworks for one RTS game in particular -- StarCraft: Brood War."}, {"paper_id": "37163636", "adju_relevance": 0, "title": "Eliciting and modelling expertise for serious games in project management", "background_label": "AbstractWithout achieving a clear understanding of the learning domain, it is difficult to develop a successful serious game that enables users to achieve the desired learning outcomes. Thus, the first step in serious game design is to establish an understandding of the particular learning domain, usually through consultation with domain experts.", "method_label": "Whilst game design is inherently a creative process, we believe the capturing of the knowledge domain can be systematised and we present a structured approach to knowledge elicitation and representation as a basis for serious game design. We have adapted and extended the applied cognitive task analysis (ACTA) method and have combined it with additional knowledge representation frameworks. We explain how the outputs of this approach can inform the game mechanic and the development of non-player characters, and apply it to the design of a serious game aimed at reducing time-to-competence in soft project management skills for professionals working in corporate environments. A total of 26 domain experts from five different countries were involved in a two-stage interview process.", "result_label": "The interviews yielded more than 300 task elements, and information about the cognition underlying the more challenging tasks. This data was incorporated into several representation frameworks and used to indicate features to be implemented in the game and the game mechanics of the supported features.", "abstract": "AbstractWithout achieving a clear understanding of the learning domain, it is difficult to develop a successful serious game that enables users to achieve the desired learning outcomes. AbstractWithout achieving a clear understanding of the learning domain, it is difficult to develop a successful serious game that enables users to achieve the desired learning outcomes. Thus, the first step in serious game design is to establish an understandding of the particular learning domain, usually through consultation with domain experts. Whilst game design is inherently a creative process, we believe the capturing of the knowledge domain can be systematised and we present a structured approach to knowledge elicitation and representation as a basis for serious game design. Whilst game design is inherently a creative process, we believe the capturing of the knowledge domain can be systematised and we present a structured approach to knowledge elicitation and representation as a basis for serious game design. We have adapted and extended the applied cognitive task analysis (ACTA) method and have combined it with additional knowledge representation frameworks. Whilst game design is inherently a creative process, we believe the capturing of the knowledge domain can be systematised and we present a structured approach to knowledge elicitation and representation as a basis for serious game design. We have adapted and extended the applied cognitive task analysis (ACTA) method and have combined it with additional knowledge representation frameworks. We explain how the outputs of this approach can inform the game mechanic and the development of non-player characters, and apply it to the design of a serious game aimed at reducing time-to-competence in soft project management skills for professionals working in corporate environments. Whilst game design is inherently a creative process, we believe the capturing of the knowledge domain can be systematised and we present a structured approach to knowledge elicitation and representation as a basis for serious game design. We have adapted and extended the applied cognitive task analysis (ACTA) method and have combined it with additional knowledge representation frameworks. We explain how the outputs of this approach can inform the game mechanic and the development of non-player characters, and apply it to the design of a serious game aimed at reducing time-to-competence in soft project management skills for professionals working in corporate environments. A total of 26 domain experts from five different countries were involved in a two-stage interview process. The interviews yielded more than 300 task elements, and information about the cognition underlying the more challenging tasks. The interviews yielded more than 300 task elements, and information about the cognition underlying the more challenging tasks. This data was incorporated into several representation frameworks and used to indicate features to be implemented in the game and the game mechanics of the supported features."}, {"paper_id": "6297134", "adju_relevance": 0, "title": "Adversarial Bandit for online interactive active learning of zero-shot spoken language understanding", "background_label": "Many state-of-the-art solutions for the understanding of speech data have in common to be probabilistic and to rely on machine learning algorithms to train their models from large amount of data. The difficulty remains in the cost of collecting and annotating such data. Another point is the time for updating an existing model to a new domain.", "method_label": "Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. To do so, this method relies on exploiting both a small-sized ontological description of the target domain and a generic word-embedding semantic space for generalization. Then, this framework has been extended to exploit user feedbacks to refine the zero-shot semantic parser parameters and increase its performance online. In this paper, we propose to drive this online adaptive process with a policy learnt using the Adversarial Bandit algorithm Exp3.", "result_label": "We show, on the second Dialog State Tracking Challenge (DSTC2) datasets, that this proposition can optimally balance the cost of gathering valuable user feedbacks and the overall performance of the spoken language understanding module.", "abstract": "Many state-of-the-art solutions for the understanding of speech data have in common to be probabilistic and to rely on machine learning algorithms to train their models from large amount of data. Many state-of-the-art solutions for the understanding of speech data have in common to be probabilistic and to rely on machine learning algorithms to train their models from large amount of data. The difficulty remains in the cost of collecting and annotating such data. Many state-of-the-art solutions for the understanding of speech data have in common to be probabilistic and to rely on machine learning algorithms to train their models from large amount of data. The difficulty remains in the cost of collecting and annotating such data. Another point is the time for updating an existing model to a new domain. Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. To do so, this method relies on exploiting both a small-sized ontological description of the target domain and a generic word-embedding semantic space for generalization. Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. To do so, this method relies on exploiting both a small-sized ontological description of the target domain and a generic word-embedding semantic space for generalization. Then, this framework has been extended to exploit user feedbacks to refine the zero-shot semantic parser parameters and increase its performance online. Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. To do so, this method relies on exploiting both a small-sized ontological description of the target domain and a generic word-embedding semantic space for generalization. Then, this framework has been extended to exploit user feedbacks to refine the zero-shot semantic parser parameters and increase its performance online. In this paper, we propose to drive this online adaptive process with a policy learnt using the Adversarial Bandit algorithm Exp3. We show, on the second Dialog State Tracking Challenge (DSTC2) datasets, that this proposition can optimally balance the cost of gathering valuable user feedbacks and the overall performance of the spoken language understanding module."}, {"paper_id": "124115275", "adju_relevance": 0, "title": "Monte Carlo Methods", "background_label": "Each function value in a stochastic program can involve a multidimensional integral in extremely high dimensions. Because Monte Carlo simulation appears to offer the best possibilities for higher dimensions (see, e.g., Deak [1988] and Asmussen and Glynn [2007]), it seems to be the natural choice for use in stochastic programs.", "method_label": "In this chapter, we describe some of the basic approaches built on sampling methods. The key feature is the use of statistical estimates to obtain confidence intervals on results. Some of the material uses probability measure theory which is necessary to develop the analytical results.", "abstract": "Each function value in a stochastic program can involve a multidimensional integral in extremely high dimensions. Each function value in a stochastic program can involve a multidimensional integral in extremely high dimensions. Because Monte Carlo simulation appears to offer the best possibilities for higher dimensions (see, e.g., Deak [1988] and Asmussen and Glynn [2007]), it seems to be the natural choice for use in stochastic programs. In this chapter, we describe some of the basic approaches built on sampling methods. In this chapter, we describe some of the basic approaches built on sampling methods. The key feature is the use of statistical estimates to obtain confidence intervals on results. In this chapter, we describe some of the basic approaches built on sampling methods. The key feature is the use of statistical estimates to obtain confidence intervals on results. Some of the material uses probability measure theory which is necessary to develop the analytical results."}, {"paper_id": "49552345", "adju_relevance": 0, "title": "TextWorld: A Learning Environment for Text-based Games", "background_label": "We introduce TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games. TextWorld is a Python library that handles interactive play-through of text games, as well as backend functions like state tracking and reward assignment.", "method_label": "It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. Its generative mechanisms give precise control over the difficulty, scope, and language of constructed games, and can be used to relax challenges inherent to commercial text games like partial observability and sparse rewards. By generating sets of varied but similar games, TextWorld can also be used to study generalization and transfer learning.", "result_label": "We cast text-based games in the Reinforcement Learning formalism, use our framework to develop a set of benchmark games, and evaluate several baseline agents on this set and the curated list.", "abstract": "We introduce TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games. We introduce TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games. TextWorld is a Python library that handles interactive play-through of text games, as well as backend functions like state tracking and reward assignment. It comes with a curated list of games whose features and challenges we have analyzed. It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. Its generative mechanisms give precise control over the difficulty, scope, and language of constructed games, and can be used to relax challenges inherent to commercial text games like partial observability and sparse rewards. It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. Its generative mechanisms give precise control over the difficulty, scope, and language of constructed games, and can be used to relax challenges inherent to commercial text games like partial observability and sparse rewards. By generating sets of varied but similar games, TextWorld can also be used to study generalization and transfer learning. We cast text-based games in the Reinforcement Learning formalism, use our framework to develop a set of benchmark games, and evaluate several baseline agents on this set and the curated list."}, {"paper_id": "9111381", "adju_relevance": 0, "title": "Confidence Driven Unsupervised Semantic Parsing", "background_label": "AbstractCurrent approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain.", "abstract": "AbstractCurrent approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain."}, {"paper_id": "2344486", "adju_relevance": 0, "title": "Learning to Shoot in First Person Shooter Games by Stabilizing Actions and Clustering Rewards for Reinforcement Learning", "background_label": "While reinforcement learning (RL) has been applied to turn-based board games for many years, more complex games involving decision-making in real-time are beginning to receive more attention. A challenge in such environments is that the time that elapses between deciding to take an action and receiving a reward based on its outcome can be longer than the interval between successive decisions.", "method_label": "We explore this in the context of a non-player character (NPC) in a modern first-person shooter game.", "abstract": "While reinforcement learning (RL) has been applied to turn-based board games for many years, more complex games involving decision-making in real-time are beginning to receive more attention. While reinforcement learning (RL) has been applied to turn-based board games for many years, more complex games involving decision-making in real-time are beginning to receive more attention. A challenge in such environments is that the time that elapses between deciding to take an action and receiving a reward based on its outcome can be longer than the interval between successive decisions. We explore this in the context of a non-player character (NPC) in a modern first-person shooter game."}, {"paper_id": "1527659", "adju_relevance": 0, "title": "On the role of tracking in stationary environments", "background_label": "It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. We present three results suggesting that this is not so.", "method_label": "First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. Our third result suggests that tracking in stationary problems could be important for metalearning research (e.g., learning to learn, feature selection, transfer). We apply a metalearning algorithm for step-size adaptation, IDBD (Sutton, 1992a), to the Black and White problem, showing that meta-learning has a dramatic long-term effect on performance whereas, on an analogous converging problem, meta-learning has only a small second-order effect.", "result_label": "This small result suggests a way of eventually overcoming a major obstacle to meta-learning research: the lack of an independent methodology for task selection.", "abstract": "It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. We present three results suggesting that this is not so. First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. Our third result suggests that tracking in stationary problems could be important for metalearning research (e.g., learning to learn, feature selection, transfer). First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. Our third result suggests that tracking in stationary problems could be important for metalearning research (e.g., learning to learn, feature selection, transfer). We apply a metalearning algorithm for step-size adaptation, IDBD (Sutton, 1992a), to the Black and White problem, showing that meta-learning has a dramatic long-term effect on performance whereas, on an analogous converging problem, meta-learning has only a small second-order effect. This small result suggests a way of eventually overcoming a major obstacle to meta-learning research: the lack of an independent methodology for task selection."}, {"paper_id": "8323579", "adju_relevance": 0, "title": "Fuego\u2014An Open-Source Framework for Board Games and Go Engine Based on Monte Carlo Tree Search", "background_label": "FUEGO is both an open-source software framework and a state-of-the-art program that plays the game of Go. The framework supports developing game engines for full-information two-player board games, and is used successfully in a substantial number of projects. The FUEGO Go program became the first program to win a game against a top professional player in 9 \u00d7 9 Go. It has won a number of strong tournaments against other programs, and is competitive for 19 \u00d7 19 as well.", "abstract": "FUEGO is both an open-source software framework and a state-of-the-art program that plays the game of Go. FUEGO is both an open-source software framework and a state-of-the-art program that plays the game of Go. The framework supports developing game engines for full-information two-player board games, and is used successfully in a substantial number of projects. FUEGO is both an open-source software framework and a state-of-the-art program that plays the game of Go. The framework supports developing game engines for full-information two-player board games, and is used successfully in a substantial number of projects. The FUEGO Go program became the first program to win a game against a top professional player in 9 \u00d7 9 Go. FUEGO is both an open-source software framework and a state-of-the-art program that plays the game of Go. The framework supports developing game engines for full-information two-player board games, and is used successfully in a substantial number of projects. The FUEGO Go program became the first program to win a game against a top professional player in 9 \u00d7 9 Go. It has won a number of strong tournaments against other programs, and is competitive for 19 \u00d7 19 as well."}, {"paper_id": "18001715", "adju_relevance": 0, "title": "When and how to help: An iterative probabilistic model for learning assistance by demonstration", "background_label": "Crafting a proper assistance policy is a difficult endeavour but essential for the development of robotic assistants. Indeed, assistance is a complex issue that depends not only on the task-at-hand, but also on the state of the user, environment and competing objectives.", "abstract": "Crafting a proper assistance policy is a difficult endeavour but essential for the development of robotic assistants. Crafting a proper assistance policy is a difficult endeavour but essential for the development of robotic assistants. Indeed, assistance is a complex issue that depends not only on the task-at-hand, but also on the state of the user, environment and competing objectives."}, {"paper_id": "12009631", "adju_relevance": 0, "title": "Affordance-based Active Belief: Recognition using visual and manual actions", "background_label": "This paper presents an active, model-based recognition system.", "method_label": "It applies information theoretic measures in a belief-driven planning framework to recognize objects using the history of visual and manual interactions and to select the most informative actions. A generalization of the aspect graph is used to construct forward models of objects that account for visual transitions. We use populations of these models to define the belief state of the recognition problem.", "abstract": "This paper presents an active, model-based recognition system. It applies information theoretic measures in a belief-driven planning framework to recognize objects using the history of visual and manual interactions and to select the most informative actions. It applies information theoretic measures in a belief-driven planning framework to recognize objects using the history of visual and manual interactions and to select the most informative actions. A generalization of the aspect graph is used to construct forward models of objects that account for visual transitions. It applies information theoretic measures in a belief-driven planning framework to recognize objects using the history of visual and manual interactions and to select the most informative actions. A generalization of the aspect graph is used to construct forward models of objects that account for visual transitions. We use populations of these models to define the belief state of the recognition problem."}, {"paper_id": "66065", "adju_relevance": 0, "title": "A Monte-Carlo approach for ghost avoidance in the Ms. Pac-Man game", "background_label": "Ms. Pac-Man is a challenging, classic arcade game that provides an interesting platform for Artificial Intelligence (AI) research.", "abstract": "Ms. Pac-Man is a challenging, classic arcade game that provides an interesting platform for Artificial Intelligence (AI) research."}, {"paper_id": "195584389", "adju_relevance": 0, "title": "SampleFix: Learning to Correct Programs by Sampling Diverse Fixes", "background_label": "Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning and NLP have rekindled the hope to eventually fully automate the process of repairing programs. A key challenge is ambiguity, as multiple codes -- or fixes -- can implement the same functionality. In addition, datasets by nature fail to capture the variance introduced by such ambiguities.", "method_label": "Therefore, we propose a deep generative model to automatically correct programming errors by learning a distribution of potential fixes. Our model is formulated as a deep conditional variational autoencoder that samples diverse fixes for the given erroneous programs. In order to account for ambiguity and inherent lack of representative datasets, we propose a novel regularizer to encourage the model to generate diverse fixes.", "result_label": "Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-the-art approaches by fixing up to 65% of the mistakes.", "abstract": "Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning and NLP have rekindled the hope to eventually fully automate the process of repairing programs. Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning and NLP have rekindled the hope to eventually fully automate the process of repairing programs. A key challenge is ambiguity, as multiple codes -- or fixes -- can implement the same functionality. Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning and NLP have rekindled the hope to eventually fully automate the process of repairing programs. A key challenge is ambiguity, as multiple codes -- or fixes -- can implement the same functionality. In addition, datasets by nature fail to capture the variance introduced by such ambiguities. Therefore, we propose a deep generative model to automatically correct programming errors by learning a distribution of potential fixes. Therefore, we propose a deep generative model to automatically correct programming errors by learning a distribution of potential fixes. Our model is formulated as a deep conditional variational autoencoder that samples diverse fixes for the given erroneous programs. Therefore, we propose a deep generative model to automatically correct programming errors by learning a distribution of potential fixes. Our model is formulated as a deep conditional variational autoencoder that samples diverse fixes for the given erroneous programs. In order to account for ambiguity and inherent lack of representative datasets, we propose a novel regularizer to encourage the model to generate diverse fixes. Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-the-art approaches by fixing up to 65% of the mistakes."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "54457345", "adju_relevance": 0, "title": "Taking the Scenic Route: Automatic Exploration for Videogames", "background_label": "Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Having access to a semantic map of reachable states in the game would enable efficient inference in these applications. However, human gameplay data is expensive to acquire relative to the coverage of a game that it provides.", "method_label": "We show that off-the-shelf automatic exploration strategies can explore with an effectiveness comparable to human gameplay on the same timescale. We contribute generic methods for quantifying exploration quality as a function of time and demonstrate our metric on several elementary techniques and human players on a collection of commercial games sampled from multiple game platforms (from Atari 2600 to Nintendo 64).", "result_label": "Emphasizing the diversity of states reached and the semantic map extracted, this work makes productive contrast with the focus on finding a behavior policy or optimizing game score used in most automatic game playing research.", "abstract": "Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Having access to a semantic map of reachable states in the game would enable efficient inference in these applications. Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Having access to a semantic map of reachable states in the game would enable efficient inference in these applications. However, human gameplay data is expensive to acquire relative to the coverage of a game that it provides. We show that off-the-shelf automatic exploration strategies can explore with an effectiveness comparable to human gameplay on the same timescale. We show that off-the-shelf automatic exploration strategies can explore with an effectiveness comparable to human gameplay on the same timescale. We contribute generic methods for quantifying exploration quality as a function of time and demonstrate our metric on several elementary techniques and human players on a collection of commercial games sampled from multiple game platforms (from Atari 2600 to Nintendo 64). Emphasizing the diversity of states reached and the semantic map extracted, this work makes productive contrast with the focus on finding a behavior policy or optimizing game score used in most automatic game playing research."}, {"paper_id": "49870586", "adju_relevance": 0, "title": "Traditional Wisdom and Monte Carlo Tree Search Face-to-Face in the Card Game Scopone", "background_label": "We present the design of a competitive artificial intelligence for Scopone, a popular Italian card game.", "method_label": "We compare rule-based players using the most established strategies (one for beginners and two for advanced players) against players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS) with different reward functions and simulation strategies. MCTS requires complete information about the game state and thus implements a cheating player while ISMCTS can deal with incomplete information and thus implements a fair player.", "result_label": "Our results show that, as expected, the cheating MCTS outperforms all the other strategies; ISMCTS is stronger than all the rule-based players implementing well-known and most advanced strategies and it also turns out to be a challenging opponent for human players.", "abstract": "We present the design of a competitive artificial intelligence for Scopone, a popular Italian card game. We compare rule-based players using the most established strategies (one for beginners and two for advanced players) against players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS) with different reward functions and simulation strategies. We compare rule-based players using the most established strategies (one for beginners and two for advanced players) against players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS) with different reward functions and simulation strategies. MCTS requires complete information about the game state and thus implements a cheating player while ISMCTS can deal with incomplete information and thus implements a fair player. Our results show that, as expected, the cheating MCTS outperforms all the other strategies; ISMCTS is stronger than all the rule-based players implementing well-known and most advanced strategies and it also turns out to be a challenging opponent for human players."}, {"paper_id": "18965413", "adju_relevance": 0, "title": "Dynamic Game Difficulty Scaling Using Adaptive Behavior-Based AI", "background_label": "Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI.", "method_label": "In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages.", "result_label": "Both algorithms are able to generalize well to a variety of opponents.", "abstract": "Games are played by a wide variety of audiences. Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents."}, {"paper_id": "4321415", "adju_relevance": 0, "title": "Learning representations by back-propagating errors", "background_label": "We describe a new learning procedure, back-propagation, for networks of neurone-like units.", "method_label": "The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.", "result_label": "The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.", "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."}, {"paper_id": "15738746", "adju_relevance": 0, "title": "Grounding the Lexical Semantics of Verbs in Visual Perception using Force Dynamics and Event Logic", "background_label": "This paper presents an implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences.", "method_label": "The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. An efficient finite representation is introduced for the infinite sets of intervals that occur when describing liquid and semi-liquid events. Additionally, an efficient procedure using this representation is presented for inferring occurrences of compound events, described with event-logic expressions, from occurrences of primitive events. Using force dynamics and event logic to specify the lexical semantics of events allows the system to be more robust than prior systems based on motion profile.", "abstract": "This paper presents an implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences. The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. An efficient finite representation is introduced for the infinite sets of intervals that occur when describing liquid and semi-liquid events. The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. An efficient finite representation is introduced for the infinite sets of intervals that occur when describing liquid and semi-liquid events. Additionally, an efficient procedure using this representation is presented for inferring occurrences of compound events, described with event-logic expressions, from occurrences of primitive events. The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. An efficient finite representation is introduced for the infinite sets of intervals that occur when describing liquid and semi-liquid events. Additionally, an efficient procedure using this representation is presented for inferring occurrences of compound events, described with event-logic expressions, from occurrences of primitive events. Using force dynamics and event logic to specify the lexical semantics of events allows the system to be more robust than prior systems based on motion profile."}, {"paper_id": "5881871", "adju_relevance": 0, "title": "Intentional Context in Situated Natural Language Learning", "background_label": "Natural language interfaces designed for situationally embedded domains (e.g. cars, videogames) must incorporate knowledge about the users' context to address the many ambiguities of situated language use.", "method_label": "We introduce a model of situated language acquisition that operates in two phases. First, intentional context is represented and inferred from user actions using probabilistic context free grammars. Then, utterances are mapped onto this representation in a noisy channel framework. The acquisition model is trained on unconstrained speech collected from subjects playing an interactive game, and tested on an understanding task.", "abstract": "Natural language interfaces designed for situationally embedded domains (e.g. Natural language interfaces designed for situationally embedded domains (e.g. cars, videogames) must incorporate knowledge about the users' context to address the many ambiguities of situated language use. We introduce a model of situated language acquisition that operates in two phases. We introduce a model of situated language acquisition that operates in two phases. First, intentional context is represented and inferred from user actions using probabilistic context free grammars. We introduce a model of situated language acquisition that operates in two phases. First, intentional context is represented and inferred from user actions using probabilistic context free grammars. Then, utterances are mapped onto this representation in a noisy channel framework. We introduce a model of situated language acquisition that operates in two phases. First, intentional context is represented and inferred from user actions using probabilistic context free grammars. Then, utterances are mapped onto this representation in a noisy channel framework. The acquisition model is trained on unconstrained speech collected from subjects playing an interactive game, and tested on an understanding task."}, {"paper_id": "14134772", "adju_relevance": 0, "title": "Photos monte carlo: A precision tool for qed corrections in z and w decays, Eur", "background_label": "We present a discussion of the precision for the PHOTOS Monte Carlo algorithm, with improved implementation of QED interference and multiple-photon radiation.", "method_label": "The main application of PHOTOS is the generation of QED radiative corrections in decays of any resonances, simulated by a\"host\"Monte Carlo generator.", "result_label": "By careful comparisons automated with the help of the MC-TESTER tool specially tailored for that purpose, we found that the precision of the current version of PHOTOS is of 0.1% in the case of Z and W decays. In the general case, the precision of PHOTOS was also improved, but this will not be quantified here.", "abstract": "We present a discussion of the precision for the PHOTOS Monte Carlo algorithm, with improved implementation of QED interference and multiple-photon radiation. The main application of PHOTOS is the generation of QED radiative corrections in decays of any resonances, simulated by a\"host\"Monte Carlo generator. By careful comparisons automated with the help of the MC-TESTER tool specially tailored for that purpose, we found that the precision of the current version of PHOTOS is of 0.1% in the case of Z and W decays. By careful comparisons automated with the help of the MC-TESTER tool specially tailored for that purpose, we found that the precision of the current version of PHOTOS is of 0.1% in the case of Z and W decays. In the general case, the precision of PHOTOS was also improved, but this will not be quantified here."}, {"paper_id": "198986427", "adju_relevance": 0, "title": "\"Deep reinforcement learning for search, recommendation, and online advertising: a survey\" by Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin with Martin Vesely as coordinator", "background_label": "Search, recommendation, and online advertising are the three most important information-providing mechanisms on the web. These information seeking techniques, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information seeking techniques.", "method_label": "These DRL based techniques have two key advantages - (1) they are able to continuously update information seeking strategies according to users' real-time feedback, and (2) they can maximize the expected cumulative long-term reward from users where reward has different definitions according to information seeking applications such as click-through rate, revenue, user satisfaction and engagement.", "result_label": "In this paper, we give an overview of deep reinforcement learning for search, recommendation, and online advertising from methodologies to applications, review representative algorithms, and discuss some appealing research directions.", "abstract": "Search, recommendation, and online advertising are the three most important information-providing mechanisms on the web. Search, recommendation, and online advertising are the three most important information-providing mechanisms on the web. These information seeking techniques, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Search, recommendation, and online advertising are the three most important information-providing mechanisms on the web. These information seeking techniques, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information seeking techniques. These DRL based techniques have two key advantages - (1) they are able to continuously update information seeking strategies according to users' real-time feedback, and (2) they can maximize the expected cumulative long-term reward from users where reward has different definitions according to information seeking applications such as click-through rate, revenue, user satisfaction and engagement. In this paper, we give an overview of deep reinforcement learning for search, recommendation, and online advertising from methodologies to applications, review representative algorithms, and discuss some appealing research directions."}, {"paper_id": "26682733", "adju_relevance": 0, "title": "The International General Game Playing Competition", "background_label": "Games have played a prominent role as a test-bed for advancements in the field of Artificial Intelligence ever since its foundation over half a century ago, resulting in highly specialized world-class game-playing systems being developed for various games. The establishment of the International General Game Playing Competition in 2005, however, resulted in a renewed interest in more general problem solving approaches to game playing.", "abstract": "Games have played a prominent role as a test-bed for advancements in the field of Artificial Intelligence ever since its foundation over half a century ago, resulting in highly specialized world-class game-playing systems being developed for various games. Games have played a prominent role as a test-bed for advancements in the field of Artificial Intelligence ever since its foundation over half a century ago, resulting in highly specialized world-class game-playing systems being developed for various games. The establishment of the International General Game Playing Competition in 2005, however, resulted in a renewed interest in more general problem solving approaches to game playing."}, {"paper_id": "49391024", "adju_relevance": 0, "title": "Game AI Research with Fast Planet Wars Variants", "background_label": "This paper describes a new implementation of Planet Wars, designed from the outset for Game AI research. The skill-depth of the game makes it a challenge for game-playing agents, and the speed of more than 1 million game ticks per second enables rapid experimentation and prototyping.", "method_label": "The parameterised nature of the game together with an interchangeable actuator model make it well suited to automated game tuning. The game is designed to be fun to play for humans, and is directly playable by General Video Game AI agents.", "abstract": "This paper describes a new implementation of Planet Wars, designed from the outset for Game AI research. This paper describes a new implementation of Planet Wars, designed from the outset for Game AI research. The skill-depth of the game makes it a challenge for game-playing agents, and the speed of more than 1 million game ticks per second enables rapid experimentation and prototyping. The parameterised nature of the game together with an interchangeable actuator model make it well suited to automated game tuning. The parameterised nature of the game together with an interchangeable actuator model make it well suited to automated game tuning. The game is designed to be fun to play for humans, and is directly playable by General Video Game AI agents."}, {"paper_id": "16532611", "adju_relevance": 0, "title": "Sample-based learning and search with permanent and transient memories", "background_label": "We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search.", "method_label": "We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. Using only the transient memory, Dyna-2 performed at least as well as UCT.", "result_label": "Using both memories combined, it significantly outperformed UCT. Our program based on Dyna-2 achieved a higher rating on the Computer Go Online Server than any handcrafted or traditional search based program.", "abstract": "We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search. We apply Dyna-2 to high performance Computer Go. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. Using only the transient memory, Dyna-2 performed at least as well as UCT. Using both memories combined, it significantly outperformed UCT. Using both memories combined, it significantly outperformed UCT. Our program based on Dyna-2 achieved a higher rating on the Computer Go Online Server than any handcrafted or traditional search based program."}, {"paper_id": "15587115", "adju_relevance": 0, "title": "Catching Up Faster by Switching Sooner: A predictive approach to adaptive estimation with an application to the AIC-BIC Dilemma", "background_label": "Prediction and estimation based on Bayesian model selection and model averaging, and derived methods such as the Bayesian information criterion BIC, do not always converge at the fastest possible rate.", "method_label": "We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods, which inspires a modification of the Bayesian predictive distribution, called the switch distribution. When used as an adaptive estimator, the switch distribution does achieve optimal cumulative risk convergence rates in non-parametric density estimation and Gaussian regression problems. We show that the minimax cumulative risk is obtained under very weak conditions and without knowledge of the underlying degree of smoothness. Unlike other adaptive model selection procedures such as the Akaike information criterion AIC and leave-one-out cross-validation, BIC and Bayes factor model selection are typically statistically consistent.", "result_label": "We show that this property is retained by the switch distribution, which thus solves the AIC\u2013BIC dilemma for cumulative risk. The switch distribution has an efficient implementation. We compare its performance with AIC, BIC and Bayesian model selection and averaging on a regression problem with simulated data.", "abstract": " Prediction and estimation based on Bayesian model selection and model averaging, and derived methods such as the Bayesian information criterion BIC, do not always converge at the fastest possible rate. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods, which inspires a modification of the Bayesian predictive distribution, called the switch distribution. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods, which inspires a modification of the Bayesian predictive distribution, called the switch distribution. When used as an adaptive estimator, the switch distribution does achieve optimal cumulative risk convergence rates in non-parametric density estimation and Gaussian regression problems. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods, which inspires a modification of the Bayesian predictive distribution, called the switch distribution. When used as an adaptive estimator, the switch distribution does achieve optimal cumulative risk convergence rates in non-parametric density estimation and Gaussian regression problems. We show that the minimax cumulative risk is obtained under very weak conditions and without knowledge of the underlying degree of smoothness. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods, which inspires a modification of the Bayesian predictive distribution, called the switch distribution. When used as an adaptive estimator, the switch distribution does achieve optimal cumulative risk convergence rates in non-parametric density estimation and Gaussian regression problems. We show that the minimax cumulative risk is obtained under very weak conditions and without knowledge of the underlying degree of smoothness. Unlike other adaptive model selection procedures such as the Akaike information criterion AIC and leave-one-out cross-validation, BIC and Bayes factor model selection are typically statistically consistent. We show that this property is retained by the switch distribution, which thus solves the AIC\u2013BIC dilemma for cumulative risk. We show that this property is retained by the switch distribution, which thus solves the AIC\u2013BIC dilemma for cumulative risk. The switch distribution has an efficient implementation. We show that this property is retained by the switch distribution, which thus solves the AIC\u2013BIC dilemma for cumulative risk. The switch distribution has an efficient implementation. We compare its performance with AIC, BIC and Bayesian model selection and averaging on a regression problem with simulated data."}, {"paper_id": "5249151", "adju_relevance": 0, "title": "Reinforcement Learning for Mapping Instructions to Actions", "background_label": "In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions.", "method_label": "We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials.", "result_label": "Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.", "abstract": "In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples."}, {"paper_id": "59419648", "adju_relevance": 0, "title": "Biasing Monte-Carlo Rollouts with Potential Field in General Video Game Playing", "background_label": "Monte-Carlo Tree Search is a famous technique for General Video Game Playing, thanks to its adaptability. However, since the rollouts are performed randomly, it may not be able to search the game state efficiently.", "method_label": "Existing research has attempted to bias the rollout by using Euclidean distances to the closest sprites as features, and training the bias weights with Evolutionary Strategy.", "result_label": "In this paper, we propose the use of potential field features instead of Euclidean distances as the rollout bias, so as to further improve the performance of Monte-Carlo Tree Search in General Video Game Playing.", "abstract": " Monte-Carlo Tree Search is a famous technique for General Video Game Playing, thanks to its adaptability. Monte-Carlo Tree Search is a famous technique for General Video Game Playing, thanks to its adaptability. However, since the rollouts are performed randomly, it may not be able to search the game state efficiently. Existing research has attempted to bias the rollout by using Euclidean distances to the closest sprites as features, and training the bias weights with Evolutionary Strategy. In this paper, we propose the use of potential field features instead of Euclidean distances as the rollout bias, so as to further improve the performance of Monte-Carlo Tree Search in General Video Game Playing."}, {"paper_id": "5313412", "adju_relevance": 0, "title": "Variations Forever: Flexibly generating rulesets from a sculptable design space of mini-games", "background_label": "Variations Forever is a novel game in which the player explores a vast design space of mini-games.", "abstract": "Variations Forever is a novel game in which the player explores a vast design space of mini-games."}, {"paper_id": "115824604", "adju_relevance": 0, "title": "Monte Carlo and Quasi-Monte Carlo Methods", "background_label": "Chapter 12 discusses Monte Carlo and quasi-Monte Carlo methods and demonstrates how these techniques can be used to compute functionals of multidimensional diffusions. In the second part of the chapter we discuss quasi-Monte Carlo methods. The focus of this part is on scrambled nets, and we show how they can produce faster convergence rates than standard Monte Carlo methods.", "method_label": "Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. 12, we focus on two novel themes which have recently emerged in the context of Monte Carlo methods, namely the exact simulation of general stochastic differential equations and multilevel methods. The chapter concludes by illustrating how to apply quasi-Monte Carlo methods under the benchmark approach introduced in Chap. 1. We recall the Minimal Market Model from Chap. 3 and price financial derivatives on realized variance in this model.", "abstract": "Chapter 12 discusses Monte Carlo and quasi-Monte Carlo methods and demonstrates how these techniques can be used to compute functionals of multidimensional diffusions. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. 12, we focus on two novel themes which have recently emerged in the context of Monte Carlo methods, namely the exact simulation of general stochastic differential equations and multilevel methods. Chapter 12 discusses Monte Carlo and quasi-Monte Carlo methods and demonstrates how these techniques can be used to compute functionals of multidimensional diffusions. In the second part of the chapter we discuss quasi-Monte Carlo methods. Chapter 12 discusses Monte Carlo and quasi-Monte Carlo methods and demonstrates how these techniques can be used to compute functionals of multidimensional diffusions. In the second part of the chapter we discuss quasi-Monte Carlo methods. The focus of this part is on scrambled nets, and we show how they can produce faster convergence rates than standard Monte Carlo methods. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. 12, we focus on two novel themes which have recently emerged in the context of Monte Carlo methods, namely the exact simulation of general stochastic differential equations and multilevel methods. The chapter concludes by illustrating how to apply quasi-Monte Carlo methods under the benchmark approach introduced in Chap. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. 12, we focus on two novel themes which have recently emerged in the context of Monte Carlo methods, namely the exact simulation of general stochastic differential equations and multilevel methods. The chapter concludes by illustrating how to apply quasi-Monte Carlo methods under the benchmark approach introduced in Chap. 1. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. 12, we focus on two novel themes which have recently emerged in the context of Monte Carlo methods, namely the exact simulation of general stochastic differential equations and multilevel methods. The chapter concludes by illustrating how to apply quasi-Monte Carlo methods under the benchmark approach introduced in Chap. 1. We recall the Minimal Market Model from Chap. Monte Carlo methods feature prominently in this book, in particular we discuss how to use Lie Symmetry methods to construct unbiased Monte Carlo estimators in Chap. 6, and we discuss how to construct unbiased Monte Carlo estimators for Wishart processes in Chap. 11. In Chap. 12, we focus on two novel themes which have recently emerged in the context of Monte Carlo methods, namely the exact simulation of general stochastic differential equations and multilevel methods. The chapter concludes by illustrating how to apply quasi-Monte Carlo methods under the benchmark approach introduced in Chap. 1. We recall the Minimal Market Model from Chap. 3 and price financial derivatives on realized variance in this model."}, {"paper_id": "88519926", "adju_relevance": 0, "title": "A Framework for Monte Carlo based Multiple Testing", "background_label": "We are concerned with a situation in which we would like to test multiple hypotheses with tests whose p-values cannot be computed explicitly but can be approximated using Monte Carlo simulation. This scenario occurs widely in practice. We are interested in obtaining the same rejections and non-rejections as the ones obtained if the p-values for all hypotheses had been available.", "abstract": "We are concerned with a situation in which we would like to test multiple hypotheses with tests whose p-values cannot be computed explicitly but can be approximated using Monte Carlo simulation. We are concerned with a situation in which we would like to test multiple hypotheses with tests whose p-values cannot be computed explicitly but can be approximated using Monte Carlo simulation. This scenario occurs widely in practice. We are concerned with a situation in which we would like to test multiple hypotheses with tests whose p-values cannot be computed explicitly but can be approximated using Monte Carlo simulation. This scenario occurs widely in practice. We are interested in obtaining the same rejections and non-rejections as the ones obtained if the p-values for all hypotheses had been available."}, {"paper_id": "298414", "adju_relevance": 0, "title": "Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games", "background_label": "Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for sequential decision-making problems such as Go and video games, but their performance can be poor when the planning depth and sampling trajectories are limited or when the rewards are sparse.", "method_label": "We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD's performance. The new method improves UCT's performance on multiple ATARI games compared to UCT without the reward bonus.", "result_label": "Combining PGRD and Deep Learning in this way should make adapting rewards for MCTS algorithms far more widely and practically applicable than before.", "abstract": "Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for sequential decision-making problems such as Go and video games, but their performance can be poor when the planning depth and sampling trajectories are limited or when the rewards are sparse. We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD's performance. We present an adaptation of PGRD (policy-gradient for reward-design) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD's performance. The new method improves UCT's performance on multiple ATARI games compared to UCT without the reward bonus. Combining PGRD and Deep Learning in this way should make adapting rewards for MCTS algorithms far more widely and practically applicable than before."}, {"paper_id": "63858579", "adju_relevance": 0, "title": "Retrospective Causal Inference with Machine Learning Ensembles: An Application to Anti-Recidivism Policies in Colombia", "background_label": "We present new methods to estimate causal effects retrospectively from micro data with the assistance of a machine learning ensemble.", "method_label": "This approach overcomes two important limitations in conventional methods like regression modeling or matching: (i) ambiguity about the pertinent retrospective counterfactuals and (ii) potential misspecification, overfitting, and otherwise bias-prone or inefficient use of a large identifying covariate set in the estimation of causal effects. Our method targets the analysis toward a well defined ``retrospective intervention effect'' (RIE) based on hypothetical population interventions and applies a machine learning ensemble that allows data to guide us, in a controlled fashion, on how to use a large identifying covariate set.", "result_label": "We illustrate with an analysis of policy options for reducing ex-combatant recidivism in Colombia.", "abstract": "We present new methods to estimate causal effects retrospectively from micro data with the assistance of a machine learning ensemble. This approach overcomes two important limitations in conventional methods like regression modeling or matching: (i) ambiguity about the pertinent retrospective counterfactuals and (ii) potential misspecification, overfitting, and otherwise bias-prone or inefficient use of a large identifying covariate set in the estimation of causal effects. This approach overcomes two important limitations in conventional methods like regression modeling or matching: (i) ambiguity about the pertinent retrospective counterfactuals and (ii) potential misspecification, overfitting, and otherwise bias-prone or inefficient use of a large identifying covariate set in the estimation of causal effects. Our method targets the analysis toward a well defined ``retrospective intervention effect'' (RIE) based on hypothetical population interventions and applies a machine learning ensemble that allows data to guide us, in a controlled fashion, on how to use a large identifying covariate set. We illustrate with an analysis of policy options for reducing ex-combatant recidivism in Colombia."}, {"paper_id": "340852", "adju_relevance": 0, "title": "Learning Dependency-Based Compositional Semantics", "background_label": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.", "abstract": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive."}, {"paper_id": "564377", "adju_relevance": 0, "title": "Lossless abstraction of imperfect information games", "background_label": "Finding an equilibrium of an extensive form game of imperfect information is a fundamental problem in computational game theory, but current techniques do not scale to large games.", "method_label": "To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. We present an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively. Its complexity is \u00f5(n2), where n is the number of nodes in a structure we call the signal tree. It is no larger than the game tree, and on nontrivial games it is drastically smaller, so GameShrink has time and space complexity sublinear in the size of the game tree.", "result_label": "Using GameShrink, we find an equilibrium to a poker game with 3.1 billion nodes\u2014over four orders of magnitude more than in the largest poker game solved previously. To address even larger games, we introduce approximation methods that do not preserve equilibrium, but nevertheless yield (ex post) provably close-to-optimal strategies.", "abstract": "Finding an equilibrium of an extensive form game of imperfect information is a fundamental problem in computational game theory, but current techniques do not scale to large games. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. We present an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. We present an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively. Its complexity is \u00f5(n2), where n is the number of nodes in a structure we call the signal tree. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. We present an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively. Its complexity is \u00f5(n2), where n is the number of nodes in a structure we call the signal tree. It is no larger than the game tree, and on nontrivial games it is drastically smaller, so GameShrink has time and space complexity sublinear in the size of the game tree. Using GameShrink, we find an equilibrium to a poker game with 3.1 billion nodes\u2014over four orders of magnitude more than in the largest poker game solved previously. Using GameShrink, we find an equilibrium to a poker game with 3.1 billion nodes\u2014over four orders of magnitude more than in the largest poker game solved previously. To address even larger games, we introduce approximation methods that do not preserve equilibrium, but nevertheless yield (ex post) provably close-to-optimal strategies."}, {"paper_id": "14674467", "adju_relevance": 0, "title": "Ensemble Determinization in Monte Carlo Tree Search for the Imperfect Information Card Game Magic: The Gathering", "background_label": "In this paper, we examine the use of Monte Carlo tree search (MCTS) for a variant of one of the most popular and profitable games in the world: the card game Magic: The Gathering (M:TG). The game tree for M:TG has a range of distinctive features, which we discuss here; it has incomplete information through the opponent's hidden cards and randomness through card drawing from a shuffled deck.", "method_label": "We investigate a wide range of approaches that use determinization, where all hidden and random information is assumed known to all players, alongside MCTS. We consider a number of variations to the rollout strategy using a range of levels of sophistication and expert knowledge, and decaying reward to encourage play urgency. We examine the effect of utilizing various pruning strategies in order to increase the information gained from each determinization, alongside methods that increase the relevance of random choices. Additionally, we deconstruct the move generation procedure into a binary yes/no decision tree and apply MCTS to this finer grained decision process.", "result_label": "We compare our modifications to a basic MCTS approach for M:TG using fixed decks, and show that significant improvements in playing strength can be obtained.", "abstract": "In this paper, we examine the use of Monte Carlo tree search (MCTS) for a variant of one of the most popular and profitable games in the world: the card game Magic: The Gathering (M:TG). In this paper, we examine the use of Monte Carlo tree search (MCTS) for a variant of one of the most popular and profitable games in the world: the card game Magic: The Gathering (M:TG). The game tree for M:TG has a range of distinctive features, which we discuss here; it has incomplete information through the opponent's hidden cards and randomness through card drawing from a shuffled deck. We investigate a wide range of approaches that use determinization, where all hidden and random information is assumed known to all players, alongside MCTS. We investigate a wide range of approaches that use determinization, where all hidden and random information is assumed known to all players, alongside MCTS. We consider a number of variations to the rollout strategy using a range of levels of sophistication and expert knowledge, and decaying reward to encourage play urgency. We investigate a wide range of approaches that use determinization, where all hidden and random information is assumed known to all players, alongside MCTS. We consider a number of variations to the rollout strategy using a range of levels of sophistication and expert knowledge, and decaying reward to encourage play urgency. We examine the effect of utilizing various pruning strategies in order to increase the information gained from each determinization, alongside methods that increase the relevance of random choices. We investigate a wide range of approaches that use determinization, where all hidden and random information is assumed known to all players, alongside MCTS. We consider a number of variations to the rollout strategy using a range of levels of sophistication and expert knowledge, and decaying reward to encourage play urgency. We examine the effect of utilizing various pruning strategies in order to increase the information gained from each determinization, alongside methods that increase the relevance of random choices. Additionally, we deconstruct the move generation procedure into a binary yes/no decision tree and apply MCTS to this finer grained decision process. We compare our modifications to a basic MCTS approach for M:TG using fixed decks, and show that significant improvements in playing strength can be obtained."}, {"paper_id": "10161834", "adju_relevance": 0, "title": "Using Reinforcement Learning to Model Incrementality in a Fast-Paced Dialogue Game", "background_label": "AbstractWe apply Reinforcement Learning (RL) to the problem of incremental dialogue policy learning in the context of a fast-paced dialogue game.", "method_label": "We compare the policy learned by RL with a high performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game. The RL policy outperforms the baseline policy in offline simulations (based on real user data). We provide a detailed comparison of the RL policy and the baseline policy, including information about how much effort and time it took to develop each one of them.", "result_label": "We also highlight the cases where the RL policy performs better, and show that understanding the RL policy can provide valuable insights which can inform the creation of an even better rule-based policy.", "abstract": "AbstractWe apply Reinforcement Learning (RL) to the problem of incremental dialogue policy learning in the context of a fast-paced dialogue game. We compare the policy learned by RL with a high performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game. We compare the policy learned by RL with a high performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game. The RL policy outperforms the baseline policy in offline simulations (based on real user data). We compare the policy learned by RL with a high performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game. The RL policy outperforms the baseline policy in offline simulations (based on real user data). We provide a detailed comparison of the RL policy and the baseline policy, including information about how much effort and time it took to develop each one of them. We also highlight the cases where the RL policy performs better, and show that understanding the RL policy can provide valuable insights which can inform the creation of an even better rule-based policy."}, {"paper_id": "2899486", "adju_relevance": 0, "title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward", "background_label": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model.", "abstract": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model."}, {"paper_id": "2937525", "adju_relevance": 0, "title": "Individual and Domain Adaptation in Sentence Planning for Dialogue", "background_label": "One of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module. This challenge arises from the need for the generator to adapt to many features of the dialogue domain, user population, and dialogue context.", "method_label": "A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group. In this paper we present and evaluate a trainable sentence planner for providing restaurant information in the MATCH dialogue system. We show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a template-based generator tuned to this domain. We also show that our method easily supports adapting the sentence planner to individuals, and that the individualized sentence planners generally perform better than models trained and tested on a population of individuals.", "result_label": "Previous work has documented and utilized individual preferences for content selection, but to our knowledge, these results provide the first demonstration of individual preferences for sentence planning operations, affecting the content order, discourse structure and sentence structure of system responses. Finally, we evaluate the contribution of different feature sets, and show that, in our application, n-gram features often do as well as features based on higher-level linguistic representations.", "abstract": "One of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module. One of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module. This challenge arises from the need for the generator to adapt to many features of the dialogue domain, user population, and dialogue context. A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group. A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group. In this paper we present and evaluate a trainable sentence planner for providing restaurant information in the MATCH dialogue system. A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group. In this paper we present and evaluate a trainable sentence planner for providing restaurant information in the MATCH dialogue system. We show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a template-based generator tuned to this domain. A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group. In this paper we present and evaluate a trainable sentence planner for providing restaurant information in the MATCH dialogue system. We show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a template-based generator tuned to this domain. We also show that our method easily supports adapting the sentence planner to individuals, and that the individualized sentence planners generally perform better than models trained and tested on a population of individuals. Previous work has documented and utilized individual preferences for content selection, but to our knowledge, these results provide the first demonstration of individual preferences for sentence planning operations, affecting the content order, discourse structure and sentence structure of system responses. Previous work has documented and utilized individual preferences for content selection, but to our knowledge, these results provide the first demonstration of individual preferences for sentence planning operations, affecting the content order, discourse structure and sentence structure of system responses. Finally, we evaluate the contribution of different feature sets, and show that, in our application, n-gram features often do as well as features based on higher-level linguistic representations."}, {"paper_id": "8618081", "adju_relevance": 0, "title": "Gameplay Analysis through State Projection", "background_label": "Analysis of gameplay data is crucial for evaluating design decisions and refining a game experience. However, identifying player strategies and finding areas of confusion is difficult because a designer may not know what queries to ask or what patterns to look for in the data.", "method_label": "To make this task easier, we present Playtracer, a method for visually analyzing play traces that is independent of a specific game's structure. Playtracer applies multidimensional scaling to cluster players and game states, providing a detailed visual representation of the paths the players take through a game. We evaluate our method by analyzing an educational puzzle game and highlighting common hypotheses, pitfalls, confusing elements, and anomalies.", "result_label": "Our results suggest that Playtracer can be an effective tool for game analysis and improvement.", "abstract": "Analysis of gameplay data is crucial for evaluating design decisions and refining a game experience. Analysis of gameplay data is crucial for evaluating design decisions and refining a game experience. However, identifying player strategies and finding areas of confusion is difficult because a designer may not know what queries to ask or what patterns to look for in the data. To make this task easier, we present Playtracer, a method for visually analyzing play traces that is independent of a specific game's structure. To make this task easier, we present Playtracer, a method for visually analyzing play traces that is independent of a specific game's structure. Playtracer applies multidimensional scaling to cluster players and game states, providing a detailed visual representation of the paths the players take through a game. To make this task easier, we present Playtracer, a method for visually analyzing play traces that is independent of a specific game's structure. Playtracer applies multidimensional scaling to cluster players and game states, providing a detailed visual representation of the paths the players take through a game. We evaluate our method by analyzing an educational puzzle game and highlighting common hypotheses, pitfalls, confusing elements, and anomalies. Our results suggest that Playtracer can be an effective tool for game analysis and improvement."}, {"paper_id": "28808621", "adju_relevance": 0, "title": "StarCraft II: A New Challenge for Reinforcement Learning", "background_label": "This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players.", "method_label": "We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player.", "result_label": "In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.", "abstract": "This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. However, when trained on the main game, these agents are unable to make significant progress. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures."}, {"paper_id": "173188489", "adju_relevance": 0, "title": "Multimodal Joint Emotion and Game Context Recognition in League of Legends Livestreams", "background_label": "Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer's emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled ('in-the-wild') conditions.", "method_label": "Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Secondly, we propose a method that exploits tensor decompositions for high-order fusion of multimodal representations.", "result_label": "The proposed method is evaluated on the problem of jointly predicting game context and player affect, compared with a set of baseline fusion approaches such as late and early fusion.", "abstract": "Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer's emotional state and behaviour via webcam footage and audio. Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer's emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled ('in-the-wild') conditions. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Secondly, we propose a method that exploits tensor decompositions for high-order fusion of multimodal representations. The proposed method is evaluated on the problem of jointly predicting game context and player affect, compared with a set of baseline fusion approaches such as late and early fusion."}, {"paper_id": "118377368", "adju_relevance": 0, "title": "Monte Carlo Method", "background_label": "The Monte Carlo technique makes use of so-called random numbers which can be defined as follows. Consider a random variable \u03c1, which is equally distributed over the interval (0, 1).", "method_label": "Its distribution function is:    $$R(r) = \\left\\{ {\\begin{array}{*{20}{c}} {0 for \\leqq 0} \\\\ {r for 0 1} \\\\ \\end{array} } \\right.$$    and the distribution is called rectangular owing to the shape of its frequency function. A sample of s observed values of \u03c1 is assumed to be taken in some way or another, for example by means of a lottery, and so a sequence of numbers (r1 r2, \u2026 , r s ) is obtained. These numbers are called random numbers.", "result_label": "They are like the list of winning numbers in a lottery: each number in the interval (rounded to a certain number of decimals) has exactly the same probability of appearing in the sequence.", "abstract": "The Monte Carlo technique makes use of so-called random numbers which can be defined as follows. The Monte Carlo technique makes use of so-called random numbers which can be defined as follows. Consider a random variable \u03c1, which is equally distributed over the interval (0, 1). Its distribution function is:    $$R(r) = \\left\\{ {\\begin{array}{*{20}{c}} {0 for \\leqq 0} \\\\ {r for 0 1} \\\\ \\end{array} } \\right.$$    and the distribution is called rectangular owing to the shape of its frequency function. Its distribution function is:    $$R(r) = \\left\\{ {\\begin{array}{*{20}{c}} {0 for \\leqq 0} \\\\ {r for 0 1} \\\\ \\end{array} } \\right.$$    and the distribution is called rectangular owing to the shape of its frequency function. A sample of s observed values of \u03c1 is assumed to be taken in some way or another, for example by means of a lottery, and so a sequence of numbers (r1 r2, \u2026 , r s ) is obtained. Its distribution function is:    $$R(r) = \\left\\{ {\\begin{array}{*{20}{c}} {0 for \\leqq 0} \\\\ {r for 0 1} \\\\ \\end{array} } \\right.$$    and the distribution is called rectangular owing to the shape of its frequency function. A sample of s observed values of \u03c1 is assumed to be taken in some way or another, for example by means of a lottery, and so a sequence of numbers (r1 r2, \u2026 , r s ) is obtained. These numbers are called random numbers. They are like the list of winning numbers in a lottery: each number in the interval (rounded to a certain number of decimals) has exactly the same probability of appearing in the sequence."}, {"paper_id": "122137669", "adju_relevance": 0, "title": "Hybrid Monte Carlo", "background_label": "Abstract I discuss the Hybrid Monte Carlo algorithm for performing lattice gauge theory calculations.", "method_label": "This is a large step method which has none of the discrete step size errors usually associated with the Molecular Dynamics, Langevin, or Hybrid algorithms. The method allows the inclusion of dynamical fermion fields in a straightforward way.", "abstract": "Abstract I discuss the Hybrid Monte Carlo algorithm for performing lattice gauge theory calculations. This is a large step method which has none of the discrete step size errors usually associated with the Molecular Dynamics, Langevin, or Hybrid algorithms. This is a large step method which has none of the discrete step size errors usually associated with the Molecular Dynamics, Langevin, or Hybrid algorithms. The method allows the inclusion of dynamical fermion fields in a straightforward way."}, {"paper_id": "62721879", "adju_relevance": 0, "title": "Chatting Pattern Based Game BOT Detection: Do They Talk Like Us?", "background_label": "Among the various security threats in online games, the use of game bots is the most serious problem. Previous studies on game bot detection have proposed many methods to find out discriminable behaviors of bots from humans based on the fact that a bot\"s playing pattern is different from that of a human.", "abstract": "Among the various security threats in online games, the use of game bots is the most serious problem. Among the various security threats in online games, the use of game bots is the most serious problem. Previous studies on game bot detection have proposed many methods to find out discriminable behaviors of bots from humans based on the fact that a bot\"s playing pattern is different from that of a human."}, {"paper_id": "14096841", "adju_relevance": 0, "title": "An Actor-Critic Algorithm for Sequence Prediction", "background_label": "Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens.", "method_label": "We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation.", "result_label": "Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "abstract": " Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling."}, {"paper_id": "13962424", "adju_relevance": 0, "title": "Modeling Individual Differences through Frequent Pattern Mining on Role-Playing Game Actions", "background_label": "There has been much work on player modeling using game behavioral data collected. Many of the previous research projects that targeted this goal used aggregate game statistics as features to develop behavior models using both statistical and machine learning techniques. While existing methods have already led to interesting findings, we suspect that aggregated features discard valuable information such as temporal or sequential patterns, which are important in deciphering information about decision-making, problem solving, or individual differences.", "method_label": "Such sequential information is critical to analyze player behaviors especially in role-playing games (RPG) where players face ample choices, experience different contexts, behave freely with individual propensities but possibly end up with similar aggregated statistics (e.g., levels, time spent). Using an RPG with multiple affordances, we designed an experiment collecting granular in-game behaviors of 64 players. Using closed sequential pattern mining and logistic regression, we developed a model that uses gameplay action sequences to predict the real world characteristics, including gender, game play expertise and five personality traits (as defined by psychology).", "result_label": "The results show that game expertise is a dominant factor that impacts in-game behaviors. The contributions of this paper are both the algorithms we developed combined with a validation procedure to determine the reliability and validity of the results, and the results themselves.", "abstract": "There has been much work on player modeling using game behavioral data collected. There has been much work on player modeling using game behavioral data collected. Many of the previous research projects that targeted this goal used aggregate game statistics as features to develop behavior models using both statistical and machine learning techniques. There has been much work on player modeling using game behavioral data collected. Many of the previous research projects that targeted this goal used aggregate game statistics as features to develop behavior models using both statistical and machine learning techniques. While existing methods have already led to interesting findings, we suspect that aggregated features discard valuable information such as temporal or sequential patterns, which are important in deciphering information about decision-making, problem solving, or individual differences. Such sequential information is critical to analyze player behaviors especially in role-playing games (RPG) where players face ample choices, experience different contexts, behave freely with individual propensities but possibly end up with similar aggregated statistics (e.g., levels, time spent). Such sequential information is critical to analyze player behaviors especially in role-playing games (RPG) where players face ample choices, experience different contexts, behave freely with individual propensities but possibly end up with similar aggregated statistics (e.g., levels, time spent). Using an RPG with multiple affordances, we designed an experiment collecting granular in-game behaviors of 64 players. Such sequential information is critical to analyze player behaviors especially in role-playing games (RPG) where players face ample choices, experience different contexts, behave freely with individual propensities but possibly end up with similar aggregated statistics (e.g., levels, time spent). Using an RPG with multiple affordances, we designed an experiment collecting granular in-game behaviors of 64 players. Using closed sequential pattern mining and logistic regression, we developed a model that uses gameplay action sequences to predict the real world characteristics, including gender, game play expertise and five personality traits (as defined by psychology). The results show that game expertise is a dominant factor that impacts in-game behaviors. The results show that game expertise is a dominant factor that impacts in-game behaviors. The contributions of this paper are both the algorithms we developed combined with a validation procedure to determine the reliability and validity of the results, and the results themselves."}, {"paper_id": "14716222", "adju_relevance": 0, "title": "The MOBO City: A Mobile Game Package for Technical Language Learning", "background_label": "Abstract-In this research we produced a mobile language learning game that is designed within a technical context. After conceptual analysis of the subject matter i.e. The action within the game is consistent to the theme. There is a story, simplifying and exaggerating real life.", "method_label": "computer's motherboard, the game was designed. Elements of control, feedback and sense of danger are incorporated into our game. By producing an engaging learning experience, vocabularies were learned incidentally.", "result_label": "Deliberate vocabulary learning games were also added to our package to help students solve their common errors.", "abstract": "Abstract-In this research we produced a mobile language learning game that is designed within a technical context. Abstract-In this research we produced a mobile language learning game that is designed within a technical context. After conceptual analysis of the subject matter i.e. computer's motherboard, the game was designed. Abstract-In this research we produced a mobile language learning game that is designed within a technical context. After conceptual analysis of the subject matter i.e. The action within the game is consistent to the theme. Abstract-In this research we produced a mobile language learning game that is designed within a technical context. After conceptual analysis of the subject matter i.e. The action within the game is consistent to the theme. There is a story, simplifying and exaggerating real life. computer's motherboard, the game was designed. Elements of control, feedback and sense of danger are incorporated into our game. computer's motherboard, the game was designed. Elements of control, feedback and sense of danger are incorporated into our game. By producing an engaging learning experience, vocabularies were learned incidentally. Deliberate vocabulary learning games were also added to our package to help students solve their common errors."}, {"paper_id": "121101759", "adju_relevance": 0, "title": "Hybrid Monte Carlo", "method_label": "Abstract We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom.", "background_label": "There are no discretization errors even for large step sizes.", "result_label": "Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.", "abstract": "Abstract We present a new method for the numerical simulation of lattice field theory. Abstract We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. Abstract We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons."}, {"paper_id": "13121800", "adju_relevance": 0, "title": "Learning the semantics of words and pictures", "background_label": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features.", "method_label": "The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition.", "abstract": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."}, {"paper_id": "101871", "adju_relevance": 0, "title": "Interactive verification of game design and playing strategies", "background_label": "Reinforcement learning is considered as one of the most suitable and prominent methods for solving game problems due to its capability to discover good strategies by extended se self-training and limited initial knowledge.", "abstract": "Reinforcement learning is considered as one of the most suitable and prominent methods for solving game problems due to its capability to discover good strategies by extended se self-training and limited initial knowledge."}, {"paper_id": "28550920", "adju_relevance": 0, "title": "Self-Learning Monte Carlo Method", "background_label": "Monte Carlo simulation is an unbiased numerical tool for studying classical and quantum many-body systems. One of its bottlenecks is the lack of general and efficient update algorithm for large size systems close to phase transition or with strong frustrations, for which local updates perform badly.", "method_label": "In this work, we propose a new general-purpose Monte Carlo method, dubbed self-learning Monte Carlo (SLMC), in which an efficient update algorithm is first learned from the training data generated in trial simulations and then used to speed up the actual simulation.", "result_label": "We demonstrate the efficiency of SLMC in a spin model at the phase transition point, achieving a 10-20 times speedup.", "abstract": "Monte Carlo simulation is an unbiased numerical tool for studying classical and quantum many-body systems. Monte Carlo simulation is an unbiased numerical tool for studying classical and quantum many-body systems. One of its bottlenecks is the lack of general and efficient update algorithm for large size systems close to phase transition or with strong frustrations, for which local updates perform badly. In this work, we propose a new general-purpose Monte Carlo method, dubbed self-learning Monte Carlo (SLMC), in which an efficient update algorithm is first learned from the training data generated in trial simulations and then used to speed up the actual simulation. We demonstrate the efficiency of SLMC in a spin model at the phase transition point, achieving a 10-20 times speedup."}, {"paper_id": "36953514", "adju_relevance": 0, "title": "Applied Optimal Control: Optimization, Estimation, and Control", "background_label": "This best-selling text focuses on the analysis and design of complicated dynamics systems. CHOICE called it \"a high-level, concise book that could well be used as a reference by engineers, applied mathematicians, and undergraduates.", "abstract": "This best-selling text focuses on the analysis and design of complicated dynamics systems. This best-selling text focuses on the analysis and design of complicated dynamics systems. CHOICE called it \"a high-level, concise book that could well be used as a reference by engineers, applied mathematicians, and undergraduates."}, {"paper_id": "201668540", "adju_relevance": 0, "title": "Interactive Language Learning by Question Answering", "background_label": "Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching.", "abstract": "Humans observe and interact with the world to acquire knowledge. Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching."}, {"paper_id": "15054631", "adju_relevance": 0, "title": "Determinization and information set Monte Carlo Tree Search for the card game Dou Di Zhu", "background_label": "Determinization is a technique for making decisions in games with stochasticity and/or imperfect information by sampling instances of the equivalent deterministic game of perfect information. Monte-Carlo Tree Search (MCTS) is an AI technique that has recently proved successful in the domain of deterministic games of perfect information.", "abstract": "Determinization is a technique for making decisions in games with stochasticity and/or imperfect information by sampling instances of the equivalent deterministic game of perfect information. Determinization is a technique for making decisions in games with stochasticity and/or imperfect information by sampling instances of the equivalent deterministic game of perfect information. Monte-Carlo Tree Search (MCTS) is an AI technique that has recently proved successful in the domain of deterministic games of perfect information."}, {"paper_id": "5667590", "adju_relevance": 0, "title": "Driving Semantic Parsing from the World's Response", "background_label": "AbstractCurrent approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers.", "abstract": "AbstractCurrent approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. AbstractCurrent approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "7775630", "adju_relevance": 0, "title": "An analysis of UCT in multi-player games", "background_label": "The UCT algorithm has been exceedingly popular for Go, a two-player game, significantly increasing the playing strength of Go programs in a very short time.", "method_label": "This paper provides an analysis of the UCT algorithm in multi-player games, showing that UCT, when run in a multi-player game, is computing a mixed-strategy equilibrium, as opposed to max n , which computes a pure-strategy equilibrium.", "result_label": "We analyze the performance of UCT in several known domains and show that it performs as well or better than existing algorithms.", "abstract": " The UCT algorithm has been exceedingly popular for Go, a two-player game, significantly increasing the playing strength of Go programs in a very short time. This paper provides an analysis of the UCT algorithm in multi-player games, showing that UCT, when run in a multi-player game, is computing a mixed-strategy equilibrium, as opposed to max n , which computes a pure-strategy equilibrium. We analyze the performance of UCT in several known domains and show that it performs as well or better than existing algorithms."}, {"paper_id": "64823044", "adju_relevance": 0, "title": "Self-learning Monte Carlo method and cumulative update in fermion systems", "method_label": "We develop the self-learning Monte Carlo (SLMC) method, a general-purpose numerical method recently introduced to simulate many-body systems, for studying interacting fermion systems. Our method uses a highly-efficient update algorithm, which we design and dub \"cumulative update\", to generate new candidate configurations in the Markov chain based on a self-learned bosonic effective model.", "result_label": "From general analysis and numerical study of the double exchange model as an example, we find the SLMC with cumulative update drastically reduces the computational cost of the simulation, while remaining statistically exact. Remarkably, its computational complexity is far less than the conventional algorithm with local updates.", "abstract": "We develop the self-learning Monte Carlo (SLMC) method, a general-purpose numerical method recently introduced to simulate many-body systems, for studying interacting fermion systems. We develop the self-learning Monte Carlo (SLMC) method, a general-purpose numerical method recently introduced to simulate many-body systems, for studying interacting fermion systems. Our method uses a highly-efficient update algorithm, which we design and dub \"cumulative update\", to generate new candidate configurations in the Markov chain based on a self-learned bosonic effective model. From general analysis and numerical study of the double exchange model as an example, we find the SLMC with cumulative update drastically reduces the computational cost of the simulation, while remaining statistically exact. From general analysis and numerical study of the double exchange model as an example, we find the SLMC with cumulative update drastically reduces the computational cost of the simulation, while remaining statistically exact. Remarkably, its computational complexity is far less than the conventional algorithm with local updates."}, {"paper_id": "1008215", "adju_relevance": 0, "title": "How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing", "method_label": "We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses.", "result_label": "We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario.", "abstract": " We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario."}, {"paper_id": "14811627", "adju_relevance": 0, "title": "Neural Networks for State Evaluation in General Game Playing", "background_label": "Unlike traditional game playing, General Game Playing is concerned with agents capable of playing classes of games. Given the rules of an unknown game, the agent is supposed to play well without human intervention. For this purpose, agent systems that use deterministic game tree search need to automatically construct a state value function to guide search.", "method_label": "Successful systems of this type use evaluation functions derived solely from the game rules, thus neglecting further improvements by experience. In addition, these functions are fixed in their form and do not necessarily capture the game's real state value function. In this work we present an approach for obtaining evaluation functions on the basis of neural networks that overcomes the aforementioned problems. A network initialization extracted from the game rules ensures reasonable behavior without the need for prior training.", "result_label": "Later training, however, can lead to significant improvements in evaluation quality, as our results indicate.", "abstract": " Unlike traditional game playing, General Game Playing is concerned with agents capable of playing classes of games. Unlike traditional game playing, General Game Playing is concerned with agents capable of playing classes of games. Given the rules of an unknown game, the agent is supposed to play well without human intervention. Unlike traditional game playing, General Game Playing is concerned with agents capable of playing classes of games. Given the rules of an unknown game, the agent is supposed to play well without human intervention. For this purpose, agent systems that use deterministic game tree search need to automatically construct a state value function to guide search. Successful systems of this type use evaluation functions derived solely from the game rules, thus neglecting further improvements by experience. Successful systems of this type use evaluation functions derived solely from the game rules, thus neglecting further improvements by experience. In addition, these functions are fixed in their form and do not necessarily capture the game's real state value function. Successful systems of this type use evaluation functions derived solely from the game rules, thus neglecting further improvements by experience. In addition, these functions are fixed in their form and do not necessarily capture the game's real state value function. In this work we present an approach for obtaining evaluation functions on the basis of neural networks that overcomes the aforementioned problems. Successful systems of this type use evaluation functions derived solely from the game rules, thus neglecting further improvements by experience. In addition, these functions are fixed in their form and do not necessarily capture the game's real state value function. In this work we present an approach for obtaining evaluation functions on the basis of neural networks that overcomes the aforementioned problems. A network initialization extracted from the game rules ensures reasonable behavior without the need for prior training. Later training, however, can lead to significant improvements in evaluation quality, as our results indicate."}, {"paper_id": "56494486", "adju_relevance": 0, "title": "The Sense of Ensemble: a Machine Learning Approach to Expressive Performance Modelling in String Quartets", "background_label": "Computational approaches for modelling expressive music performance have produced systems that emulate music expression, but few steps have been taken in the domain of ensemble performance.", "method_label": "In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In order to address the problem of inter-dependence among musicians we propose the introduction of inter-voice contextual attributes. We evaluate the method on data extracted from multi-modal recordings of string quartet performances in two different conditions: solo and ensemble. We used machine-learning algorithms to produce computational models for predicting intensity, timing deviations, vibrato extent, and bowing speed of each note.", "result_label": "As a result, the introduced inter-voice contextual attributes generally improved the prediction of the expressive parameters. Furthermore, results on attribute selection show ...", "abstract": "Computational approaches for modelling expressive music performance have produced systems that emulate music expression, but few steps have been taken in the domain of ensemble performance. In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In order to address the problem of inter-dependence among musicians we propose the introduction of inter-voice contextual attributes. In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In order to address the problem of inter-dependence among musicians we propose the introduction of inter-voice contextual attributes. We evaluate the method on data extracted from multi-modal recordings of string quartet performances in two different conditions: solo and ensemble. In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In order to address the problem of inter-dependence among musicians we propose the introduction of inter-voice contextual attributes. We evaluate the method on data extracted from multi-modal recordings of string quartet performances in two different conditions: solo and ensemble. We used machine-learning algorithms to produce computational models for predicting intensity, timing deviations, vibrato extent, and bowing speed of each note. As a result, the introduced inter-voice contextual attributes generally improved the prediction of the expressive parameters. As a result, the introduced inter-voice contextual attributes generally improved the prediction of the expressive parameters. Furthermore, results on attribute selection show ..."}, {"paper_id": "6975204", "adju_relevance": 0, "title": "Generic Heuristic Approach to General Game Playing", "background_label": "General Game Playing (GGP) is a specially designed environment for creating and testing competitive agents which can play variety of games.", "abstract": " General Game Playing (GGP) is a specially designed environment for creating and testing competitive agents which can play variety of games."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "52922277", "adju_relevance": 0, "title": "Zooming Network", "background_label": "Structural information is important in natural language understanding. Although some current neural net-based models have a limited ability to take local syntactic information, they fail to use high-level and large-scale structures of documents. This information is valuable for text understanding since it contains the author's strategy to express information, in building an effective representation and forming appropriate output modes.", "abstract": "Structural information is important in natural language understanding. Structural information is important in natural language understanding. Although some current neural net-based models have a limited ability to take local syntactic information, they fail to use high-level and large-scale structures of documents. Structural information is important in natural language understanding. Although some current neural net-based models have a limited ability to take local syntactic information, they fail to use high-level and large-scale structures of documents. This information is valuable for text understanding since it contains the author's strategy to express information, in building an effective representation and forming appropriate output modes."}, {"paper_id": "52222250", "adju_relevance": 0, "title": "General Win Prediction from Agent Experience", "background_label": "The question of whether the correct algorithm is used for the problem at hand usually comes at the end of execution, when the algorithm\u2019s ability to solve the problem (or not) can be verified. But what if this question could be answered in advance, with enough notice to make changes in the approach in order for it to be more successful?", "abstract": "The question of whether the correct algorithm is used for the problem at hand usually comes at the end of execution, when the algorithm\u2019s ability to solve the problem (or not) can be verified. The question of whether the correct algorithm is used for the problem at hand usually comes at the end of execution, when the algorithm\u2019s ability to solve the problem (or not) can be verified. But what if this question could be answered in advance, with enough notice to make changes in the approach in order for it to be more successful?"}, {"paper_id": "32274875", "adju_relevance": 0, "title": "Using reinforcement learning to learn how to play text-based games", "background_label": "The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. Text-based games with multiple endings and rewards are a promising platform for this task, since their feedback allows us to employ reinforcement learning techniques to jointly learn text representations and control policies.", "method_label": "We present a general text game playing agent, testing its generalisation and transfer learning performance and showing its ability to play multiple games at once.", "result_label": "We also present pyfiction, an open-source library for universal access to different text games that could, together with our agent that implements its interface, serve as a baseline for future research.", "abstract": "The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. Text-based games with multiple endings and rewards are a promising platform for this task, since their feedback allows us to employ reinforcement learning techniques to jointly learn text representations and control policies. We present a general text game playing agent, testing its generalisation and transfer learning performance and showing its ability to play multiple games at once. We also present pyfiction, an open-source library for universal access to different text games that could, together with our agent that implements its interface, serve as a baseline for future research."}, {"paper_id": "16933934", "adju_relevance": 0, "title": "General Game Playing: An Overview and Open Problems", "background_label": "General Game Playing has emerged, in recent years, as a challenging testbed for Artificial Intelligence research.", "abstract": "General Game Playing has emerged, in recent years, as a challenging testbed for Artificial Intelligence research."}, {"paper_id": "13976617", "adju_relevance": 0, "title": "Valuation of a CDO and an n th to Default CDS Without Monte Carlo Simulation", "background_label": "In this paper we develop two fast procedures for valuing tranches of collateralized debt obligations and n th to default swaps.", "method_label": "The procedures are based on a factor copula model of times to default and are alternatives to using fast Fourier transforms. One involves calculating the probability distribution of the number of defaults by a certain time using a recurrence relationship; the other involves using a \u201cprobability bucketing\u201d numerical procedure to build up the loss distribution. We show how many different copula models can be generated by using different distributional assumptions within the factor model. We examine the impact on valuations of default probabilities, default correlations, the copula model chosen, and a correlation of recovery rates with default probabilities.", "result_label": "Finally we look at the market pricing of index tranches and conclude that a \u201cdouble tdistribution\u201d copula fits the prices reasonably well.", "abstract": "In this paper we develop two fast procedures for valuing tranches of collateralized debt obligations and n th to default swaps. The procedures are based on a factor copula model of times to default and are alternatives to using fast Fourier transforms. The procedures are based on a factor copula model of times to default and are alternatives to using fast Fourier transforms. One involves calculating the probability distribution of the number of defaults by a certain time using a recurrence relationship; the other involves using a \u201cprobability bucketing\u201d numerical procedure to build up the loss distribution. The procedures are based on a factor copula model of times to default and are alternatives to using fast Fourier transforms. One involves calculating the probability distribution of the number of defaults by a certain time using a recurrence relationship; the other involves using a \u201cprobability bucketing\u201d numerical procedure to build up the loss distribution. We show how many different copula models can be generated by using different distributional assumptions within the factor model. The procedures are based on a factor copula model of times to default and are alternatives to using fast Fourier transforms. One involves calculating the probability distribution of the number of defaults by a certain time using a recurrence relationship; the other involves using a \u201cprobability bucketing\u201d numerical procedure to build up the loss distribution. We show how many different copula models can be generated by using different distributional assumptions within the factor model. We examine the impact on valuations of default probabilities, default correlations, the copula model chosen, and a correlation of recovery rates with default probabilities. Finally we look at the market pricing of index tranches and conclude that a \u201cdouble tdistribution\u201d copula fits the prices reasonably well."}]