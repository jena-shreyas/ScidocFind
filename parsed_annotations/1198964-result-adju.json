[{"paper_id": "1198964", "title": "Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference", "background_label": "The run time complexity of state-of-the-art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance.", "method_label": "We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time.", "result_label": "In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n^2) directed MST inference.", "abstract": "The run time complexity of state-of-the-art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). The run time complexity of state-of-the-art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. The run time complexity of state-of-the-art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n^2) directed MST inference."}, {"paper_id": "17254305", "adju_relevance": 2, "title": "Efficient Third-Order Dependency Parsers", "background_label": "AbstractWe present algorithms for higher-order dependency parsing that are \"third-order\" in the sense that they can evaluate substructures containing three dependencies, and \"efficient\" in the sense that they require only O(n 4 ) time.", "method_label": "Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.", "result_label": "We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.", "abstract": "AbstractWe present algorithms for higher-order dependency parsing that are \"third-order\" in the sense that they can evaluate substructures containing three dependencies, and \"efficient\" in the sense that they require only O(n 4 ) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively."}, {"paper_id": "8000929", "adju_relevance": 2, "title": "Experiments with a Higher-Order Projective Dependency Parser", "background_label": "AbstractWe present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children.", "method_label": "We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron.", "result_label": "Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007) , our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.", "abstract": "AbstractWe present experiments with a dependency parsing model defined on rich factors. AbstractWe present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007) , our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech."}, {"paper_id": "59669834", "adju_relevance": 2, "title": "Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing", "background_label": "In this thesis we develop a discriminative learning method for dependency parsing using online large-margin training combined with spanning tree inference algorithms. Unfortunately, the non-projective problem then becomes NP-hard so we provide structurally motivated approximate algorithms.", "method_label": "We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. Having defined a set of parsing algorithms, we will also define a rich feature set and train various parsers using the online large-margin learning framework. We then compare our trained dependency parsers to other state-of-the-art parsers on 14 diverse languages: Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, English, German, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. Having built an efficient and accurate discriminative dependency parser, this thesis will then turn to improving and applying the parser. First we will show how additional resources can provide useful features to increase parsing accuracy and to adapt parsers to new domains. We will also argue that the robustness of discriminative inference-based learning algorithms lend themselves well to dependency parsing when feature representations or structural constraints do not allow for tractable parsing algorithms.", "result_label": "Finally, we integrate our parsing models into a state-of-the-art sentence compression system to show its applicability to a real world problem.", "abstract": "In this thesis we develop a discriminative learning method for dependency parsing using online large-margin training combined with spanning tree inference algorithms. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. In this thesis we develop a discriminative learning method for dependency parsing using online large-margin training combined with spanning tree inference algorithms. Unfortunately, the non-projective problem then becomes NP-hard so we provide structurally motivated approximate algorithms. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. Having defined a set of parsing algorithms, we will also define a rich feature set and train various parsers using the online large-margin learning framework. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. Having defined a set of parsing algorithms, we will also define a rich feature set and train various parsers using the online large-margin learning framework. We then compare our trained dependency parsers to other state-of-the-art parsers on 14 diverse languages: Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, English, German, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. Having defined a set of parsing algorithms, we will also define a rich feature set and train various parsers using the online large-margin learning framework. We then compare our trained dependency parsers to other state-of-the-art parsers on 14 diverse languages: Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, English, German, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. Having built an efficient and accurate discriminative dependency parser, this thesis will then turn to improving and applying the parser. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. Having defined a set of parsing algorithms, we will also define a rich feature set and train various parsers using the online large-margin learning framework. We then compare our trained dependency parsers to other state-of-the-art parsers on 14 diverse languages: Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, English, German, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. Having built an efficient and accurate discriminative dependency parser, this thesis will then turn to improving and applying the parser. First we will show how additional resources can provide useful features to increase parsing accuracy and to adapt parsers to new domains. We will show that this method provides state-of-the-art accuracy, is extensible through the feature set and can be implemented efficiently. Furthermore, we display the language independent nature of the method by evaluating it on over a dozen diverse languages as well as show its practical applicability through integration into a sentence compression system. We start by presenting an online large-margin learning framework that is a generalization of the work of Crammer and Singer [34, 37] to structured outputs, such as sequences and parse trees. This will lead to the heart of this thesis---discriminative dependency parsing. Here we will formulate dependency parsing in a spanning tree framework, yielding efficient parsing algorithms for both projective and non-projective tree structures. We will then extend the parsing algorithm to incorporate features over larger substructures without an increase in computational complexity for the projective case. Having defined a set of parsing algorithms, we will also define a rich feature set and train various parsers using the online large-margin learning framework. We then compare our trained dependency parsers to other state-of-the-art parsers on 14 diverse languages: Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, English, German, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. Having built an efficient and accurate discriminative dependency parser, this thesis will then turn to improving and applying the parser. First we will show how additional resources can provide useful features to increase parsing accuracy and to adapt parsers to new domains. We will also argue that the robustness of discriminative inference-based learning algorithms lend themselves well to dependency parsing when feature representations or structural constraints do not allow for tractable parsing algorithms. Finally, we integrate our parsing models into a state-of-the-art sentence compression system to show its applicability to a real world problem."}, {"paper_id": "14574911", "adju_relevance": 2, "title": "On Dependency Analysis via Contractions and Weighted FSTs", "background_label": "Abstract Arc contractions in syntactic dependency graphs can be used to decide which graphs are trees. The paper observes that these contractions can be expressed with weighted finite-state transducers (weighted FST) that operate on stringencoded trees.", "method_label": "The observation gives rise to a finite-state parsing algorithm that computes the parse forest and extracts the best parses from it. The algorithm is customizable to functional and bilexical dependency parsing, and it can be extended to non-projective parsing via a multi-planar encoding with prior results on high recall.", "result_label": "Our experiments support an analysis of projective parsing according to which the worst-case time complexity of the algorithm is quadratic to the sentence length, and linear to the overlapping arcs and the number of functional categories of the arcs. The results suggest several interesting directions towards efficient and highprecision dependency parsing that takes advantage of the flexibility and the demonstrated ambiguity-packing capacity of such a parser.", "abstract": "Abstract Arc contractions in syntactic dependency graphs can be used to decide which graphs are trees. Abstract Arc contractions in syntactic dependency graphs can be used to decide which graphs are trees. The paper observes that these contractions can be expressed with weighted finite-state transducers (weighted FST) that operate on stringencoded trees. The observation gives rise to a finite-state parsing algorithm that computes the parse forest and extracts the best parses from it. The observation gives rise to a finite-state parsing algorithm that computes the parse forest and extracts the best parses from it. The algorithm is customizable to functional and bilexical dependency parsing, and it can be extended to non-projective parsing via a multi-planar encoding with prior results on high recall. Our experiments support an analysis of projective parsing according to which the worst-case time complexity of the algorithm is quadratic to the sentence length, and linear to the overlapping arcs and the number of functional categories of the arcs. Our experiments support an analysis of projective parsing according to which the worst-case time complexity of the algorithm is quadratic to the sentence length, and linear to the overlapping arcs and the number of functional categories of the arcs. The results suggest several interesting directions towards efficient and highprecision dependency parsing that takes advantage of the flexibility and the demonstrated ambiguity-packing capacity of such a parser."}, {"paper_id": "11748996", "adju_relevance": 2, "title": "Dependency Parsing with Undirected Graphs", "background_label": "AbstractWe introduce a new approach to transitionbased dependency parsing in which the parser does not directly construct a dependency structure, but rather an undirected graph, which is then converted into a directed dependency tree in a post-processing step.", "method_label": "This alleviates error propagation, since undirected parsers do not need to observe the single-head constraint.Undirected parsers can be obtained by simplifying existing transition-based parsers satisfying certain conditions. We apply this approach to obtain undirected variants of the planar and 2-planar parsers and of Covington's non-projective parser.", "result_label": "We perform experiments on several datasets from the CoNLL-X shared task, showing that these variants outperform the original directed algorithms in most of the cases.", "abstract": "AbstractWe introduce a new approach to transitionbased dependency parsing in which the parser does not directly construct a dependency structure, but rather an undirected graph, which is then converted into a directed dependency tree in a post-processing step. This alleviates error propagation, since undirected parsers do not need to observe the single-head constraint.Undirected parsers can be obtained by simplifying existing transition-based parsers satisfying certain conditions. This alleviates error propagation, since undirected parsers do not need to observe the single-head constraint.Undirected parsers can be obtained by simplifying existing transition-based parsers satisfying certain conditions. We apply this approach to obtain undirected variants of the planar and 2-planar parsers and of Covington's non-projective parser. We perform experiments on several datasets from the CoNLL-X shared task, showing that these variants outperform the original directed algorithms in most of the cases."}, {"paper_id": "13174124", "adju_relevance": 2, "title": "Constrained Arc-Eager Dependency Parsing", "background_label": "Arc-eager dependency parsers process sentences in a single left-to-right pass over the input and have linear time complexity with greedy decoding or beam search.", "method_label": "We show how such parsers can be constrained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser.", "abstract": "Arc-eager dependency parsers process sentences in a single left-to-right pass over the input and have linear time complexity with greedy decoding or beam search. We show how such parsers can be constrained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. We show how such parsers can be constrained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser."}, {"paper_id": "621320", "adju_relevance": 2, "title": "Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers", "background_label": "AbstractWe present fast, accurate, direct nonprojective dependency parsers with thirdorder features.", "method_label": "Our approach uses AD 3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models.", "result_label": "Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German).", "abstract": "AbstractWe present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD 3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German)."}, {"paper_id": "12675146", "adju_relevance": 2, "title": "Fourth-Order Dependency Parsing", "background_label": "ABSTRACTWe present and implement a fourth-order projective dependency parsing algorithm that effectively utilizes both \"grand-sibling\" style and \"tri-sibling\" style interactions of third-order and \"grand-tri-sibling\" style interactions of forth-order factored parts for performance enhancement.", "method_label": "This algorithm requires O(n 5 ) time and O(n 4 ) space. We implement and evaluate the parser on two languages-English and Chinese, both achieving state-of-the-art accuracy.", "result_label": "This results show that a higher-order (\u22654) dependency parser gives performance improvement over all previous lower-order parsers.", "abstract": "ABSTRACTWe present and implement a fourth-order projective dependency parsing algorithm that effectively utilizes both \"grand-sibling\" style and \"tri-sibling\" style interactions of third-order and \"grand-tri-sibling\" style interactions of forth-order factored parts for performance enhancement. This algorithm requires O(n 5 ) time and O(n 4 ) space. This algorithm requires O(n 5 ) time and O(n 4 ) space. We implement and evaluate the parser on two languages-English and Chinese, both achieving state-of-the-art accuracy. This results show that a higher-order (\u22654) dependency parser gives performance improvement over all previous lower-order parsers."}, {"paper_id": "14081838", "adju_relevance": 2, "title": "Dependency Parsing as Head Selection", "background_label": "Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence.", "method_label": "Our model which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf N}eural {\\bf Se}lection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, \\textsc{DeNSe} generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm.", "result_label": "We evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.", "abstract": "Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf N}eural {\\bf Se}lection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Our model which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf N}eural {\\bf Se}lection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, \\textsc{DeNSe} generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. We evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art."}, {"paper_id": "11905341", "adju_relevance": 2, "title": "Vine Pruning for Efficient Multi-Pass Dependency Parsing", "background_label": "AbstractCoarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy.", "method_label": "We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search.", "result_label": "Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.", "abstract": "AbstractCoarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages."}, {"paper_id": "3146611", "adju_relevance": 2, "title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing", "background_label": "AbstractWe present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built.", "method_label": "A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed.", "result_label": "The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.", "abstract": "AbstractWe present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. AbstractWe present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models."}, {"paper_id": "14415278", "adju_relevance": 2, "title": "High-order Graph-based Neural Dependency Parsing", "method_label": "Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks.", "result_label": "Compared to existing work, our parsers give competitive performance with much more efficient inference.", "abstract": " Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks. Compared to existing work, our parsers give competitive performance with much more efficient inference."}, {"paper_id": "802998", "adju_relevance": 2, "title": "Online Learning of Approximate Dependency Parsing Algorithms", "background_label": "AbstractIn this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.", "method_label": "We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.", "result_label": "We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.", "abstract": "AbstractIn this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. AbstractIn this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish."}, {"paper_id": "6681594", "adju_relevance": 2, "title": "Non-Projective Dependency Parsing using Spanning Tree Algorithms", "background_label": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time.", "method_label": "More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm.", "result_label": "We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies."}, {"paper_id": "6469543", "adju_relevance": 2, "title": "Generalized Higher-Order Dependency Parsing with Cube Pruning", "background_label": "AbstractState-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant.", "abstract": "AbstractState-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. AbstractState-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant."}, {"paper_id": "1286199", "adju_relevance": 1, "title": "Compressing Optimal Paths with Run Length Encoding", "background_label": "AbstractWe introduce a novel approach to Compressed Path Databases, space efficient oracles used to very quickly identify the first edge on a shortest path.", "method_label": "Our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. Space consumption is competitive, due to a compression approach that rearranges rows and columns in a first-move matrix and then performs run length encoding (RLE) on the contents of the matrix. One variant of our implemented system was, by a convincing margin, the fastest entry in the 2014 Grid-Based Path Planning Competition.We give a first tractability analysis for the compression scheme used by our algorithm. We study the complexity of computing a database of minimum size for general directed and undirected graphs.", "result_label": "We find that in both cases the problem is NP-complete. We also show that, for graphs which can be decomposed along articulation points, the problem can be decomposed into independent parts, with a corresponding reduction in its level of difficulty. In particular, this leads to simple and tractable algorithms with linear running time which yield optimal compression results for trees.", "abstract": "AbstractWe introduce a novel approach to Compressed Path Databases, space efficient oracles used to very quickly identify the first edge on a shortest path. Our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. Our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. Space consumption is competitive, due to a compression approach that rearranges rows and columns in a first-move matrix and then performs run length encoding (RLE) on the contents of the matrix. Our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. Space consumption is competitive, due to a compression approach that rearranges rows and columns in a first-move matrix and then performs run length encoding (RLE) on the contents of the matrix. One variant of our implemented system was, by a convincing margin, the fastest entry in the 2014 Grid-Based Path Planning Competition.We give a first tractability analysis for the compression scheme used by our algorithm. Our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. Space consumption is competitive, due to a compression approach that rearranges rows and columns in a first-move matrix and then performs run length encoding (RLE) on the contents of the matrix. One variant of our implemented system was, by a convincing margin, the fastest entry in the 2014 Grid-Based Path Planning Competition.We give a first tractability analysis for the compression scheme used by our algorithm. We study the complexity of computing a database of minimum size for general directed and undirected graphs. We find that in both cases the problem is NP-complete. We find that in both cases the problem is NP-complete. We also show that, for graphs which can be decomposed along articulation points, the problem can be decomposed into independent parts, with a corresponding reduction in its level of difficulty. We find that in both cases the problem is NP-complete. We also show that, for graphs which can be decomposed along articulation points, the problem can be decomposed into independent parts, with a corresponding reduction in its level of difficulty. In particular, this leads to simple and tractable algorithms with linear running time which yield optimal compression results for trees."}, {"paper_id": "2347857", "adju_relevance": 1, "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "background_label": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime.", "method_label": "It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation.", "abstract": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation."}, {"paper_id": "12233172", "adju_relevance": 1, "title": "Parsing to Noncrossing Dependency Graphs", "background_label": "We study the generalization of maximum spanning tree dependency parsing to maximum acyclic subgraphs. Because the underlying optimization problem is intractable even under an arc-factored model, we consider the restriction to noncrossing dependency graphs.", "abstract": "We study the generalization of maximum spanning tree dependency parsing to maximum acyclic subgraphs. We study the generalization of maximum spanning tree dependency parsing to maximum acyclic subgraphs. Because the underlying optimization problem is intractable even under an arc-factored model, we consider the restriction to noncrossing dependency graphs."}, {"paper_id": "17650336", "adju_relevance": 1, "title": "The complexity of finding the maximum spanning DAG and other restrictions for DAG parsing of natural language", "background_label": "Recently, there has been renewed interest in semantic dependency parsing, among which one of the paradigms focuses on parsing directed acyclic graphs (DAGs). Consideration of the decoding problem in natural language semantic dependency parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities. In particular, the computational complexity (and approximability) of the problem has not been addressed in the literature to date.", "method_label": "This paper helps to fill this gap, showing that this general problem is APX-hard, and is NP-hard even under the planar restriction, in the graphtheoretic sense. On the other hand, we show that under the restriction of projectivity, the problem has a straightforward O(n 3 ) algorithm.", "result_label": "We also give some empirical evidence of the algorithmic importance of these graph restrictions, on data from the SemEval 2014 task 8 on Broad Coverage Semantic Dependency Parsing.", "abstract": "Recently, there has been renewed interest in semantic dependency parsing, among which one of the paradigms focuses on parsing directed acyclic graphs (DAGs). Recently, there has been renewed interest in semantic dependency parsing, among which one of the paradigms focuses on parsing directed acyclic graphs (DAGs). Consideration of the decoding problem in natural language semantic dependency parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities. Recently, there has been renewed interest in semantic dependency parsing, among which one of the paradigms focuses on parsing directed acyclic graphs (DAGs). Consideration of the decoding problem in natural language semantic dependency parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities. In particular, the computational complexity (and approximability) of the problem has not been addressed in the literature to date. This paper helps to fill this gap, showing that this general problem is APX-hard, and is NP-hard even under the planar restriction, in the graphtheoretic sense. This paper helps to fill this gap, showing that this general problem is APX-hard, and is NP-hard even under the planar restriction, in the graphtheoretic sense. On the other hand, we show that under the restriction of projectivity, the problem has a straightforward O(n 3 ) algorithm. We also give some empirical evidence of the algorithmic importance of these graph restrictions, on data from the SemEval 2014 task 8 on Broad Coverage Semantic Dependency Parsing."}, {"paper_id": "974611", "adju_relevance": 1, "title": "Memory-Based Dependency Parsing", "background_label": "AbstractThis paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text.", "method_label": "Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank.", "result_label": "The evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further.", "abstract": "AbstractThis paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text. Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank. The evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further."}, {"paper_id": "6183590", "adju_relevance": 1, "title": "Decremental Single-Source Shortest Paths on Undirected Graphs in Near-Linear Total Update Time", "background_label": "In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the fastest known algorithm for three decades. At the cost of a $(1+\\epsilon)$-approximation factor, the running time was recently improved to $n^{2+o(1)}$ by Bernstein and Roditty [SODA 2011]. The only prior work on weighted graphs in $o(m n)$ time is the $m n^{0.9 + o(1)}$-time algorithm by Henzinger et al. [STOC 2014, ICALP 2015] which works for directed graphs with quasi-polynomial edge weights. The expected running time bound of our algorithm holds against an oblivious adversary.", "method_label": "In this paper, we bring the running time down to near-linear: We give a $(1+\\epsilon)$-approximation algorithm with $m^{1+o(1)}$ expected total update time, thus obtaining near-linear time. In contrast to the previous results which rely on maintaining a sparse emulator, our algorithm relies on maintaining a so-called sparse $(h, \\epsilon)$-hop set introduced by Cohen [JACM 2000] in the PRAM literature.", "result_label": "Moreover, we obtain $m^{1+o(1)} \\log W$ time for the weighted case, where the edge weights are integers from $1$ to $W$.", "abstract": "In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the fastest known algorithm for three decades. In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the fastest known algorithm for three decades. At the cost of a $(1+\\epsilon)$-approximation factor, the running time was recently improved to $n^{2+o(1)}$ by Bernstein and Roditty [SODA 2011]. In this paper, we bring the running time down to near-linear: We give a $(1+\\epsilon)$-approximation algorithm with $m^{1+o(1)}$ expected total update time, thus obtaining near-linear time. Moreover, we obtain $m^{1+o(1)} \\log W$ time for the weighted case, where the edge weights are integers from $1$ to $W$. In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the fastest known algorithm for three decades. At the cost of a $(1+\\epsilon)$-approximation factor, the running time was recently improved to $n^{2+o(1)}$ by Bernstein and Roditty [SODA 2011]. The only prior work on weighted graphs in $o(m n)$ time is the $m n^{0.9 + o(1)}$-time algorithm by Henzinger et al. In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the fastest known algorithm for three decades. At the cost of a $(1+\\epsilon)$-approximation factor, the running time was recently improved to $n^{2+o(1)}$ by Bernstein and Roditty [SODA 2011]. The only prior work on weighted graphs in $o(m n)$ time is the $m n^{0.9 + o(1)}$-time algorithm by Henzinger et al. [STOC 2014, ICALP 2015] which works for directed graphs with quasi-polynomial edge weights. In the decremental single-source shortest paths (SSSP) problem we want to maintain the distances between a given source node $s$ and every other node in an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the fastest known algorithm for three decades. At the cost of a $(1+\\epsilon)$-approximation factor, the running time was recently improved to $n^{2+o(1)}$ by Bernstein and Roditty [SODA 2011]. The only prior work on weighted graphs in $o(m n)$ time is the $m n^{0.9 + o(1)}$-time algorithm by Henzinger et al. [STOC 2014, ICALP 2015] which works for directed graphs with quasi-polynomial edge weights. The expected running time bound of our algorithm holds against an oblivious adversary. In this paper, we bring the running time down to near-linear: We give a $(1+\\epsilon)$-approximation algorithm with $m^{1+o(1)}$ expected total update time, thus obtaining near-linear time. In contrast to the previous results which rely on maintaining a sparse emulator, our algorithm relies on maintaining a so-called sparse $(h, \\epsilon)$-hop set introduced by Cohen [JACM 2000] in the PRAM literature."}, {"paper_id": "24774", "adju_relevance": 1, "title": "Linear expected-time algorithms for connectivity problems (Extended Abstract)", "background_label": "Researchers in recent years have developed many graph algorithms that are fast in the worst case, but little work has been done on graph algorithms that are fast on the average. (Exceptions include the work of Angluin and Valiant [1], Karp [7], and Schnorr [9].)", "abstract": "Researchers in recent years have developed many graph algorithms that are fast in the worst case, but little work has been done on graph algorithms that are fast on the average. Researchers in recent years have developed many graph algorithms that are fast in the worst case, but little work has been done on graph algorithms that are fast on the average. (Exceptions include the work of Angluin and Valiant [1], Karp [7], and Schnorr [9].)"}, {"paper_id": "48359110", "adju_relevance": 1, "title": "Broadcast and minimum spanning tree with $o(m)$ messages in the asynchronous CONGEST model", "background_label": "We provide the first asynchronous distributed algorithms to compute broadcast and minimum spanning tree with $o(m)$ bits of communication, in a graph with $n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of communication are required for any algorithm that constructs a broadcast tree. In 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have initial knowledge of their neighbors' identities it is possible to construct MST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. In the CONGEST model messages are of size $O(\\log n)$. However, no algorithm with $o(m)$ messages were known for the asynchronous case.", "method_label": "Here, we provide an algorithm that uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous CONGEST model. Our algorithm is randomized Monte Carlo and outputs MST with high probability. We will provide an algorithm for computing a spanning tree with $O(n^{3/2} \\log^{3/2} n)$ messages.", "result_label": "Given a spanning tree, we can compute MST with $\\tilde{O}(n)$ messages.", "abstract": "We provide the first asynchronous distributed algorithms to compute broadcast and minimum spanning tree with $o(m)$ bits of communication, in a graph with $n$ nodes and $m$ edges. We provide the first asynchronous distributed algorithms to compute broadcast and minimum spanning tree with $o(m)$ bits of communication, in a graph with $n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of communication are required for any algorithm that constructs a broadcast tree. We provide the first asynchronous distributed algorithms to compute broadcast and minimum spanning tree with $o(m)$ bits of communication, in a graph with $n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of communication are required for any algorithm that constructs a broadcast tree. In 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have initial knowledge of their neighbors' identities it is possible to construct MST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. We provide the first asynchronous distributed algorithms to compute broadcast and minimum spanning tree with $o(m)$ bits of communication, in a graph with $n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of communication are required for any algorithm that constructs a broadcast tree. In 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have initial knowledge of their neighbors' identities it is possible to construct MST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. In the CONGEST model messages are of size $O(\\log n)$. We provide the first asynchronous distributed algorithms to compute broadcast and minimum spanning tree with $o(m)$ bits of communication, in a graph with $n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of communication are required for any algorithm that constructs a broadcast tree. In 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have initial knowledge of their neighbors' identities it is possible to construct MST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. In the CONGEST model messages are of size $O(\\log n)$. However, no algorithm with $o(m)$ messages were known for the asynchronous case. Here, we provide an algorithm that uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous CONGEST model. Here, we provide an algorithm that uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous CONGEST model. Our algorithm is randomized Monte Carlo and outputs MST with high probability. Here, we provide an algorithm that uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous CONGEST model. Our algorithm is randomized Monte Carlo and outputs MST with high probability. We will provide an algorithm for computing a spanning tree with $O(n^{3/2} \\log^{3/2} n)$ messages. Given a spanning tree, we can compute MST with $\\tilde{O}(n)$ messages."}, {"paper_id": "12313253", "adju_relevance": 1, "title": "Dependency Parsing by Belief Propagation", "background_label": "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference.", "method_label": "As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods.", "result_label": "Incorporating additional features would increase the runtime additively rather than multiplicatively.", "abstract": "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively."}, {"paper_id": "1334893", "adju_relevance": 1, "title": "Branch and Bound Algorithm for Dependency Parsing with Non-local Features", "background_label": "Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference.", "method_label": "In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&B) framework where non-local features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets.", "result_label": "We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to non-projective dependency parsing or other graphical models.", "abstract": "Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&B) framework where non-local features are bounded by a linear combination of local features. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&B) framework where non-local features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&B) framework where non-local features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to non-projective dependency parsing or other graphical models."}, {"paper_id": "12044793", "adju_relevance": 1, "title": "Edge-disjoint spanning trees and depth-first search", "method_label": "The algorithm uses depthfirst search and an efficient method for computing disjoint set unions.", "result_label": "It requires O (e\u03b1(e, n)) time and O(e) space to analyze a graph with n vertices and e edges, where \u03b1 (e, n) is a very slowly growing function related to a functional inverse of Ackermann's function.", "abstract": " The algorithm uses depthfirst search and an efficient method for computing disjoint set unions. It requires O (e\u03b1(e, n)) time and O(e) space to analyze a graph with n vertices and e edges, where \u03b1 (e, n) is a very slowly growing function related to a functional inverse of Ackermann's function."}, {"paper_id": "1584288", "adju_relevance": 1, "title": "A Crossing-Sensitive Third-Order Factorization for Dependency Parsing", "background_label": "Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. For graph-based non-projective parsers, wider factorizations have so far implied large increases in the computational complexity of the parsing problem.", "abstract": "Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. For graph-based non-projective parsers, wider factorizations have so far implied large increases in the computational complexity of the parsing problem."}, {"paper_id": "5625678", "adju_relevance": 1, "title": "K-best Spanning Tree Parsing", "background_label": "AbstractThis paper introduces a Maximum Entropy dependency parser based on an efficient kbest Maximum Spanning Tree (MST) algorithm.", "method_label": "Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. This motivates our parsing approach, which is based on reranking the kbest parses generated by an edge-factored model.", "result_label": "Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English).", "abstract": "AbstractThis paper introduces a Maximum Entropy dependency parser based on an efficient kbest Maximum Spanning Tree (MST) algorithm. Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. This motivates our parsing approach, which is based on reranking the kbest parses generated by an edge-factored model. Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English)."}, {"paper_id": "991005", "adju_relevance": 1, "title": "Concise Integer Linear Programming Formulations for Dependency Parsing", "background_label": "We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program.", "method_label": "Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation.", "result_label": "We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.", "abstract": "We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods."}, {"paper_id": "14940991", "adju_relevance": 1, "title": "On maximum spanning DAG algorithms for semantic DAG parsing", "background_label": "Consideration of the decoding problem in semantic parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities that haven\u2019t been fully addressed in the literature to date, among which are its actual appropriateness for the decoding task in semantic parsing, not to mention an explicit proof of its complexity (and its approximability).", "abstract": "Consideration of the decoding problem in semantic parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities that haven\u2019t been fully addressed in the literature to date, among which are its actual appropriateness for the decoding task in semantic parsing, not to mention an explicit proof of its complexity (and its approximability)."}, {"paper_id": "8778603", "adju_relevance": 1, "title": "Semantic Dependency Graph Parsing Using Tree Approximations", "background_label": "AbstractIn this contribution, we deal with graph parsing, i.e., mapping input strings to graph-structured output representations, using tree approximations.", "method_label": "We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We focus on undirected base cycles in the SDP graphs, and discover strong connections to grammatical control and coordination. Second, we make use of the approximations in a statistical parsing scenario. In it, we convert the training set graphs to dependency trees, and use the resulting treebanks to build standard dependency tree parsers. We perform lossy graph reconstructions on parser outputs, and evaluate our models as dependency graph parsers.", "result_label": "Our system outperforms the baselines by a large margin, and evaluates as the best non-voting tree approximation-based parser on the SemEval 2014 data, scoring at just over 81% in labeled F 1 .", "abstract": "AbstractIn this contribution, we deal with graph parsing, i.e., mapping input strings to graph-structured output representations, using tree approximations. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We focus on undirected base cycles in the SDP graphs, and discover strong connections to grammatical control and coordination. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We focus on undirected base cycles in the SDP graphs, and discover strong connections to grammatical control and coordination. Second, we make use of the approximations in a statistical parsing scenario. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We focus on undirected base cycles in the SDP graphs, and discover strong connections to grammatical control and coordination. Second, we make use of the approximations in a statistical parsing scenario. In it, we convert the training set graphs to dependency trees, and use the resulting treebanks to build standard dependency tree parsers. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We focus on undirected base cycles in the SDP graphs, and discover strong connections to grammatical control and coordination. Second, we make use of the approximations in a statistical parsing scenario. In it, we convert the training set graphs to dependency trees, and use the resulting treebanks to build standard dependency tree parsers. We perform lossy graph reconstructions on parser outputs, and evaluate our models as dependency graph parsers. Our system outperforms the baselines by a large margin, and evaluates as the best non-voting tree approximation-based parser on the SemEval 2014 data, scoring at just over 81% in labeled F 1 ."}, {"paper_id": "3143429", "adju_relevance": 1, "title": "Arc-Eager Parsing with the Tree Constraint", "background_label": "The arc-eager system for transition-based dependency parsing is widely used in natural language processing despite the fact that it does not guarantee that the output is a well-formed dependency tree.", "method_label": "We propose a simple modification to the original system that enforces the tree constraint without requiring any modification to the parser training procedure.", "result_label": "Experiments on multiple languages show that the method on average achieves 72% of the error reduction possible and consistently outperforms the standard heuristic in current use.", "abstract": "The arc-eager system for transition-based dependency parsing is widely used in natural language processing despite the fact that it does not guarantee that the output is a well-formed dependency tree. We propose a simple modification to the original system that enforces the tree constraint without requiring any modification to the parser training procedure. Experiments on multiple languages show that the method on average achieves 72% of the error reduction possible and consistently outperforms the standard heuristic in current use."}, {"paper_id": "269533", "adju_relevance": 1, "title": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set", "background_label": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al.", "method_label": "(2011) to produce the first implementation of worst-case O(n^3) exact decoders for arc-hybrid and arc-eager transition systems. With our minimal features, we also present O(n^3) global training methods.", "result_label": "Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the\"second-best-in-class\"result on the English Penn Treebank.", "abstract": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case O(n^3) exact decoders for arc-hybrid and arc-eager transition systems. (2011) to produce the first implementation of worst-case O(n^3) exact decoders for arc-hybrid and arc-eager transition systems. With our minimal features, we also present O(n^3) global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the\"second-best-in-class\"result on the English Penn Treebank."}, {"paper_id": "62608205", "adju_relevance": 0, "title": "Incremental computation of dominator trees", "background_label": "Data flow analysis based on an incremental approach may require that the dominator tree be correctly maintained at all times. Previous solutions to the problem of incrementally maintaining dominator trees were restricted to reducible flowgraphs.", "method_label": "In this paper we present a new algorithm for incrementally maintaining the dominator tree of an arbitrary flowgraph, either reducible or irreducible, based on a program representation called the DJ-graph. For the case where an edge is inserted, our algorithm is also faster than previous approaches (in the worst case).", "result_label": "For the deletion case, our algorithm is likely to run fast on the average cases.", "abstract": "Data flow analysis based on an incremental approach may require that the dominator tree be correctly maintained at all times. Data flow analysis based on an incremental approach may require that the dominator tree be correctly maintained at all times. Previous solutions to the problem of incrementally maintaining dominator trees were restricted to reducible flowgraphs. In this paper we present a new algorithm for incrementally maintaining the dominator tree of an arbitrary flowgraph, either reducible or irreducible, based on a program representation called the DJ-graph. In this paper we present a new algorithm for incrementally maintaining the dominator tree of an arbitrary flowgraph, either reducible or irreducible, based on a program representation called the DJ-graph. For the case where an edge is inserted, our algorithm is also faster than previous approaches (in the worst case). For the deletion case, our algorithm is likely to run fast on the average cases."}, {"paper_id": "15484396", "adju_relevance": 0, "title": "XML Compression via Directed Acyclic Graphs", "background_label": "Unranked node-labeled trees can be represented using their minimal dag (directed acyclic graph). For XML this achieves high compression ratios due to their repetitive mark up. Unranked trees are often represented through first child/next sibling (fcns) encoded binary trees.", "abstract": "Unranked node-labeled trees can be represented using their minimal dag (directed acyclic graph). Unranked node-labeled trees can be represented using their minimal dag (directed acyclic graph). For XML this achieves high compression ratios due to their repetitive mark up. Unranked node-labeled trees can be represented using their minimal dag (directed acyclic graph). For XML this achieves high compression ratios due to their repetitive mark up. Unranked trees are often represented through first child/next sibling (fcns) encoded binary trees."}, {"paper_id": "120445825", "adju_relevance": 0, "title": "A Minimal Spanning Tree algorithm for source detection in \u03b3-ray images", "method_label": "We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for finding clusters in a given set of points. This algorithm is applied to \u03b3-ray bi-dimensional images where the points correspond to the arrival direction of photons, and the possible sources are associated with the regions where they clusterize. Some filters to select these clusters and to reduce the spurious detections are introduced. An empirical study of the statistical properties of MST on random fields is carried out in order to derive some criteria to estimate the best filter values. We also introduce two parameters useful to verify the goodness of candidate sources.", "result_label": "To show how the MST algorithm works in practice, we present an application to an EGRET observation of the Virgo field, at high Galactic latitude and with a low and rather uniform background, in which several sources are detected.", "abstract": "We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for finding clusters in a given set of points. We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for finding clusters in a given set of points. This algorithm is applied to \u03b3-ray bi-dimensional images where the points correspond to the arrival direction of photons, and the possible sources are associated with the regions where they clusterize. We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for finding clusters in a given set of points. This algorithm is applied to \u03b3-ray bi-dimensional images where the points correspond to the arrival direction of photons, and the possible sources are associated with the regions where they clusterize. Some filters to select these clusters and to reduce the spurious detections are introduced. We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for finding clusters in a given set of points. This algorithm is applied to \u03b3-ray bi-dimensional images where the points correspond to the arrival direction of photons, and the possible sources are associated with the regions where they clusterize. Some filters to select these clusters and to reduce the spurious detections are introduced. An empirical study of the statistical properties of MST on random fields is carried out in order to derive some criteria to estimate the best filter values. We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for finding clusters in a given set of points. This algorithm is applied to \u03b3-ray bi-dimensional images where the points correspond to the arrival direction of photons, and the possible sources are associated with the regions where they clusterize. Some filters to select these clusters and to reduce the spurious detections are introduced. An empirical study of the statistical properties of MST on random fields is carried out in order to derive some criteria to estimate the best filter values. We also introduce two parameters useful to verify the goodness of candidate sources. To show how the MST algorithm works in practice, we present an application to an EGRET observation of the Virgo field, at high Galactic latitude and with a low and rather uniform background, in which several sources are detected."}, {"paper_id": "9822933", "adju_relevance": 0, "title": "Flow-Based Algorithms for Local Graph Clustering", "background_label": "Given a subset S of vertices of an undirected graph G, the cut-improvement problem asks us to find a subset S that is similar to A but has smaller conductance. All previously known local algorithms for graph partitioning are random-walk based and can only guarantee an output conductance of O(\\sqrt{OPT}) when the target set has conductance OPT \\in [0,1]. Very recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT / \\sqrt{CONN}) where the internal connectivity parameter CONN \\in [0,1] is defined as the reciprocal of the mixing time of the random walk over the graph induced by the target set.", "method_label": "A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. that runs in time dependent on the size of the input set A rather than on the size of the entire graph. Moreover, LocalImprove achieves this local behaviour while essentially matching the same theoretical guarantee as the global algorithm of Andersen and Lang. In this work, we show how to use LocalImprove to obtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This yields the first flow-based algorithm.", "result_label": "The main application of LocalImprove is to the design of better local-graph-partitioning algorithms. Moreover, its performance strictly outperforms the ones based on random walks and surprisingly matches that of the best known global algorithm, which is SDP-based, in this parameter regime [MMV12]. Finally, our results show that spectral methods are not the only viable approach to the construction of local graph partitioning algorithm and open door to the study of algorithms with even better approximation and locality guarantees.", "abstract": "Given a subset S of vertices of an undirected graph G, the cut-improvement problem asks us to find a subset S that is similar to A but has smaller conductance. A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. that runs in time dependent on the size of the input set A rather than on the size of the entire graph. A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. that runs in time dependent on the size of the input set A rather than on the size of the entire graph. Moreover, LocalImprove achieves this local behaviour while essentially matching the same theoretical guarantee as the global algorithm of Andersen and Lang. The main application of LocalImprove is to the design of better local-graph-partitioning algorithms. Given a subset S of vertices of an undirected graph G, the cut-improvement problem asks us to find a subset S that is similar to A but has smaller conductance. All previously known local algorithms for graph partitioning are random-walk based and can only guarantee an output conductance of O(\\sqrt{OPT}) when the target set has conductance OPT \\in [0,1]. Given a subset S of vertices of an undirected graph G, the cut-improvement problem asks us to find a subset S that is similar to A but has smaller conductance. All previously known local algorithms for graph partitioning are random-walk based and can only guarantee an output conductance of O(\\sqrt{OPT}) when the target set has conductance OPT \\in [0,1]. Very recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT / \\sqrt{CONN}) where the internal connectivity parameter CONN \\in [0,1] is defined as the reciprocal of the mixing time of the random walk over the graph induced by the target set. A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. that runs in time dependent on the size of the input set A rather than on the size of the entire graph. Moreover, LocalImprove achieves this local behaviour while essentially matching the same theoretical guarantee as the global algorithm of Andersen and Lang. In this work, we show how to use LocalImprove to obtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. that runs in time dependent on the size of the input set A rather than on the size of the entire graph. Moreover, LocalImprove achieves this local behaviour while essentially matching the same theoretical guarantee as the global algorithm of Andersen and Lang. In this work, we show how to use LocalImprove to obtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This yields the first flow-based algorithm. The main application of LocalImprove is to the design of better local-graph-partitioning algorithms. Moreover, its performance strictly outperforms the ones based on random walks and surprisingly matches that of the best known global algorithm, which is SDP-based, in this parameter regime [MMV12]. The main application of LocalImprove is to the design of better local-graph-partitioning algorithms. Moreover, its performance strictly outperforms the ones based on random walks and surprisingly matches that of the best known global algorithm, which is SDP-based, in this parameter regime [MMV12]. Finally, our results show that spectral methods are not the only viable approach to the construction of local graph partitioning algorithm and open door to the study of algorithms with even better approximation and locality guarantees."}, {"paper_id": "35618095", "adju_relevance": 0, "title": "Efficient algorithms for finding minimum spanning trees in undirected and directed graphs", "background_label": "Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue). Their data structure, theFibonacci heap (or F-heap) supports arbitrary deletion inO(logn) amortized time and other heap operations inO(1) amortized time.", "abstract": "Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue). Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue). Their data structure, theFibonacci heap (or F-heap) supports arbitrary deletion inO(logn) amortized time and other heap operations inO(1) amortized time."}, {"paper_id": "1787614", "adju_relevance": 0, "title": "Using Optimal Dependency-Trees for Combinatorial Optimization: Learning the Structure of the Search Space", "background_label": "Many combinatorial optimization algorithms have no mechanism for capturing inter-parameter dependencies. However, modeling such dependencies may allow an algorithm to concentrate its sampling more effectively on regions of the search space which have appeared promising in the past.", "method_label": "We present an algorithm which incrementally learns pairwise probability distributions from good solutions seen so far, uses these statistics to generate optimal (in terms of maximum likelihood) dependency trees to model these distributions, and then stochastically generates new candidate solutions from these trees. We test this algorithm on a variety of optimization problems.", "result_label": "Our results indicate superior performance over other tested algorithms that either (1) do not explicitly use these dependencies, or (2) use these dependencies to generate a more restricted class of dependency graphs.", "abstract": "Many combinatorial optimization algorithms have no mechanism for capturing inter-parameter dependencies. Many combinatorial optimization algorithms have no mechanism for capturing inter-parameter dependencies. However, modeling such dependencies may allow an algorithm to concentrate its sampling more effectively on regions of the search space which have appeared promising in the past. We present an algorithm which incrementally learns pairwise probability distributions from good solutions seen so far, uses these statistics to generate optimal (in terms of maximum likelihood) dependency trees to model these distributions, and then stochastically generates new candidate solutions from these trees. We present an algorithm which incrementally learns pairwise probability distributions from good solutions seen so far, uses these statistics to generate optimal (in terms of maximum likelihood) dependency trees to model these distributions, and then stochastically generates new candidate solutions from these trees. We test this algorithm on a variety of optimization problems. Our results indicate superior performance over other tested algorithms that either (1) do not explicitly use these dependencies, or (2) use these dependencies to generate a more restricted class of dependency graphs."}, {"paper_id": "660097", "adju_relevance": 0, "title": "Minimum Spanning Tree Based Clustering Algorithms", "background_label": "The minimum spanning tree clustering algorithm is known to be capable of detecting clusters with irregular boundaries.", "abstract": "The minimum spanning tree clustering algorithm is known to be capable of detecting clusters with irregular boundaries."}, {"paper_id": "519025", "adju_relevance": 0, "title": "A simpler minimum spanning tree verification algorithm", "background_label": "The problem considered here is that of determining whether a given spanning tree is a minimal spanning tree. In 1984 Koml\u00f3s presented an algorithm which required only a linear number of comparisons, but nonlinear overhead to determine which comparisons to make.", "method_label": "We simplify his algorithm and give a linear-time procedure for its implementation in the unit cost RAM model. The procedure uses table lookup of a few simple functions, which we precompute in time linear in the size of the tree.", "abstract": "The problem considered here is that of determining whether a given spanning tree is a minimal spanning tree. The problem considered here is that of determining whether a given spanning tree is a minimal spanning tree. In 1984 Koml\u00f3s presented an algorithm which required only a linear number of comparisons, but nonlinear overhead to determine which comparisons to make. We simplify his algorithm and give a linear-time procedure for its implementation in the unit cost RAM model. We simplify his algorithm and give a linear-time procedure for its implementation in the unit cost RAM model. The procedure uses table lookup of a few simple functions, which we precompute in time linear in the size of the tree."}, {"paper_id": "202144185", "adju_relevance": 0, "title": "LR-Drawings of Ordered Rooted Binary Trees and Near-Linear Area Drawings of Outerplanar Graphs", "background_label": "Abstract We study the width requirements of LR-drawings of n-node ordered rooted binary trees; these are the drawings produced by a family of tree drawing algorithms introduced by Chan, who showed how to construct LR-drawings with width O ( n 0.48 ) .", "method_label": "We prove that LR-drawings with minimum width can be constructed in O ( n 1.48 ) time. Further, we show an infinite family of n-node ordered rooted binary trees requiring \u03a9 ( n 0.418 ) width in any LR-drawing and we present the results of an experimental evaluation that allowed us to determine the minimum width of all the ordered rooted binary trees with up to 455 nodes. We also show that, if n-node ordered rooted binary trees have LR-drawings with f ( n ) width, for any function f ( n ) , then n-vertex outerplanar graphs have outerplanar straight-line drawings in O ( f ( n ) ) area.", "result_label": "Finally, we prove that every n-vertex outerplanar graph has an outerplanar straight-line drawing in O ( n \u22c5 2 2 log 2 \u2061 n log \u2061 n ) area.", "abstract": "Abstract We study the width requirements of LR-drawings of n-node ordered rooted binary trees; these are the drawings produced by a family of tree drawing algorithms introduced by Chan, who showed how to construct LR-drawings with width O ( n 0.48 ) . We prove that LR-drawings with minimum width can be constructed in O ( n 1.48 ) time. We prove that LR-drawings with minimum width can be constructed in O ( n 1.48 ) time. Further, we show an infinite family of n-node ordered rooted binary trees requiring \u03a9 ( n 0.418 ) width in any LR-drawing and we present the results of an experimental evaluation that allowed us to determine the minimum width of all the ordered rooted binary trees with up to 455 nodes. We prove that LR-drawings with minimum width can be constructed in O ( n 1.48 ) time. Further, we show an infinite family of n-node ordered rooted binary trees requiring \u03a9 ( n 0.418 ) width in any LR-drawing and we present the results of an experimental evaluation that allowed us to determine the minimum width of all the ordered rooted binary trees with up to 455 nodes. We also show that, if n-node ordered rooted binary trees have LR-drawings with f ( n ) width, for any function f ( n ) , then n-vertex outerplanar graphs have outerplanar straight-line drawings in O ( f ( n ) ) area. Finally, we prove that every n-vertex outerplanar graph has an outerplanar straight-line drawing in O ( n \u22c5 2 2 log 2 \u2061 n log \u2061 n ) area."}, {"paper_id": "11208684", "adju_relevance": 0, "title": "Fast edge splitting and Edmonds' arborescence construction for unweighted graphs", "background_label": "Given an unweighted undirected or directed graph with <i>n</i> vertices, <i>m</i> edges and edge connectivity <i>c</i>, we present a new deterministic algorithm for edge splitting. Our first application is a sub-quadratic (in <i>n</i>) algorithm to construct Edmonds' arborescences. A classical result of Edmonds [5] shows that an unweighted directed graph with <i>c</i> edge-disjoint paths from any particular vertex <i>r</i> to every other vertex has exactly <i>c</i> edge-disjoint arborescences rooted at <i>r.</i> For a <i>c</i> edge connected unweighted undirected graph, the same theorem holds on the digraph obtained by replacing each undirected edge by two directed edges, one in each direction.", "method_label": "Our algorithm splits-off any specified subset <i>S</i> of vertices satisfying standard conditions (even degree for the undirected case and in-degree \u2265 out-degree for the directed case) while maintaining connectivity <i>c</i> for vertices outside <i>S</i> in \u00d5(<i>m</i>+<i>nc</i><sup>2</sup>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. This improves the current best deterministic time bounds due to Gabow [8], who splits-off a <i>single</i> vertex in \u00d5(<i>nc</i><sup>2</sup>+<i>m</i>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. The current fastest construction of these arborescences by Gabow [7] takes <i>\u00d5</i>(<i>n</i><sup>2</sup><i>c</i><sup>2</sup>) time. Our algorithm takes \u00d5(<i>nc</i><sup>3</sup>+<i>m</i>) time for the undirected case and \u00d5(<i>nc</i><sup>4</sup>+<i>mc</i>) time for the directed case. The second application of our splitting algorithm is a new Steiner edge connectivity algorithm for undirected graphs which matches the best known bound of \u00d5(<i>nc</i><sup>2</sup> + <i>m</i>) time due to Bhalgat et al [3].", "result_label": "Further, for appropriate ranges of <i>n</i>, <i>c</i>, |<i>S</i>| it improves the current best randomized bounds due to Bencz\u00far and Karger [2], who split-off a single vertex in an undirected graph in \u00d5(<i>n</i><sup>2</sup>) Monte Carlo time. We give two applications of our edge splitting algorithms. Finally, our algorithm can also be viewed as an alternative proof for existential edge splitting theorems due to Lov\u00e1sz [9] and Mader [11].", "abstract": "Given an unweighted undirected or directed graph with <i>n</i> vertices, <i>m</i> edges and edge connectivity <i>c</i>, we present a new deterministic algorithm for edge splitting. Our algorithm splits-off any specified subset <i>S</i> of vertices satisfying standard conditions (even degree for the undirected case and in-degree \u2265 out-degree for the directed case) while maintaining connectivity <i>c</i> for vertices outside <i>S</i> in \u00d5(<i>m</i>+<i>nc</i><sup>2</sup>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. Our algorithm splits-off any specified subset <i>S</i> of vertices satisfying standard conditions (even degree for the undirected case and in-degree \u2265 out-degree for the directed case) while maintaining connectivity <i>c</i> for vertices outside <i>S</i> in \u00d5(<i>m</i>+<i>nc</i><sup>2</sup>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. This improves the current best deterministic time bounds due to Gabow [8], who splits-off a <i>single</i> vertex in \u00d5(<i>nc</i><sup>2</sup>+<i>m</i>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. Further, for appropriate ranges of <i>n</i>, <i>c</i>, |<i>S</i>| it improves the current best randomized bounds due to Bencz\u00far and Karger [2], who split-off a single vertex in an undirected graph in \u00d5(<i>n</i><sup>2</sup>) Monte Carlo time. Further, for appropriate ranges of <i>n</i>, <i>c</i>, |<i>S</i>| it improves the current best randomized bounds due to Bencz\u00far and Karger [2], who split-off a single vertex in an undirected graph in \u00d5(<i>n</i><sup>2</sup>) Monte Carlo time. We give two applications of our edge splitting algorithms. Given an unweighted undirected or directed graph with <i>n</i> vertices, <i>m</i> edges and edge connectivity <i>c</i>, we present a new deterministic algorithm for edge splitting. Our first application is a sub-quadratic (in <i>n</i>) algorithm to construct Edmonds' arborescences. Given an unweighted undirected or directed graph with <i>n</i> vertices, <i>m</i> edges and edge connectivity <i>c</i>, we present a new deterministic algorithm for edge splitting. Our first application is a sub-quadratic (in <i>n</i>) algorithm to construct Edmonds' arborescences. A classical result of Edmonds [5] shows that an unweighted directed graph with <i>c</i> edge-disjoint paths from any particular vertex <i>r</i> to every other vertex has exactly <i>c</i> edge-disjoint arborescences rooted at <i>r.</i> For a <i>c</i> edge connected unweighted undirected graph, the same theorem holds on the digraph obtained by replacing each undirected edge by two directed edges, one in each direction. Our algorithm splits-off any specified subset <i>S</i> of vertices satisfying standard conditions (even degree for the undirected case and in-degree \u2265 out-degree for the directed case) while maintaining connectivity <i>c</i> for vertices outside <i>S</i> in \u00d5(<i>m</i>+<i>nc</i><sup>2</sup>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. This improves the current best deterministic time bounds due to Gabow [8], who splits-off a <i>single</i> vertex in \u00d5(<i>nc</i><sup>2</sup>+<i>m</i>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. The current fastest construction of these arborescences by Gabow [7] takes <i>\u00d5</i>(<i>n</i><sup>2</sup><i>c</i><sup>2</sup>) time. Our algorithm splits-off any specified subset <i>S</i> of vertices satisfying standard conditions (even degree for the undirected case and in-degree \u2265 out-degree for the directed case) while maintaining connectivity <i>c</i> for vertices outside <i>S</i> in \u00d5(<i>m</i>+<i>nc</i><sup>2</sup>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. This improves the current best deterministic time bounds due to Gabow [8], who splits-off a <i>single</i> vertex in \u00d5(<i>nc</i><sup>2</sup>+<i>m</i>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. The current fastest construction of these arborescences by Gabow [7] takes <i>\u00d5</i>(<i>n</i><sup>2</sup><i>c</i><sup>2</sup>) time. Our algorithm takes \u00d5(<i>nc</i><sup>3</sup>+<i>m</i>) time for the undirected case and \u00d5(<i>nc</i><sup>4</sup>+<i>mc</i>) time for the directed case. Our algorithm splits-off any specified subset <i>S</i> of vertices satisfying standard conditions (even degree for the undirected case and in-degree \u2265 out-degree for the directed case) while maintaining connectivity <i>c</i> for vertices outside <i>S</i> in \u00d5(<i>m</i>+<i>nc</i><sup>2</sup>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. This improves the current best deterministic time bounds due to Gabow [8], who splits-off a <i>single</i> vertex in \u00d5(<i>nc</i><sup>2</sup>+<i>m</i>) time for an undirected graph and \u00d5(<i>mc</i>) time for a directed graph. The current fastest construction of these arborescences by Gabow [7] takes <i>\u00d5</i>(<i>n</i><sup>2</sup><i>c</i><sup>2</sup>) time. Our algorithm takes \u00d5(<i>nc</i><sup>3</sup>+<i>m</i>) time for the undirected case and \u00d5(<i>nc</i><sup>4</sup>+<i>mc</i>) time for the directed case. The second application of our splitting algorithm is a new Steiner edge connectivity algorithm for undirected graphs which matches the best known bound of \u00d5(<i>nc</i><sup>2</sup> + <i>m</i>) time due to Bhalgat et al [3]. Further, for appropriate ranges of <i>n</i>, <i>c</i>, |<i>S</i>| it improves the current best randomized bounds due to Bencz\u00far and Karger [2], who split-off a single vertex in an undirected graph in \u00d5(<i>n</i><sup>2</sup>) Monte Carlo time. We give two applications of our edge splitting algorithms. Finally, our algorithm can also be viewed as an alternative proof for existential edge splitting theorems due to Lov\u00e1sz [9] and Mader [11]."}, {"paper_id": "17892857", "adju_relevance": 0, "title": "The first approximated distributed algorithm for the minimum degree spanning tree problem on general graphs", "background_label": "In this paper we present the first distributed algorithm on general graphs for the minimum degree spanning tree problem. The problem is NP-hard in sequential.", "method_label": "Our algorithm gives a spanning tree of a degree at most 1 from the optimal. The resulting distributed algorithm is asynchronous, it works for named asynchronous arbitrary networks and achieves O(|V|) time complexity and O(|V| |E|) message complexity.", "abstract": "In this paper we present the first distributed algorithm on general graphs for the minimum degree spanning tree problem. In this paper we present the first distributed algorithm on general graphs for the minimum degree spanning tree problem. The problem is NP-hard in sequential. Our algorithm gives a spanning tree of a degree at most 1 from the optimal. Our algorithm gives a spanning tree of a degree at most 1 from the optimal. The resulting distributed algorithm is asynchronous, it works for named asynchronous arbitrary networks and achieves O(|V|) time complexity and O(|V| |E|) message complexity."}, {"paper_id": "16595283", "adju_relevance": 0, "title": "Vector Space Embedding of Undirected Graphs with Fixed-cardinality Vertex Sequences for Classification", "background_label": "Simple weighted undirected graphs with a fixed number of vertices and fixed vertex orderings can be used to represent data and patterns in a wide variety of scientific and engineering domains. Classification of such graphs by existing graph matching methods perform rather poorly because they do not exploit their specificity. As an alternative, methods relying on vector-space embedding hold promising potential.", "method_label": "We propose two such techniques that can be deployed as a front-end for any pattern recognition classifiers: one has low computational cost but generates high-dimensional spaces, while the other is more computationally demanding but can yield relatively low-dimensional vector space representations.", "result_label": "We show experimental results on an fMRI brain state decoding task and discuss the shortfalls of graph edit distance for the type of graph under consideration.", "abstract": "Simple weighted undirected graphs with a fixed number of vertices and fixed vertex orderings can be used to represent data and patterns in a wide variety of scientific and engineering domains. Simple weighted undirected graphs with a fixed number of vertices and fixed vertex orderings can be used to represent data and patterns in a wide variety of scientific and engineering domains. Classification of such graphs by existing graph matching methods perform rather poorly because they do not exploit their specificity. Simple weighted undirected graphs with a fixed number of vertices and fixed vertex orderings can be used to represent data and patterns in a wide variety of scientific and engineering domains. Classification of such graphs by existing graph matching methods perform rather poorly because they do not exploit their specificity. As an alternative, methods relying on vector-space embedding hold promising potential. We propose two such techniques that can be deployed as a front-end for any pattern recognition classifiers: one has low computational cost but generates high-dimensional spaces, while the other is more computationally demanding but can yield relatively low-dimensional vector space representations. We show experimental results on an fMRI brain state decoding task and discuss the shortfalls of graph edit distance for the type of graph under consideration."}, {"paper_id": "14325210", "adju_relevance": 0, "title": "Planar Subgraph Isomorphism Revisited", "background_label": "The problem of Subgraph Isomorphism is defined as follows: Given a pattern H and a host graph G on n vertices, does G contain a subgraph that is isomorphic to H? Eppstein [SODA 95, J'GAA 99] gives the first linear time algorithm for subgraph isomorphism for a fixed-size pattern, say of order k, and arbitrary planar host graph, improving upon the O(n^\\sqrt{k})-time algorithm when using the ``Color-coding'' technique of Alon et al [J'ACM 95].", "method_label": "Eppstein's algorithm runs in time k^O(k) n, that is, the dependency on k is superexponential. We solve an open problem posed in Eppstein's paper and improve the running time to 2^O(k) n, that is, single exponential in k while keeping the term in n linear. Next to deciding subgraph isomorphism, we can construct a solution and enumerate all solutions in the same asymptotic running time. We may list w subgraphs with an additive term O(w k) in the running time of our algorithm. We introduce the technique of\"embedded dynamic programming\"on a suitably structured graph decomposition, which exploits the topology of the underlying embeddings of the subgraph pattern (rather than of the host graph).", "result_label": "To achieve our results, we give an upper bound on the number of partial solutions in each dynamic programming step as a function of pattern size--as it turns out, for the planar subgraph isomorphism problem, that function is single exponential in the number of vertices in the pattern.", "abstract": "The problem of Subgraph Isomorphism is defined as follows: Given a pattern H and a host graph G on n vertices, does G contain a subgraph that is isomorphic to H? The problem of Subgraph Isomorphism is defined as follows: Given a pattern H and a host graph G on n vertices, does G contain a subgraph that is isomorphic to H? Eppstein [SODA 95, J'GAA 99] gives the first linear time algorithm for subgraph isomorphism for a fixed-size pattern, say of order k, and arbitrary planar host graph, improving upon the O(n^\\sqrt{k})-time algorithm when using the ``Color-coding'' technique of Alon et al [J'ACM 95]. Eppstein's algorithm runs in time k^O(k) n, that is, the dependency on k is superexponential. Eppstein's algorithm runs in time k^O(k) n, that is, the dependency on k is superexponential. We solve an open problem posed in Eppstein's paper and improve the running time to 2^O(k) n, that is, single exponential in k while keeping the term in n linear. Eppstein's algorithm runs in time k^O(k) n, that is, the dependency on k is superexponential. We solve an open problem posed in Eppstein's paper and improve the running time to 2^O(k) n, that is, single exponential in k while keeping the term in n linear. Next to deciding subgraph isomorphism, we can construct a solution and enumerate all solutions in the same asymptotic running time. Eppstein's algorithm runs in time k^O(k) n, that is, the dependency on k is superexponential. We solve an open problem posed in Eppstein's paper and improve the running time to 2^O(k) n, that is, single exponential in k while keeping the term in n linear. Next to deciding subgraph isomorphism, we can construct a solution and enumerate all solutions in the same asymptotic running time. We may list w subgraphs with an additive term O(w k) in the running time of our algorithm. Eppstein's algorithm runs in time k^O(k) n, that is, the dependency on k is superexponential. We solve an open problem posed in Eppstein's paper and improve the running time to 2^O(k) n, that is, single exponential in k while keeping the term in n linear. Next to deciding subgraph isomorphism, we can construct a solution and enumerate all solutions in the same asymptotic running time. We may list w subgraphs with an additive term O(w k) in the running time of our algorithm. We introduce the technique of\"embedded dynamic programming\"on a suitably structured graph decomposition, which exploits the topology of the underlying embeddings of the subgraph pattern (rather than of the host graph). To achieve our results, we give an upper bound on the number of partial solutions in each dynamic programming step as a function of pattern size--as it turns out, for the planar subgraph isomorphism problem, that function is single exponential in the number of vertices in the pattern."}, {"paper_id": "3058746", "adju_relevance": 0, "title": "Image registration with minimum spanning tree algorithm", "background_label": "Registration is a fundamental task in image processing and quite a few registration techniques have been developed in various fields.", "method_label": "In this paper we propose a novel graph-representation method for image registration with Renyi entropy as the dissimilarity metric between the images. The image matching is performed by minimizing the length of the minimum spanning tree (MST) which spans the graph generated from the overlapped images. Our method also takes advantage of the minimum k-point spanning tree (k-MST) approach to robustify the registration against spurious discrepancies in the images. The proposed algorithm is tested in two applications: registering magnetic resonance (MR) images, and registering an electro-optical image with a terrain height map.", "result_label": "In both cases the algorithm is shown to be accurate and robust.", "abstract": "Registration is a fundamental task in image processing and quite a few registration techniques have been developed in various fields. In this paper we propose a novel graph-representation method for image registration with Renyi entropy as the dissimilarity metric between the images. In this paper we propose a novel graph-representation method for image registration with Renyi entropy as the dissimilarity metric between the images. The image matching is performed by minimizing the length of the minimum spanning tree (MST) which spans the graph generated from the overlapped images. In this paper we propose a novel graph-representation method for image registration with Renyi entropy as the dissimilarity metric between the images. The image matching is performed by minimizing the length of the minimum spanning tree (MST) which spans the graph generated from the overlapped images. Our method also takes advantage of the minimum k-point spanning tree (k-MST) approach to robustify the registration against spurious discrepancies in the images. In this paper we propose a novel graph-representation method for image registration with Renyi entropy as the dissimilarity metric between the images. The image matching is performed by minimizing the length of the minimum spanning tree (MST) which spans the graph generated from the overlapped images. Our method also takes advantage of the minimum k-point spanning tree (k-MST) approach to robustify the registration against spurious discrepancies in the images. The proposed algorithm is tested in two applications: registering magnetic resonance (MR) images, and registering an electro-optical image with a terrain height map. In both cases the algorithm is shown to be accurate and robust."}, {"paper_id": "1365409", "adju_relevance": 0, "title": "Solving Shortest Paths Efficiently on Nearly Acyclic Directed Graphs", "background_label": "AbstractShortest path problems can be solved very efficiently when a directed graph is nearly acyclic. Earlier results defined a graph decomposition, now called the 1-dominator set, which consists of a unique collection of acyclic structures with each single acyclic structure dominated by a single associated trigger vertex. In this framework, a specialised shortest path algorithm only spends delete-min operations on trigger vertices, thereby making the computation of shortest paths through non-trigger vertices easier.", "method_label": "A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. Here m and n respectively denote the number of edges and vertices in the graph, while r denotes the number of trigger vertices. A new algorithm presented in this paper computes the 1-dominator set in just O(m) time. This can be integrated as part of the O(m+r log r) time spent solving single-source, improving on the value of r obtained by the earlier tree-decomposition single-source algorithm. In addition, a new bi-directional form of 1-dominator set is presented, which further improves the value of r by defining acyclic structures in both directions over edges in the graph. The bi-directional 1-dominator set can similarly be computed in O(m) time and included as part of the O(m + r log r) time spent computing single-source.", "result_label": "This paper also presents a new all-pairs algorithm under the more general framework where r is defined as the size of any predetermined feedback vertex set of the graph, improving the previous all-pairs time complexity from O(mn + nr 2 ) to O(mn + r 3 ).", "abstract": "AbstractShortest path problems can be solved very efficiently when a directed graph is nearly acyclic. AbstractShortest path problems can be solved very efficiently when a directed graph is nearly acyclic. Earlier results defined a graph decomposition, now called the 1-dominator set, which consists of a unique collection of acyclic structures with each single acyclic structure dominated by a single associated trigger vertex. AbstractShortest path problems can be solved very efficiently when a directed graph is nearly acyclic. Earlier results defined a graph decomposition, now called the 1-dominator set, which consists of a unique collection of acyclic structures with each single acyclic structure dominated by a single associated trigger vertex. In this framework, a specialised shortest path algorithm only spends delete-min operations on trigger vertices, thereby making the computation of shortest paths through non-trigger vertices easier. A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. Here m and n respectively denote the number of edges and vertices in the graph, while r denotes the number of trigger vertices. A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. Here m and n respectively denote the number of edges and vertices in the graph, while r denotes the number of trigger vertices. A new algorithm presented in this paper computes the 1-dominator set in just O(m) time. A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. Here m and n respectively denote the number of edges and vertices in the graph, while r denotes the number of trigger vertices. A new algorithm presented in this paper computes the 1-dominator set in just O(m) time. This can be integrated as part of the O(m+r log r) time spent solving single-source, improving on the value of r obtained by the earlier tree-decomposition single-source algorithm. A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. Here m and n respectively denote the number of edges and vertices in the graph, while r denotes the number of trigger vertices. A new algorithm presented in this paper computes the 1-dominator set in just O(m) time. This can be integrated as part of the O(m+r log r) time spent solving single-source, improving on the value of r obtained by the earlier tree-decomposition single-source algorithm. In addition, a new bi-directional form of 1-dominator set is presented, which further improves the value of r by defining acyclic structures in both directions over edges in the graph. A previously presented algorithm computed the 1-dominator set in O(mn) worst-case time, which allowed it to be integrated as part of an O(mn + nr log r) time all-pairs algorithm. Here m and n respectively denote the number of edges and vertices in the graph, while r denotes the number of trigger vertices. A new algorithm presented in this paper computes the 1-dominator set in just O(m) time. This can be integrated as part of the O(m+r log r) time spent solving single-source, improving on the value of r obtained by the earlier tree-decomposition single-source algorithm. In addition, a new bi-directional form of 1-dominator set is presented, which further improves the value of r by defining acyclic structures in both directions over edges in the graph. The bi-directional 1-dominator set can similarly be computed in O(m) time and included as part of the O(m + r log r) time spent computing single-source. This paper also presents a new all-pairs algorithm under the more general framework where r is defined as the size of any predetermined feedback vertex set of the graph, improving the previous all-pairs time complexity from O(mn + nr 2 ) to O(mn + r 3 )."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "9063046", "adju_relevance": 0, "title": "Triggered Minimum Spanning Tree for distributed coverage with connectivity maintenance", "background_label": "We study the problem of distributed coverage with connectivity maintenance for robot networks with range-limited communication and sensing.We let the robots perform coverage while keeping a set of communication links. The structure that ensures that the network does not get disconnected, and that provides the highest freedom of robot motions is the Minimum-distance Spanning Tree (MST). As robots move to perform the coverage task, their distances change as well and this MST becomes obsolete. Computing the MST at each coverage step may be too communication demanding.", "abstract": "We study the problem of distributed coverage with connectivity maintenance for robot networks with range-limited communication and sensing.We let the robots perform coverage while keeping a set of communication links. We study the problem of distributed coverage with connectivity maintenance for robot networks with range-limited communication and sensing.We let the robots perform coverage while keeping a set of communication links. The structure that ensures that the network does not get disconnected, and that provides the highest freedom of robot motions is the Minimum-distance Spanning Tree (MST). We study the problem of distributed coverage with connectivity maintenance for robot networks with range-limited communication and sensing.We let the robots perform coverage while keeping a set of communication links. The structure that ensures that the network does not get disconnected, and that provides the highest freedom of robot motions is the Minimum-distance Spanning Tree (MST). As robots move to perform the coverage task, their distances change as well and this MST becomes obsolete. We study the problem of distributed coverage with connectivity maintenance for robot networks with range-limited communication and sensing.We let the robots perform coverage while keeping a set of communication links. The structure that ensures that the network does not get disconnected, and that provides the highest freedom of robot motions is the Minimum-distance Spanning Tree (MST). As robots move to perform the coverage task, their distances change as well and this MST becomes obsolete. Computing the MST at each coverage step may be too communication demanding."}, {"paper_id": "21756035", "adju_relevance": 0, "title": "Approximate Graph Edit Distance in Quadratic Time.", "background_label": "Graph edit distance is one of the most flexible and general graph matching models available. The major drawback of graph edit distance, however, is its computational complexity that restricts its applicability to graphs of rather small size. Recently the authors of the present paper introduced a general approximation framework for the graph edit distance problem.", "abstract": "Graph edit distance is one of the most flexible and general graph matching models available. Graph edit distance is one of the most flexible and general graph matching models available. The major drawback of graph edit distance, however, is its computational complexity that restricts its applicability to graphs of rather small size. Graph edit distance is one of the most flexible and general graph matching models available. The major drawback of graph edit distance, however, is its computational complexity that restricts its applicability to graphs of rather small size. Recently the authors of the present paper introduced a general approximation framework for the graph edit distance problem."}, {"paper_id": "18208524", "adju_relevance": 0, "title": "Sensitivity Analysis of Minimum Spanning Trees in Sub-Inverse-Ackermann Time", "background_label": "We present a deterministic algorithm for computing the sensitivity of a minimum spanning tree (MST) or shortest path tree in $O(m\\log\\alpha(m,n))$ time, where $\\alpha$ is the inverse-Ackermann function.", "method_label": "This improves upon a long standing bound of $O(m\\alpha(m,n))$ established by Tarjan. Our algorithms are based on an efficient split-findmin data structure, which maintains a collection of sequences of weighted elements that may be split into smaller subsequences. As far as we are aware, our split-findmin algorithm is the first with superlinear but sub-inverse-Ackermann complexity. We also give a reduction from MST sensitivity to the MST problem itself.", "result_label": "Together with the randomized linear time MST algorithm of Karger, Klein, and Tarjan, this gives another randomized linear time MST sensitivity algoritm.", "abstract": "We present a deterministic algorithm for computing the sensitivity of a minimum spanning tree (MST) or shortest path tree in $O(m\\log\\alpha(m,n))$ time, where $\\alpha$ is the inverse-Ackermann function. This improves upon a long standing bound of $O(m\\alpha(m,n))$ established by Tarjan. This improves upon a long standing bound of $O(m\\alpha(m,n))$ established by Tarjan. Our algorithms are based on an efficient split-findmin data structure, which maintains a collection of sequences of weighted elements that may be split into smaller subsequences. This improves upon a long standing bound of $O(m\\alpha(m,n))$ established by Tarjan. Our algorithms are based on an efficient split-findmin data structure, which maintains a collection of sequences of weighted elements that may be split into smaller subsequences. As far as we are aware, our split-findmin algorithm is the first with superlinear but sub-inverse-Ackermann complexity. This improves upon a long standing bound of $O(m\\alpha(m,n))$ established by Tarjan. Our algorithms are based on an efficient split-findmin data structure, which maintains a collection of sequences of weighted elements that may be split into smaller subsequences. As far as we are aware, our split-findmin algorithm is the first with superlinear but sub-inverse-Ackermann complexity. We also give a reduction from MST sensitivity to the MST problem itself. Together with the randomized linear time MST algorithm of Karger, Klein, and Tarjan, this gives another randomized linear time MST sensitivity algoritm."}, {"paper_id": "8386282", "adju_relevance": 0, "title": "A characterization of minimum spanning tree-like metric spaces", "background_label": "Recent years have witnessed a surge of biological interest in the minimum spanning tree (MST) problem for its relevance to automatic model construction using the distances between data points. Despite the increasing use of MST algorithms for this purpose, the goodness-of-fit of an MST to the data is often elusive because no quantitative criteria have been developed to measure it.", "method_label": "Motivated by this, we provide a necessary and sufficient condition to ensure that a metric space on n points can be represented by a fully labeled tree on n vertices, and thereby determine when an MST preserves all pairwise distances between points in a finite metric space.", "abstract": "Recent years have witnessed a surge of biological interest in the minimum spanning tree (MST) problem for its relevance to automatic model construction using the distances between data points. Recent years have witnessed a surge of biological interest in the minimum spanning tree (MST) problem for its relevance to automatic model construction using the distances between data points. Despite the increasing use of MST algorithms for this purpose, the goodness-of-fit of an MST to the data is often elusive because no quantitative criteria have been developed to measure it. Motivated by this, we provide a necessary and sufficient condition to ensure that a metric space on n points can be represented by a fully labeled tree on n vertices, and thereby determine when an MST preserves all pairwise distances between points in a finite metric space."}, {"paper_id": "8203079", "adju_relevance": 0, "title": "I/O efficient: computing SCCs in massive graphs", "background_label": "A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, or reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. Therefore, finding all SCCs in a directed graph G is a critical operation. In this paper, we study new I/O efficient semi-external algorithms to find all SCCs for a massive directed graph G that cannot reside in main memory entirely. To overcome the deficiency of the existing DFS based semi-external algorithm that heavily relies on a total order, we explore a weak order based on which we investigate new algorithms.", "method_label": "The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. In addition, we propose a new single phase algorithm, which combines the tree construction and tree search phases into a single phase, with three new optimization techniques. They are early acceptance, early rejection, and batch processing. By the single phase algorithm with the new optimization techniques, we can significantly reduce the number of I/Os and CPU cost.", "result_label": "However, when a graph cannot resident entirely in the main memory, the existing external or semi-external algorithms to find all SCCs have limitation to achieve high I/O efficiency. We conduct extensive experimental studies using 4 real datasets including a massive real dataset, and several synthetic datasets to confirm the I/O efficiency of our approaches.", "abstract": "A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, or reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, or reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. Therefore, finding all SCCs in a directed graph G is a critical operation. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. However, when a graph cannot resident entirely in the main memory, the existing external or semi-external algorithms to find all SCCs have limitation to achieve high I/O efficiency. A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, or reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. Therefore, finding all SCCs in a directed graph G is a critical operation. In this paper, we study new I/O efficient semi-external algorithms to find all SCCs for a massive directed graph G that cannot reside in main memory entirely. A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, or reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. Therefore, finding all SCCs in a directed graph G is a critical operation. In this paper, we study new I/O efficient semi-external algorithms to find all SCCs for a massive directed graph G that cannot reside in main memory entirely. To overcome the deficiency of the existing DFS based semi-external algorithm that heavily relies on a total order, we explore a weak order based on which we investigate new algorithms. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. We propose a new two phase algorithm, namely, tree construction and tree search. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. In addition, we propose a new single phase algorithm, which combines the tree construction and tree search phases into a single phase, with three new optimization techniques. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. In addition, we propose a new single phase algorithm, which combines the tree construction and tree search phases into a single phase, with three new optimization techniques. They are early acceptance, early rejection, and batch processing. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. In addition, we propose a new single phase algorithm, which combines the tree construction and tree search phases into a single phase, with three new optimization techniques. They are early acceptance, early rejection, and batch processing. By the single phase algorithm with the new optimization techniques, we can significantly reduce the number of I/Os and CPU cost. However, when a graph cannot resident entirely in the main memory, the existing external or semi-external algorithms to find all SCCs have limitation to achieve high I/O efficiency. We conduct extensive experimental studies using 4 real datasets including a massive real dataset, and several synthetic datasets to confirm the I/O efficiency of our approaches."}, {"paper_id": "14135246", "adju_relevance": 0, "title": "Maximum Matching on Trees in the Online Preemptive and the Incremental Dynamic Graph Models", "background_label": "We study the Maximum Cardinality Matching (MCM) and the Maximum Weight Matching (MWM) problems, on trees and on some special classes of graphs, in the Online Preemptive and the Incremental Dynamic Graph models. In the {\\em Online Preemptive} model, the edges of a graph are revealed one by one and the algorithm is required to always maintain a valid matching. The same lower bound applies for MWM. In this paper we show that some of the results can be improved in the case of trees and some special classes of graphs. In the online preemptive model, we present a $64/33$-competitive (in expectation) randomized algorithm for MCM on trees.", "method_label": "On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. In this model, the complexity of the problems is settled for deterministic algorithms. Epstein et al. Inspired by the above mentioned algorithm for MCM, we present the main result of the paper, a randomized algorithm for MCM with a\"worst case\"update time of $O(1)$, in the incremental dynamic graph model, which is $3/2$-approximate (in expectation) on trees, and $1.8$-approximate (in expectation) on general graphs with maximum degree $3$. Note that this algorithm works only against an oblivious adversary. Hence, we derandomize this algorithm, and give a $(3/2 + \\epsilon)$-approximate deterministic algorithm for MCM on trees, with an amortized update time of $O(1/\\epsilon)$.", "result_label": "gave a $5.356$-competitive randomized algorithm for MWM, and also proved a lower bound of $1.693$ for MCM. We also present a minor result for MWM in the online preemptive model, a $3$-competitive (in expectation) randomized algorithm on growing trees (where the input revealed upto any stage is always a tree, i.e. a new edge never connects two disconnected trees).", "abstract": "We study the Maximum Cardinality Matching (MCM) and the Maximum Weight Matching (MWM) problems, on trees and on some special classes of graphs, in the Online Preemptive and the Incremental Dynamic Graph models. We study the Maximum Cardinality Matching (MCM) and the Maximum Weight Matching (MWM) problems, on trees and on some special classes of graphs, in the Online Preemptive and the Incremental Dynamic Graph models. In the {\\em Online Preemptive} model, the edges of a graph are revealed one by one and the algorithm is required to always maintain a valid matching. On seeing an edge, the algorithm has to either accept or reject the edge. On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. In this model, the complexity of the problems is settled for deterministic algorithms. On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. In this model, the complexity of the problems is settled for deterministic algorithms. Epstein et al. gave a $5.356$-competitive randomized algorithm for MWM, and also proved a lower bound of $1.693$ for MCM. We study the Maximum Cardinality Matching (MCM) and the Maximum Weight Matching (MWM) problems, on trees and on some special classes of graphs, in the Online Preemptive and the Incremental Dynamic Graph models. In the {\\em Online Preemptive} model, the edges of a graph are revealed one by one and the algorithm is required to always maintain a valid matching. The same lower bound applies for MWM. We study the Maximum Cardinality Matching (MCM) and the Maximum Weight Matching (MWM) problems, on trees and on some special classes of graphs, in the Online Preemptive and the Incremental Dynamic Graph models. In the {\\em Online Preemptive} model, the edges of a graph are revealed one by one and the algorithm is required to always maintain a valid matching. The same lower bound applies for MWM. In this paper we show that some of the results can be improved in the case of trees and some special classes of graphs. We study the Maximum Cardinality Matching (MCM) and the Maximum Weight Matching (MWM) problems, on trees and on some special classes of graphs, in the Online Preemptive and the Incremental Dynamic Graph models. In the {\\em Online Preemptive} model, the edges of a graph are revealed one by one and the algorithm is required to always maintain a valid matching. The same lower bound applies for MWM. In this paper we show that some of the results can be improved in the case of trees and some special classes of graphs. In the online preemptive model, we present a $64/33$-competitive (in expectation) randomized algorithm for MCM on trees. On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. In this model, the complexity of the problems is settled for deterministic algorithms. Epstein et al. Inspired by the above mentioned algorithm for MCM, we present the main result of the paper, a randomized algorithm for MCM with a\"worst case\"update time of $O(1)$, in the incremental dynamic graph model, which is $3/2$-approximate (in expectation) on trees, and $1.8$-approximate (in expectation) on general graphs with maximum degree $3$. On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. In this model, the complexity of the problems is settled for deterministic algorithms. Epstein et al. Inspired by the above mentioned algorithm for MCM, we present the main result of the paper, a randomized algorithm for MCM with a\"worst case\"update time of $O(1)$, in the incremental dynamic graph model, which is $3/2$-approximate (in expectation) on trees, and $1.8$-approximate (in expectation) on general graphs with maximum degree $3$. Note that this algorithm works only against an oblivious adversary. On seeing an edge, the algorithm has to either accept or reject the edge. If accepted, then the adjacent edges are discarded, and all rejections are permanent. In this model, the complexity of the problems is settled for deterministic algorithms. Epstein et al. Inspired by the above mentioned algorithm for MCM, we present the main result of the paper, a randomized algorithm for MCM with a\"worst case\"update time of $O(1)$, in the incremental dynamic graph model, which is $3/2$-approximate (in expectation) on trees, and $1.8$-approximate (in expectation) on general graphs with maximum degree $3$. Note that this algorithm works only against an oblivious adversary. Hence, we derandomize this algorithm, and give a $(3/2 + \\epsilon)$-approximate deterministic algorithm for MCM on trees, with an amortized update time of $O(1/\\epsilon)$. gave a $5.356$-competitive randomized algorithm for MWM, and also proved a lower bound of $1.693$ for MCM. We also present a minor result for MWM in the online preemptive model, a $3$-competitive (in expectation) randomized algorithm on growing trees (where the input revealed upto any stage is always a tree, i.e. gave a $5.356$-competitive randomized algorithm for MWM, and also proved a lower bound of $1.693$ for MCM. We also present a minor result for MWM in the online preemptive model, a $3$-competitive (in expectation) randomized algorithm on growing trees (where the input revealed upto any stage is always a tree, i.e. a new edge never connects two disconnected trees)."}, {"paper_id": "16129556", "adju_relevance": 0, "title": "An Efficient Algorithm for Solving Pseudo Clique Enumeration Problem", "background_label": "The problem of finding dense structures in a given graph is quite basic in informatics including data mining and data engineering. Clique is a popular model to represent dense structures, and widely used because of its simplicity and ease in handling. Pseudo cliques are natural extension of cliques which are subgraphs obtained by removing small number of edges from cliques.", "method_label": "We here define a pseudo clique by a subgraph such that the ratio of the number of its edges compared to that of the clique with the same number of vertices is no less than a given threshold value.", "abstract": "The problem of finding dense structures in a given graph is quite basic in informatics including data mining and data engineering. The problem of finding dense structures in a given graph is quite basic in informatics including data mining and data engineering. Clique is a popular model to represent dense structures, and widely used because of its simplicity and ease in handling. The problem of finding dense structures in a given graph is quite basic in informatics including data mining and data engineering. Clique is a popular model to represent dense structures, and widely used because of its simplicity and ease in handling. Pseudo cliques are natural extension of cliques which are subgraphs obtained by removing small number of edges from cliques. We here define a pseudo clique by a subgraph such that the ratio of the number of its edges compared to that of the clique with the same number of vertices is no less than a given threshold value."}, {"paper_id": "1230550", "adju_relevance": 0, "title": "An evolutionary algorithm with solution archives and bounding extension for the generalized minimum spanning tree problem", "background_label": "We consider the recently proposed concept of enhancing an evolutionary algorithm (EA) with a complete solution archive. It stores evaluated solutions during the optimization in order to detect duplicates and to efficiently transform them into yet unconsidered solutions.", "method_label": "For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. This extension enables the EA to concentrate the search on promising areas of the solution space. Similarly to the classical branch-and-bound technique, bounds are obtained via primal and dual heuristics. As an application we consider the generalized minimum spanning tree problem where we are given a graph with nodes partitioned into clusters and exactly one node from each cluster must be connected in the cheapest way. As the EA uses operators based on two dual representations, we exploit two corresponding tries that complement each other.", "result_label": "Test results on TSPlib instances document the strength of this concept and that it can compete with the leading metaheuristics for this problem in the literature.", "abstract": "We consider the recently proposed concept of enhancing an evolutionary algorithm (EA) with a complete solution archive. We consider the recently proposed concept of enhancing an evolutionary algorithm (EA) with a complete solution archive. It stores evaluated solutions during the optimization in order to detect duplicates and to efficiently transform them into yet unconsidered solutions. For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. This extension enables the EA to concentrate the search on promising areas of the solution space. For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. This extension enables the EA to concentrate the search on promising areas of the solution space. Similarly to the classical branch-and-bound technique, bounds are obtained via primal and dual heuristics. For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. This extension enables the EA to concentrate the search on promising areas of the solution space. Similarly to the classical branch-and-bound technique, bounds are obtained via primal and dual heuristics. As an application we consider the generalized minimum spanning tree problem where we are given a graph with nodes partitioned into clusters and exactly one node from each cluster must be connected in the cheapest way. For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. This extension enables the EA to concentrate the search on promising areas of the solution space. Similarly to the classical branch-and-bound technique, bounds are obtained via primal and dual heuristics. As an application we consider the generalized minimum spanning tree problem where we are given a graph with nodes partitioned into clusters and exactly one node from each cluster must be connected in the cheapest way. As the EA uses operators based on two dual representations, we exploit two corresponding tries that complement each other. Test results on TSPlib instances document the strength of this concept and that it can compete with the leading metaheuristics for this problem in the literature."}, {"paper_id": "6909069", "adju_relevance": 0, "title": "Parallel Clustering Algorithm for Large Data Sets with Applications in Bioinformatics", "background_label": "Large sets of bioinformatical data provide a challenge in time consumption while solving the cluster identification problem, and that is why a parallel algorithm is so needed for identifying dense clusters in a noisy background.", "method_label": "Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our high-level strategy for the parallel MST construction algorithm is to first partition the graph, then construct MSTs for the partitioned subgraphs and auxiliary bipartite graphs based on the subgraphs, and finally merge these MSTs to derive an MST of the original graph.", "result_label": "The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. We have implemented the clustering algorithm as the software CLUMP.", "abstract": "Large sets of bioinformatical data provide a challenge in time consumption while solving the cluster identification problem, and that is why a parallel algorithm is so needed for identifying dense clusters in a noisy background. Our algorithm works on a graph representation of the data set to be analyzed. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our high-level strategy for the parallel MST construction algorithm is to first partition the graph, then construct MSTs for the partitioned subgraphs and auxiliary bipartite graphs based on the subgraphs, and finally merge these MSTs to derive an MST of the original graph. The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. We have implemented the clustering algorithm as the software CLUMP."}, {"paper_id": "8348754", "adju_relevance": 0, "title": "A Fast Quartet Tree Heuristic for Hierarchical Clustering", "background_label": "The Minimum Quartet Tree Cost problem is to construct an optimal weight tree from the $3{n \\choose 4}$ weighted quartet topologies on $n$ objects, where optimality means that the summed weight of the embedded quartet topologies is optimal (so it can be the case that the optimal tree embeds all quartets as nonoptimal topologies).", "method_label": "We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We also present a greatly improved heuristic, reducing the running time by a factor of order a thousand to ten thousand. All this is implemented and available, as part of the CompLearn package.", "result_label": "We compare performance and running time of the original and improved versions with those of UPGMA, BioNJ, and NJ, as implemented in the SplitsTree package on genomic data for which the latter are optimized.", "abstract": "The Minimum Quartet Tree Cost problem is to construct an optimal weight tree from the $3{n \\choose 4}$ weighted quartet topologies on $n$ objects, where optimality means that the summed weight of the embedded quartet topologies is optimal (so it can be the case that the optimal tree embeds all quartets as nonoptimal topologies). We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We also present a greatly improved heuristic, reducing the running time by a factor of order a thousand to ten thousand. We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We also present a greatly improved heuristic, reducing the running time by a factor of order a thousand to ten thousand. All this is implemented and available, as part of the CompLearn package. We compare performance and running time of the original and improved versions with those of UPGMA, BioNJ, and NJ, as implemented in the SplitsTree package on genomic data for which the latter are optimized."}, {"paper_id": "17070346", "adju_relevance": 0, "title": "Finding Optimal 1-Endpoint-Crossing Trees", "background_label": "Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. For 95.8\u201399.8% of dependency parses in various natural language treebanks, whenever an edge is crossed, the edges that cross it all have a common vertex.", "method_label": "The optimal dependency tree that satisfies this 1-Endpoint-Crossing property can be found with an O(n4) parsing algorithm that recursively combines forests over intervals with one exterior point.", "result_label": "1-Endpoint-Crossing trees also have natural connections to linguistics and another class of graphs that has been studied in NLP.", "abstract": "Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. For 95.8\u201399.8% of dependency parses in various natural language treebanks, whenever an edge is crossed, the edges that cross it all have a common vertex. The optimal dependency tree that satisfies this 1-Endpoint-Crossing property can be found with an O(n4) parsing algorithm that recursively combines forests over intervals with one exterior point. 1-Endpoint-Crossing trees also have natural connections to linguistics and another class of graphs that has been studied in NLP."}, {"paper_id": "1515006", "adju_relevance": 0, "title": "Directed Acyclic Graph Continuous Max-Flow Image Segmentation for Unconstrained Label Orderings", "background_label": "Label ordering, the specification of subset\u2013superset relationships for segmentation labels, has been of increasing interest in image segmentation as they allow for complex regions to be represented as a collection of simple parts. Recent advances in continuous max-flow segmentation have widely expanded the possible label orderings from binary background/foreground problems to extendable frameworks in which the label ordering can be specified.", "abstract": "Label ordering, the specification of subset\u2013superset relationships for segmentation labels, has been of increasing interest in image segmentation as they allow for complex regions to be represented as a collection of simple parts. Label ordering, the specification of subset\u2013superset relationships for segmentation labels, has been of increasing interest in image segmentation as they allow for complex regions to be represented as a collection of simple parts. Recent advances in continuous max-flow segmentation have widely expanded the possible label orderings from binary background/foreground problems to extendable frameworks in which the label ordering can be specified."}, {"paper_id": "7604320", "adju_relevance": 0, "title": "MST-based Visual Parsing of Online Handwritten Mathematical Expressions", "method_label": "We develop a Maximum Spanning Tree (MST) based parser using Edmonds' algorithm, which extracts an MST from a directed Line-of-Sight graph in two passes. First, symbols are segmented by grouping input strokes, and then symbols and symbol pair spatial relationships are labeled. The time complexity of our MST-based parsing is lower than the time complexity of CYK parsing with 2-D Context-Free grammars. Also, our MST-based parser obtains higher formula structure and expression rates than published techniques using CYK parsing when starting from valid symbols. This parsing technique could be extended to include n-grams or other language constraints, and might be used for other notations.", "abstract": "We develop a Maximum Spanning Tree (MST) based parser using Edmonds' algorithm, which extracts an MST from a directed Line-of-Sight graph in two passes. We develop a Maximum Spanning Tree (MST) based parser using Edmonds' algorithm, which extracts an MST from a directed Line-of-Sight graph in two passes. First, symbols are segmented by grouping input strokes, and then symbols and symbol pair spatial relationships are labeled. We develop a Maximum Spanning Tree (MST) based parser using Edmonds' algorithm, which extracts an MST from a directed Line-of-Sight graph in two passes. First, symbols are segmented by grouping input strokes, and then symbols and symbol pair spatial relationships are labeled. The time complexity of our MST-based parsing is lower than the time complexity of CYK parsing with 2-D Context-Free grammars. We develop a Maximum Spanning Tree (MST) based parser using Edmonds' algorithm, which extracts an MST from a directed Line-of-Sight graph in two passes. First, symbols are segmented by grouping input strokes, and then symbols and symbol pair spatial relationships are labeled. The time complexity of our MST-based parsing is lower than the time complexity of CYK parsing with 2-D Context-Free grammars. Also, our MST-based parser obtains higher formula structure and expression rates than published techniques using CYK parsing when starting from valid symbols. We develop a Maximum Spanning Tree (MST) based parser using Edmonds' algorithm, which extracts an MST from a directed Line-of-Sight graph in two passes. First, symbols are segmented by grouping input strokes, and then symbols and symbol pair spatial relationships are labeled. The time complexity of our MST-based parsing is lower than the time complexity of CYK parsing with 2-D Context-Free grammars. Also, our MST-based parser obtains higher formula structure and expression rates than published techniques using CYK parsing when starting from valid symbols. This parsing technique could be extended to include n-grams or other language constraints, and might be used for other notations."}, {"paper_id": "22188406", "adju_relevance": 0, "title": "Graph Homomorphism Reconfiguration and Frozen $H$-Colourings", "background_label": "For a fixed graph $H$, the reconfiguration problem for $H$-colourings (i.e.", "abstract": "For a fixed graph $H$, the reconfiguration problem for $H$-colourings (i.e."}, {"paper_id": "13362841", "adju_relevance": 0, "title": "Parsing with Traces: An $O(n^4)$ Algorithm and a Structural Representation", "background_label": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.", "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons."}, {"paper_id": "13653862", "adju_relevance": 0, "title": "Graph sketching-based Space-efficient Data Clustering", "background_label": "In this paper, we address the problem of recovering arbitrary-shaped data clusters from datasets while facing \\emph{high space constraints}, as this is for instance the case in many real-world applications when analysis algorithms are directly deployed on resources-limited mobile devices collecting the data.", "method_label": "We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. An approximate MST is retrieved by following the dynamic \\emph{semi-streaming} model in handling the dissimilarity graph $\\mathcal{G}$ as a stream of edge weight updates which is sketched in one pass over the data into a compact structure requiring $O(N \\operatorname{polylog}(N))$ space, far better than the theoretical memory cost $O(N^2)$ of $\\mathcal{G}$. The recovered approximate MST $\\mathcal{T}$ as input, DBMSTClu then successfully detects the right number of nonconvex clusters by performing relevant cuts on $\\mathcal{T}$ in a time linear in $N$.", "result_label": "We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.", "abstract": "In this paper, we address the problem of recovering arbitrary-shaped data clusters from datasets while facing \\emph{high space constraints}, as this is for instance the case in many real-world applications when analysis algorithms are directly deployed on resources-limited mobile devices collecting the data. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. An approximate MST is retrieved by following the dynamic \\emph{semi-streaming} model in handling the dissimilarity graph $\\mathcal{G}$ as a stream of edge weight updates which is sketched in one pass over the data into a compact structure requiring $O(N \\operatorname{polylog}(N))$ space, far better than the theoretical memory cost $O(N^2)$ of $\\mathcal{G}$. We present DBMSTClu a new space-efficient density-based \\emph{non-parametric} method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a \\emph{sketched} version of the dissimilarity graph $\\mathcal{G}$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. An approximate MST is retrieved by following the dynamic \\emph{semi-streaming} model in handling the dissimilarity graph $\\mathcal{G}$ as a stream of edge weight updates which is sketched in one pass over the data into a compact structure requiring $O(N \\operatorname{polylog}(N))$ space, far better than the theoretical memory cost $O(N^2)$ of $\\mathcal{G}$. The recovered approximate MST $\\mathcal{T}$ as input, DBMSTClu then successfully detects the right number of nonconvex clusters by performing relevant cuts on $\\mathcal{T}$ in a time linear in $N$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets."}, {"paper_id": "18100002", "adju_relevance": 0, "title": "Graph theory for image analysis: an approach based on the shortest spanning tree", "background_label": "The paper describes methods of image segmentation and edge detection based on graph-theoretic representations of images.", "method_label": "The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. These further methods make use of global pictorial information, removing many of the problems of the SST segmentation in its simple form and of other pixel linking algorithms.", "result_label": "An important feature in all of the proposed methods is that regions may be described in a hierarchical way.", "abstract": "The paper describes methods of image segmentation and edge detection based on graph-theoretic representations of images. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. These further methods make use of global pictorial information, removing many of the problems of the SST segmentation in its simple form and of other pixel linking algorithms. An important feature in all of the proposed methods is that regions may be described in a hierarchical way."}, {"paper_id": "1668009", "adju_relevance": 0, "title": "Fast minimum spanning tree for large graphs on the GPU", "background_label": "Graphics Processor Units are used for many general purpose processing due to high compute power available on them. Regular, data-parallel algorithms map well to the SIMD architecture of current GPU. Irregular algorithms on discrete structures like graphs are harder to map to them. Efficient data-mapping primitives can play crucial role in mapping such algorithms onto the GPU.", "method_label": "In this paper, we present a minimum spanning tree algorithm on Nvidia GPUs under CUDA, as a recursive formulation of Bor\u016fvka's approach for undirected graphs. We implement it using scalable primitives such as scan, segmented scan and split. The irregular steps of supervertex formation and recursive graph construction are mapped to primitives like split to categories involving vertex ids and edge weights.", "result_label": "We obtain 30 to 50 times speedup over the CPU implementation on most graphs and 3 to 10 times speedup over our previous GPU implementation. We construct the minimum spanning tree on a 5 million node and 30 million edge graph in under 1 second on one quarter of the Tesla S1070 GPU.", "abstract": "Graphics Processor Units are used for many general purpose processing due to high compute power available on them. Graphics Processor Units are used for many general purpose processing due to high compute power available on them. Regular, data-parallel algorithms map well to the SIMD architecture of current GPU. Graphics Processor Units are used for many general purpose processing due to high compute power available on them. Regular, data-parallel algorithms map well to the SIMD architecture of current GPU. Irregular algorithms on discrete structures like graphs are harder to map to them. Graphics Processor Units are used for many general purpose processing due to high compute power available on them. Regular, data-parallel algorithms map well to the SIMD architecture of current GPU. Irregular algorithms on discrete structures like graphs are harder to map to them. Efficient data-mapping primitives can play crucial role in mapping such algorithms onto the GPU. In this paper, we present a minimum spanning tree algorithm on Nvidia GPUs under CUDA, as a recursive formulation of Bor\u016fvka's approach for undirected graphs. In this paper, we present a minimum spanning tree algorithm on Nvidia GPUs under CUDA, as a recursive formulation of Bor\u016fvka's approach for undirected graphs. We implement it using scalable primitives such as scan, segmented scan and split. In this paper, we present a minimum spanning tree algorithm on Nvidia GPUs under CUDA, as a recursive formulation of Bor\u016fvka's approach for undirected graphs. We implement it using scalable primitives such as scan, segmented scan and split. The irregular steps of supervertex formation and recursive graph construction are mapped to primitives like split to categories involving vertex ids and edge weights. We obtain 30 to 50 times speedup over the CPU implementation on most graphs and 3 to 10 times speedup over our previous GPU implementation. We obtain 30 to 50 times speedup over the CPU implementation on most graphs and 3 to 10 times speedup over our previous GPU implementation. We construct the minimum spanning tree on a 5 million node and 30 million edge graph in under 1 second on one quarter of the Tesla S1070 GPU."}, {"paper_id": "8896250", "adju_relevance": 0, "title": "Tree Filtering: Efficient Structure-Preserving Smoothing With a Minimum Spanning Tree", "background_label": "We present a new efficient edge-preserving filter-\u201ctree filter\u201d-to achieve strong image smoothing.", "abstract": "We present a new efficient edge-preserving filter-\u201ctree filter\u201d-to achieve strong image smoothing."}, {"paper_id": "8293478", "adju_relevance": 0, "title": "The Minset-Poset Approach to Representations of Graph Connectivity", "background_label": "Various instances of the minimal-set poset (minset-poset for short) have been proposed in the literature, e.g., the representation of Picard and Queyranne for all st-minimum cuts of a flow network. We begin with an explanation of why this poset structure is common. We also construct the cactus representation for weighted graphs in time O(nm log(n2/m)), the same bound as a previously known algorithm but in linear space O(m). The latter bound also holds for constructing the minset poset for any weighted digraph; the former bound also holds for constructing the nodes of that poset for any unweighted digraph.", "method_label": "We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. For directed and undirected graphs, weighted and unweighted, we achieve the time of the preceding two bounds, i.e., essentially the best-known bounds to compute the edge connectivity itself. Some constructions of minset posets for graph rigidity are also sketched. For vertex connectivity, the minset poset is proved to be a slight variant of the dominator tree. This leads to an algorithm to construct the dominator tree in time O(m) on a RAM.", "result_label": "We use the poset algorithms to construct the cactus representation for unweighted graphs in time O(m+\u03bb2 n log (n/\u03bb)) (\u03bb is the edge connectivity) improving the previous bound O(\u03bb n2) for all but the densest graphs. (The algorithm is included in the appendix, since other linear-time algorithms of similar simplicity have recently been presented.)", "abstract": "Various instances of the minimal-set poset (minset-poset for short) have been proposed in the literature, e.g., the representation of Picard and Queyranne for all st-minimum cuts of a flow network. Various instances of the minimal-set poset (minset-poset for short) have been proposed in the literature, e.g., the representation of Picard and Queyranne for all st-minimum cuts of a flow network. We begin with an explanation of why this poset structure is common. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. We use the poset algorithms to construct the cactus representation for unweighted graphs in time O(m+\u03bb2 n log (n/\u03bb)) (\u03bb is the edge connectivity) improving the previous bound O(\u03bb n2) for all but the densest graphs. Various instances of the minimal-set poset (minset-poset for short) have been proposed in the literature, e.g., the representation of Picard and Queyranne for all st-minimum cuts of a flow network. We begin with an explanation of why this poset structure is common. We also construct the cactus representation for weighted graphs in time O(nm log(n2/m)), the same bound as a previously known algorithm but in linear space O(m). Various instances of the minimal-set poset (minset-poset for short) have been proposed in the literature, e.g., the representation of Picard and Queyranne for all st-minimum cuts of a flow network. We begin with an explanation of why this poset structure is common. We also construct the cactus representation for weighted graphs in time O(nm log(n2/m)), the same bound as a previously known algorithm but in linear space O(m). The latter bound also holds for constructing the minset poset for any weighted digraph; the former bound also holds for constructing the nodes of that poset for any unweighted digraph. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. For directed and undirected graphs, weighted and unweighted, we achieve the time of the preceding two bounds, i.e., essentially the best-known bounds to compute the edge connectivity itself. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. For directed and undirected graphs, weighted and unweighted, we achieve the time of the preceding two bounds, i.e., essentially the best-known bounds to compute the edge connectivity itself. Some constructions of minset posets for graph rigidity are also sketched. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. For directed and undirected graphs, weighted and unweighted, we achieve the time of the preceding two bounds, i.e., essentially the best-known bounds to compute the edge connectivity itself. Some constructions of minset posets for graph rigidity are also sketched. For vertex connectivity, the minset poset is proved to be a slight variant of the dominator tree. We show any family of sets F that can be defined by a \u201clabelling algorithm\u201d (e.g., the Ford-Fulkerson labelling algorithm for maximum network flow) has an algorithm that constructs the minset poset for F. We implement this algorithm to efficiently find the nodes of the poset when F is the family of minimum edge cuts of an unweighted graph; we also give related algorithms to construct the entire poset for weighted graphs. The rest of the article discusses applications to edge- and vertex connectivity, both combinatorial and algorithmic, that we now describe. For digraphs, a natural interpretation of the minset poset represents all minimum edge cuts. In the special case of undirected graphs, the minset poset is proved to be a variant of the well-known cactus representation of all mincuts. The poset is used in algorithms to increase the edge connectivity of a graph by adding the fewest edges possible. For directed and undirected graphs, weighted and unweighted, we achieve the time of the preceding two bounds, i.e., essentially the best-known bounds to compute the edge connectivity itself. Some constructions of minset posets for graph rigidity are also sketched. For vertex connectivity, the minset poset is proved to be a slight variant of the dominator tree. This leads to an algorithm to construct the dominator tree in time O(m) on a RAM. We use the poset algorithms to construct the cactus representation for unweighted graphs in time O(m+\u03bb2 n log (n/\u03bb)) (\u03bb is the edge connectivity) improving the previous bound O(\u03bb n2) for all but the densest graphs. (The algorithm is included in the appendix, since other linear-time algorithms of similar simplicity have recently been presented.)"}, {"paper_id": "53236530", "adju_relevance": 0, "title": "The Sparsest Additive Spanner via Multiple Weighted BFS Trees", "background_label": "Spanners are fundamental graph structures that sparsify graphs at the cost of small stretch. In particular, in recent years, many sequential algorithms constructing additive all-pairs spanners were designed, providing very sparse small-stretch subgraphs. Remarkably, it was then shown that the known (+6)-spanner constructions are essentially the sparsest possible, that is, a larger additive stretch cannot guarantee a sparser spanner, which brought the stretch-sparsity trade-off to its limit. Distributed constructions of spanners are also abundant. However, for additive spanners, while there were algorithms constructing (+2) and (+4)-all-pairs spanners, the sparsest case of (+6)-spanners remained elusive.", "method_label": "We remedy this by designing a new sequential algorithm for constructing a (+6)-spanner with the essentially-optimal sparsity of roughly O(n^{4/3}) edges.", "abstract": "Spanners are fundamental graph structures that sparsify graphs at the cost of small stretch. Spanners are fundamental graph structures that sparsify graphs at the cost of small stretch. In particular, in recent years, many sequential algorithms constructing additive all-pairs spanners were designed, providing very sparse small-stretch subgraphs. Spanners are fundamental graph structures that sparsify graphs at the cost of small stretch. In particular, in recent years, many sequential algorithms constructing additive all-pairs spanners were designed, providing very sparse small-stretch subgraphs. Remarkably, it was then shown that the known (+6)-spanner constructions are essentially the sparsest possible, that is, a larger additive stretch cannot guarantee a sparser spanner, which brought the stretch-sparsity trade-off to its limit. Spanners are fundamental graph structures that sparsify graphs at the cost of small stretch. In particular, in recent years, many sequential algorithms constructing additive all-pairs spanners were designed, providing very sparse small-stretch subgraphs. Remarkably, it was then shown that the known (+6)-spanner constructions are essentially the sparsest possible, that is, a larger additive stretch cannot guarantee a sparser spanner, which brought the stretch-sparsity trade-off to its limit. Distributed constructions of spanners are also abundant. Spanners are fundamental graph structures that sparsify graphs at the cost of small stretch. In particular, in recent years, many sequential algorithms constructing additive all-pairs spanners were designed, providing very sparse small-stretch subgraphs. Remarkably, it was then shown that the known (+6)-spanner constructions are essentially the sparsest possible, that is, a larger additive stretch cannot guarantee a sparser spanner, which brought the stretch-sparsity trade-off to its limit. Distributed constructions of spanners are also abundant. However, for additive spanners, while there were algorithms constructing (+2) and (+4)-all-pairs spanners, the sparsest case of (+6)-spanners remained elusive. We remedy this by designing a new sequential algorithm for constructing a (+6)-spanner with the essentially-optimal sparsity of roughly O(n^{4/3}) edges."}, {"paper_id": "32617931", "adju_relevance": 0, "title": "On Computing the Hyperbolicity of Real-World Graphs", "background_label": "The (Gromov) hyperbolicity is a topological property of a graph, which has been recently applied in several different contexts, such as the design of routing schemes, network security, computational biology, the analysis of graph algorithms, and the classification of complex networks. Computing the hyperbolicity of a graph can be very time consuming: indeed, the best available algorithm has running-time O(n 3.69 ), which is clearly prohibitive for big graphs.", "method_label": "In this paper, we provide a new and more efficient algorithm: although its worst-case complexity is O(n 4 ), in practice it is much faster, allowing, for the first time, the computation of the hyperbolicity of graphs with up to 200,000 nodes.", "result_label": "We experimentally show that our new algorithm drastically outperforms the best previously available algorithms, by analyzing a big dataset of real-world networks. Finally, we apply the new algorithm to compute the hyperbolicity of random graphs generated with the Erd\u00f6s-Renyi model, the Chung-Lu model, and the Configuration Model.", "abstract": " The (Gromov) hyperbolicity is a topological property of a graph, which has been recently applied in several different contexts, such as the design of routing schemes, network security, computational biology, the analysis of graph algorithms, and the classification of complex networks. The (Gromov) hyperbolicity is a topological property of a graph, which has been recently applied in several different contexts, such as the design of routing schemes, network security, computational biology, the analysis of graph algorithms, and the classification of complex networks. Computing the hyperbolicity of a graph can be very time consuming: indeed, the best available algorithm has running-time O(n 3.69 ), which is clearly prohibitive for big graphs. In this paper, we provide a new and more efficient algorithm: although its worst-case complexity is O(n 4 ), in practice it is much faster, allowing, for the first time, the computation of the hyperbolicity of graphs with up to 200,000 nodes. We experimentally show that our new algorithm drastically outperforms the best previously available algorithms, by analyzing a big dataset of real-world networks. We experimentally show that our new algorithm drastically outperforms the best previously available algorithms, by analyzing a big dataset of real-world networks. Finally, we apply the new algorithm to compute the hyperbolicity of random graphs generated with the Erd\u00f6s-Renyi model, the Chung-Lu model, and the Configuration Model."}, {"paper_id": "39321987", "adju_relevance": 0, "title": "Minor Excluded Network Families Admit Fast Distributed Algorithms", "background_label": "Distributed network optimization algorithms, such as minimum spanning tree, minimum cut, and shortest path, are an active research area in distributed computing. This paper presents a fast distributed algorithm for such problems in the CONGEST model, on networks that exclude a fixed minor. On general graphs, many optimization problems, including the ones mentioned above, require $\\tilde\\Omega(\\sqrt n)$ rounds of communication in the CONGEST model, even if the network graph has a much smaller diameter.", "method_label": "Naturally, the next step in algorithm design is to design efficient algorithms which bypass this lower bound on a restricted class of graphs.", "abstract": "Distributed network optimization algorithms, such as minimum spanning tree, minimum cut, and shortest path, are an active research area in distributed computing. Distributed network optimization algorithms, such as minimum spanning tree, minimum cut, and shortest path, are an active research area in distributed computing. This paper presents a fast distributed algorithm for such problems in the CONGEST model, on networks that exclude a fixed minor. Distributed network optimization algorithms, such as minimum spanning tree, minimum cut, and shortest path, are an active research area in distributed computing. This paper presents a fast distributed algorithm for such problems in the CONGEST model, on networks that exclude a fixed minor. On general graphs, many optimization problems, including the ones mentioned above, require $\\tilde\\Omega(\\sqrt n)$ rounds of communication in the CONGEST model, even if the network graph has a much smaller diameter. Naturally, the next step in algorithm design is to design efficient algorithms which bypass this lower bound on a restricted class of graphs."}, {"paper_id": "1994530", "adju_relevance": 0, "title": "On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing", "background_label": "AbstractThis paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems.", "method_label": "The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation.", "result_label": "We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.", "abstract": "AbstractThis paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger."}, {"paper_id": "18591076", "adju_relevance": 0, "title": "Local search with edge weighting and configuration checking heuristics for minimum vertex cover", "background_label": "The Minimum Vertex Cover (MVC) problem is a well-known combinatorial optimization problem of great importance in theory and applications. In recent years, local search has been shown to be an effective and promising approach to solve hard problems, such as MVC.", "abstract": "The Minimum Vertex Cover (MVC) problem is a well-known combinatorial optimization problem of great importance in theory and applications. The Minimum Vertex Cover (MVC) problem is a well-known combinatorial optimization problem of great importance in theory and applications. In recent years, local search has been shown to be an effective and promising approach to solve hard problems, such as MVC."}, {"paper_id": "14892145", "adju_relevance": 0, "title": "A Local Algorithm for Constructing Spanners in Minor-Free Graphs", "background_label": "Constructing a spanning tree of a graph is one of the most basic tasks in graph theory. We consider this problem in the setting of local algorithms: one wants to quickly determine whether a given edge $e$ is in a specific spanning tree, without computing the whole spanning tree, but rather by inspecting the local neighborhood of $e$.", "abstract": "Constructing a spanning tree of a graph is one of the most basic tasks in graph theory. Constructing a spanning tree of a graph is one of the most basic tasks in graph theory. We consider this problem in the setting of local algorithms: one wants to quickly determine whether a given edge $e$ is in a specific spanning tree, without computing the whole spanning tree, but rather by inspecting the local neighborhood of $e$."}, {"paper_id": "15668657", "adju_relevance": 0, "title": "An Efficient Algorithm for Solving the Dyck-CFL Reachability Problem on Trees", "background_label": "Abstract. The context-free language (CFL) reachability problem is well known and studied in computer science, as a fundamental problem underlying many important static analyses such as points-to-analysis. Solving the CFL reachability problem in the general case is very hard. Popular solutions resorting to a graph traversal exhibit a time complexity of O(k 3 n 3 ) for a grammar of size k. For Dyck CFLs, a particular class of CFLs, this complexity can be reduced to O(kn 3 ).", "method_label": "Only recently the first subcubic algorithm was proposed by Chaudhuri, dividing the complexity of predating solutions by a factor of log n. In this paper we propose an effective algorithm for solving the CFL reachability problem for Dyck languages when the considered graph is a bidirected tree with specific constraints. Our solution pre-processes the graph in O(n log n log k) time in a space of O(n log n), after which any Dyck-CFL reachability query can be answered in O(1) time, while a na\u00efve online algorithm will require O(n) time to answer a query or require O(n 2 ) to store the pre-computed results for all pairs of nodes.", "abstract": "Abstract. Abstract. The context-free language (CFL) reachability problem is well known and studied in computer science, as a fundamental problem underlying many important static analyses such as points-to-analysis. Abstract. The context-free language (CFL) reachability problem is well known and studied in computer science, as a fundamental problem underlying many important static analyses such as points-to-analysis. Solving the CFL reachability problem in the general case is very hard. Abstract. The context-free language (CFL) reachability problem is well known and studied in computer science, as a fundamental problem underlying many important static analyses such as points-to-analysis. Solving the CFL reachability problem in the general case is very hard. Popular solutions resorting to a graph traversal exhibit a time complexity of O(k 3 n 3 ) for a grammar of size k. For Dyck CFLs, a particular class of CFLs, this complexity can be reduced to O(kn 3 ). Only recently the first subcubic algorithm was proposed by Chaudhuri, dividing the complexity of predating solutions by a factor of log n. In this paper we propose an effective algorithm for solving the CFL reachability problem for Dyck languages when the considered graph is a bidirected tree with specific constraints. Only recently the first subcubic algorithm was proposed by Chaudhuri, dividing the complexity of predating solutions by a factor of log n. In this paper we propose an effective algorithm for solving the CFL reachability problem for Dyck languages when the considered graph is a bidirected tree with specific constraints. Our solution pre-processes the graph in O(n log n log k) time in a space of O(n log n), after which any Dyck-CFL reachability query can be answered in O(1) time, while a na\u00efve online algorithm will require O(n) time to answer a query or require O(n 2 ) to store the pre-computed results for all pairs of nodes."}, {"paper_id": "3597365", "adju_relevance": 0, "title": "KickStarter: Fast and Accurate Computations on Streaming Graphs via Trimmed Approximations", "background_label": "Continuous processing of a streaming graph maintains an approximate result of the iterative computation on a recent version of the graph. Upon a user query, the accurate result on the current graph can be quickly computed by feeding the approximate results to the iterative computation --- a form of incremental computation that corrects the (small amount of) error in the approximate result. Despite the effectiveness of this approach in processing growing graphs, it is generally not applicable when edge deletions are present --- existing approximations can lead to either incorrect results (e.g., monotonic computations terminate at an incorrect minima/maxima) or poor performance (e.g., with approximations, convergence takes longer than performing the computation from scratch).", "method_label": "This paper presents KickStarter, a runtime technique that can trim the approximate values for a subset of vertices impacted by the deleted edges. The trimmed approximation is both safe and profitable, enabling the computation to produce correct results and converge quickly. KickStarter works for a class of monotonic graph algorithms and can be readily incorporated in any existing streaming graph system.", "result_label": "Our experiments with four streaming algorithms on five large graphs demonstrate that trimming not only produces correct results but also accelerates these algorithms by 8.5--23.7x.", "abstract": "Continuous processing of a streaming graph maintains an approximate result of the iterative computation on a recent version of the graph. Continuous processing of a streaming graph maintains an approximate result of the iterative computation on a recent version of the graph. Upon a user query, the accurate result on the current graph can be quickly computed by feeding the approximate results to the iterative computation --- a form of incremental computation that corrects the (small amount of) error in the approximate result. Continuous processing of a streaming graph maintains an approximate result of the iterative computation on a recent version of the graph. Upon a user query, the accurate result on the current graph can be quickly computed by feeding the approximate results to the iterative computation --- a form of incremental computation that corrects the (small amount of) error in the approximate result. Despite the effectiveness of this approach in processing growing graphs, it is generally not applicable when edge deletions are present --- existing approximations can lead to either incorrect results (e.g., monotonic computations terminate at an incorrect minima/maxima) or poor performance (e.g., with approximations, convergence takes longer than performing the computation from scratch). This paper presents KickStarter, a runtime technique that can trim the approximate values for a subset of vertices impacted by the deleted edges. This paper presents KickStarter, a runtime technique that can trim the approximate values for a subset of vertices impacted by the deleted edges. The trimmed approximation is both safe and profitable, enabling the computation to produce correct results and converge quickly. This paper presents KickStarter, a runtime technique that can trim the approximate values for a subset of vertices impacted by the deleted edges. The trimmed approximation is both safe and profitable, enabling the computation to produce correct results and converge quickly. KickStarter works for a class of monotonic graph algorithms and can be readily incorporated in any existing streaming graph system. Our experiments with four streaming algorithms on five large graphs demonstrate that trimming not only produces correct results but also accelerates these algorithms by 8.5--23.7x."}, {"paper_id": "40601787", "adju_relevance": 0, "title": "New Models of the Generalized Minimum Spanning Tree Problem", "background_label": "We consider a generalization of the Minimum Spanning Tree Problem, called the Generalized Minimum Spanning Tree Problem, denoted by GMST. It is known that the GMST problem is NP-hard. We present a stronger result regarding its complexity, namely, the GMST problem is NP-hard even on trees as well an exact exponential time algorithm for the problem based on dynamic programming.", "method_label": "We describe new mixed integer programming models of the GMST problem, mainly containing a polynomial number of constraints. We establish relationships between the polytopes corresponding to their linear relaxations. Based on a new model of the GMST we present a solution procedure that solves the problem to optimality for graphs with nodes up to 240.", "result_label": "We discuss the advantages of our method in comparison with earlier methods.", "abstract": "We consider a generalization of the Minimum Spanning Tree Problem, called the Generalized Minimum Spanning Tree Problem, denoted by GMST. We consider a generalization of the Minimum Spanning Tree Problem, called the Generalized Minimum Spanning Tree Problem, denoted by GMST. It is known that the GMST problem is NP-hard. We consider a generalization of the Minimum Spanning Tree Problem, called the Generalized Minimum Spanning Tree Problem, denoted by GMST. It is known that the GMST problem is NP-hard. We present a stronger result regarding its complexity, namely, the GMST problem is NP-hard even on trees as well an exact exponential time algorithm for the problem based on dynamic programming. We describe new mixed integer programming models of the GMST problem, mainly containing a polynomial number of constraints. We describe new mixed integer programming models of the GMST problem, mainly containing a polynomial number of constraints. We establish relationships between the polytopes corresponding to their linear relaxations. We describe new mixed integer programming models of the GMST problem, mainly containing a polynomial number of constraints. We establish relationships between the polytopes corresponding to their linear relaxations. Based on a new model of the GMST we present a solution procedure that solves the problem to optimality for graphs with nodes up to 240. We discuss the advantages of our method in comparison with earlier methods."}, {"paper_id": "3442928", "adju_relevance": 0, "title": "An Improved Algorithm for Incremental DFS Tree in Undirected Graphs", "background_label": "Depth first search (DFS) tree is one of the most well-known data structures for designing efficient graph algorithms. Given an undirected graph $G=(V,E)$ with $n$ vertices and $m$ edges, the textbook algorithm takes $O(n+m)$ time to construct a DFS tree.", "abstract": "Depth first search (DFS) tree is one of the most well-known data structures for designing efficient graph algorithms. Depth first search (DFS) tree is one of the most well-known data structures for designing efficient graph algorithms. Given an undirected graph $G=(V,E)$ with $n$ vertices and $m$ edges, the textbook algorithm takes $O(n+m)$ time to construct a DFS tree."}, {"paper_id": "35924921", "adju_relevance": 0, "title": "Approximating the Minimum k-way Cut in a Graph via Minimum 3-way Cuts", "background_label": "For an edge weighted undirected graph G and an integer k > 2, a k-way cut is a set of edges whose removal leaves G with at least k components.", "abstract": "For an edge weighted undirected graph G and an integer k > 2, a k-way cut is a set of edges whose removal leaves G with at least k components."}, {"paper_id": "17165618", "adju_relevance": 0, "title": "Greedy Learning of Markov Network Structure", "background_label": "We propose a new yet natural algorithm for learning the graph structure of general discrete graphical models (a.k.a. Markov random fields) from samples.", "method_label": "Our algorithm finds the neighborhood of a node by sequentially adding nodes that produce the largest reduction in empirical conditional entropy; it is greedy in the sense that the choice of addition is based only on the reduction achieved at that iteration. Its sequential nature gives it a lower computational complexity as compared to other existing comparison-based techniques, all of which involve exhaustive searches over every node set of a certain size. Our main result characterizes the sample complexity of this procedure, as a function of node degrees, graph size and girth in factor-graph representation. We subsequently specialize this result to the case of Ising models, where we provide a simple transparent characterization of sample complexity as a function of model and graph parameters.", "result_label": "For tree graphs, our algorithm is the same as the classical Chow-Liu algorithm, and in that sense can be considered the extension of the same to graphs with cycles.", "abstract": "We propose a new yet natural algorithm for learning the graph structure of general discrete graphical models (a.k.a. We propose a new yet natural algorithm for learning the graph structure of general discrete graphical models (a.k.a. Markov random fields) from samples. Our algorithm finds the neighborhood of a node by sequentially adding nodes that produce the largest reduction in empirical conditional entropy; it is greedy in the sense that the choice of addition is based only on the reduction achieved at that iteration. Our algorithm finds the neighborhood of a node by sequentially adding nodes that produce the largest reduction in empirical conditional entropy; it is greedy in the sense that the choice of addition is based only on the reduction achieved at that iteration. Its sequential nature gives it a lower computational complexity as compared to other existing comparison-based techniques, all of which involve exhaustive searches over every node set of a certain size. Our algorithm finds the neighborhood of a node by sequentially adding nodes that produce the largest reduction in empirical conditional entropy; it is greedy in the sense that the choice of addition is based only on the reduction achieved at that iteration. Its sequential nature gives it a lower computational complexity as compared to other existing comparison-based techniques, all of which involve exhaustive searches over every node set of a certain size. Our main result characterizes the sample complexity of this procedure, as a function of node degrees, graph size and girth in factor-graph representation. Our algorithm finds the neighborhood of a node by sequentially adding nodes that produce the largest reduction in empirical conditional entropy; it is greedy in the sense that the choice of addition is based only on the reduction achieved at that iteration. Its sequential nature gives it a lower computational complexity as compared to other existing comparison-based techniques, all of which involve exhaustive searches over every node set of a certain size. Our main result characterizes the sample complexity of this procedure, as a function of node degrees, graph size and girth in factor-graph representation. We subsequently specialize this result to the case of Ising models, where we provide a simple transparent characterization of sample complexity as a function of model and graph parameters. For tree graphs, our algorithm is the same as the classical Chow-Liu algorithm, and in that sense can be considered the extension of the same to graphs with cycles."}, {"paper_id": "2011169", "adju_relevance": 0, "title": "Parse, Price and Cut\u2013-Delayed Column and Row Generation for Graph Based Parsers", "background_label": "AbstractGraph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored-without any loss of optimality guarantees and, hence, accuracy.", "method_label": "This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6-13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation.", "result_label": "We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed.", "abstract": "AbstractGraph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. AbstractGraph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored-without any loss of optimality guarantees and, hence, accuracy. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6-13% of the second order edges of the full model. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6-13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation. We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed."}, {"paper_id": "58004296", "adju_relevance": 0, "title": "Generalization of Dijkstra\u0092S Algorithm for Extraction of Shortest Paths in Directed multigraphs", "background_label": "ABSTRACTThe classical Dijkstra's algorithm to find the shortest path in graphs is not applicable to multigraphs. In this study the authors generalize the classical Dijkstra's algorithm to make it applicable to directed multigraphs.", "method_label": "The modified algorithm is called by Generalized Dijkstra's algorithm or GD Algorithm (GDA in short). The GDA outputs the shortest paths and the corresponding min cost.", "result_label": "It is claimed that GDA may play a major role in many application areas of computer science, communication, transportation systems, in particular in those networks which cannot be modeled into graphs but into multigraphs.", "abstract": "ABSTRACTThe classical Dijkstra's algorithm to find the shortest path in graphs is not applicable to multigraphs. ABSTRACTThe classical Dijkstra's algorithm to find the shortest path in graphs is not applicable to multigraphs. In this study the authors generalize the classical Dijkstra's algorithm to make it applicable to directed multigraphs. The modified algorithm is called by Generalized Dijkstra's algorithm or GD Algorithm (GDA in short). The modified algorithm is called by Generalized Dijkstra's algorithm or GD Algorithm (GDA in short). The GDA outputs the shortest paths and the corresponding min cost. It is claimed that GDA may play a major role in many application areas of computer science, communication, transportation systems, in particular in those networks which cannot be modeled into graphs but into multigraphs."}, {"paper_id": "9696739", "adju_relevance": 0, "title": "A Parallel Implementation of Gomory-Hu's Cut Tree Algorithm", "background_label": "Cut trees are a compact representation of the edge-connectivity between every pair of vertices of an undirected graph, and have a large number of applications. In this work a parallel version of the well known Gomory-Hu cut tree algorithm is presented.", "method_label": "The parallel strategy is based on the master/slave model. The strategy is optimistic in the sense that the master process manipulates the tree being constructed and the slaves solve minimum s-t-cuts independently. Another version is proposed that employs a heuristic that enumerates all (up to a limit) of the minimum s-t-cuts in order to choose the most balanced one.", "result_label": "The algorithm was implemented and extensive experimental results are presented, including a comparison with Gusfielda\u0302s cut tree algorithm. Parallel versions of these algorithms have achieved significant speedups on real and synthetic graphs. We discuss the trade-offs between the two alternatives, each of which presents better results given the characteristics of the input graph. In particular, the existence of balanced cuts clearly gives an advantage to Gomory-Hua\u0302salgorithm.", "abstract": "Cut trees are a compact representation of the edge-connectivity between every pair of vertices of an undirected graph, and have a large number of applications. Cut trees are a compact representation of the edge-connectivity between every pair of vertices of an undirected graph, and have a large number of applications. In this work a parallel version of the well known Gomory-Hu cut tree algorithm is presented. The parallel strategy is based on the master/slave model. The parallel strategy is based on the master/slave model. The strategy is optimistic in the sense that the master process manipulates the tree being constructed and the slaves solve minimum s-t-cuts independently. The parallel strategy is based on the master/slave model. The strategy is optimistic in the sense that the master process manipulates the tree being constructed and the slaves solve minimum s-t-cuts independently. Another version is proposed that employs a heuristic that enumerates all (up to a limit) of the minimum s-t-cuts in order to choose the most balanced one. The algorithm was implemented and extensive experimental results are presented, including a comparison with Gusfielda\u0302s cut tree algorithm. The algorithm was implemented and extensive experimental results are presented, including a comparison with Gusfielda\u0302s cut tree algorithm. Parallel versions of these algorithms have achieved significant speedups on real and synthetic graphs. The algorithm was implemented and extensive experimental results are presented, including a comparison with Gusfielda\u0302s cut tree algorithm. Parallel versions of these algorithms have achieved significant speedups on real and synthetic graphs. We discuss the trade-offs between the two alternatives, each of which presents better results given the characteristics of the input graph. The algorithm was implemented and extensive experimental results are presented, including a comparison with Gusfielda\u0302s cut tree algorithm. Parallel versions of these algorithms have achieved significant speedups on real and synthetic graphs. We discuss the trade-offs between the two alternatives, each of which presents better results given the characteristics of the input graph. In particular, the existence of balanced cuts clearly gives an advantage to Gomory-Hua\u0302salgorithm."}, {"paper_id": "7295712", "adju_relevance": 0, "title": "The minimum area spanning tree problem", "background_label": "AbstractWe define and study the Minimum Area Spanning Tree (mast) problem.", "method_label": "Given a set P of n points in the plane, find a spanning tree T of P of minimum area, where the area of a spanning tree is the area of the union of the n \u2212 1 disks whose diameters are the edges in T . We prove that the minimum spanning tree of P is a constant-factor approximation for mast. We then apply this result to obtain a constantfactor approximation for the Minimum Area Range Assignment (mara) problem and for the Minimum Area Connected Disk Graph (macdg) problem.", "result_label": "The former problem is a variant of the power assignment problem in radio networks, and the latter problem is a related natural problem.", "abstract": "AbstractWe define and study the Minimum Area Spanning Tree (mast) problem. Given a set P of n points in the plane, find a spanning tree T of P of minimum area, where the area of a spanning tree is the area of the union of the n \u2212 1 disks whose diameters are the edges in T . Given a set P of n points in the plane, find a spanning tree T of P of minimum area, where the area of a spanning tree is the area of the union of the n \u2212 1 disks whose diameters are the edges in T . We prove that the minimum spanning tree of P is a constant-factor approximation for mast. Given a set P of n points in the plane, find a spanning tree T of P of minimum area, where the area of a spanning tree is the area of the union of the n \u2212 1 disks whose diameters are the edges in T . We prove that the minimum spanning tree of P is a constant-factor approximation for mast. We then apply this result to obtain a constantfactor approximation for the Minimum Area Range Assignment (mara) problem and for the Minimum Area Connected Disk Graph (macdg) problem. The former problem is a variant of the power assignment problem in radio networks, and the latter problem is a related natural problem."}, {"paper_id": "15973618", "adju_relevance": 0, "title": "Inferring a Tree from Walks", "background_label": "AbstractA walk in an undirected edge-colored graph G is a path containing all edges of G. The tree inference from a walk is, given a string x of colors, finding the smallest tree that realizes a walk whose sequence of edge-colors coincides with x.", "method_label": "We prove that the problem is solvable in O(n) time, where n is the length of a given string, We furthermore consider the problem of inferring a tree from a finite number of partial walks, where a partial walk in G is a path in G. We show that the problem turns to be NP-complete even if the number of colors is restricted to 3.", "result_label": "It is also shown that the problem of inferring a linear chain from partial walks is NP-complete, while the linear chain inference from a single walk is known to be solvable in polynomial time.", "abstract": "AbstractA walk in an undirected edge-colored graph G is a path containing all edges of G. The tree inference from a walk is, given a string x of colors, finding the smallest tree that realizes a walk whose sequence of edge-colors coincides with x. We prove that the problem is solvable in O(n) time, where n is the length of a given string, We furthermore consider the problem of inferring a tree from a finite number of partial walks, where a partial walk in G is a path in G. We show that the problem turns to be NP-complete even if the number of colors is restricted to 3. It is also shown that the problem of inferring a linear chain from partial walks is NP-complete, while the linear chain inference from a single walk is known to be solvable in polynomial time."}, {"paper_id": "33167313", "adju_relevance": 0, "title": "Upward drawings of triconnected digraphs", "background_label": "A polynomial-time algorithm for testing if a triconnected directed graph has an upward drkwing is presented. An upward drkwing is a planar drkwing such that all the edges flow in a common direction (e.g., from bottom to top). The problem arises in the fields of automatic graph drkwing and ordered sets, and has been open for several years.", "method_label": "The proposed algorithm is based on a new combinatorial characterization that maps the problem into a max-flow problem on a sparse network; the time complexity isO(n+r 2) , wheren is the number of vertices andr is the number of sources and sinks of the directed graph.", "result_label": "If the directed graph has an upward drkwing, the algorithm allows us to construct one easily.", "abstract": "A polynomial-time algorithm for testing if a triconnected directed graph has an upward drkwing is presented. A polynomial-time algorithm for testing if a triconnected directed graph has an upward drkwing is presented. An upward drkwing is a planar drkwing such that all the edges flow in a common direction (e.g., from bottom to top). A polynomial-time algorithm for testing if a triconnected directed graph has an upward drkwing is presented. An upward drkwing is a planar drkwing such that all the edges flow in a common direction (e.g., from bottom to top). The problem arises in the fields of automatic graph drkwing and ordered sets, and has been open for several years. The proposed algorithm is based on a new combinatorial characterization that maps the problem into a max-flow problem on a sparse network; the time complexity isO(n+r 2) , wheren is the number of vertices andr is the number of sources and sinks of the directed graph. If the directed graph has an upward drkwing, the algorithm allows us to construct one easily."}, {"paper_id": "6666052", "adju_relevance": 0, "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "background_label": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders.", "method_label": "To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings.", "result_label": "The parser's implementation is available for download at the first author's webpage.", "abstract": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage."}, {"paper_id": "25677591", "adju_relevance": 0, "title": "Using the Minimum Description Length Principle to Infer Reduced Ordered Decision Graphs", "background_label": "We propose an algorithm for the inference of decision graphs from a set of labeled instances.", "abstract": "We propose an algorithm for the inference of decision graphs from a set of labeled instances."}, {"paper_id": "15407650", "adju_relevance": 0, "title": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles", "background_label": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed state-of-the-art approaches in constituency parsing.", "method_label": "To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n^3) oracles for standard dependency parsing.", "result_label": "Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.", "abstract": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed state-of-the-art approaches in constituency parsing. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n^3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data."}, {"paper_id": "3679", "adju_relevance": 0, "title": "Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs and its Relation to Linear Programs with Integer Solutions", "background_label": "We consider the general problem of finding the minimum weight $\\bm$-matching on arbitrary graphs.", "method_label": "We prove that, whenever the linear programming (LP) relaxation of the problem has no fractional solutions, then the belief propagation (BP) algorithm converges to the correct solution. We also show that when the LP relaxation has a fractional solution then the BP algorithm can be used to solve the LP relaxation. Our proof is based on the notion of graph covers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara 2007}.", "result_label": "These results are notable in the following regards: (1) It is one of a very small number of proofs showing correctness of BP without any constraint on the graph structure. (2) Variants of the proof work for both synchronous and asynchronous BP; it is the first proof of convergence and correctness of an asynchronous BP algorithm for a combinatorial optimization problem.", "abstract": "We consider the general problem of finding the minimum weight $\\bm$-matching on arbitrary graphs. We prove that, whenever the linear programming (LP) relaxation of the problem has no fractional solutions, then the belief propagation (BP) algorithm converges to the correct solution. We prove that, whenever the linear programming (LP) relaxation of the problem has no fractional solutions, then the belief propagation (BP) algorithm converges to the correct solution. We also show that when the LP relaxation has a fractional solution then the BP algorithm can be used to solve the LP relaxation. We prove that, whenever the linear programming (LP) relaxation of the problem has no fractional solutions, then the belief propagation (BP) algorithm converges to the correct solution. We also show that when the LP relaxation has a fractional solution then the BP algorithm can be used to solve the LP relaxation. Our proof is based on the notion of graph covers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara 2007}. These results are notable in the following regards: (1) It is one of a very small number of proofs showing correctness of BP without any constraint on the graph structure. These results are notable in the following regards: (1) It is one of a very small number of proofs showing correctness of BP without any constraint on the graph structure. (2) Variants of the proof work for both synchronous and asynchronous BP; it is the first proof of convergence and correctness of an asynchronous BP algorithm for a combinatorial optimization problem."}, {"paper_id": "1191614", "adju_relevance": 0, "title": "Optimal structure identification with greedy search", "background_label": "In this paper we prove the so-called \"Meek Conjecture\". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H.", "method_label": "As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain.", "result_label": "Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.", "abstract": "In this paper we prove the so-called \"Meek Conjecture\". In this paper we prove the so-called \"Meek Conjecture\". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes."}, {"paper_id": "118426174", "adju_relevance": 0, "title": "QuickCSG: Arbitrary and Faster Boolean Combinations of N Solids", "background_label": "While studied over several decades, the computation of boolean operations on polyhedra is almost always addressed by focusing on the case of two polyhedra. For multiple input polyhedra and an arbitrary boolean operation to be applied, the operation is decomposed over a binary CSG tree, each node being processed separately in quasilinear time. For large trees, this is both error prone due to intermediate geometry and error accumulation, and inefficient because each node yields a specific overhead.", "abstract": "While studied over several decades, the computation of boolean operations on polyhedra is almost always addressed by focusing on the case of two polyhedra. While studied over several decades, the computation of boolean operations on polyhedra is almost always addressed by focusing on the case of two polyhedra. For multiple input polyhedra and an arbitrary boolean operation to be applied, the operation is decomposed over a binary CSG tree, each node being processed separately in quasilinear time. While studied over several decades, the computation of boolean operations on polyhedra is almost always addressed by focusing on the case of two polyhedra. For multiple input polyhedra and an arbitrary boolean operation to be applied, the operation is decomposed over a binary CSG tree, each node being processed separately in quasilinear time. For large trees, this is both error prone due to intermediate geometry and error accumulation, and inefficient because each node yields a specific overhead."}, {"paper_id": "3655067", "adju_relevance": 0, "title": "A million variables and more: the Fast Greedy Equivalence Search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images", "background_label": "We describe two modifications that parallelize and reorganize caching in the well-known Greedy Equivalence Search algorithm for discovering directed acyclic graphs on random variables from sample values.", "method_label": "We apply one of these modifications, the Fast Greedy Equivalence Search (fGES) assuming faithfulness, to an i.i.d. sample of 1000 units to recover with high precision and good recall an average degree 2 directed acyclic graph with one million Gaussian variables. We describe a modification of the algorithm to rapidly find the Markov Blanket of any variable in a high dimensional system. Using 51,000 voxels that parcellate an entire human cortex, we apply the fGES algorithm to blood oxygenation level-dependent time series obtained from resting state fMRI.", "abstract": "We describe two modifications that parallelize and reorganize caching in the well-known Greedy Equivalence Search algorithm for discovering directed acyclic graphs on random variables from sample values. We apply one of these modifications, the Fast Greedy Equivalence Search (fGES) assuming faithfulness, to an i.i.d. We apply one of these modifications, the Fast Greedy Equivalence Search (fGES) assuming faithfulness, to an i.i.d. sample of 1000 units to recover with high precision and good recall an average degree 2 directed acyclic graph with one million Gaussian variables. We apply one of these modifications, the Fast Greedy Equivalence Search (fGES) assuming faithfulness, to an i.i.d. sample of 1000 units to recover with high precision and good recall an average degree 2 directed acyclic graph with one million Gaussian variables. We describe a modification of the algorithm to rapidly find the Markov Blanket of any variable in a high dimensional system. We apply one of these modifications, the Fast Greedy Equivalence Search (fGES) assuming faithfulness, to an i.i.d. sample of 1000 units to recover with high precision and good recall an average degree 2 directed acyclic graph with one million Gaussian variables. We describe a modification of the algorithm to rapidly find the Markov Blanket of any variable in a high dimensional system. Using 51,000 voxels that parcellate an entire human cortex, we apply the fGES algorithm to blood oxygenation level-dependent time series obtained from resting state fMRI."}, {"paper_id": "125785378", "adju_relevance": 0, "title": "Universal Method for Stochastic Composite Optimization Problems", "method_label": "A fast gradient method requiring only one projection is proposed for smooth convex optimization problems. The method has a visual geometric interpretation, so it is called the method of similar triangles (MST). Composite, adaptive, and universal versions of MST are suggested. Based on MST, a universal method is proposed for the first time for strongly convex problems (this method is continuous with respect to the strong convexity parameter of the smooth part of the functional). It is shown how the universal version of MST can be applied to stochastic optimization problems.", "abstract": "A fast gradient method requiring only one projection is proposed for smooth convex optimization problems. A fast gradient method requiring only one projection is proposed for smooth convex optimization problems. The method has a visual geometric interpretation, so it is called the method of similar triangles (MST). A fast gradient method requiring only one projection is proposed for smooth convex optimization problems. The method has a visual geometric interpretation, so it is called the method of similar triangles (MST). Composite, adaptive, and universal versions of MST are suggested. A fast gradient method requiring only one projection is proposed for smooth convex optimization problems. The method has a visual geometric interpretation, so it is called the method of similar triangles (MST). Composite, adaptive, and universal versions of MST are suggested. Based on MST, a universal method is proposed for the first time for strongly convex problems (this method is continuous with respect to the strong convexity parameter of the smooth part of the functional). A fast gradient method requiring only one projection is proposed for smooth convex optimization problems. The method has a visual geometric interpretation, so it is called the method of similar triangles (MST). Composite, adaptive, and universal versions of MST are suggested. Based on MST, a universal method is proposed for the first time for strongly convex problems (this method is continuous with respect to the strong convexity parameter of the smooth part of the functional). It is shown how the universal version of MST can be applied to stochastic optimization problems."}, {"paper_id": "57825707", "adju_relevance": 0, "title": "Depth First Search in the Semi-streaming Model", "background_label": "Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. The classical DFS algorithm requires $O(m+n)$ time for a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm is allowed several passes (preferably single) over the input graph having a restriction on the size of local space used. However, it remains an open problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even in any relaxed streaming environment. We first describe an extremely simple algorithm that requires at most $\\lceil n/k\\rceil$ passes using $O(nk)$ space, where $k$ is any positive integer.", "method_label": "Trivially, a DFS tree can be computed using a single pass using $O(m)$ space. In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$ passes, where each pass adds one vertex to the DFS tree. We then improve this algorithm by using more involved techniques to reduce the number of passes to $\\lceil h/k\\rceil$ under similar space constraints, where $h$ is the height of the computed DFS tree. In particular, this algorithm improves the bounds for the case where the computed DFS tree is shallow (having $o(n)$ height). Moreover, this algorithm is presented as a framework that allows the flexibility of using any algorithm to maintain a DFS tree of a stored sparser subgraph as a black box, which may be of independent interest.", "result_label": "We present the first semi-streaming algorithms that compute a DFS tree of an undirected graph in $o(n)$ passes using $o(m)$ space. Both these algorithms essentially demonstrate the existence of a trade-off between the space and number of passes required for computing a DFS tree. Furthermore, we evaluate these algorithms experimentally which reveals their exceptional performance in practice. For both random and real graphs, they require merely a few passes even when allowed just $O(n)$ space.", "abstract": "Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. The classical DFS algorithm requires $O(m+n)$ time for a graph having $n$ vertices and $m$ edges. Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. The classical DFS algorithm requires $O(m+n)$ time for a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm is allowed several passes (preferably single) over the input graph having a restriction on the size of local space used. Trivially, a DFS tree can be computed using a single pass using $O(m)$ space. Trivially, a DFS tree can be computed using a single pass using $O(m)$ space. In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$ passes, where each pass adds one vertex to the DFS tree. Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. The classical DFS algorithm requires $O(m+n)$ time for a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm is allowed several passes (preferably single) over the input graph having a restriction on the size of local space used. However, it remains an open problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even in any relaxed streaming environment. We present the first semi-streaming algorithms that compute a DFS tree of an undirected graph in $o(n)$ passes using $o(m)$ space. Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. The classical DFS algorithm requires $O(m+n)$ time for a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm is allowed several passes (preferably single) over the input graph having a restriction on the size of local space used. However, it remains an open problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even in any relaxed streaming environment. We first describe an extremely simple algorithm that requires at most $\\lceil n/k\\rceil$ passes using $O(nk)$ space, where $k$ is any positive integer. Trivially, a DFS tree can be computed using a single pass using $O(m)$ space. In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$ passes, where each pass adds one vertex to the DFS tree. We then improve this algorithm by using more involved techniques to reduce the number of passes to $\\lceil h/k\\rceil$ under similar space constraints, where $h$ is the height of the computed DFS tree. Trivially, a DFS tree can be computed using a single pass using $O(m)$ space. In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$ passes, where each pass adds one vertex to the DFS tree. We then improve this algorithm by using more involved techniques to reduce the number of passes to $\\lceil h/k\\rceil$ under similar space constraints, where $h$ is the height of the computed DFS tree. In particular, this algorithm improves the bounds for the case where the computed DFS tree is shallow (having $o(n)$ height). Trivially, a DFS tree can be computed using a single pass using $O(m)$ space. In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$ passes, where each pass adds one vertex to the DFS tree. We then improve this algorithm by using more involved techniques to reduce the number of passes to $\\lceil h/k\\rceil$ under similar space constraints, where $h$ is the height of the computed DFS tree. In particular, this algorithm improves the bounds for the case where the computed DFS tree is shallow (having $o(n)$ height). Moreover, this algorithm is presented as a framework that allows the flexibility of using any algorithm to maintain a DFS tree of a stored sparser subgraph as a black box, which may be of independent interest. We present the first semi-streaming algorithms that compute a DFS tree of an undirected graph in $o(n)$ passes using $o(m)$ space. Both these algorithms essentially demonstrate the existence of a trade-off between the space and number of passes required for computing a DFS tree. We present the first semi-streaming algorithms that compute a DFS tree of an undirected graph in $o(n)$ passes using $o(m)$ space. Both these algorithms essentially demonstrate the existence of a trade-off between the space and number of passes required for computing a DFS tree. Furthermore, we evaluate these algorithms experimentally which reveals their exceptional performance in practice. We present the first semi-streaming algorithms that compute a DFS tree of an undirected graph in $o(n)$ passes using $o(m)$ space. Both these algorithms essentially demonstrate the existence of a trade-off between the space and number of passes required for computing a DFS tree. Furthermore, we evaluate these algorithms experimentally which reveals their exceptional performance in practice. For both random and real graphs, they require merely a few passes even when allowed just $O(n)$ space."}, {"paper_id": "17223042", "adju_relevance": 0, "title": "A highly asynchronous minimum spanning tree protocol", "background_label": "In this paper, we present an efficient distributed protocol for constructing a minimum-weight spanning tree (MST). Gallager, Humblet and Spira [5] proposed a protocol for this problem with time and message complexitiesO(N logN) andO(E+NlogN) respectively. A protocol withO(N) time complexity was proposed by Awerbuch [1].", "method_label": "We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We give a protocol which requiresO(min(N, (D+d)logN)) time andO(E+NlogNlogN/loglogN) messages. The protocol constructs a minimum spanning tree by growing disjoint subtrees of the MST (which are referred to asfragments). Fragments having the same minimum-weight outgoing edge are combined until a single fragment which spans the entire network remains. The protocols in [5] and [1] enforce a balanced growth of fragments. We relax the requirement of balanced growth and obtain a highly asynchronous protocol.", "result_label": "In this protocol, fast growing fragments combine more often and there-fore speed up the execution.", "abstract": "In this paper, we present an efficient distributed protocol for constructing a minimum-weight spanning tree (MST). In this paper, we present an efficient distributed protocol for constructing a minimum-weight spanning tree (MST). Gallager, Humblet and Spira [5] proposed a protocol for this problem with time and message complexitiesO(N logN) andO(E+NlogN) respectively. In this paper, we present an efficient distributed protocol for constructing a minimum-weight spanning tree (MST). Gallager, Humblet and Spira [5] proposed a protocol for this problem with time and message complexitiesO(N logN) andO(E+NlogN) respectively. A protocol withO(N) time complexity was proposed by Awerbuch [1]. We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We give a protocol which requiresO(min(N, (D+d)logN)) time andO(E+NlogNlogN/loglogN) messages. We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We give a protocol which requiresO(min(N, (D+d)logN)) time andO(E+NlogNlogN/loglogN) messages. The protocol constructs a minimum spanning tree by growing disjoint subtrees of the MST (which are referred to asfragments). We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We give a protocol which requiresO(min(N, (D+d)logN)) time andO(E+NlogNlogN/loglogN) messages. The protocol constructs a minimum spanning tree by growing disjoint subtrees of the MST (which are referred to asfragments). Fragments having the same minimum-weight outgoing edge are combined until a single fragment which spans the entire network remains. We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We give a protocol which requiresO(min(N, (D+d)logN)) time andO(E+NlogNlogN/loglogN) messages. The protocol constructs a minimum spanning tree by growing disjoint subtrees of the MST (which are referred to asfragments). Fragments having the same minimum-weight outgoing edge are combined until a single fragment which spans the entire network remains. The protocols in [5] and [1] enforce a balanced growth of fragments. We show that the time complexity of the protocol in [5] can also be expressed asO((D+d) logN), whereD is the maximum degree of a node andd is a diameter of the MST and therefore this protocol performs better than the protocol in [1] wheneverD+d<N/logN. We give a protocol which requiresO(min(N, (D+d)logN)) time andO(E+NlogNlogN/loglogN) messages. The protocol constructs a minimum spanning tree by growing disjoint subtrees of the MST (which are referred to asfragments). Fragments having the same minimum-weight outgoing edge are combined until a single fragment which spans the entire network remains. The protocols in [5] and [1] enforce a balanced growth of fragments. We relax the requirement of balanced growth and obtain a highly asynchronous protocol. In this protocol, fast growing fragments combine more often and there-fore speed up the execution."}, {"paper_id": "1779773", "adju_relevance": 0, "title": "Dual Decomposition with Many Overlapping Components", "background_label": "AbstractDual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient.", "method_label": "We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes.", "result_label": "We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.", "abstract": "AbstractDual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. AbstractDual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "26604137", "adju_relevance": 0, "title": "Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence", "method_label": "We present a new method for the joint task of tagging and non-projective dependency parsing. The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through Lagrangian relaxation.", "background_label": "We demonstrate its usefulness with an application to discontinu-ous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly.", "result_label": "We evaluate this model and obtain state-of-the-art results despite strong independence assumptions.", "abstract": "We present a new method for the joint task of tagging and non-projective dependency parsing. We demonstrate its usefulness with an application to discontinu-ous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly. We present a new method for the joint task of tagging and non-projective dependency parsing. The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through Lagrangian relaxation. We evaluate this model and obtain state-of-the-art results despite strong independence assumptions."}, {"paper_id": "578137", "adju_relevance": 0, "title": "The Euler Path to Static Level-Ancestors", "background_label": "Suppose that a rooted tree T is given for preprocessing.", "abstract": "Suppose that a rooted tree T is given for preprocessing."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "15790317", "adju_relevance": 0, "title": "The probabilistic minimum spanning tree problem", "background_label": "In this paper we consider a natural probabilistic variation of the classical minimum spanning tree problem (MST), which we call the probabilistic minimum spanning tree problem (PMST). In particular, we consider the case where not all the points are deterministically present, but are present with certain probability.", "method_label": "We discuss the applications of the PMST and find a closed-form expression for the expected length of a given spanning tree. Based on these expressions, we prove that the problem is NP-complete. We further examine some interesting combinatorial properties of the problem, establish the relation of the PMST with the MST and the network design problem, and examine some cases where the problem is solvable in polynomial time. We finally characterize the asymptotic behavior of reoptimization strategies, in which we find the MST or the Steiner tree, respectively, among the points that are present on a particular instance, and the PMST, in the case in which points are randomly distributed in the Euclidean plane and in the case in which the costs of the arcs are randomly distributed.", "result_label": "In both cases the PMST is within constant factors from both strategies.", "abstract": "In this paper we consider a natural probabilistic variation of the classical minimum spanning tree problem (MST), which we call the probabilistic minimum spanning tree problem (PMST). In this paper we consider a natural probabilistic variation of the classical minimum spanning tree problem (MST), which we call the probabilistic minimum spanning tree problem (PMST). In particular, we consider the case where not all the points are deterministically present, but are present with certain probability. We discuss the applications of the PMST and find a closed-form expression for the expected length of a given spanning tree. We discuss the applications of the PMST and find a closed-form expression for the expected length of a given spanning tree. Based on these expressions, we prove that the problem is NP-complete. We discuss the applications of the PMST and find a closed-form expression for the expected length of a given spanning tree. Based on these expressions, we prove that the problem is NP-complete. We further examine some interesting combinatorial properties of the problem, establish the relation of the PMST with the MST and the network design problem, and examine some cases where the problem is solvable in polynomial time. We discuss the applications of the PMST and find a closed-form expression for the expected length of a given spanning tree. Based on these expressions, we prove that the problem is NP-complete. We further examine some interesting combinatorial properties of the problem, establish the relation of the PMST with the MST and the network design problem, and examine some cases where the problem is solvable in polynomial time. We finally characterize the asymptotic behavior of reoptimization strategies, in which we find the MST or the Steiner tree, respectively, among the points that are present on a particular instance, and the PMST, in the case in which points are randomly distributed in the Euclidean plane and in the case in which the costs of the arcs are randomly distributed. In both cases the PMST is within constant factors from both strategies."}, {"paper_id": "88524777", "adju_relevance": 0, "title": "Hieroglyph: Hierarchical Glia Graph Skeletonization and Matching", "background_label": "Automatic 3D reconstruction of glia morphology is a powerful tool necessary for investigating the role of microglia in neurological disorders in the central nervous system. Current glia skeleton reconstruction techniques fail to capture an accurate tracing of the processes over time, useful for the study of the microglia motility and morphology in the brain during healthy and diseased states.", "method_label": "We propose Hieroglyph, a fully automatic temporal 3D skeleton reconstruction algorithm for glia imaged via 3D multiphoton microscopy. Hieroglyph yielded a 21% performance increase compared to state of the art automatic skeleton reconstruction methods and outperforms the state of the art in different measures of consistency on datasets of 3D images of microglia.", "result_label": "The results from this method provide a 3D graph and digital reconstruction of glia useful for a myriad of morphological analyses that could impact studies in brain immunology and disease.", "abstract": "Automatic 3D reconstruction of glia morphology is a powerful tool necessary for investigating the role of microglia in neurological disorders in the central nervous system. Automatic 3D reconstruction of glia morphology is a powerful tool necessary for investigating the role of microglia in neurological disorders in the central nervous system. Current glia skeleton reconstruction techniques fail to capture an accurate tracing of the processes over time, useful for the study of the microglia motility and morphology in the brain during healthy and diseased states. We propose Hieroglyph, a fully automatic temporal 3D skeleton reconstruction algorithm for glia imaged via 3D multiphoton microscopy. We propose Hieroglyph, a fully automatic temporal 3D skeleton reconstruction algorithm for glia imaged via 3D multiphoton microscopy. Hieroglyph yielded a 21% performance increase compared to state of the art automatic skeleton reconstruction methods and outperforms the state of the art in different measures of consistency on datasets of 3D images of microglia. The results from this method provide a 3D graph and digital reconstruction of glia useful for a myriad of morphological analyses that could impact studies in brain immunology and disease."}, {"paper_id": "1637866", "adju_relevance": 0, "title": "Dual Decomposition for Parsing with Non-Projective Head Automata", "background_label": "AbstractThis paper introduces algorithms for nonprojective parsing based on dual decomposition.", "abstract": "AbstractThis paper introduces algorithms for nonprojective parsing based on dual decomposition."}, {"paper_id": "39118261", "adju_relevance": 0, "title": "Separator Based Sparsification. I. Planary Testing and Minimum Spanning Trees", "background_label": "We describe algorithms and data structures for maintaining a dynamic planar graph subject to edge insertions and edge deletions that preserve planarity but that can change the embedding.", "method_label": "We give a fully dynamic planarity testing algorithm that maintains a graph subject to edge insertions and deletions and that allows queries that test whether the graph is currently planar, or whether a potential new edge would violate planarity, in O(n 1\u00c22 ) amortized time per update or query. We give fully dynamic algorithms for maintaining the connected components, the best swap and the minimum spanning forest of a planar graph in O(log n) worst-case time per insertion and O(log 2 n) per deletion. Finally, we give fully dynamic algorithms for maintaining the 2-edge-connected components of a planar graph in O(log n) amortized time per insertion and O(log 2 n) per deletion. All of the data structures, except for the one that answers planarity queries, handle only insertions that keep the graph planar.", "result_label": "All our algorithms improve previous bounds. The improvements are based upon a new type of sparsification combined with several properties of separators in planar graphs. ]", "abstract": "We describe algorithms and data structures for maintaining a dynamic planar graph subject to edge insertions and edge deletions that preserve planarity but that can change the embedding. We give a fully dynamic planarity testing algorithm that maintains a graph subject to edge insertions and deletions and that allows queries that test whether the graph is currently planar, or whether a potential new edge would violate planarity, in O(n 1\u00c22 ) amortized time per update or query. We give a fully dynamic planarity testing algorithm that maintains a graph subject to edge insertions and deletions and that allows queries that test whether the graph is currently planar, or whether a potential new edge would violate planarity, in O(n 1\u00c22 ) amortized time per update or query. We give fully dynamic algorithms for maintaining the connected components, the best swap and the minimum spanning forest of a planar graph in O(log n) worst-case time per insertion and O(log 2 n) per deletion. We give a fully dynamic planarity testing algorithm that maintains a graph subject to edge insertions and deletions and that allows queries that test whether the graph is currently planar, or whether a potential new edge would violate planarity, in O(n 1\u00c22 ) amortized time per update or query. We give fully dynamic algorithms for maintaining the connected components, the best swap and the minimum spanning forest of a planar graph in O(log n) worst-case time per insertion and O(log 2 n) per deletion. Finally, we give fully dynamic algorithms for maintaining the 2-edge-connected components of a planar graph in O(log n) amortized time per insertion and O(log 2 n) per deletion. We give a fully dynamic planarity testing algorithm that maintains a graph subject to edge insertions and deletions and that allows queries that test whether the graph is currently planar, or whether a potential new edge would violate planarity, in O(n 1\u00c22 ) amortized time per update or query. We give fully dynamic algorithms for maintaining the connected components, the best swap and the minimum spanning forest of a planar graph in O(log n) worst-case time per insertion and O(log 2 n) per deletion. Finally, we give fully dynamic algorithms for maintaining the 2-edge-connected components of a planar graph in O(log n) amortized time per insertion and O(log 2 n) per deletion. All of the data structures, except for the one that answers planarity queries, handle only insertions that keep the graph planar. All our algorithms improve previous bounds. All our algorithms improve previous bounds. The improvements are based upon a new type of sparsification combined with several properties of separators in planar graphs. ]"}, {"paper_id": "10412124", "adju_relevance": 0, "title": "Deterministic Partially Dynamic Single Source Shortest Paths for Sparse Graphs", "method_label": "(Our algorithm can also be modified to work in the incremental setting, where the graph is initially empty and subject to a sequence of online adversarial edge insertions.) In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. Later papers presented conditional lower bounds showing that O(mn) is optimal up to log factors. In SODA 2011, Bernstein and Roditty showed how to bypass these lower bounds and improve upon the Even and Shiloach O(mn) total update time bound by allowing a (1 + \u03f5) approximation.", "abstract": " (Our algorithm can also be modified to work in the incremental setting, where the graph is initially empty and subject to a sequence of online adversarial edge insertions.) (Our algorithm can also be modified to work in the incremental setting, where the graph is initially empty and subject to a sequence of online adversarial edge insertions.) In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. (Our algorithm can also be modified to work in the incremental setting, where the graph is initially empty and subject to a sequence of online adversarial edge insertions.) In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. Later papers presented conditional lower bounds showing that O(mn) is optimal up to log factors. (Our algorithm can also be modified to work in the incremental setting, where the graph is initially empty and subject to a sequence of online adversarial edge insertions.) In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. Later papers presented conditional lower bounds showing that O(mn) is optimal up to log factors. In SODA 2011, Bernstein and Roditty showed how to bypass these lower bounds and improve upon the Even and Shiloach O(mn) total update time bound by allowing a (1 + \u03f5) approximation."}, {"paper_id": "88512466", "adju_relevance": 0, "title": "Learning Loosely Connected Markov Random Fields", "background_label": "We consider the structure learning problem for graphical models that we call loosely connected Markov random fields, in which the number of short paths between any pair of nodes is small, and present a new conditional independence test based algorithm for learning the underlying graph structure.", "method_label": "The novel maximization step in our algorithm ensures that the true edges are detected correctly even when there are short cycles in the graph. The number of samples required by our algorithm is C*log p, where p is the size of the graph and the constant C depends on the parameters of the model. We show that several previously studied models are examples of loosely connected Markov random fields, and our algorithm achieves the same or lower computational complexity than the previously designed algorithms for individual cases.", "result_label": "We also get new results for more general graphical models, in particular, our algorithm learns general Ising models on the Erdos-Renyi random graph G(p, c/p) correctly with running time O(np^5).", "abstract": "We consider the structure learning problem for graphical models that we call loosely connected Markov random fields, in which the number of short paths between any pair of nodes is small, and present a new conditional independence test based algorithm for learning the underlying graph structure. The novel maximization step in our algorithm ensures that the true edges are detected correctly even when there are short cycles in the graph. The novel maximization step in our algorithm ensures that the true edges are detected correctly even when there are short cycles in the graph. The number of samples required by our algorithm is C*log p, where p is the size of the graph and the constant C depends on the parameters of the model. The novel maximization step in our algorithm ensures that the true edges are detected correctly even when there are short cycles in the graph. The number of samples required by our algorithm is C*log p, where p is the size of the graph and the constant C depends on the parameters of the model. We show that several previously studied models are examples of loosely connected Markov random fields, and our algorithm achieves the same or lower computational complexity than the previously designed algorithms for individual cases. We also get new results for more general graphical models, in particular, our algorithm learns general Ising models on the Erdos-Renyi random graph G(p, c/p) correctly with running time O(np^5)."}, {"paper_id": "21542371", "adju_relevance": 0, "title": "Theory of minimum spanning trees I: Mean-field theory and strongly disordered spin-glass model", "background_label": "The minimum spanning tree (MST) is a combinatorial optimization problem: given a connected graph with a real weight (\"cost\") on each edge, find the spanning tree that minimizes the sum of the total cost of the occupied edges.", "method_label": "We consider the random MST, in which the edge costs are (quenched) independent random variables.", "abstract": "The minimum spanning tree (MST) is a combinatorial optimization problem: given a connected graph with a real weight (\"cost\") on each edge, find the spanning tree that minimizes the sum of the total cost of the occupied edges. We consider the random MST, in which the edge costs are (quenched) independent random variables."}, {"paper_id": "4776668", "adju_relevance": 0, "title": "Drain: An Online Log Parsing Approach with Fixed Depth Tree", "background_label": "Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. Most of the existing log parsing methods focus on offline, batch processing of logs.", "method_label": "A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. We evaluate Drain on five real-world log data sets with more than 10 million raw log messages.", "result_label": "The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. We also conduct a case study on an anomaly detection task using Drain in the parsing step, which determines the effectiveness of Drain in log analysis.", "abstract": "Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. Most of the existing log parsing methods focus on offline, batch processing of logs. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. We evaluate Drain on five real-world log data sets with more than 10 million raw log messages. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. We also conduct a case study on an anomaly detection task using Drain in the parsing step, which determines the effectiveness of Drain in log analysis."}, {"paper_id": "18752624", "adju_relevance": 0, "title": "Linear algorithm for lexicographic enumeration of CFG parse trees", "background_label": "We study CFG parse tree enumeration in this paper. By dividing the set of all parse trees into infinite hierarchies according to height of parse tree, the hierarchical lexicographic order on the set of parse trees is established.", "method_label": "Then grammar-based algorithms for counting and enumerating CFG parse trees in this order are presented. To generate a parse tree of height n, the time complexity is O(n). If \u03c4 is a lowest parse tree for its yield, then O(n) =O(\u2225 gt \u2225+ 1), where \u2225\u03c4 \u2225 is the length of the sentence (yield) generated by \u03c4. The sentence can be obtained as a by-product of the parse tree.", "result_label": "To compute sentence from its parse tree (needn\u2019t be lowest one), the time complexity is O(node)+O(\u2225\u03c4 \u2225 + 1), where node is the number of non-leaf nodes of parse tree \u03c4. To generate both a complete lowest parse tree and its yield at the same time, the time complexity is O(\u2225\u03c4 \u2225 + 1).", "abstract": "We study CFG parse tree enumeration in this paper. We study CFG parse tree enumeration in this paper. By dividing the set of all parse trees into infinite hierarchies according to height of parse tree, the hierarchical lexicographic order on the set of parse trees is established. Then grammar-based algorithms for counting and enumerating CFG parse trees in this order are presented. Then grammar-based algorithms for counting and enumerating CFG parse trees in this order are presented. To generate a parse tree of height n, the time complexity is O(n). Then grammar-based algorithms for counting and enumerating CFG parse trees in this order are presented. To generate a parse tree of height n, the time complexity is O(n). If \u03c4 is a lowest parse tree for its yield, then O(n) =O(\u2225 gt \u2225+ 1), where \u2225\u03c4 \u2225 is the length of the sentence (yield) generated by \u03c4. Then grammar-based algorithms for counting and enumerating CFG parse trees in this order are presented. To generate a parse tree of height n, the time complexity is O(n). If \u03c4 is a lowest parse tree for its yield, then O(n) =O(\u2225 gt \u2225+ 1), where \u2225\u03c4 \u2225 is the length of the sentence (yield) generated by \u03c4. The sentence can be obtained as a by-product of the parse tree. To compute sentence from its parse tree (needn\u2019t be lowest one), the time complexity is O(node)+O(\u2225\u03c4 \u2225 + 1), where node is the number of non-leaf nodes of parse tree \u03c4. To compute sentence from its parse tree (needn\u2019t be lowest one), the time complexity is O(node)+O(\u2225\u03c4 \u2225 + 1), where node is the number of non-leaf nodes of parse tree \u03c4. To generate both a complete lowest parse tree and its yield at the same time, the time complexity is O(\u2225\u03c4 \u2225 + 1)."}, {"paper_id": "3262717", "adju_relevance": 0, "title": "Three New Probabilistic Models for Dependency Parsing: An Exploration", "background_label": "After presenting a novel O(n^3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it.", "method_label": "We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank).", "result_label": "In these results, the generative (i.e., top-down) model performs significantly better than the others, and does about equally well at assigning part-of-speech tags.", "abstract": "After presenting a novel O(n^3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative (i.e., top-down) model performs significantly better than the others, and does about equally well at assigning part-of-speech tags."}, {"paper_id": "7534444", "adju_relevance": 0, "title": "Dependency Parsing with Dilated Iterated Graph CNNs", "background_label": "Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in GPU hardware have enabled neural networks to achieve significant gains over the previous best models, these models still fail to leverage GPUs' capability for massive parallelism due to their requirement of sequential processing of the sentence.", "method_label": "In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing.", "result_label": "In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best neural network parsers.", "abstract": "Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in GPU hardware have enabled neural networks to achieve significant gains over the previous best models, these models still fail to leverage GPUs' capability for massive parallelism due to their requirement of sequential processing of the sentence. In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best neural network parsers."}, {"paper_id": "14724867", "adju_relevance": 0, "title": "Real-Time Salient Object Detection with a Minimum Spanning Tree", "background_label": "In this paper, we present a real-time salient object detection system based on the minimum spanning tree. Due to the fact that background regions are typically connected to the image boundaries, salient objects can be extracted by computing the distances to the boundaries. However, measuring the image boundary connectivity efficiently is a challenging problem.", "method_label": "Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Instead, we propose an exact and iteration free solution on a minimum spanning tree. The minimum spanning tree representation of an image inherently reveals the object geometry information in a scene. Meanwhile, it largely reduces the search space of shortest paths, resulting an efficient and high quality distance transform algorithm. We further introduce a boundary dissimilarity measure to compliment the shortage of distance transform for salient object detection.", "result_label": "Extensive evaluations show that the proposed algorithm achieves the leading performance compared to the state-of-the-art methods in terms of efficiency and accuracy.", "abstract": "In this paper, we present a real-time salient object detection system based on the minimum spanning tree. In this paper, we present a real-time salient object detection system based on the minimum spanning tree. Due to the fact that background regions are typically connected to the image boundaries, salient objects can be extracted by computing the distances to the boundaries. In this paper, we present a real-time salient object detection system based on the minimum spanning tree. Due to the fact that background regions are typically connected to the image boundaries, salient objects can be extracted by computing the distances to the boundaries. However, measuring the image boundary connectivity efficiently is a challenging problem. Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Instead, we propose an exact and iteration free solution on a minimum spanning tree. Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Instead, we propose an exact and iteration free solution on a minimum spanning tree. The minimum spanning tree representation of an image inherently reveals the object geometry information in a scene. Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Instead, we propose an exact and iteration free solution on a minimum spanning tree. The minimum spanning tree representation of an image inherently reveals the object geometry information in a scene. Meanwhile, it largely reduces the search space of shortest paths, resulting an efficient and high quality distance transform algorithm. Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Instead, we propose an exact and iteration free solution on a minimum spanning tree. The minimum spanning tree representation of an image inherently reveals the object geometry information in a scene. Meanwhile, it largely reduces the search space of shortest paths, resulting an efficient and high quality distance transform algorithm. We further introduce a boundary dissimilarity measure to compliment the shortage of distance transform for salient object detection. Extensive evaluations show that the proposed algorithm achieves the leading performance compared to the state-of-the-art methods in terms of efficiency and accuracy."}, {"paper_id": "12003625", "adju_relevance": 0, "title": "An OpenFlow-Based Dynamic Path Adjustment Algorithm for Multicast Spanning Trees", "background_label": "Multicast transmission can effectively reduce the cost of network resources in delivering packets to a group of receivers. However, it is difficult to adjust the path of the generated multicast spanning tree (MST) if the multicast source and group members are unchanged. OpenFlow has been a key technology to realize the software defined networking and makes the flow control feasible.", "abstract": "Multicast transmission can effectively reduce the cost of network resources in delivering packets to a group of receivers. Multicast transmission can effectively reduce the cost of network resources in delivering packets to a group of receivers. However, it is difficult to adjust the path of the generated multicast spanning tree (MST) if the multicast source and group members are unchanged. Multicast transmission can effectively reduce the cost of network resources in delivering packets to a group of receivers. However, it is difficult to adjust the path of the generated multicast spanning tree (MST) if the multicast source and group members are unchanged. OpenFlow has been a key technology to realize the software defined networking and makes the flow control feasible."}, {"paper_id": "3396562", "adju_relevance": 0, "title": "Spanning Tree Congestion and Computation of Generalized Gy\\H{o}ri-Lov\\'{a}sz Partition", "background_label": "We study a natural problem in graph sparsification, the Spanning Tree Congestion (\\STC) problem. Informally, the \\STC problem seeks a spanning tree with no tree-edge \\emph{routing} too many of the original edges. The root of this problem dates back to at least 30 years ago, motivated by applications in network design, parallel computing and circuit design.", "method_label": "Variants of the problem have also seen algorithmic applications as a preprocessing step of several important graph algorithms.", "abstract": "We study a natural problem in graph sparsification, the Spanning Tree Congestion (\\STC) problem. We study a natural problem in graph sparsification, the Spanning Tree Congestion (\\STC) problem. Informally, the \\STC problem seeks a spanning tree with no tree-edge \\emph{routing} too many of the original edges. We study a natural problem in graph sparsification, the Spanning Tree Congestion (\\STC) problem. Informally, the \\STC problem seeks a spanning tree with no tree-edge \\emph{routing} too many of the original edges. The root of this problem dates back to at least 30 years ago, motivated by applications in network design, parallel computing and circuit design. Variants of the problem have also seen algorithmic applications as a preprocessing step of several important graph algorithms."}, {"paper_id": "1550476", "adju_relevance": 0, "title": "Symbolic Probabilistic Inference with Evidence Potential", "background_label": "Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same features. With the clique tree, various probability distributions are computed and stored in each clique. This is the ?pre-processing? step of SEPI.", "method_label": "We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm. Once this step is done, the query can then be computed. To process a query, a recursive process similar to the SPI algorithm is used. The queries are directed to the root clique and decomposed into queries for the clique's subtrees until a particular query can be answered at the clique at which it is directed. The algorithm and the computation are simple.", "result_label": "The SEPI algorithm will be presented in this paper along with several examples.", "abstract": "Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same features. We call the extension symbolic evidence potential inference (SEPI). We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm. Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same features. With the clique tree, various probability distributions are computed and stored in each clique. Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same features. With the clique tree, various probability distributions are computed and stored in each clique. This is the ?pre-processing? Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2] has focused attention on the importance of resolving general queries in Bayesian networks. SPI applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. In response to this research we have extended the evidence potential algorithm [3] with the same features. With the clique tree, various probability distributions are computed and stored in each clique. This is the ?pre-processing? step of SEPI. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm. Once this step is done, the query can then be computed. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm. Once this step is done, the query can then be computed. To process a query, a recursive process similar to the SPI algorithm is used. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm. Once this step is done, the query can then be computed. To process a query, a recursive process similar to the SPI algorithm is used. The queries are directed to the root clique and decomposed into queries for the clique's subtrees until a particular query can be answered at the clique at which it is directed. We call the extension symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic queries and is incremental with respect to queries and observations. While in SPI, operations are done on a search tree constructed from the nodes of the original network, in SEPI, a clique-tree structure obtained from the evidence potential algorithm [3] is the basic framework for recursive query processing. In this paper, we describe the systematic query and caching procedure of SEPI. SEPI begins with finding a clique tree from a Bayesian network-the standard procedure of the evidence potential algorithm. Once this step is done, the query can then be computed. To process a query, a recursive process similar to the SPI algorithm is used. The queries are directed to the root clique and decomposed into queries for the clique's subtrees until a particular query can be answered at the clique at which it is directed. The algorithm and the computation are simple. The SEPI algorithm will be presented in this paper along with several examples."}, {"paper_id": "921607", "adju_relevance": 0, "title": "Better \u03f5-Dependencies for Offline Approximate Nearest Neighbor Search, Euclidean Minimum Spanning Trees, and \u03f5-Kernels", "background_label": "Recently, Arya, da Fonseca, and Mount [STOC 2011, SODA 2012] made notable progress in improving the &epsis;-dependencies in the space/query-time tradeoffs for (1 + &epsis;)-factor approximate nearest neighbor search in fixed-dimensional Euclidean spaces. However, &epsis;-dependencies in the preprocessing time were not considered, and so their data structures cannot be used to derive faster algorithms for offline proximity problems. Known algorithms for many such problems, including approximate bichromatic closest pair (BCP) and approximate Euclidean minimum spanning trees (EMST), typically have factors near (1/&epsis;)d/2\u00b1O(1) in the running time when the dimension d is a constant.", "method_label": "We describe a technique that breaks the (1/&epsis;)d/2 barrier and yields new results for many well-known proximity problems, including:  \u2022 an O((1/&epsis;)d/3+O(1) n)-time randomized algorithm for approximate BCP,  \u2022 an O((1/&epsis;)d/3+O(1) n log n)-time algorithm for approximate EMST, and  \u2022 an O(n log n + (1/&epsis;)d/3+O(1) n)-time algorithm to answer n approximate nearest neighbor queries on n points. Using additional bit-packing tricks, we can shave off the log n factor for EMST, and even move most of the &epsis;-factors to a sublinear term.", "result_label": "The improvement arises from a new time bound for exact \"discrete Voronoi diagrams\", which were previously used in the construction of &epsis;-kernels (or extent-based coresets), a well-known tool for another class of fundamental problems. This connection leads to more results, including:  \u2022 a streaming algorithm to maintain an approximate diameter in O((1/&epsis;)d/3+O(1)) time per point using O((1/&epsis;)d/2+O(1)) space, and  \u2022 a streaming algorithm to maintain an &epsis;-kernel in O((1/&epsis;)d/4+O(1)) time per point using O((1/&epsis;)d/2+O(1)) space.", "abstract": "Recently, Arya, da Fonseca, and Mount [STOC 2011, SODA 2012] made notable progress in improving the &epsis;-dependencies in the space/query-time tradeoffs for (1 + &epsis;)-factor approximate nearest neighbor search in fixed-dimensional Euclidean spaces. Recently, Arya, da Fonseca, and Mount [STOC 2011, SODA 2012] made notable progress in improving the &epsis;-dependencies in the space/query-time tradeoffs for (1 + &epsis;)-factor approximate nearest neighbor search in fixed-dimensional Euclidean spaces. However, &epsis;-dependencies in the preprocessing time were not considered, and so their data structures cannot be used to derive faster algorithms for offline proximity problems. Recently, Arya, da Fonseca, and Mount [STOC 2011, SODA 2012] made notable progress in improving the &epsis;-dependencies in the space/query-time tradeoffs for (1 + &epsis;)-factor approximate nearest neighbor search in fixed-dimensional Euclidean spaces. However, &epsis;-dependencies in the preprocessing time were not considered, and so their data structures cannot be used to derive faster algorithms for offline proximity problems. Known algorithms for many such problems, including approximate bichromatic closest pair (BCP) and approximate Euclidean minimum spanning trees (EMST), typically have factors near (1/&epsis;)d/2\u00b1O(1) in the running time when the dimension d is a constant. We describe a technique that breaks the (1/&epsis;)d/2 barrier and yields new results for many well-known proximity problems, including:  \u2022 an O((1/&epsis;)d/3+O(1) n)-time randomized algorithm for approximate BCP,  \u2022 an O((1/&epsis;)d/3+O(1) n log n)-time algorithm for approximate EMST, and  \u2022 an O(n log n + (1/&epsis;)d/3+O(1) n)-time algorithm to answer n approximate nearest neighbor queries on n points. We describe a technique that breaks the (1/&epsis;)d/2 barrier and yields new results for many well-known proximity problems, including:  \u2022 an O((1/&epsis;)d/3+O(1) n)-time randomized algorithm for approximate BCP,  \u2022 an O((1/&epsis;)d/3+O(1) n log n)-time algorithm for approximate EMST, and  \u2022 an O(n log n + (1/&epsis;)d/3+O(1) n)-time algorithm to answer n approximate nearest neighbor queries on n points. Using additional bit-packing tricks, we can shave off the log n factor for EMST, and even move most of the &epsis;-factors to a sublinear term. The improvement arises from a new time bound for exact \"discrete Voronoi diagrams\", which were previously used in the construction of &epsis;-kernels (or extent-based coresets), a well-known tool for another class of fundamental problems. The improvement arises from a new time bound for exact \"discrete Voronoi diagrams\", which were previously used in the construction of &epsis;-kernels (or extent-based coresets), a well-known tool for another class of fundamental problems. This connection leads to more results, including:  \u2022 a streaming algorithm to maintain an approximate diameter in O((1/&epsis;)d/3+O(1)) time per point using O((1/&epsis;)d/2+O(1)) space, and  \u2022 a streaming algorithm to maintain an &epsis;-kernel in O((1/&epsis;)d/4+O(1)) time per point using O((1/&epsis;)d/2+O(1)) space."}, {"paper_id": "52004253", "adju_relevance": 0, "title": "Testing Graph Clusterability: Algorithms and Lower Bounds", "background_label": "We consider the problem of testing graph cluster structure: given access to a graph $G=(V, E)$, can we quickly determine whether the graph can be partitioned into a few clusters with good inner conductance, or is far from any such graph? This is a generalization of the well-studied problem of testing graph expansion, where one wants to distinguish between the graph having good expansion (i.e.\\ being a good single cluster) and the graph having a sparse cut (i.e.\\ being a union of at least two clusters).", "method_label": "A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. This yields a very efficient testing algorithm, but only works if the cluster structure is very strong: it is necessary to assume that the gap between conductances of accepted and rejected graphs is at least logarithmic in the size of the graph $G$. In this paper we show how one can leverage more refined geometric information, namely angles as opposed to distances, to obtain a sublinear time tester that works even when the gap is a sufficiently large constant. Our tester is based on the singular value decomposition of a natural matrix derived from random walk transition probabilities from a small sample of seed nodes. We complement our algorithm with a matching lower bound on the query complexity of testing clusterability.", "result_label": "Our lower bound is based on a novel property testing problem, which we analyze using Fourier analytic tools. As a byproduct of our techniques, we also achieve new lower bounds for the problem of approximating MAX-CUT value in sublinear time.", "abstract": "We consider the problem of testing graph cluster structure: given access to a graph $G=(V, E)$, can we quickly determine whether the graph can be partitioned into a few clusters with good inner conductance, or is far from any such graph? We consider the problem of testing graph cluster structure: given access to a graph $G=(V, E)$, can we quickly determine whether the graph can be partitioned into a few clusters with good inner conductance, or is far from any such graph? This is a generalization of the well-studied problem of testing graph expansion, where one wants to distinguish between the graph having good expansion (i.e.\\ being a good single cluster) and the graph having a sparse cut (i.e.\\ being a union of at least two clusters). A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. This yields a very efficient testing algorithm, but only works if the cluster structure is very strong: it is necessary to assume that the gap between conductances of accepted and rejected graphs is at least logarithmic in the size of the graph $G$. A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. This yields a very efficient testing algorithm, but only works if the cluster structure is very strong: it is necessary to assume that the gap between conductances of accepted and rejected graphs is at least logarithmic in the size of the graph $G$. In this paper we show how one can leverage more refined geometric information, namely angles as opposed to distances, to obtain a sublinear time tester that works even when the gap is a sufficiently large constant. A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. This yields a very efficient testing algorithm, but only works if the cluster structure is very strong: it is necessary to assume that the gap between conductances of accepted and rejected graphs is at least logarithmic in the size of the graph $G$. In this paper we show how one can leverage more refined geometric information, namely angles as opposed to distances, to obtain a sublinear time tester that works even when the gap is a sufficiently large constant. Our tester is based on the singular value decomposition of a natural matrix derived from random walk transition probabilities from a small sample of seed nodes. A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. This yields a very efficient testing algorithm, but only works if the cluster structure is very strong: it is necessary to assume that the gap between conductances of accepted and rejected graphs is at least logarithmic in the size of the graph $G$. In this paper we show how one can leverage more refined geometric information, namely angles as opposed to distances, to obtain a sublinear time tester that works even when the gap is a sufficiently large constant. Our tester is based on the singular value decomposition of a natural matrix derived from random walk transition probabilities from a small sample of seed nodes. We complement our algorithm with a matching lower bound on the query complexity of testing clusterability. Our lower bound is based on a novel property testing problem, which we analyze using Fourier analytic tools. Our lower bound is based on a novel property testing problem, which we analyze using Fourier analytic tools. As a byproduct of our techniques, we also achieve new lower bounds for the problem of approximating MAX-CUT value in sublinear time."}, {"paper_id": "9659057", "adju_relevance": 0, "title": "A tree-decomposed transfer matrix for computing exact Potts model partition functions for arbitrary graphs, with applications to planar graph colourings", "background_label": "Combining tree decomposition and transfer matrix techniques provides a very general algorithm for computing exact partition functions of statistical models defined on arbitrary graphs. The algorithm is particularly efficient in the case of planar graphs.", "method_label": "We illustrate it by computing the Potts model partition functions and chromatic polynomials (the number of proper vertex colourings using Q colours) for large samples of random planar graphs with up to N=100 vertices.", "result_label": "In the latter case, our algorithm yields a sub-exponential average running time of ~ exp(1.516 sqrt(N)), a substantial improvement over the exponential running time ~ exp(0.245 N) provided by the hitherto best known algorithm. We study the statistics of chromatic roots of random planar graphs in some detail, comparing the findings with results for finite pieces of a regular lattice.", "abstract": "Combining tree decomposition and transfer matrix techniques provides a very general algorithm for computing exact partition functions of statistical models defined on arbitrary graphs. Combining tree decomposition and transfer matrix techniques provides a very general algorithm for computing exact partition functions of statistical models defined on arbitrary graphs. The algorithm is particularly efficient in the case of planar graphs. We illustrate it by computing the Potts model partition functions and chromatic polynomials (the number of proper vertex colourings using Q colours) for large samples of random planar graphs with up to N=100 vertices. In the latter case, our algorithm yields a sub-exponential average running time of ~ exp(1.516 sqrt(N)), a substantial improvement over the exponential running time ~ exp(0.245 N) provided by the hitherto best known algorithm. In the latter case, our algorithm yields a sub-exponential average running time of ~ exp(1.516 sqrt(N)), a substantial improvement over the exponential running time ~ exp(0.245 N) provided by the hitherto best known algorithm. We study the statistics of chromatic roots of random planar graphs in some detail, comparing the findings with results for finite pieces of a regular lattice."}, {"paper_id": "44110554", "adju_relevance": 0, "title": "An Improved Algorithm for Incremental DFS Tree in Undirected Graphs", "background_label": "Depth first search (DFS) tree is one of the most well-known data structures for designing efficient graph algorithms. Given an undirected graph $G=(V,E)$ with $n$ vertices and $m$ edges, the textbook algorithm takes $O(n+m)$ time to construct a DFS tree.", "abstract": "Depth first search (DFS) tree is one of the most well-known data structures for designing efficient graph algorithms. Depth first search (DFS) tree is one of the most well-known data structures for designing efficient graph algorithms. Given an undirected graph $G=(V,E)$ with $n$ vertices and $m$ edges, the textbook algorithm takes $O(n+m)$ time to construct a DFS tree."}, {"paper_id": "653597", "adju_relevance": 0, "title": "Distributed verification of minimum spanning trees", "background_label": "The problem of verifying a Minimum Spanning Tree (MST) was introduced by Tarjan in a sequential setting. Given a graph and a tree that spans it, the algorithm is required to check whether this tree is an MST.", "abstract": "The problem of verifying a Minimum Spanning Tree (MST) was introduced by Tarjan in a sequential setting. The problem of verifying a Minimum Spanning Tree (MST) was introduced by Tarjan in a sequential setting. Given a graph and a tree that spans it, the algorithm is required to check whether this tree is an MST."}, {"paper_id": "12523814", "adju_relevance": 0, "title": "RTED: A Robust Algorithm for the Tree Edit Distance", "background_label": "We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity, but the worst case happens frequently, or they are very efficient for some tree shapes, but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms.", "method_label": "In this paper we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of RTED is smaller or equal to the complexity of the best competitors for any input instance, i.e., RTED is both efficient and worst-case optimal. We introduce the class of LRH (Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit distance algorithms presented in literature.", "result_label": "We prove that RTED outperforms all previously proposed LRH algorithms in terms of runtime complexity. In our experiments on synthetic and real world data we empirically evaluate our solution and compare it to the state-of-the-art.", "abstract": "We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity, but the worst case happens frequently, or they are very efficient for some tree shapes, but degenerate for others. We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity, but the worst case happens frequently, or they are very efficient for some tree shapes, but degenerate for others. This leads to unpredictable and often infeasible runtimes. We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity, but the worst case happens frequently, or they are very efficient for some tree shapes, but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms. In this paper we present RTED, a robust tree edit distance algorithm. In this paper we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of RTED is smaller or equal to the complexity of the best competitors for any input instance, i.e., RTED is both efficient and worst-case optimal. In this paper we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of RTED is smaller or equal to the complexity of the best competitors for any input instance, i.e., RTED is both efficient and worst-case optimal. We introduce the class of LRH (Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit distance algorithms presented in literature. We prove that RTED outperforms all previously proposed LRH algorithms in terms of runtime complexity. We prove that RTED outperforms all previously proposed LRH algorithms in terms of runtime complexity. In our experiments on synthetic and real world data we empirically evaluate our solution and compare it to the state-of-the-art."}, {"paper_id": "6884123", "adju_relevance": 0, "title": "Graph Partitioning with Natural Cuts", "background_label": "We present a novel approach to graph partitioning based on the notion of \\emph{natural cuts}. Our algorithm, called PUNCH, has two phases.", "method_label": "The first phase performs a series of minimum-cut computations to identify and contract dense regions of the graph. This reduces the graph size, but preserves its general structure. The second phase uses a combination of greedy and local search heuristics to assemble the final partition. The algorithm performs especially well on road networks, which have an abundance of natural cuts (such as bridges, mountain passes, and ferries).", "result_label": "In a few minutes, it obtains the best known partitions for continental-sized networks, significantly improving on previous results.", "abstract": "We present a novel approach to graph partitioning based on the notion of \\emph{natural cuts}. We present a novel approach to graph partitioning based on the notion of \\emph{natural cuts}. Our algorithm, called PUNCH, has two phases. The first phase performs a series of minimum-cut computations to identify and contract dense regions of the graph. The first phase performs a series of minimum-cut computations to identify and contract dense regions of the graph. This reduces the graph size, but preserves its general structure. The first phase performs a series of minimum-cut computations to identify and contract dense regions of the graph. This reduces the graph size, but preserves its general structure. The second phase uses a combination of greedy and local search heuristics to assemble the final partition. The first phase performs a series of minimum-cut computations to identify and contract dense regions of the graph. This reduces the graph size, but preserves its general structure. The second phase uses a combination of greedy and local search heuristics to assemble the final partition. The algorithm performs especially well on road networks, which have an abundance of natural cuts (such as bridges, mountain passes, and ferries). In a few minutes, it obtains the best known partitions for continental-sized networks, significantly improving on previous results."}, {"paper_id": "844854", "adju_relevance": 0, "title": "Isoperimetric partitioning: A new algorithm for graph partitioning", "background_label": "We present a new algorithm for graph partitioning based on optimization of the combinatorial isoperimetric constant. It is shown empirically that this algorithm is competitive with other global partitioning algorithms in terms of partition quality. The isoperimetric algorithm for graph partitioning is implemented in our publicly available Graph Analysis Toolbox Methods of graph partitioning take different forms, depending on the number of partitions required-whether or not the nodes have coordinates-and the cardinality constraints of the sets. In this paper, we use the term partition to refer to the assignment of each node in the vertex set into two (not necessarily equal) parts.", "method_label": "The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. An exact circuit analogy to the algorithm is also developed with a natural random walks interpretation. We propose an algorithm termed isoperimetric partitioning, since it is derived and motivated by the isoperimetric constant defined for continuous manifolds [15] . The isoperimetric algorithm most closely resembles spectral partitioning in its use and ability to create hybrids with other algorithms (e.g., multilevel spectral partitioning [40] and geometric-spectral partitioning [14] ). However, it requires the solution to a large, sparse system of equations rather than solving the eigenvector problem for a large, sparse matrix. This difference leads to improved speed and numerical stability.The paper is organized as follows: we begin by deriving the isoperimetric algorithm from the isoperimetric constant of a graph in section 2, followed in section 3 by two physical analogies for the isoperimetric algorithm, and in section 4 by proving a few formal properties of the algorithm.", "result_label": "In section 5, we review the most popular and", "abstract": " We present a new algorithm for graph partitioning based on optimization of the combinatorial isoperimetric constant. We present a new algorithm for graph partitioning based on optimization of the combinatorial isoperimetric constant. It is shown empirically that this algorithm is competitive with other global partitioning algorithms in terms of partition quality. The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. An exact circuit analogy to the algorithm is also developed with a natural random walks interpretation. We present a new algorithm for graph partitioning based on optimization of the combinatorial isoperimetric constant. It is shown empirically that this algorithm is competitive with other global partitioning algorithms in terms of partition quality. The isoperimetric algorithm for graph partitioning is implemented in our publicly available Graph Analysis Toolbox Methods of graph partitioning take different forms, depending on the number of partitions required-whether or not the nodes have coordinates-and the cardinality constraints of the sets. We present a new algorithm for graph partitioning based on optimization of the combinatorial isoperimetric constant. It is shown empirically that this algorithm is competitive with other global partitioning algorithms in terms of partition quality. The isoperimetric algorithm for graph partitioning is implemented in our publicly available Graph Analysis Toolbox Methods of graph partitioning take different forms, depending on the number of partitions required-whether or not the nodes have coordinates-and the cardinality constraints of the sets. In this paper, we use the term partition to refer to the assignment of each node in the vertex set into two (not necessarily equal) parts. The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. An exact circuit analogy to the algorithm is also developed with a natural random walks interpretation. We propose an algorithm termed isoperimetric partitioning, since it is derived and motivated by the isoperimetric constant defined for continuous manifolds [15] . The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. An exact circuit analogy to the algorithm is also developed with a natural random walks interpretation. We propose an algorithm termed isoperimetric partitioning, since it is derived and motivated by the isoperimetric constant defined for continuous manifolds [15] . The isoperimetric algorithm most closely resembles spectral partitioning in its use and ability to create hybrids with other algorithms (e.g., multilevel spectral partitioning [40] and geometric-spectral partitioning [14] ). The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. An exact circuit analogy to the algorithm is also developed with a natural random walks interpretation. We propose an algorithm termed isoperimetric partitioning, since it is derived and motivated by the isoperimetric constant defined for continuous manifolds [15] . The isoperimetric algorithm most closely resembles spectral partitioning in its use and ability to create hybrids with other algorithms (e.g., multilevel spectral partitioning [40] and geometric-spectral partitioning [14] ). However, it requires the solution to a large, sparse system of equations rather than solving the eigenvector problem for a large, sparse matrix. The isoperimetric algorithm is easy to parallelize, does not require coordinate information, and handles nonplanar graphs, weighted graphs, and families of graphs which are known to cause problems for other methods. Compared to spectral partitioning, the isoperimetric algorithm is faster and more stable. An exact circuit analogy to the algorithm is also developed with a natural random walks interpretation. We propose an algorithm termed isoperimetric partitioning, since it is derived and motivated by the isoperimetric constant defined for continuous manifolds [15] . The isoperimetric algorithm most closely resembles spectral partitioning in its use and ability to create hybrids with other algorithms (e.g., multilevel spectral partitioning [40] and geometric-spectral partitioning [14] ). However, it requires the solution to a large, sparse system of equations rather than solving the eigenvector problem for a large, sparse matrix. This difference leads to improved speed and numerical stability.The paper is organized as follows: we begin by deriving the isoperimetric algorithm from the isoperimetric constant of a graph in section 2, followed in section 3 by two physical analogies for the isoperimetric algorithm, and in section 4 by proving a few formal properties of the algorithm. In section 5, we review the most popular and"}, {"paper_id": "410716", "adju_relevance": 0, "title": "Generalizing Swendsen-Wang to sampling arbitrary posterior probabilities", "background_label": "Many vision tasks can be formulated as graph partition problems that minimize energy functions. For such problems, the Gibbs sampler provides a general solution but is very slow, while other methods, such as Ncut and graph cuts are computationally effective but only work for specific energy forms and are not generally applicable.", "abstract": "Many vision tasks can be formulated as graph partition problems that minimize energy functions. Many vision tasks can be formulated as graph partition problems that minimize energy functions. For such problems, the Gibbs sampler provides a general solution but is very slow, while other methods, such as Ncut and graph cuts are computationally effective but only work for specific energy forms and are not generally applicable."}, {"paper_id": "18658744", "adju_relevance": 0, "title": "LCA Queries in Directed Acyclic Graphs", "method_label": "We present two methods for finding a lowest common ancestor (LCA) for each pair of vertices of a directed acyclic graph (dag) on n vertices and m edges.The first method is surprisingly natural and solves the all-pairs LCA problem for the input dag on n vertices and m edges in time O(nm). As a corollary, we obtain an O(n 2 )-time algorithm for finding genealogical distances considerably improving the previously known O(n 2.575 ) timebound for this problem.The second method relies on a novel reduction of the all-pairs LCA problem to the problem of finding maximum witnesses for Boolean matrix product. We solve the latter problem and hence also the all-pairs LCA problem in time O(n 2+ 1 4\u2212\u03c9 ), where \u03c9 = 2.376 is the exponent of the fastest known matrix multiplication algorithm.", "result_label": "This improves the previously known O(n w+3 2 ) time-bound for the general all-pairs LCA problem in dags.", "abstract": " We present two methods for finding a lowest common ancestor (LCA) for each pair of vertices of a directed acyclic graph (dag) on n vertices and m edges.The first method is surprisingly natural and solves the all-pairs LCA problem for the input dag on n vertices and m edges in time O(nm). We present two methods for finding a lowest common ancestor (LCA) for each pair of vertices of a directed acyclic graph (dag) on n vertices and m edges.The first method is surprisingly natural and solves the all-pairs LCA problem for the input dag on n vertices and m edges in time O(nm). As a corollary, we obtain an O(n 2 )-time algorithm for finding genealogical distances considerably improving the previously known O(n 2.575 ) timebound for this problem.The second method relies on a novel reduction of the all-pairs LCA problem to the problem of finding maximum witnesses for Boolean matrix product. We present two methods for finding a lowest common ancestor (LCA) for each pair of vertices of a directed acyclic graph (dag) on n vertices and m edges.The first method is surprisingly natural and solves the all-pairs LCA problem for the input dag on n vertices and m edges in time O(nm). As a corollary, we obtain an O(n 2 )-time algorithm for finding genealogical distances considerably improving the previously known O(n 2.575 ) timebound for this problem.The second method relies on a novel reduction of the all-pairs LCA problem to the problem of finding maximum witnesses for Boolean matrix product. We solve the latter problem and hence also the all-pairs LCA problem in time O(n 2+ 1 4\u2212\u03c9 ), where \u03c9 = 2.376 is the exponent of the fastest known matrix multiplication algorithm. This improves the previously known O(n w+3 2 ) time-bound for the general all-pairs LCA problem in dags."}, {"paper_id": "146808166", "adju_relevance": 0, "title": "LCuts: Linear Clustering of Bacteria using Recursive Graph Cuts", "background_label": "Bacterial biofilm segmentation poses significant challenges due to lack of apparent structure, poor imaging resolution, limited contrast between conterminous cells and high density of cells that overlap. Although there exist bacterial segmentation algorithms in the existing art, they fail to delineate cells in dense biofilms, especially in 3D imaging scenarios in which the cells are growing and subdividing in a complex manner.", "method_label": "A graph-based data clustering method, LCuts, is presented with the application on bacterial cell segmentation. By constructing a weighted graph with node features in locations and principal orientations, the proposed method can automatically classify and detect differently oriented aggregations of linear structures (represent by bacteria in the application). The method assists in the assessment of several facets, such as bacterium tracking, cluster growth, and mapping of migration patterns of bacterial biofilms.", "result_label": "Quantitative and qualitative measures for 2D data demonstrate the superiority of proposed method over the state of the art. Preliminary 3D results exhibit reliable classification of the cells with 97% accuracy.", "abstract": "Bacterial biofilm segmentation poses significant challenges due to lack of apparent structure, poor imaging resolution, limited contrast between conterminous cells and high density of cells that overlap. Bacterial biofilm segmentation poses significant challenges due to lack of apparent structure, poor imaging resolution, limited contrast between conterminous cells and high density of cells that overlap. Although there exist bacterial segmentation algorithms in the existing art, they fail to delineate cells in dense biofilms, especially in 3D imaging scenarios in which the cells are growing and subdividing in a complex manner. A graph-based data clustering method, LCuts, is presented with the application on bacterial cell segmentation. A graph-based data clustering method, LCuts, is presented with the application on bacterial cell segmentation. By constructing a weighted graph with node features in locations and principal orientations, the proposed method can automatically classify and detect differently oriented aggregations of linear structures (represent by bacteria in the application). A graph-based data clustering method, LCuts, is presented with the application on bacterial cell segmentation. By constructing a weighted graph with node features in locations and principal orientations, the proposed method can automatically classify and detect differently oriented aggregations of linear structures (represent by bacteria in the application). The method assists in the assessment of several facets, such as bacterium tracking, cluster growth, and mapping of migration patterns of bacterial biofilms. Quantitative and qualitative measures for 2D data demonstrate the superiority of proposed method over the state of the art. Quantitative and qualitative measures for 2D data demonstrate the superiority of proposed method over the state of the art. Preliminary 3D results exhibit reliable classification of the cells with 97% accuracy."}, {"paper_id": "23620538", "adju_relevance": 0, "title": "Linear Time Computation of Moments in Sum-Product Networks", "background_label": "Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2).", "method_label": "We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear.", "result_label": "We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs.", "abstract": "Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2). We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear. We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs."}, {"paper_id": "7412465", "adju_relevance": 0, "title": "The minimum spanning tree: an unbiased method for brain network analysis.", "background_label": "The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. However, studies on brain networks have reported contradictory findings, and do not easily converge to a clear concept of the structural and functional network organization of the brain. It has recently been suggested that the minimum spanning tree (MST) may help to increase comparability between studies. We then explored the behavior of MST and conventional network-characteristics for simulated regular and scale-free networks that were gradually rewired to random networks. Surprisingly, although most connections are discarded during construction of the MST, MST characteristics were equally sensitive to alterations in network topology as the conventional graph theoretical measures. The MST characteristics diameter and leaf fraction were very strongly related to changes in the characteristic path length when the network changed from a regular to a random configuration.", "method_label": "The MST is an acyclic sub-network that connects all nodes and may solve several methodological limitations of previous work, such as sensitivity to alterations in connection strength (for weighted networks) or link density (for unweighted networks), which may occur concomitantly with alterations in network topology under empirical conditions.", "result_label": "If analysis of MSTs avoids these methodological limitations, understanding the relationship between MST characteristics and conventional network measures is crucial for interpreting MST brain network studies. Here, we firstly demonstrated that the MST is insensitive to alterations in connection strength or link density. Similarly, MST degree, diameter, and leaf fraction were very strongly related to the degree of scale-free networks that were rewired to random networks. Analysis of the MST is especially suitable for the comparison of brain networks, as it avoids methodological biases. Even though the MST does not utilize all the connections in the network, it still provides a, mathematically defined and unbiased, sub-network with characteristics that can provide similar information about network topology as conventional graph measures.", "abstract": "The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. However, studies on brain networks have reported contradictory findings, and do not easily converge to a clear concept of the structural and functional network organization of the brain. The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. However, studies on brain networks have reported contradictory findings, and do not easily converge to a clear concept of the structural and functional network organization of the brain. It has recently been suggested that the minimum spanning tree (MST) may help to increase comparability between studies. The MST is an acyclic sub-network that connects all nodes and may solve several methodological limitations of previous work, such as sensitivity to alterations in connection strength (for weighted networks) or link density (for unweighted networks), which may occur concomitantly with alterations in network topology under empirical conditions. If analysis of MSTs avoids these methodological limitations, understanding the relationship between MST characteristics and conventional network measures is crucial for interpreting MST brain network studies. If analysis of MSTs avoids these methodological limitations, understanding the relationship between MST characteristics and conventional network measures is crucial for interpreting MST brain network studies. Here, we firstly demonstrated that the MST is insensitive to alterations in connection strength or link density. The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. However, studies on brain networks have reported contradictory findings, and do not easily converge to a clear concept of the structural and functional network organization of the brain. It has recently been suggested that the minimum spanning tree (MST) may help to increase comparability between studies. We then explored the behavior of MST and conventional network-characteristics for simulated regular and scale-free networks that were gradually rewired to random networks. The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. However, studies on brain networks have reported contradictory findings, and do not easily converge to a clear concept of the structural and functional network organization of the brain. It has recently been suggested that the minimum spanning tree (MST) may help to increase comparability between studies. We then explored the behavior of MST and conventional network-characteristics for simulated regular and scale-free networks that were gradually rewired to random networks. Surprisingly, although most connections are discarded during construction of the MST, MST characteristics were equally sensitive to alterations in network topology as the conventional graph theoretical measures. The brain is increasingly studied with graph theoretical approaches, which can be used to characterize network topology. However, studies on brain networks have reported contradictory findings, and do not easily converge to a clear concept of the structural and functional network organization of the brain. It has recently been suggested that the minimum spanning tree (MST) may help to increase comparability between studies. We then explored the behavior of MST and conventional network-characteristics for simulated regular and scale-free networks that were gradually rewired to random networks. Surprisingly, although most connections are discarded during construction of the MST, MST characteristics were equally sensitive to alterations in network topology as the conventional graph theoretical measures. The MST characteristics diameter and leaf fraction were very strongly related to changes in the characteristic path length when the network changed from a regular to a random configuration. If analysis of MSTs avoids these methodological limitations, understanding the relationship between MST characteristics and conventional network measures is crucial for interpreting MST brain network studies. Here, we firstly demonstrated that the MST is insensitive to alterations in connection strength or link density. Similarly, MST degree, diameter, and leaf fraction were very strongly related to the degree of scale-free networks that were rewired to random networks. If analysis of MSTs avoids these methodological limitations, understanding the relationship between MST characteristics and conventional network measures is crucial for interpreting MST brain network studies. Here, we firstly demonstrated that the MST is insensitive to alterations in connection strength or link density. Similarly, MST degree, diameter, and leaf fraction were very strongly related to the degree of scale-free networks that were rewired to random networks. Analysis of the MST is especially suitable for the comparison of brain networks, as it avoids methodological biases. If analysis of MSTs avoids these methodological limitations, understanding the relationship between MST characteristics and conventional network measures is crucial for interpreting MST brain network studies. Here, we firstly demonstrated that the MST is insensitive to alterations in connection strength or link density. Similarly, MST degree, diameter, and leaf fraction were very strongly related to the degree of scale-free networks that were rewired to random networks. Analysis of the MST is especially suitable for the comparison of brain networks, as it avoids methodological biases. Even though the MST does not utilize all the connections in the network, it still provides a, mathematically defined and unbiased, sub-network with characteristics that can provide similar information about network topology as conventional graph measures."}, {"paper_id": "2686760", "adju_relevance": 0, "title": "An Algorithm for Finding Minimum d-Separating Sets in Belief Networks", "background_label": "The criterion commonly used in directed acyclic graphs (dags) for testing graphical independence is the well-known d-separation criterion. It allows us to build graphical representations of dependency models (usually probabilistic dependency models) in the form of belief networks, which make easy interpretation and management of independence relationships possible, without reference to numerical parameters (conditional probabilities).", "abstract": "The criterion commonly used in directed acyclic graphs (dags) for testing graphical independence is the well-known d-separation criterion. The criterion commonly used in directed acyclic graphs (dags) for testing graphical independence is the well-known d-separation criterion. It allows us to build graphical representations of dependency models (usually probabilistic dependency models) in the form of belief networks, which make easy interpretation and management of independence relationships possible, without reference to numerical parameters (conditional probabilities)."}, {"paper_id": "16137272", "adju_relevance": 0, "title": "Linear Time LexDFS on Cocomparability Graphs", "background_label": "Lexicographic depth first search (LexDFS) is a graph search protocol which has already proved to be a powerful tool on cocomparability graphs. Cocomparability graphs have been well studied by investigating their complements (comparability graphs) and their corresponding posets. Recently however LexDFS has led to a number of elegant polynomial and near linear time algorithms on cocomparability graphs when used as a preprocessing step [2, 3, 11].", "method_label": "The nonlinear runtime of some of these results is a consequence of complexity of this preprocessing step.", "result_label": "We present the first linear time algorithm to compute a LexDFS cocomparability ordering, therefore answering a problem raised in [2] and helping achieve the first linear time algorithms for the minimum path cover problem, and thus the Hamilton path problem, the maximum independent set problem and the minimum clique cover for this graph family.", "abstract": "Lexicographic depth first search (LexDFS) is a graph search protocol which has already proved to be a powerful tool on cocomparability graphs. Lexicographic depth first search (LexDFS) is a graph search protocol which has already proved to be a powerful tool on cocomparability graphs. Cocomparability graphs have been well studied by investigating their complements (comparability graphs) and their corresponding posets. Lexicographic depth first search (LexDFS) is a graph search protocol which has already proved to be a powerful tool on cocomparability graphs. Cocomparability graphs have been well studied by investigating their complements (comparability graphs) and their corresponding posets. Recently however LexDFS has led to a number of elegant polynomial and near linear time algorithms on cocomparability graphs when used as a preprocessing step [2, 3, 11]. The nonlinear runtime of some of these results is a consequence of complexity of this preprocessing step. We present the first linear time algorithm to compute a LexDFS cocomparability ordering, therefore answering a problem raised in [2] and helping achieve the first linear time algorithms for the minimum path cover problem, and thus the Hamilton path problem, the maximum independent set problem and the minimum clique cover for this graph family."}, {"paper_id": "2956441", "adju_relevance": 0, "title": "Faster Algorithms for the Maximum Common Subtree Isomorphism Problem", "background_label": "The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. Confining to trees renders polynomial time algorithms possible and is of fundamental importance for approaches on more general graph classes. Various variants of this problem in trees have been intensively studied. a weight function on the mapped vertices and edges. For trees of order $n$ and maximum degree $\\Delta$ our algorithm achieves a running time of $\\mathcal{O}(n^2\\Delta)$ by exploiting the structure of the matching instances arising as subproblems. Thus our algorithm outperforms the best previously known approaches.", "result_label": "We consider the general case, where trees are neither rooted nor ordered and the isomorphism is maximum w.r.t. Combining a polynomial-delay algorithm for the enumeration of all maximum common subtree isomorphisms with central ideas of our new algorithm leads to an improvement of its running time from $\\mathcal{O}(n^6+Tn^2)$ to $\\mathcal{O}(n^3+Tn\\Delta)$, where $n$ is the order of the larger tree, $T$ is the number of different solutions, and $\\Delta$ is the minimum of the maximum degrees of the input trees. Our theoretical results are supplemented by an experimental evaluation on synthetic and real-world instances.", "method_label": "No faster algorithm is possible for trees of bounded degree and for trees of unbounded degree we show that a further reduction of the running time would directly improve the best known approach to the assignment problem.", "abstract": "The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. Confining to trees renders polynomial time algorithms possible and is of fundamental importance for approaches on more general graph classes. The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. Confining to trees renders polynomial time algorithms possible and is of fundamental importance for approaches on more general graph classes. Various variants of this problem in trees have been intensively studied. We consider the general case, where trees are neither rooted nor ordered and the isomorphism is maximum w.r.t. The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. Confining to trees renders polynomial time algorithms possible and is of fundamental importance for approaches on more general graph classes. Various variants of this problem in trees have been intensively studied. a weight function on the mapped vertices and edges. The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. Confining to trees renders polynomial time algorithms possible and is of fundamental importance for approaches on more general graph classes. Various variants of this problem in trees have been intensively studied. a weight function on the mapped vertices and edges. For trees of order $n$ and maximum degree $\\Delta$ our algorithm achieves a running time of $\\mathcal{O}(n^2\\Delta)$ by exploiting the structure of the matching instances arising as subproblems. The maximum common subtree isomorphism problem asks for the largest possible isomorphism between subtrees of two given input trees. This problem is a natural restriction of the maximum common subgraph problem, which is ${\\sf NP}$-hard in general graphs. Confining to trees renders polynomial time algorithms possible and is of fundamental importance for approaches on more general graph classes. Various variants of this problem in trees have been intensively studied. a weight function on the mapped vertices and edges. For trees of order $n$ and maximum degree $\\Delta$ our algorithm achieves a running time of $\\mathcal{O}(n^2\\Delta)$ by exploiting the structure of the matching instances arising as subproblems. Thus our algorithm outperforms the best previously known approaches. No faster algorithm is possible for trees of bounded degree and for trees of unbounded degree we show that a further reduction of the running time would directly improve the best known approach to the assignment problem. We consider the general case, where trees are neither rooted nor ordered and the isomorphism is maximum w.r.t. Combining a polynomial-delay algorithm for the enumeration of all maximum common subtree isomorphisms with central ideas of our new algorithm leads to an improvement of its running time from $\\mathcal{O}(n^6+Tn^2)$ to $\\mathcal{O}(n^3+Tn\\Delta)$, where $n$ is the order of the larger tree, $T$ is the number of different solutions, and $\\Delta$ is the minimum of the maximum degrees of the input trees. We consider the general case, where trees are neither rooted nor ordered and the isomorphism is maximum w.r.t. Combining a polynomial-delay algorithm for the enumeration of all maximum common subtree isomorphisms with central ideas of our new algorithm leads to an improvement of its running time from $\\mathcal{O}(n^6+Tn^2)$ to $\\mathcal{O}(n^3+Tn\\Delta)$, where $n$ is the order of the larger tree, $T$ is the number of different solutions, and $\\Delta$ is the minimum of the maximum degrees of the input trees. Our theoretical results are supplemented by an experimental evaluation on synthetic and real-world instances."}, {"paper_id": "15475535", "adju_relevance": 0, "title": "On Dynamic Shortest Paths Problems", "background_label": "We obtain the following results related to dynamic versions of the shortest-paths problem: (i) Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems.", "method_label": "(ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance.", "result_label": "A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4).", "abstract": "We obtain the following results related to dynamic versions of the shortest-paths problem: (i) Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We obtain the following results related to dynamic versions of the shortest-paths problem: (i) Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\\tilde {O}(m\\sqrt{n})$ (we use $\\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance."}, {"paper_id": "6494947", "adju_relevance": 0, "title": "A Simpler Linear Time Algorithm for Embedding Graphs into an Arbitrary Surface and the Genus of Graphs of Bounded Tree-Width", "background_label": "For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. 2. The hidden constant (depending on the genus g of the surface S) is much smaller. It is singly exponential in g, while it is doubly exponential in Mohar's algorithm.", "method_label": "In this paper, we give a new linear time algorithm for the same problem. The advantages of our algorithm are the following: 1. As a spinoff of our main result, we give another linear time algorithm, which is of independent interest. This algorithm computes the genus and constructs minimum genus embeddings of graphs of bounded tree-width.", "result_label": "The proof is considerably simpler: it needs only about 10 pages, and some results (with rather accessible proofs) from graph minors theory, while Mohar's original algorithm and its proof occupy more than 100 pages in total. This resolves a conjecture by Neil Robertson and solves one of the most annoying long standing open question about complexity of algorithms on graphs of bounded tree-width.", "abstract": "For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. In this paper, we give a new linear time algorithm for the same problem. In this paper, we give a new linear time algorithm for the same problem. The advantages of our algorithm are the following: 1. The proof is considerably simpler: it needs only about 10 pages, and some results (with rather accessible proofs) from graph minors theory, while Mohar's original algorithm and its proof occupy more than 100 pages in total. For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. 2. For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. 2. The hidden constant (depending on the genus g of the surface S) is much smaller. For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. 2. The hidden constant (depending on the genus g of the surface S) is much smaller. It is singly exponential in g, while it is doubly exponential in Mohar's algorithm. In this paper, we give a new linear time algorithm for the same problem. The advantages of our algorithm are the following: 1. As a spinoff of our main result, we give another linear time algorithm, which is of independent interest. In this paper, we give a new linear time algorithm for the same problem. The advantages of our algorithm are the following: 1. As a spinoff of our main result, we give another linear time algorithm, which is of independent interest. This algorithm computes the genus and constructs minimum genus embeddings of graphs of bounded tree-width. The proof is considerably simpler: it needs only about 10 pages, and some results (with rather accessible proofs) from graph minors theory, while Mohar's original algorithm and its proof occupy more than 100 pages in total. This resolves a conjecture by Neil Robertson and solves one of the most annoying long standing open question about complexity of algorithms on graphs of bounded tree-width."}, {"paper_id": "6854669", "adju_relevance": 0, "title": "A Unifying Formalism for Shortest Path Problems with Expensive Edge Evaluations via Lazy Best-First Search over Paths with Edge Selectors", "background_label": "While the shortest path problem has myriad applications, the computational efficiency of suitable algorithms depends intimately on the underlying problem domain. In this paper, we focus on domains where evaluating the edge weight function dominates algorithm running time.", "method_label": "Inspired by approaches in robotic motion planning, we define and investigate the Lazy Shortest Path class of algorithms which is differentiated by the choice of an edge selector function. We show that several algorithms in the literature are equivalent to this lazy algorithm for appropriate choice of this selector.", "result_label": "Further, we propose various novel selectors inspired by sampling and statistical mechanics, and find that these selectors outperform existing algorithms on a set of example problems.", "abstract": "While the shortest path problem has myriad applications, the computational efficiency of suitable algorithms depends intimately on the underlying problem domain. While the shortest path problem has myriad applications, the computational efficiency of suitable algorithms depends intimately on the underlying problem domain. In this paper, we focus on domains where evaluating the edge weight function dominates algorithm running time. Inspired by approaches in robotic motion planning, we define and investigate the Lazy Shortest Path class of algorithms which is differentiated by the choice of an edge selector function. Inspired by approaches in robotic motion planning, we define and investigate the Lazy Shortest Path class of algorithms which is differentiated by the choice of an edge selector function. We show that several algorithms in the literature are equivalent to this lazy algorithm for appropriate choice of this selector. Further, we propose various novel selectors inspired by sampling and statistical mechanics, and find that these selectors outperform existing algorithms on a set of example problems."}, {"paper_id": "9894130", "adju_relevance": 0, "title": "A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian Networks", "background_label": "Exact Bayesian structure discovery in Bayesian networks requires exponential time and space. Using dynamic programming (DP), the fastest known sequential algorithm computes the exact posterior probabilities of structural features in $O(2(d+1)n2^n)$ time and space, if the number of nodes (variables) in the Bayesian network is $n$ and the in-degree (the number of parents) per node is bounded by a constant $d$.", "method_label": "Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Our algorithm is based the observation that the subproblems in the sequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way to coordinate the computation of correlated DP procedures such that large amount of data exchange is suppressed. Further, we develop parallel techniques for two variants of the well-known \\emph{zeta transform}, which have applications outside the context of Bayesian networks. We demonstrate the capability of our algorithm on datasets with up to 33 variables and its scalability on up to 2048 processors.", "result_label": "We apply our algorithm to a biological data set for discovering the yeast pheromone response pathways.", "abstract": "Exact Bayesian structure discovery in Bayesian networks requires exponential time and space. Exact Bayesian structure discovery in Bayesian networks requires exponential time and space. Using dynamic programming (DP), the fastest known sequential algorithm computes the exact posterior probabilities of structural features in $O(2(d+1)n2^n)$ time and space, if the number of nodes (variables) in the Bayesian network is $n$ and the in-degree (the number of parents) per node is bounded by a constant $d$. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Our algorithm is based the observation that the subproblems in the sequential DP algorithm constitute a $n$-$D$ hypercube. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Our algorithm is based the observation that the subproblems in the sequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way to coordinate the computation of correlated DP procedures such that large amount of data exchange is suppressed. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Our algorithm is based the observation that the subproblems in the sequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way to coordinate the computation of correlated DP procedures such that large amount of data exchange is suppressed. Further, we develop parallel techniques for two variants of the well-known \\emph{zeta transform}, which have applications outside the context of Bayesian networks. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. That is, if $p=2^k$ processors are used, the run-time reduces to $O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per processor. Our algorithm is based the observation that the subproblems in the sequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way to coordinate the computation of correlated DP procedures such that large amount of data exchange is suppressed. Further, we develop parallel techniques for two variants of the well-known \\emph{zeta transform}, which have applications outside the context of Bayesian networks. We demonstrate the capability of our algorithm on datasets with up to 33 variables and its scalability on up to 2048 processors. We apply our algorithm to a biological data set for discovering the yeast pheromone response pathways."}, {"paper_id": "12980973", "adju_relevance": 0, "title": "Minimum spanning trees on random networks", "background_label": "We show that the geometry of minimum spanning trees (MST) on random graphs is universal. Due to this geometric universality, we are able to characterise the energy of MST using a scaling distribution ($P(\\epsilon)$) found using uniform disorder.", "method_label": "We show that the MST energy for other disorder distributions is simply related to $P(\\epsilon)$.", "result_label": "We discuss the relationship to invasion percolation (IP), to the directed polymer in a random media (DPRM) and the implications for the broader issue of universality in disordered systems.", "abstract": "We show that the geometry of minimum spanning trees (MST) on random graphs is universal. We show that the geometry of minimum spanning trees (MST) on random graphs is universal. Due to this geometric universality, we are able to characterise the energy of MST using a scaling distribution ($P(\\epsilon)$) found using uniform disorder. We show that the MST energy for other disorder distributions is simply related to $P(\\epsilon)$. We discuss the relationship to invasion percolation (IP), to the directed polymer in a random media (DPRM) and the implications for the broader issue of universality in disordered systems."}, {"paper_id": "16143419", "adju_relevance": 0, "title": "An O*(2^n ) Algorithm for Graph Coloring and Other Partitioning Problems via Inclusion--Exclusion", "background_label": "We use the principle of inclusion and exclusion, combined with polynomial time segmentation and fast Mobius transform, to solve the generic problem of summing or optimizing over the partitions of n elements into a given number of weighted subsets. This problem subsumes various classical graph partitioning problems, such as graph coloring, domatic partitioning, and max k-cut, as well as machine learning problems like decision graph learning and model-based data clustering.", "result_label": "Our algorithm runs in O*(2n) time, thus substantially improving on the usual O*(3n)-time dynamic programming algorithm; the notation O* suppresses factors polynomial in n. This result improves, e.g., Byskov's recent record for graph coloring from O*(2.4023n) to O*(2n). We note that twenty five years ago, R. M. Karp used inclusion-exclusion in a similar fashion to reduce the space requirement of the usual dynamic programming algorithms from exponential to polynomial", "abstract": "We use the principle of inclusion and exclusion, combined with polynomial time segmentation and fast Mobius transform, to solve the generic problem of summing or optimizing over the partitions of n elements into a given number of weighted subsets. We use the principle of inclusion and exclusion, combined with polynomial time segmentation and fast Mobius transform, to solve the generic problem of summing or optimizing over the partitions of n elements into a given number of weighted subsets. This problem subsumes various classical graph partitioning problems, such as graph coloring, domatic partitioning, and max k-cut, as well as machine learning problems like decision graph learning and model-based data clustering. Our algorithm runs in O*(2n) time, thus substantially improving on the usual O*(3n)-time dynamic programming algorithm; the notation O* suppresses factors polynomial in n. This result improves, e.g., Byskov's recent record for graph coloring from O*(2.4023n) to O*(2n). Our algorithm runs in O*(2n) time, thus substantially improving on the usual O*(3n)-time dynamic programming algorithm; the notation O* suppresses factors polynomial in n. This result improves, e.g., Byskov's recent record for graph coloring from O*(2.4023n) to O*(2n). We note that twenty five years ago, R. M. Karp used inclusion-exclusion in a similar fashion to reduce the space requirement of the usual dynamic programming algorithms from exponential to polynomial"}, {"paper_id": "6029138", "adju_relevance": 0, "title": "A constant-factor approximation algorithm for the k MST problem (extended abstract)", "background_label": "R. RavitGiven an undirected graph with non-negative edge costs and an integer k, the k-MST problem is that of finding a tree of minimum cost on k nodes.", "method_label": "This problem is known to be NP-hard.We present a simple approximation algorithm that finds a solution whose cost is less than 17 times the cost of the optimum.This improves upon previous performance ratios for this problem -O(w) due to Ravi et al., 0(log2 k) due to Awerbuch et al, and the previous best bound of O(log k) due to Rajagopalan and Vazirani. Given any O < cr < 1, we first present a bicriteria approximation algorithm that~o~tputs a tree on p z cYk vertices of total cost at most~1~, where L is the cost of the optimal k-MST.", "result_label": "The running time of the algorithm is 0(rz2 log2 n) on an n-node graph.We then show how to use this algorithm to derive a constant factor approximation algorithm for the k-MST problem.The main subroutine in our algorithm is identical to an approximation algorithm of Goemans and Williamson for the prize-collecting Steiner tree problem.", "abstract": " R. RavitGiven an undirected graph with non-negative edge costs and an integer k, the k-MST problem is that of finding a tree of minimum cost on k nodes. This problem is known to be NP-hard.We present a simple approximation algorithm that finds a solution whose cost is less than 17 times the cost of the optimum.This improves upon previous performance ratios for this problem -O(w) due to Ravi et al., 0(log2 k) due to Awerbuch et al, and the previous best bound of O(log k) due to Rajagopalan and Vazirani. This problem is known to be NP-hard.We present a simple approximation algorithm that finds a solution whose cost is less than 17 times the cost of the optimum.This improves upon previous performance ratios for this problem -O(w) due to Ravi et al., 0(log2 k) due to Awerbuch et al, and the previous best bound of O(log k) due to Rajagopalan and Vazirani. Given any O < cr < 1, we first present a bicriteria approximation algorithm that~o~tputs a tree on p z cYk vertices of total cost at most~1~, where L is the cost of the optimal k-MST. The running time of the algorithm is 0(rz2 log2 n) on an n-node graph.We then show how to use this algorithm to derive a constant factor approximation algorithm for the k-MST problem.The main subroutine in our algorithm is identical to an approximation algorithm of Goemans and Williamson for the prize-collecting Steiner tree problem."}, {"paper_id": "5729059", "adju_relevance": 0, "title": "Parallel Maximum Clique Algorithms with Applications to Network Analysis and Storage", "background_label": "We propose a fast, parallel maximum clique algorithm for large sparse graphs that is designed to exploit characteristics of social and information networks.", "method_label": "The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Our method employs a branch and bound strategy with novel and aggressive pruning techniques. For instance, we use the core number of a vertex in combination with a good heuristic clique finder to efficiently remove the vast majority of the search space. In addition, we parallelize the exploration of the search tree. During the search, processes immediately communicate changes to upper and lower bounds on the size of maximum clique, which occasionally results in a super-linear speedup because vertices with large search spaces can be pruned by other processes. We apply the algorithm to two problems: to compute temporal strong components and to compress graphs.", "abstract": "We propose a fast, parallel maximum clique algorithm for large sparse graphs that is designed to exploit characteristics of social and information networks. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Our method employs a branch and bound strategy with novel and aggressive pruning techniques. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Our method employs a branch and bound strategy with novel and aggressive pruning techniques. For instance, we use the core number of a vertex in combination with a good heuristic clique finder to efficiently remove the vast majority of the search space. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Our method employs a branch and bound strategy with novel and aggressive pruning techniques. For instance, we use the core number of a vertex in combination with a good heuristic clique finder to efficiently remove the vast majority of the search space. In addition, we parallelize the exploration of the search tree. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Our method employs a branch and bound strategy with novel and aggressive pruning techniques. For instance, we use the core number of a vertex in combination with a good heuristic clique finder to efficiently remove the vast majority of the search space. In addition, we parallelize the exploration of the search tree. During the search, processes immediately communicate changes to upper and lower bounds on the size of maximum clique, which occasionally results in a super-linear speedup because vertices with large search spaces can be pruned by other processes. The method exhibits a roughly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Our method employs a branch and bound strategy with novel and aggressive pruning techniques. For instance, we use the core number of a vertex in combination with a good heuristic clique finder to efficiently remove the vast majority of the search space. In addition, we parallelize the exploration of the search tree. During the search, processes immediately communicate changes to upper and lower bounds on the size of maximum clique, which occasionally results in a super-linear speedup because vertices with large search spaces can be pruned by other processes. We apply the algorithm to two problems: to compute temporal strong components and to compress graphs."}, {"paper_id": "2628370", "adju_relevance": 0, "title": "On perturbation theory and an algorithm for maximal clique enumeration in uncertain and noisy graphs", "background_label": "The maximal clique enumeration (MCE) problem can be used to find very tightly-coupled collections of objects inside a network or graph of relationships. However, when such networks are based on noisy or uncertain data, the solutions to the MCE problem for several closely related graphs may be necessary to accurately define the collections.", "abstract": "The maximal clique enumeration (MCE) problem can be used to find very tightly-coupled collections of objects inside a network or graph of relationships. The maximal clique enumeration (MCE) problem can be used to find very tightly-coupled collections of objects inside a network or graph of relationships. However, when such networks are based on noisy or uncertain data, the solutions to the MCE problem for several closely related graphs may be necessary to accurately define the collections."}, {"paper_id": "7969169", "adju_relevance": 0, "title": "TREE-PUZZLE: maximum likelihood phylogenetic analysis using quartets and parallel computing.", "background_label": "SUMMARY TREE-PUZZLE is a program package for quartet-based maximum-likelihood phylogenetic analysis (formerly PUZZLE, Strimmer and von Haeseler, Mol. Biol. Evol., 13, 964-969, 1996) that provides methods for reconstruction, comparison, and testing of trees and models on DNA as well as protein sequences.", "method_label": "To reduce waiting time for larger datasets the tree reconstruction part of the software has been parallelized using message passing that runs on clusters of workstations as well as parallel computers.", "abstract": "SUMMARY TREE-PUZZLE is a program package for quartet-based maximum-likelihood phylogenetic analysis (formerly PUZZLE, Strimmer and von Haeseler, Mol. SUMMARY TREE-PUZZLE is a program package for quartet-based maximum-likelihood phylogenetic analysis (formerly PUZZLE, Strimmer and von Haeseler, Mol. Biol. SUMMARY TREE-PUZZLE is a program package for quartet-based maximum-likelihood phylogenetic analysis (formerly PUZZLE, Strimmer and von Haeseler, Mol. Biol. Evol., 13, 964-969, 1996) that provides methods for reconstruction, comparison, and testing of trees and models on DNA as well as protein sequences. To reduce waiting time for larger datasets the tree reconstruction part of the software has been parallelized using message passing that runs on clusters of workstations as well as parallel computers."}, {"paper_id": "33240098", "adju_relevance": 0, "title": "Graph Matching with Adaptive and Branching Path Following", "background_label": "Graph matching aims at establishing correspondences between graph elements, and is widely used in many computer vision tasks. Among recently proposed graph matching algorithms, those utilizing the path following strategy have attracted special research attentions due to their exhibition of state-of-the-art performances. However, the paths computed in these algorithms often contain singular points, which could hurt the matching performance if not dealt properly.", "method_label": "To deal with this issue, we propose a novel path following strategy, named branching path following  (BPF), to improve graph matching accuracy. In particular, we first propose a singular point detector by solving a KKT system, and then design a branch switching method to seek for better paths at singular points. Moreover, to reduce the computational burden of the BPF strategy, an adaptive path estimation (APE) strategy is integrated into BPF to accelerate the convergence of searching along each path. A new graph matching algorithm named ABPF-G is developed by applying APE and BPF to a recently proposed path following algorithm named GNCCP (Liu & Qiao 2014).", "result_label": "Experimental results reveal how our approach consistently outperforms state-of-the-art algorithms for graph matching on five public benchmark datasets.", "abstract": "Graph matching aims at establishing correspondences between graph elements, and is widely used in many computer vision tasks. Graph matching aims at establishing correspondences between graph elements, and is widely used in many computer vision tasks. Among recently proposed graph matching algorithms, those utilizing the path following strategy have attracted special research attentions due to their exhibition of state-of-the-art performances. Graph matching aims at establishing correspondences between graph elements, and is widely used in many computer vision tasks. Among recently proposed graph matching algorithms, those utilizing the path following strategy have attracted special research attentions due to their exhibition of state-of-the-art performances. However, the paths computed in these algorithms often contain singular points, which could hurt the matching performance if not dealt properly. To deal with this issue, we propose a novel path following strategy, named branching path following  (BPF), to improve graph matching accuracy. To deal with this issue, we propose a novel path following strategy, named branching path following  (BPF), to improve graph matching accuracy. In particular, we first propose a singular point detector by solving a KKT system, and then design a branch switching method to seek for better paths at singular points. To deal with this issue, we propose a novel path following strategy, named branching path following  (BPF), to improve graph matching accuracy. In particular, we first propose a singular point detector by solving a KKT system, and then design a branch switching method to seek for better paths at singular points. Moreover, to reduce the computational burden of the BPF strategy, an adaptive path estimation (APE) strategy is integrated into BPF to accelerate the convergence of searching along each path. To deal with this issue, we propose a novel path following strategy, named branching path following  (BPF), to improve graph matching accuracy. In particular, we first propose a singular point detector by solving a KKT system, and then design a branch switching method to seek for better paths at singular points. Moreover, to reduce the computational burden of the BPF strategy, an adaptive path estimation (APE) strategy is integrated into BPF to accelerate the convergence of searching along each path. A new graph matching algorithm named ABPF-G is developed by applying APE and BPF to a recently proposed path following algorithm named GNCCP (Liu & Qiao 2014). Experimental results reveal how our approach consistently outperforms state-of-the-art algorithms for graph matching on five public benchmark datasets."}, {"paper_id": "6547412", "adju_relevance": 0, "title": "A Dynamic Separator Algorithm", "background_label": "Abstract Our work is based on the pioneering work in sphere separators done by Miller, Teng, Vavasis et al, [8, 12] , who gave efficient static (fixed input) algorithms for finding sphere separators of sizeWe present dynamic algorithms which maintain separators for a dynamically changing graph.", "method_label": "Our algorithms answer queries and process insertions and deletions to the input set, If the total input size and number of queries is n, our algorithm is polylog, that is, it takes 1We also give a general technique for transforming a class of expected time randomized incremental algorithms that use random sampling to incremental algorithms with high likelihood time bounds.", "result_label": "In particular, we show how we can maintain separators in time O(log 3 n) with high likelihood.Our results can be applied to generate dynamic algorithms for a wide variety of combinatorial and numerical problems, whose underlying associated dynamic graph is a kneighborhood graph, such as solving linear systems and monoid path problems.", "abstract": "Abstract Our work is based on the pioneering work in sphere separators done by Miller, Teng, Vavasis et al, [8, 12] , who gave efficient static (fixed input) algorithms for finding sphere separators of sizeWe present dynamic algorithms which maintain separators for a dynamically changing graph. Our algorithms answer queries and process insertions and deletions to the input set, If the total input size and number of queries is n, our algorithm is polylog, that is, it takes Our algorithms answer queries and process insertions and deletions to the input set, If the total input size and number of queries is n, our algorithm is polylog, that is, it takes 1We also give a general technique for transforming a class of expected time randomized incremental algorithms that use random sampling to incremental algorithms with high likelihood time bounds. In particular, we show how we can maintain separators in time O(log 3 n) with high likelihood.Our results can be applied to generate dynamic algorithms for a wide variety of combinatorial and numerical problems, whose underlying associated dynamic graph is a kneighborhood graph, such as solving linear systems and monoid path problems."}, {"paper_id": "7235055", "adju_relevance": 0, "title": "A Faster Parameterized Algorithm for Treedepth", "background_label": "The width measure \\emph{treedepth}, also known as vertex ranking, centered coloring and elimination tree height, is a well-established notion which has recently seen a resurgence of interest. We present an algorithm which---given as input an $n$-vertex graph, a tree decomposition of the graph of width $w$, and an integer $t$---decides Treedepth, i.e.", "method_label": "whether the treedepth of the graph is at most $t$, in time $2^{O(wt)} \\cdot n$. If necessary, a witness structure for the treedepth can be constructed in the same running time. In conjunction with previous results we provide a simple algorithm and a fast algorithm which decide treedepth in time $2^{2^{O(t)}} \\cdot n$ and $2^{O(t^2)} \\cdot n$, respectively, which do not require a tree decomposition as part of their input. The former answers an open question posed by Ossona de Mendez and Nesetril as to whether deciding Treedepth admits an algorithm with a linear running time (for every fixed $t$) that does not rely on Courcelle's Theorem or other heavy machinery.", "result_label": "For chordal graphs we can prove a running time of $2^{O(t \\log t)}\\cdot n$ for the same algorithm.", "abstract": "The width measure \\emph{treedepth}, also known as vertex ranking, centered coloring and elimination tree height, is a well-established notion which has recently seen a resurgence of interest. The width measure \\emph{treedepth}, also known as vertex ranking, centered coloring and elimination tree height, is a well-established notion which has recently seen a resurgence of interest. We present an algorithm which---given as input an $n$-vertex graph, a tree decomposition of the graph of width $w$, and an integer $t$---decides Treedepth, i.e. whether the treedepth of the graph is at most $t$, in time $2^{O(wt)} \\cdot n$. whether the treedepth of the graph is at most $t$, in time $2^{O(wt)} \\cdot n$. If necessary, a witness structure for the treedepth can be constructed in the same running time. whether the treedepth of the graph is at most $t$, in time $2^{O(wt)} \\cdot n$. If necessary, a witness structure for the treedepth can be constructed in the same running time. In conjunction with previous results we provide a simple algorithm and a fast algorithm which decide treedepth in time $2^{2^{O(t)}} \\cdot n$ and $2^{O(t^2)} \\cdot n$, respectively, which do not require a tree decomposition as part of their input. whether the treedepth of the graph is at most $t$, in time $2^{O(wt)} \\cdot n$. If necessary, a witness structure for the treedepth can be constructed in the same running time. In conjunction with previous results we provide a simple algorithm and a fast algorithm which decide treedepth in time $2^{2^{O(t)}} \\cdot n$ and $2^{O(t^2)} \\cdot n$, respectively, which do not require a tree decomposition as part of their input. The former answers an open question posed by Ossona de Mendez and Nesetril as to whether deciding Treedepth admits an algorithm with a linear running time (for every fixed $t$) that does not rely on Courcelle's Theorem or other heavy machinery. For chordal graphs we can prove a running time of $2^{O(t \\log t)}\\cdot n$ for the same algorithm."}, {"paper_id": "49428113", "adju_relevance": 0, "title": "Linear-Time Online Algorithm Inferring the Shortest Path from a Walk", "background_label": "We consider the problem of inferring an edge-labeled graph from the sequence of edge labels seen in a walk of that graph. It has been known that this problem is solvable in $O(n \\log n)$ time when the targets are path or cycle graphs.", "method_label": "This paper presents an online algorithm for the problem of this restricted case that runs in $O(n)$ time, based on Manacher's algorithm for computing all the maximal palindromes in a string.", "abstract": "We consider the problem of inferring an edge-labeled graph from the sequence of edge labels seen in a walk of that graph. We consider the problem of inferring an edge-labeled graph from the sequence of edge labels seen in a walk of that graph. It has been known that this problem is solvable in $O(n \\log n)$ time when the targets are path or cycle graphs. This paper presents an online algorithm for the problem of this restricted case that runs in $O(n)$ time, based on Manacher's algorithm for computing all the maximal palindromes in a string."}, {"paper_id": "10921461", "adju_relevance": 0, "title": "A clique-based method using dynamic programming for computing edit distance between unordered trees.", "background_label": "Many kinds of tree-structured data, such as RNA secondary structures, have become available due to the progress of techniques in the field of molecular biology. To analyze the tree-structured data, various measures for computing the similarity between them have been developed and applied. Among them, tree edit distance is one of the most widely used measures. However, the tree edit distance problem for unordered trees is NP-hard. Recently, a practical method called clique-based algorithm has been proposed, but it is not fast for large trees.", "result_label": "Therefore, it is required to develop efficient algorithms for the problem. In particular, for hard instances, the improved method achieved more than 100 times speed-up.", "method_label": "This article presents an improved clique-based method for the tree edit distance problem for unordered trees. The improved method is obtained by introducing a dynamic programming scheme and heuristic techniques to the previous clique-based method. To evaluate the efficiency of the improved method, we applied the method to comparison of real tree structured data such as glycan structures. For large tree-structures, the improved method is much faster than the previous method.", "abstract": "Many kinds of tree-structured data, such as RNA secondary structures, have become available due to the progress of techniques in the field of molecular biology. Many kinds of tree-structured data, such as RNA secondary structures, have become available due to the progress of techniques in the field of molecular biology. To analyze the tree-structured data, various measures for computing the similarity between them have been developed and applied. Many kinds of tree-structured data, such as RNA secondary structures, have become available due to the progress of techniques in the field of molecular biology. To analyze the tree-structured data, various measures for computing the similarity between them have been developed and applied. Among them, tree edit distance is one of the most widely used measures. Many kinds of tree-structured data, such as RNA secondary structures, have become available due to the progress of techniques in the field of molecular biology. To analyze the tree-structured data, various measures for computing the similarity between them have been developed and applied. Among them, tree edit distance is one of the most widely used measures. However, the tree edit distance problem for unordered trees is NP-hard. Therefore, it is required to develop efficient algorithms for the problem. Many kinds of tree-structured data, such as RNA secondary structures, have become available due to the progress of techniques in the field of molecular biology. To analyze the tree-structured data, various measures for computing the similarity between them have been developed and applied. Among them, tree edit distance is one of the most widely used measures. However, the tree edit distance problem for unordered trees is NP-hard. Recently, a practical method called clique-based algorithm has been proposed, but it is not fast for large trees. This article presents an improved clique-based method for the tree edit distance problem for unordered trees. This article presents an improved clique-based method for the tree edit distance problem for unordered trees. The improved method is obtained by introducing a dynamic programming scheme and heuristic techniques to the previous clique-based method. This article presents an improved clique-based method for the tree edit distance problem for unordered trees. The improved method is obtained by introducing a dynamic programming scheme and heuristic techniques to the previous clique-based method. To evaluate the efficiency of the improved method, we applied the method to comparison of real tree structured data such as glycan structures. This article presents an improved clique-based method for the tree edit distance problem for unordered trees. The improved method is obtained by introducing a dynamic programming scheme and heuristic techniques to the previous clique-based method. To evaluate the efficiency of the improved method, we applied the method to comparison of real tree structured data such as glycan structures. For large tree-structures, the improved method is much faster than the previous method. Therefore, it is required to develop efficient algorithms for the problem. In particular, for hard instances, the improved method achieved more than 100 times speed-up."}, {"paper_id": "17331227", "adju_relevance": 0, "title": "A Fixed-Parameter Approach for Weighted Cluster Editing", "background_label": "Clustering objects with respect to a given similarity or distance measure is a problem often encountered in computational biology. Several well-known clustering algorithms are based on transforming the input matrix into a weighted graph although the resulting WEIGHTED CLUSTER EDITING problem is computationally hard: here, we transform the input graph into a disjoint union of cliques such that the sum of weights of all modified edges is minimized.", "method_label": "We present fixed-parameter algorithms for this problem which guarantee to find an optimal solution in provable worst-case running time. We introduce a new data reduction operation (merging vertices) that has no counterpart in the unweighted case and strongly cuts down running times in practice. We have applied our algorithms to both artificial and biological data. Despite the complexity of the problem, our method often allows exact computation of optimal solutions in reasonable running time.", "abstract": "Clustering objects with respect to a given similarity or distance measure is a problem often encountered in computational biology. Clustering objects with respect to a given similarity or distance measure is a problem often encountered in computational biology. Several well-known clustering algorithms are based on transforming the input matrix into a weighted graph although the resulting WEIGHTED CLUSTER EDITING problem is computationally hard: here, we transform the input graph into a disjoint union of cliques such that the sum of weights of all modified edges is minimized. We present fixed-parameter algorithms for this problem which guarantee to find an optimal solution in provable worst-case running time. We present fixed-parameter algorithms for this problem which guarantee to find an optimal solution in provable worst-case running time. We introduce a new data reduction operation (merging vertices) that has no counterpart in the unweighted case and strongly cuts down running times in practice. We present fixed-parameter algorithms for this problem which guarantee to find an optimal solution in provable worst-case running time. We introduce a new data reduction operation (merging vertices) that has no counterpart in the unweighted case and strongly cuts down running times in practice. We have applied our algorithms to both artificial and biological data. We present fixed-parameter algorithms for this problem which guarantee to find an optimal solution in provable worst-case running time. We introduce a new data reduction operation (merging vertices) that has no counterpart in the unweighted case and strongly cuts down running times in practice. We have applied our algorithms to both artificial and biological data. Despite the complexity of the problem, our method often allows exact computation of optimal solutions in reasonable running time."}, {"paper_id": "122744233", "adju_relevance": 0, "title": "Depth-first discovery algorithm for incremental topological sorting of directed acyclic graphs", "background_label": "We study the problem of incrementally maintaining a topological sorting in a large DAG. The Discovery Algorithm (DA) of Alpern et al. [Proc. 1st Annual ACM-SIAM Symp. on Discrete Algorithms, 1990, pp.", "method_label": "32-42] computes a cover K of nodes such that a solution to the modified problem can be found by changing node priorities within K only. It achieves a runtime complexity that is polynomially bounded in terms of the minimal cover size k.The temporary space complexity of DA grows quickly with increasing number of added edges and cover size. We introduce the Depth-First Discovery Algorithm (DFDA), which uses depth-first search to reduce the temporary space of DA from O(|A| \u00d7 ||-K||) to O(|A| + ||K||), where |A| is the number of edges to add and ||K|| is the extended size of the cover.", "result_label": "DFDA is simpler than DA and performs better in our empirical tests.", "abstract": "We study the problem of incrementally maintaining a topological sorting in a large DAG. We study the problem of incrementally maintaining a topological sorting in a large DAG. The Discovery Algorithm (DA) of Alpern et al. We study the problem of incrementally maintaining a topological sorting in a large DAG. The Discovery Algorithm (DA) of Alpern et al. [Proc. We study the problem of incrementally maintaining a topological sorting in a large DAG. The Discovery Algorithm (DA) of Alpern et al. [Proc. 1st Annual ACM-SIAM Symp. We study the problem of incrementally maintaining a topological sorting in a large DAG. The Discovery Algorithm (DA) of Alpern et al. [Proc. 1st Annual ACM-SIAM Symp. on Discrete Algorithms, 1990, pp. 32-42] computes a cover K of nodes such that a solution to the modified problem can be found by changing node priorities within K only. 32-42] computes a cover K of nodes such that a solution to the modified problem can be found by changing node priorities within K only. It achieves a runtime complexity that is polynomially bounded in terms of the minimal cover size k.The temporary space complexity of DA grows quickly with increasing number of added edges and cover size. 32-42] computes a cover K of nodes such that a solution to the modified problem can be found by changing node priorities within K only. It achieves a runtime complexity that is polynomially bounded in terms of the minimal cover size k.The temporary space complexity of DA grows quickly with increasing number of added edges and cover size. We introduce the Depth-First Discovery Algorithm (DFDA), which uses depth-first search to reduce the temporary space of DA from O(|A| \u00d7 ||-K||) to O(|A| + ||K||), where |A| is the number of edges to add and ||K|| is the extended size of the cover. DFDA is simpler than DA and performs better in our empirical tests."}, {"paper_id": "2829078", "adju_relevance": 0, "title": "I/O-efficient shortest path algorithms for undirected graphs with random or bounded edge lengths", "background_label": "We present I/O-efficient single-source shortest path algorithms for undirected graphs.", "abstract": "We present I/O-efficient single-source shortest path algorithms for undirected graphs."}, {"paper_id": "289184", "adju_relevance": 0, "title": "Graph rigidity, Cyclic Belief Propagation and Point Pattern Matching", "background_label": "A recent paper \\cite{CaeCaeSchBar06} proposed a provably optimal, polynomial time method for performing near-isometric point pattern matching by means of exact probabilistic inference in a chordal graphical model. Their fundamental result is that the chordal graph in question is shown to be globally rigid, implying that exact inference provides the same matching solution as exact inference in a complete graphical model. This implies that the algorithm is optimal when there is no noise in the point patterns.", "method_label": "In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in \\cite{CaeCaeSchBar06}: its maximal clique size is smaller, rendering inference significantly more efficient. However, our graph is not chordal and thus standard Junction Tree algorithms cannot be directly applied. Nevertheless, we show that loopy belief propagation in such a graph converges to the optimal solution. This allows us to retain the optimality guarantee in the noiseless case, while substantially reducing both memory requirements and processing time.", "result_label": "Our experimental results show that the accuracy of the proposed solution is indistinguishable from that of \\cite{CaeCaeSchBar06} when there is noise in the point patterns.", "abstract": "A recent paper \\cite{CaeCaeSchBar06} proposed a provably optimal, polynomial time method for performing near-isometric point pattern matching by means of exact probabilistic inference in a chordal graphical model. A recent paper \\cite{CaeCaeSchBar06} proposed a provably optimal, polynomial time method for performing near-isometric point pattern matching by means of exact probabilistic inference in a chordal graphical model. Their fundamental result is that the chordal graph in question is shown to be globally rigid, implying that exact inference provides the same matching solution as exact inference in a complete graphical model. A recent paper \\cite{CaeCaeSchBar06} proposed a provably optimal, polynomial time method for performing near-isometric point pattern matching by means of exact probabilistic inference in a chordal graphical model. Their fundamental result is that the chordal graph in question is shown to be globally rigid, implying that exact inference provides the same matching solution as exact inference in a complete graphical model. This implies that the algorithm is optimal when there is no noise in the point patterns. In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in \\cite{CaeCaeSchBar06}: its maximal clique size is smaller, rendering inference significantly more efficient. In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in \\cite{CaeCaeSchBar06}: its maximal clique size is smaller, rendering inference significantly more efficient. However, our graph is not chordal and thus standard Junction Tree algorithms cannot be directly applied. In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in \\cite{CaeCaeSchBar06}: its maximal clique size is smaller, rendering inference significantly more efficient. However, our graph is not chordal and thus standard Junction Tree algorithms cannot be directly applied. Nevertheless, we show that loopy belief propagation in such a graph converges to the optimal solution. In this paper, we present a new graph which is also globally rigid but has an advantage over the graph proposed in \\cite{CaeCaeSchBar06}: its maximal clique size is smaller, rendering inference significantly more efficient. However, our graph is not chordal and thus standard Junction Tree algorithms cannot be directly applied. Nevertheless, we show that loopy belief propagation in such a graph converges to the optimal solution. This allows us to retain the optimality guarantee in the noiseless case, while substantially reducing both memory requirements and processing time. Our experimental results show that the accuracy of the proposed solution is indistinguishable from that of \\cite{CaeCaeSchBar06} when there is noise in the point patterns."}, {"paper_id": "10991505", "adju_relevance": 0, "title": "Exploiting Structure in Parsing to 1-Endpoint-Crossing Graphs", "background_label": "AbstractDeep dependency parsing can be cast as the search for maximum acyclic subgraphs in weighted digraphs. Because this search problem is intractable in the general case, we consider its restriction to the class of 1-endpoint-crossing (1ec) graphs, which has high coverage on standard data sets.", "abstract": "AbstractDeep dependency parsing can be cast as the search for maximum acyclic subgraphs in weighted digraphs. AbstractDeep dependency parsing can be cast as the search for maximum acyclic subgraphs in weighted digraphs. Because this search problem is intractable in the general case, we consider its restriction to the class of 1-endpoint-crossing (1ec) graphs, which has high coverage on standard data sets."}, {"paper_id": "186025", "adju_relevance": 0, "title": "Fast euclidean minimum spanning tree: algorithm, analysis, and applications", "background_label": "The Euclidean Minimum Spanning Tree problem has applications in a wide range of fields, and many efficient algorithms have been developed to solve it. We present a new, fast, general EMST algorithm, motivated by the clustering and analysis of astronomical data. Large-scale astronomical surveys, including the Sloan Digital Sky Survey, and large simulations of the early universe, such as the Millennium Simulation, can contain millions of points and fill terabytes of storage.", "method_label": "Traditional EMST methods scale quadratically, and more advanced methods lack rigorous runtime guarantees.", "result_label": "We present a new dual-tree algorithm for efficiently computing the EMST, use adaptive algorithm analysis to prove the tightest (and possibly optimal) runtime bound for the EMST problem to-date, and demonstrate the scalability of our method on astronomical data sets.", "abstract": "The Euclidean Minimum Spanning Tree problem has applications in a wide range of fields, and many efficient algorithms have been developed to solve it. The Euclidean Minimum Spanning Tree problem has applications in a wide range of fields, and many efficient algorithms have been developed to solve it. We present a new, fast, general EMST algorithm, motivated by the clustering and analysis of astronomical data. The Euclidean Minimum Spanning Tree problem has applications in a wide range of fields, and many efficient algorithms have been developed to solve it. We present a new, fast, general EMST algorithm, motivated by the clustering and analysis of astronomical data. Large-scale astronomical surveys, including the Sloan Digital Sky Survey, and large simulations of the early universe, such as the Millennium Simulation, can contain millions of points and fill terabytes of storage. Traditional EMST methods scale quadratically, and more advanced methods lack rigorous runtime guarantees. We present a new dual-tree algorithm for efficiently computing the EMST, use adaptive algorithm analysis to prove the tightest (and possibly optimal) runtime bound for the EMST problem to-date, and demonstrate the scalability of our method on astronomical data sets."}, {"paper_id": "134808496", "adju_relevance": 0, "title": "A spatial one-to-many flow layout algorithm using triangulation, approximate Steiner trees, and path smoothing", "background_label": "ABSTRACTMapping spatial flow remains a challenge despite noticeable progress in recent years.", "abstract": "ABSTRACTMapping spatial flow remains a challenge despite noticeable progress in recent years."}, {"paper_id": "12710022", "adju_relevance": 0, "title": "Parallel implementation of Bouvka's minimum spanning tree algorithm", "background_label": "We study parallel algorithms for the minimum spanning tree problem, based on the sequential algorithm of O. Boruvka (1926). The target architectures for our algorithm are asynchronous, distributed-memory machines. Analysis of our parallel algorithm on a simple model that is reminiscent of the LogP model, shows that in principle a speedup proportional to the number of processors can be achieved, but that communication costs can be significant.", "method_label": "To reduce these costs, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms. We also consider empirically the effects of data imbalance on the running time. For the graphs used in our experiments, load balancing schemes result in little improvement in running times.", "result_label": "Our implementations on sparse graphs with 64,000 vertices on Thinking Machine's CM-5 achieve a speedup factor of about 4 on 16 processors. On this environment, packaging of messages turns out to be the most effective way to reduce communication costs.", "abstract": "We study parallel algorithms for the minimum spanning tree problem, based on the sequential algorithm of O. Boruvka (1926). We study parallel algorithms for the minimum spanning tree problem, based on the sequential algorithm of O. Boruvka (1926). The target architectures for our algorithm are asynchronous, distributed-memory machines. We study parallel algorithms for the minimum spanning tree problem, based on the sequential algorithm of O. Boruvka (1926). The target architectures for our algorithm are asynchronous, distributed-memory machines. Analysis of our parallel algorithm on a simple model that is reminiscent of the LogP model, shows that in principle a speedup proportional to the number of processors can be achieved, but that communication costs can be significant. To reduce these costs, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms. To reduce these costs, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms. We also consider empirically the effects of data imbalance on the running time. To reduce these costs, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms. We also consider empirically the effects of data imbalance on the running time. For the graphs used in our experiments, load balancing schemes result in little improvement in running times. Our implementations on sparse graphs with 64,000 vertices on Thinking Machine's CM-5 achieve a speedup factor of about 4 on 16 processors. Our implementations on sparse graphs with 64,000 vertices on Thinking Machine's CM-5 achieve a speedup factor of about 4 on 16 processors. On this environment, packaging of messages turns out to be the most effective way to reduce communication costs."}, {"paper_id": "17316509", "adju_relevance": 0, "title": "Robust estimation of point process intensity features using k-minimal spanning trees", "background_label": "Minimal spanning trees (MST) have been applied to multi-dimensional random processes for pattern recognition and randomness testing.", "method_label": "We present a robust version of the MST to estimate complexity features of a point process intensity function under an epsilon contaminated model for the intensity. The principal feature considered is the Renyi entropy of the mixture and a strongly consistent entropy estimator is given which depends on the data only through the total length of the MST passing through the data points. Robustification of the MST estimator is achieved by applying the theory of k-minimum MST.", "abstract": "Minimal spanning trees (MST) have been applied to multi-dimensional random processes for pattern recognition and randomness testing. We present a robust version of the MST to estimate complexity features of a point process intensity function under an epsilon contaminated model for the intensity. We present a robust version of the MST to estimate complexity features of a point process intensity function under an epsilon contaminated model for the intensity. The principal feature considered is the Renyi entropy of the mixture and a strongly consistent entropy estimator is given which depends on the data only through the total length of the MST passing through the data points. We present a robust version of the MST to estimate complexity features of a point process intensity function under an epsilon contaminated model for the intensity. The principal feature considered is the Renyi entropy of the mixture and a strongly consistent entropy estimator is given which depends on the data only through the total length of the MST passing through the data points. Robustification of the MST estimator is achieved by applying the theory of k-minimum MST."}, {"paper_id": "2869969", "adju_relevance": 0, "title": "Quadratic-Time Dependency Parsing for Machine Translation", "background_label": "Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently, McDonald et al. (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages.", "method_label": "This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores.", "result_label": "Our results show that augmenting a state-of-the-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets.", "abstract": "Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently, McDonald et al. Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently, McDonald et al. (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently, McDonald et al. (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-of-the-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets."}, {"paper_id": "26198019", "adju_relevance": 0, "title": "Minimum cost spanning tree games", "background_label": "We consider the problem of cost allocation among users of a minimum cost spanning tree network. It is formulated as a cooperative game in characteristic function form, referred to as a minimum cost spanning tree (m.c.s.t.) game. game is never empty.", "method_label": "We show that the core of a m.c.s.t. In fact, a point in the core can be read directly from any minimum cost spanning tree graph associated with the problem. For m.c.s.t. games with efficient coalition structures we define and construct m.c.s.t. games on the components of the structure.", "result_label": "We show that the core and the nucleolus of the original game are the cartesian products of the cores and the nucleoli, respectively, of the induced games on the components of the efficient coalition structure.", "abstract": "We consider the problem of cost allocation among users of a minimum cost spanning tree network. We consider the problem of cost allocation among users of a minimum cost spanning tree network. It is formulated as a cooperative game in characteristic function form, referred to as a minimum cost spanning tree (m.c.s.t.) We consider the problem of cost allocation among users of a minimum cost spanning tree network. It is formulated as a cooperative game in characteristic function form, referred to as a minimum cost spanning tree (m.c.s.t.) game. We show that the core of a m.c.s.t. We consider the problem of cost allocation among users of a minimum cost spanning tree network. It is formulated as a cooperative game in characteristic function form, referred to as a minimum cost spanning tree (m.c.s.t.) game. game is never empty. We show that the core of a m.c.s.t. In fact, a point in the core can be read directly from any minimum cost spanning tree graph associated with the problem. We show that the core of a m.c.s.t. In fact, a point in the core can be read directly from any minimum cost spanning tree graph associated with the problem. For m.c.s.t. We show that the core of a m.c.s.t. In fact, a point in the core can be read directly from any minimum cost spanning tree graph associated with the problem. For m.c.s.t. games with efficient coalition structures we define and construct m.c.s.t. We show that the core of a m.c.s.t. In fact, a point in the core can be read directly from any minimum cost spanning tree graph associated with the problem. For m.c.s.t. games with efficient coalition structures we define and construct m.c.s.t. games on the components of the structure. We show that the core and the nucleolus of the original game are the cartesian products of the cores and the nucleoli, respectively, of the induced games on the components of the efficient coalition structure."}, {"paper_id": "16515149", "adju_relevance": 0, "title": "A Distributed Parallel Algorithm for Minimum Spanning Tree Problem", "background_label": "In this paper we present and evaluate a parallel algorithm for solving a minimum spanning tree (MST) problem for supercomputers with distributed memory.", "method_label": "The algorithm relies on the relaxation of the message processing order requirement for one specific message type compared to the original GHS (Gallager, Humblet, Spira) algorithm. Our algorithm adopts hashing and message compression optimization techniques as well.", "result_label": "To the best of our knowledge, this is the first parallel implementation of the GHS algorithm that linearly scales to more than 32 nodes (256 cores) of Infiniband cluster.", "abstract": "In this paper we present and evaluate a parallel algorithm for solving a minimum spanning tree (MST) problem for supercomputers with distributed memory. The algorithm relies on the relaxation of the message processing order requirement for one specific message type compared to the original GHS (Gallager, Humblet, Spira) algorithm. The algorithm relies on the relaxation of the message processing order requirement for one specific message type compared to the original GHS (Gallager, Humblet, Spira) algorithm. Our algorithm adopts hashing and message compression optimization techniques as well. To the best of our knowledge, this is the first parallel implementation of the GHS algorithm that linearly scales to more than 32 nodes (256 cores) of Infiniband cluster."}, {"paper_id": "13936127", "adju_relevance": 0, "title": "The Leaf Path Projection View Of Parse Trees: Exploring String Kernels For HPSG Parse Selection", "background_label": "AbstractWe present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree.", "abstract": "AbstractWe present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree."}, {"paper_id": "10733914", "adju_relevance": 0, "title": "An Almost-Linear-Time Algorithm for Approximate Max Flow in Undirected Graphs, and its Multicommodity Generalizations", "background_label": "In this paper, we introduce a new framework for approximately solving flow problems in capacitated, undirected graphs and apply it to provide asymptotically faster algorithms for the maximum $s$-$t$ flow and maximum concurrent multicommodity flow problems.", "method_label": "For graphs with $n$ vertices and $m$ edges, it allows us to find an $\\epsilon$-approximate maximum $s$-$t$ flow in time $O(m^{1+o(1)}\\epsilon^{-2})$, improving on the previous best bound of $\\tilde{O}(mn^{1/3} poly(1/\\epsilon))$.", "abstract": "In this paper, we introduce a new framework for approximately solving flow problems in capacitated, undirected graphs and apply it to provide asymptotically faster algorithms for the maximum $s$-$t$ flow and maximum concurrent multicommodity flow problems. For graphs with $n$ vertices and $m$ edges, it allows us to find an $\\epsilon$-approximate maximum $s$-$t$ flow in time $O(m^{1+o(1)}\\epsilon^{-2})$, improving on the previous best bound of $\\tilde{O}(mn^{1/3} poly(1/\\epsilon))$."}, {"paper_id": "120484095", "adju_relevance": 0, "title": "The quadratic minimum spanning tree problem", "background_label": "This article introduces a new optimization problem that involves searching for the spanning tree of minimum cost under a quadratic cost structure. This quadratic minimum spanning tree problem is proven to be NP-hard.", "method_label": "A technique for generating lower bounds for this problem is discussed and incorporated into branch-and-bound schemes for obtaining exact solutions. Two heuristic algorithms are also developed. Computational experience with both exact and heuristic algorithms is reported.", "abstract": "This article introduces a new optimization problem that involves searching for the spanning tree of minimum cost under a quadratic cost structure. This article introduces a new optimization problem that involves searching for the spanning tree of minimum cost under a quadratic cost structure. This quadratic minimum spanning tree problem is proven to be NP-hard. A technique for generating lower bounds for this problem is discussed and incorporated into branch-and-bound schemes for obtaining exact solutions. A technique for generating lower bounds for this problem is discussed and incorporated into branch-and-bound schemes for obtaining exact solutions. Two heuristic algorithms are also developed. A technique for generating lower bounds for this problem is discussed and incorporated into branch-and-bound schemes for obtaining exact solutions. Two heuristic algorithms are also developed. Computational experience with both exact and heuristic algorithms is reported."}, {"paper_id": "198968031", "adju_relevance": 0, "title": "Faster asynchronous MST and low diameter tree construction with sublinear communication", "background_label": "Building a spanning tree, minimum spanning tree (MST), and BFS tree in a distributed network are fundamental problems which are still not fully understood in terms of time and communication cost. x The first work to succeed in computing a spanning tree with communication sublinear in the number of edges in an asynchronous CONGEST network appeared in DISC 2018. That algorithm which constructs an MST is sequential in the worst case; its running time is proportional to the total number of messages sent. Our paper matches its message complexity but brings the running time down to linear in $n$.", "method_label": "Our techniques can also be used to provide an asynchronous algorithm with sublinear communication to construct a tree in which the distance from a source to each node is within an additive term of $\\sqrt{n}$ of its actual distance.", "abstract": "Building a spanning tree, minimum spanning tree (MST), and BFS tree in a distributed network are fundamental problems which are still not fully understood in terms of time and communication cost. Building a spanning tree, minimum spanning tree (MST), and BFS tree in a distributed network are fundamental problems which are still not fully understood in terms of time and communication cost. x The first work to succeed in computing a spanning tree with communication sublinear in the number of edges in an asynchronous CONGEST network appeared in DISC 2018. Building a spanning tree, minimum spanning tree (MST), and BFS tree in a distributed network are fundamental problems which are still not fully understood in terms of time and communication cost. x The first work to succeed in computing a spanning tree with communication sublinear in the number of edges in an asynchronous CONGEST network appeared in DISC 2018. That algorithm which constructs an MST is sequential in the worst case; its running time is proportional to the total number of messages sent. Building a spanning tree, minimum spanning tree (MST), and BFS tree in a distributed network are fundamental problems which are still not fully understood in terms of time and communication cost. x The first work to succeed in computing a spanning tree with communication sublinear in the number of edges in an asynchronous CONGEST network appeared in DISC 2018. That algorithm which constructs an MST is sequential in the worst case; its running time is proportional to the total number of messages sent. Our paper matches its message complexity but brings the running time down to linear in $n$. Our techniques can also be used to provide an asynchronous algorithm with sublinear communication to construct a tree in which the distance from a source to each node is within an additive term of $\\sqrt{n}$ of its actual distance."}, {"paper_id": "2639671", "adju_relevance": 0, "title": "Cut Tree Construction from Massive Graphs", "background_label": "The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. Cut trees are a powerful back-end for graph management and mining, as they support various procedures related to the minimum cut, maximum flow, and connectivity. However, the crucial drawback with cut trees is the computational cost of their construction. In theory, a cut tree is built by applying a maximum flow algorithm for $n$ times, where $n$ is the number of vertices. Therefore, naive implementations of this approach result in cubic time complexity, which is obviously too slow for today's large-scale graphs.", "method_label": "To address this issue, in the present study, we propose a new cut-tree construction algorithm tailored to real-world networks.", "result_label": "Using a series of experiments, we demonstrate that the proposed algorithm is several orders of magnitude faster than previous algorithms and it can construct cut trees for billion-scale graphs.", "abstract": "The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. Cut trees are a powerful back-end for graph management and mining, as they support various procedures related to the minimum cut, maximum flow, and connectivity. The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. Cut trees are a powerful back-end for graph management and mining, as they support various procedures related to the minimum cut, maximum flow, and connectivity. However, the crucial drawback with cut trees is the computational cost of their construction. The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. Cut trees are a powerful back-end for graph management and mining, as they support various procedures related to the minimum cut, maximum flow, and connectivity. However, the crucial drawback with cut trees is the computational cost of their construction. In theory, a cut tree is built by applying a maximum flow algorithm for $n$ times, where $n$ is the number of vertices. The construction of cut trees (also known as Gomory-Hu trees) for a given graph enables the minimum-cut size of the original graph to be obtained for any pair of vertices. Cut trees are a powerful back-end for graph management and mining, as they support various procedures related to the minimum cut, maximum flow, and connectivity. However, the crucial drawback with cut trees is the computational cost of their construction. In theory, a cut tree is built by applying a maximum flow algorithm for $n$ times, where $n$ is the number of vertices. Therefore, naive implementations of this approach result in cubic time complexity, which is obviously too slow for today's large-scale graphs. To address this issue, in the present study, we propose a new cut-tree construction algorithm tailored to real-world networks. Using a series of experiments, we demonstrate that the proposed algorithm is several orders of magnitude faster than previous algorithms and it can construct cut trees for billion-scale graphs."}, {"paper_id": "6670331", "adju_relevance": 0, "title": "Estimation of Renyi information divergence via pruned minimal spanning trees", "background_label": "In this paper we develop robust estimators of the Renyi information divergence (I-divergence) given a reference distribution and a random sample from an unknown distribution.", "method_label": "Estimation is performed by constructing a minimal spanning tree (MST) passing through the random sample points and applying a change of measure which flattens the reference distribution. In a mixture model where the reference distribution is contaminated by an unknown noise distribution one can use these results to reject noise samples by implementing a greedy algorithm for pruning the k-longest branches of the MST, resulting in a tree called the k-MST.", "result_label": "We illustrate this procedure in the context of density discrimination and robust clustering for a planar mixture model.", "abstract": "In this paper we develop robust estimators of the Renyi information divergence (I-divergence) given a reference distribution and a random sample from an unknown distribution. Estimation is performed by constructing a minimal spanning tree (MST) passing through the random sample points and applying a change of measure which flattens the reference distribution. Estimation is performed by constructing a minimal spanning tree (MST) passing through the random sample points and applying a change of measure which flattens the reference distribution. In a mixture model where the reference distribution is contaminated by an unknown noise distribution one can use these results to reject noise samples by implementing a greedy algorithm for pruning the k-longest branches of the MST, resulting in a tree called the k-MST. We illustrate this procedure in the context of density discrimination and robust clustering for a planar mixture model."}, {"paper_id": "6276962", "adju_relevance": 0, "title": "A Minimum Spanning Tree Algorithm with Inverse-Ackermann Type Complexity", "background_label": "A deterministic algorithm for computing a minimum spanning tree of a connected graph is presented.", "method_label": "Its running time is <italic>0</italic>(<italic>m</italic> \u03b1(<italic>m, n</italic>)), where  \u03b1 is the classical functional inverse of Ackermann's function and <italic>n</italic> (respectively, <italic>m</italic>) is the number of vertices (respectively, edges). The algorithm is comparison-based : it uses pointers, not arrays, and it makes no numeric assumptions on the edge costs.", "abstract": "A deterministic algorithm for computing a minimum spanning tree of a connected graph is presented. Its running time is <italic>0</italic>(<italic>m</italic> \u03b1(<italic>m, n</italic>)), where  \u03b1 is the classical functional inverse of Ackermann's function and <italic>n</italic> (respectively, <italic>m</italic>) is the number of vertices (respectively, edges). Its running time is <italic>0</italic>(<italic>m</italic> \u03b1(<italic>m, n</italic>)), where  \u03b1 is the classical functional inverse of Ackermann's function and <italic>n</italic> (respectively, <italic>m</italic>) is the number of vertices (respectively, edges). The algorithm is comparison-based : it uses pointers, not arrays, and it makes no numeric assumptions on the edge costs."}, {"paper_id": "15379542", "adju_relevance": 0, "title": "Directed Steiner Tree and the Lasserre Hierarchy", "background_label": "The goal for the Directed Steiner Tree problem is to find a minimum cost tree in a directed graph G=(V,E) that connects all terminals X to a given root r. It is well known that modulo a logarithmic factor it suffices to consider acyclic graphs where the nodes are arranged in L<= log |X| levels. Unfortunately the natural LP formulation has a |X|^(1/2) integrality gap already for 5 levels.", "method_label": "We show that for every L, the O(L)-round Lasserre Strengthening of this LP has integrality gap O(L log |X|).", "result_label": "This provides a polynomial time |X|^{epsilon}-approximation and a O(log^3 |X|) approximation in O(n^{log |X|) time, matching the best known approximation guarantee obtained by a greedy algorithm of Charikar et al.", "abstract": "The goal for the Directed Steiner Tree problem is to find a minimum cost tree in a directed graph G=(V,E) that connects all terminals X to a given root r. It is well known that modulo a logarithmic factor it suffices to consider acyclic graphs where the nodes are arranged in L<= log |X| levels. The goal for the Directed Steiner Tree problem is to find a minimum cost tree in a directed graph G=(V,E) that connects all terminals X to a given root r. It is well known that modulo a logarithmic factor it suffices to consider acyclic graphs where the nodes are arranged in L<= log |X| levels. Unfortunately the natural LP formulation has a |X|^(1/2) integrality gap already for 5 levels. We show that for every L, the O(L)-round Lasserre Strengthening of this LP has integrality gap O(L log |X|). This provides a polynomial time |X|^{epsilon}-approximation and a O(log^3 |X|) approximation in O(n^{log |X|) time, matching the best known approximation guarantee obtained by a greedy algorithm of Charikar et al."}, {"paper_id": "61727075", "adju_relevance": 0, "title": "Optimal Basic Block Reordering via Hammock Decomposition", "background_label": "Many optimizing compilers use basic block reordering to reduce conditional branch misprediction penalties, decrease the number of unconditional branches and to improve the instruction cache performance. To perform this optimization one has to solve an NP-hard problem of finding a covering of a weighted control flow graph with a node-disjoint set of simple paths with maximal total weight the problem is usually solved heuristically.", "method_label": "We present the precise method which decomposes the initial graph into the set of subgraphs of certain kind \u2014 hammocks \u2014 and then solves a subproblem for each subgraph by an exponential branch-and-bound algorithm. The algorithm was tested on several real-world C programs, and its running time was as small as the rest of the compilation time for 91% of the tested graphs.", "result_label": "Unfortunately it still failed to finish in a reasonable time for the remaining graphs since they didn\u2019t have enough hammocks. We consider fixing up this drawback as a future work.", "abstract": "Many optimizing compilers use basic block reordering to reduce conditional branch misprediction penalties, decrease the number of unconditional branches and to improve the instruction cache performance. Many optimizing compilers use basic block reordering to reduce conditional branch misprediction penalties, decrease the number of unconditional branches and to improve the instruction cache performance. To perform this optimization one has to solve an NP-hard problem of finding a covering of a weighted control flow graph with a node-disjoint set of simple paths with maximal total weight the problem is usually solved heuristically. We present the precise method which decomposes the initial graph into the set of subgraphs of certain kind \u2014 hammocks \u2014 and then solves a subproblem for each subgraph by an exponential branch-and-bound algorithm. We present the precise method which decomposes the initial graph into the set of subgraphs of certain kind \u2014 hammocks \u2014 and then solves a subproblem for each subgraph by an exponential branch-and-bound algorithm. The algorithm was tested on several real-world C programs, and its running time was as small as the rest of the compilation time for 91% of the tested graphs. Unfortunately it still failed to finish in a reasonable time for the remaining graphs since they didn\u2019t have enough hammocks. Unfortunately it still failed to finish in a reasonable time for the remaining graphs since they didn\u2019t have enough hammocks. We consider fixing up this drawback as a future work."}, {"paper_id": "120965672", "adju_relevance": 0, "title": "The construction of optimal multiway search trees and the monotonicity principle", "background_label": "Given Nweighted keysN+1 missing-key weights and a branching factor t the application of dynamic programming yields algorithms for constructing optimal binary search trees (t = 2), optimal multi-way search trees (t>2), and optimal leaf search trees (or alphabetic code trees) with leaf weights only.", "method_label": "The basic running time in all cases is O(N 3)(in terms of the number of keys), but it can be reduced to O(N 2) for binary search trees by a \u201cmonotonicity\u201d principle which restricts the number of candidates for the root at each step in the construction. This principle can also be applied for multiway search trees when the missing-key weights are zero.", "result_label": "However it does not extend to optimal multiway search trees in general, as we demonstrate; in particular, there is no monotonicity principle for alphabetic code trees, contrary to what is claimed in [5].", "abstract": "Given Nweighted keysN+1 missing-key weights and a branching factor t the application of dynamic programming yields algorithms for constructing optimal binary search trees (t = 2), optimal multi-way search trees (t>2), and optimal leaf search trees (or alphabetic code trees) with leaf weights only. The basic running time in all cases is O(N 3)(in terms of the number of keys), but it can be reduced to O(N 2) for binary search trees by a \u201cmonotonicity\u201d principle which restricts the number of candidates for the root at each step in the construction. The basic running time in all cases is O(N 3)(in terms of the number of keys), but it can be reduced to O(N 2) for binary search trees by a \u201cmonotonicity\u201d principle which restricts the number of candidates for the root at each step in the construction. This principle can also be applied for multiway search trees when the missing-key weights are zero. However it does not extend to optimal multiway search trees in general, as we demonstrate; in particular, there is no monotonicity principle for alphabetic code trees, contrary to what is claimed in [5]."}, {"paper_id": "42994", "adju_relevance": 0, "title": "Local MST computation with short advice", "background_label": "We use the recently introduced <i>advising scheme</i> framework for measuring the difficulty of locally distributively computing a Minimum Spanning Tree (MST). An (<i>m</i>,<i>t</i>)-advising scheme for a distributed problem <i>P</i> is a way, for every possible input <i>I</i> of <i>P</i>, to provide an \"advice\" (i.e., a bit string) about <i>I</i> to each node so that: (1) the maximum size of the advices is at most <i>m</i> bits, and (2) the problem <i>P</i> can be solved distributively in at most <i>t</i> rounds using the advices as inputs. In case of MST, the output returned by each node of a weighted graph <i>G</i> is the edge leading to its parent in some rooted MST <i>T</i> of <i>G</i>. Clearly, there is a trivial (log <i>n</i>,0)-advising scheme for MST (each node is given the local port number of the edge leading to the root of some MST <i>T</i>), and it is known that any (0,<i>t</i>)-advising scheme satisfies <i>t</i> \u2265 \u03a9 (\u221a<i>n</i>).", "abstract": "We use the recently introduced <i>advising scheme</i> framework for measuring the difficulty of locally distributively computing a Minimum Spanning Tree (MST). We use the recently introduced <i>advising scheme</i> framework for measuring the difficulty of locally distributively computing a Minimum Spanning Tree (MST). An (<i>m</i>,<i>t</i>)-advising scheme for a distributed problem <i>P</i> is a way, for every possible input <i>I</i> of <i>P</i>, to provide an \"advice\" (i.e., a bit string) about <i>I</i> to each node so that: (1) the maximum size of the advices is at most <i>m</i> bits, and (2) the problem <i>P</i> can be solved distributively in at most <i>t</i> rounds using the advices as inputs. We use the recently introduced <i>advising scheme</i> framework for measuring the difficulty of locally distributively computing a Minimum Spanning Tree (MST). An (<i>m</i>,<i>t</i>)-advising scheme for a distributed problem <i>P</i> is a way, for every possible input <i>I</i> of <i>P</i>, to provide an \"advice\" (i.e., a bit string) about <i>I</i> to each node so that: (1) the maximum size of the advices is at most <i>m</i> bits, and (2) the problem <i>P</i> can be solved distributively in at most <i>t</i> rounds using the advices as inputs. In case of MST, the output returned by each node of a weighted graph <i>G</i> is the edge leading to its parent in some rooted MST <i>T</i> of <i>G</i>. We use the recently introduced <i>advising scheme</i> framework for measuring the difficulty of locally distributively computing a Minimum Spanning Tree (MST). An (<i>m</i>,<i>t</i>)-advising scheme for a distributed problem <i>P</i> is a way, for every possible input <i>I</i> of <i>P</i>, to provide an \"advice\" (i.e., a bit string) about <i>I</i> to each node so that: (1) the maximum size of the advices is at most <i>m</i> bits, and (2) the problem <i>P</i> can be solved distributively in at most <i>t</i> rounds using the advices as inputs. In case of MST, the output returned by each node of a weighted graph <i>G</i> is the edge leading to its parent in some rooted MST <i>T</i> of <i>G</i>. Clearly, there is a trivial (log <i>n</i>,0)-advising scheme for MST (each node is given the local port number of the edge leading to the root of some MST <i>T</i>), and it is known that any (0,<i>t</i>)-advising scheme satisfies <i>t</i> \u2265 \u03a9 (\u221a<i>n</i>)."}, {"paper_id": "57588770", "adju_relevance": 0, "title": "Algorithm of documents clustering based on minimum spanning tree", "background_label": "This paper puts forward a method of document clustering based on minimum spanning tree (MST) in vector space model (VSM).", "method_label": "This algorithm adopts classical VSM and combines with the method of MST in graph theory.", "result_label": "The quality and performance of document clustering are higher than other traditional clustering methods.", "abstract": "This paper puts forward a method of document clustering based on minimum spanning tree (MST) in vector space model (VSM). This algorithm adopts classical VSM and combines with the method of MST in graph theory. The quality and performance of document clustering are higher than other traditional clustering methods."}, {"paper_id": "5362916", "adju_relevance": 0, "title": "An optimal minimum spanning tree algorithm", "background_label": "We establish that the algorithmic complexity of the minimumspanning tree problem is equal to its decision-tree complexity.Specifically, we present a deterministic algorithm to find aminimum spanning tree of a graph with <i>n</i> vertices and<i>m</i> edges that runs in time<i>O</i>(<i>T</i><sup>*</sup>(<i>m,n</i>)) where<i>T</i><sup>*</sup> is the minimum number of edge-weightcomparisons needed to determine the solution. The algorithm isquite simple and can be implemented on a pointer machine.Althoughour time bound is optimal, the exact function describing it is notknown at present.", "method_label": "The current best bounds known for<i>T</i><sup>*</sup> are <i>T</i><sup>*</sup>(<i>m,n</i>) =\u03a9(<i>m</i>) and <i>T</i><sup>*</sup>(<i>m,n</i>) =<i>O</i>(<i>m</i> \u2219 \u03b1(<i>m,n</i>)), where \u03b1 is acertain natural inverse of Ackermann's function.Even under theassumption that <i>T</i><sup>*</sup> is superlinear, we show thatif the input graph is selected from <i>G</i><sub><i>n,m</i></sub>,our algorithm runs in linear time with high probability, regardlessof <i>n</i>, <i>m</i>, or the permutation of edge weights.", "abstract": "We establish that the algorithmic complexity of the minimumspanning tree problem is equal to its decision-tree complexity.Specifically, we present a deterministic algorithm to find aminimum spanning tree of a graph with <i>n</i> vertices and<i>m</i> edges that runs in time<i>O</i>(<i>T</i><sup>*</sup>(<i>m,n</i>)) where<i>T</i><sup>*</sup> is the minimum number of edge-weightcomparisons needed to determine the solution. We establish that the algorithmic complexity of the minimumspanning tree problem is equal to its decision-tree complexity.Specifically, we present a deterministic algorithm to find aminimum spanning tree of a graph with <i>n</i> vertices and<i>m</i> edges that runs in time<i>O</i>(<i>T</i><sup>*</sup>(<i>m,n</i>)) where<i>T</i><sup>*</sup> is the minimum number of edge-weightcomparisons needed to determine the solution. The algorithm isquite simple and can be implemented on a pointer machine.Althoughour time bound is optimal, the exact function describing it is notknown at present. The current best bounds known for<i>T</i><sup>*</sup> are <i>T</i><sup>*</sup>(<i>m,n</i>) =\u03a9(<i>m</i>) and <i>T</i><sup>*</sup>(<i>m,n</i>) =<i>O</i>(<i>m</i> \u2219 \u03b1(<i>m,n</i>)), where \u03b1 is acertain natural inverse of Ackermann's function.Even under theassumption that <i>T</i><sup>*</sup> is superlinear, we show thatif the input graph is selected from <i>G</i><sub><i>n,m</i></sub>,our algorithm runs in linear time with high probability, regardlessof <i>n</i>, <i>m</i>, or the permutation of edge weights."}, {"paper_id": "13944414", "adju_relevance": 0, "title": "A Bit-Parallel Russian Dolls Search for a Maximum Cardinality Clique in a Graph", "background_label": "Finding the clique of maximum cardinality in an arbitrary graph is an NP-Hard problem that has many applications, which has motivated studies to solve it exactly despite its difficulty. The great majority of algorithms proposed in the literature are based on the Branch and Bound method.", "method_label": "In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. When compared to Branch and Bound, the main difference of the Russian Dolls method is that the nodes of its search tree correspond to decision subproblems, instead of the optimization subproblems of the Branch and Bound method. In comparison to a first implementation of this Russian Dolls method from the literature, several improvements are presented. Some of them are adaptations of techniques already employed successfully in Branch and Bound algorithms, like the use of approximate coloring for pruning purposes and bit-parallel operations. Two different coloring heuristics are tested: the standard greedy and the greedy with recoloring. Other improvements are directly related to the Russian Dolls scheme: the adoption of recursive calls where each subproblem (doll) is solved itself via the same principles than the Russian Dolls Search and the application of an elimination rule allowing not to generate a significant number of dolls.", "result_label": "Results of computational experiments show that the algorithm outperforms the best exact combinatorial algorithms in the literature for the great majority of the dense graphs tested, being more than twice faster in several cases.", "abstract": "Finding the clique of maximum cardinality in an arbitrary graph is an NP-Hard problem that has many applications, which has motivated studies to solve it exactly despite its difficulty. Finding the clique of maximum cardinality in an arbitrary graph is an NP-Hard problem that has many applications, which has motivated studies to solve it exactly despite its difficulty. The great majority of algorithms proposed in the literature are based on the Branch and Bound method. In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. When compared to Branch and Bound, the main difference of the Russian Dolls method is that the nodes of its search tree correspond to decision subproblems, instead of the optimization subproblems of the Branch and Bound method. In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. When compared to Branch and Bound, the main difference of the Russian Dolls method is that the nodes of its search tree correspond to decision subproblems, instead of the optimization subproblems of the Branch and Bound method. In comparison to a first implementation of this Russian Dolls method from the literature, several improvements are presented. In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. When compared to Branch and Bound, the main difference of the Russian Dolls method is that the nodes of its search tree correspond to decision subproblems, instead of the optimization subproblems of the Branch and Bound method. In comparison to a first implementation of this Russian Dolls method from the literature, several improvements are presented. Some of them are adaptations of techniques already employed successfully in Branch and Bound algorithms, like the use of approximate coloring for pruning purposes and bit-parallel operations. In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. When compared to Branch and Bound, the main difference of the Russian Dolls method is that the nodes of its search tree correspond to decision subproblems, instead of the optimization subproblems of the Branch and Bound method. In comparison to a first implementation of this Russian Dolls method from the literature, several improvements are presented. Some of them are adaptations of techniques already employed successfully in Branch and Bound algorithms, like the use of approximate coloring for pruning purposes and bit-parallel operations. Two different coloring heuristics are tested: the standard greedy and the greedy with recoloring. In this paper, we propose an exact algorithm for the maximum clique problem based on the Russian Dolls Search method. When compared to Branch and Bound, the main difference of the Russian Dolls method is that the nodes of its search tree correspond to decision subproblems, instead of the optimization subproblems of the Branch and Bound method. In comparison to a first implementation of this Russian Dolls method from the literature, several improvements are presented. Some of them are adaptations of techniques already employed successfully in Branch and Bound algorithms, like the use of approximate coloring for pruning purposes and bit-parallel operations. Two different coloring heuristics are tested: the standard greedy and the greedy with recoloring. Other improvements are directly related to the Russian Dolls scheme: the adoption of recursive calls where each subproblem (doll) is solved itself via the same principles than the Russian Dolls Search and the application of an elimination rule allowing not to generate a significant number of dolls. Results of computational experiments show that the algorithm outperforms the best exact combinatorial algorithms in the literature for the great majority of the dense graphs tested, being more than twice faster in several cases."}, {"paper_id": "542323", "adju_relevance": 0, "title": "Embedded trees: estimation of Gaussian Processes on graphs with cycles", "background_label": "Graphical models provide a powerful general framework for encoding the structure of large-scale estimation problems. However, the graphs describing typical real-world phenomena contain many cycles, making direct estimation procedures prohibitively costly.", "abstract": "Graphical models provide a powerful general framework for encoding the structure of large-scale estimation problems. Graphical models provide a powerful general framework for encoding the structure of large-scale estimation problems. However, the graphs describing typical real-world phenomena contain many cycles, making direct estimation procedures prohibitively costly."}, {"paper_id": "9033974", "adju_relevance": 0, "title": "Minimum spanning trees for graphs with random edge lengths", "background_label": "The theory of the minimal spanning tree (MST) of a connected graph whose edges are assigned lengths according to independent identically distributed random variables is developed from two directions.", "method_label": "First, it is shown how the Tutte polynomial for a connected graph can be used to provide an exact formula for the length of the minimal spanning tree under the model of uniformly distributed edge lengths. Second, it is shown how the theory of local weak convergence provides a systematic approach to the asymptotic theory of the length of the MST and related power sums.", "result_label": "Consequences of these investigations include (1) the exact rational determination of the expected length of the MST for the complete graph Kn for 2 \u2264 n \u2264 9 and (2) refinements of the results of Penrose (1998) for the MST of the d-cube and results of Beveridge, Frieze, and McDiarmid (1998) and Frieze, Ruzink6, and Thoma (2000) for graphs with modest expansion properties. In most cases, the results reviewed here have not reached their final form, and they should be viewed as part of work-in-progress.", "abstract": "The theory of the minimal spanning tree (MST) of a connected graph whose edges are assigned lengths according to independent identically distributed random variables is developed from two directions. First, it is shown how the Tutte polynomial for a connected graph can be used to provide an exact formula for the length of the minimal spanning tree under the model of uniformly distributed edge lengths. First, it is shown how the Tutte polynomial for a connected graph can be used to provide an exact formula for the length of the minimal spanning tree under the model of uniformly distributed edge lengths. Second, it is shown how the theory of local weak convergence provides a systematic approach to the asymptotic theory of the length of the MST and related power sums. Consequences of these investigations include (1) the exact rational determination of the expected length of the MST for the complete graph Kn for 2 \u2264 n \u2264 9 and (2) refinements of the results of Penrose (1998) for the MST of the d-cube and results of Beveridge, Frieze, and McDiarmid (1998) and Frieze, Ruzink6, and Thoma (2000) for graphs with modest expansion properties. Consequences of these investigations include (1) the exact rational determination of the expected length of the MST for the complete graph Kn for 2 \u2264 n \u2264 9 and (2) refinements of the results of Penrose (1998) for the MST of the d-cube and results of Beveridge, Frieze, and McDiarmid (1998) and Frieze, Ruzink6, and Thoma (2000) for graphs with modest expansion properties. In most cases, the results reviewed here have not reached their final form, and they should be viewed as part of work-in-progress."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "3343953", "adju_relevance": 0, "title": "A New Approach of Arc Skeletonization for Tree-like Objects Using Minimum Cost Path", "background_label": "Traditional arc skeletonization algorithms using the principle of Blum's transform, often, produce unwanted spurious branches due to boundary irregularities and digital effects on objects and other artifacts.", "abstract": "Traditional arc skeletonization algorithms using the principle of Blum's transform, often, produce unwanted spurious branches due to boundary irregularities and digital effects on objects and other artifacts."}, {"paper_id": "7304828", "adju_relevance": 0, "title": "A Junction Tree Framework for Undirected Graphical Model Selection", "background_label": "An undirected graphical model is a joint probability distribution defined on an undirected graph G*, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables.", "abstract": "An undirected graphical model is a joint probability distribution defined on an undirected graph G*, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables."}, {"paper_id": "115178641", "adju_relevance": 0, "title": "Improving access to organized information", "background_label": "We introduce several new models and methods for improving access to organized information. The first model, Constrained Subtree Selection (CSS), has applications in web site design and the reorganization of directory structures. Even though CSS remains NP-hard for constant degree DAGs, we give an O(log(k)\u03b3(d+1)) approximation for any G with maximum degree d, provided that \u03b3 favors nodes with at most k links. Finally, we give a complete characterization of the optimal trees for two special cases: (1) linear degree cost in unconstrained graphs and uniform probability distributions, and (2) logarithmic degree cost in arbitrary DAGs and uniform probability distributions.", "method_label": "Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. CT generalizes the well-studied Decision Tree (DT) problem. Our results resolve two open problems: We give a ln n + 1-approximation for DT and show DT does not have a polynomial time approximation scheme unless P=NP. Our work, while providing the first non-trivial upper and lower bounds on approximating DT, also demonstrates that DT and a subtly different problem which also bears the name decision tree have fundamentally different approximation complexity. We complement the above models with a new pruning method for k nearest neighbor queries on R-trees. We show that an extension to a popular depth-first 1-nearest neighbor query results in a theoretically better search.", "result_label": "Turning to algorithms, we give a polynomial time solution for instances of CSS where G does not constrain the choice of subtrees and \u03b3 favors nodes with at most k links. We call this extension Promise-Pruning and construct a class of R-trees where its application reduces the search space exponentially.", "abstract": "We introduce several new models and methods for improving access to organized information. We introduce several new models and methods for improving access to organized information. The first model, Constrained Subtree Selection (CSS), has applications in web site design and the reorganization of directory structures. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. Turning to algorithms, we give a polynomial time solution for instances of CSS where G does not constrain the choice of subtrees and \u03b3 favors nodes with at most k links. We introduce several new models and methods for improving access to organized information. The first model, Constrained Subtree Selection (CSS), has applications in web site design and the reorganization of directory structures. Even though CSS remains NP-hard for constant degree DAGs, we give an O(log(k)\u03b3(d+1)) approximation for any G with maximum degree d, provided that \u03b3 favors nodes with at most k links. We introduce several new models and methods for improving access to organized information. The first model, Constrained Subtree Selection (CSS), has applications in web site design and the reorganization of directory structures. Even though CSS remains NP-hard for constant degree DAGs, we give an O(log(k)\u03b3(d+1)) approximation for any G with maximum degree d, provided that \u03b3 favors nodes with at most k links. Finally, we give a complete characterization of the optimal trees for two special cases: (1) linear degree cost in unconstrained graphs and uniform probability distributions, and (2) logarithmic degree cost in arbitrary DAGs and uniform probability distributions. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. CT generalizes the well-studied Decision Tree (DT) problem. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. CT generalizes the well-studied Decision Tree (DT) problem. Our results resolve two open problems: We give a ln n + 1-approximation for DT and show DT does not have a polynomial time approximation scheme unless P=NP. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. CT generalizes the well-studied Decision Tree (DT) problem. Our results resolve two open problems: We give a ln n + 1-approximation for DT and show DT does not have a polynomial time approximation scheme unless P=NP. Our work, while providing the first non-trivial upper and lower bounds on approximating DT, also demonstrates that DT and a subtly different problem which also bears the name decision tree have fundamentally different approximation complexity. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. CT generalizes the well-studied Decision Tree (DT) problem. Our results resolve two open problems: We give a ln n + 1-approximation for DT and show DT does not have a polynomial time approximation scheme unless P=NP. Our work, while providing the first non-trivial upper and lower bounds on approximating DT, also demonstrates that DT and a subtly different problem which also bears the name decision tree have fundamentally different approximation complexity. We complement the above models with a new pruning method for k nearest neighbor queries on R-trees. Given a hierarchy represented as a rooted DAG G with n weighted leaves, one selects a subtree of the transitive closure of G that minimizes the expected path cost. Path cost is the sum of the degree costs along a path from the root to a leaf. Degree cost, \u03b3, is a function of the out degree of a node. We give a sufficient condition for \u03b3 that makes CSS NP-complete. This result holds even when the leaves have equal weight. The second problem, Category Tree (CT), seeks a decision tree for categorical data where internal nodes are categories, edges are appropriate values for the categories, and leaves are data items. CT generalizes the well-studied Decision Tree (DT) problem. Our results resolve two open problems: We give a ln n + 1-approximation for DT and show DT does not have a polynomial time approximation scheme unless P=NP. Our work, while providing the first non-trivial upper and lower bounds on approximating DT, also demonstrates that DT and a subtly different problem which also bears the name decision tree have fundamentally different approximation complexity. We complement the above models with a new pruning method for k nearest neighbor queries on R-trees. We show that an extension to a popular depth-first 1-nearest neighbor query results in a theoretically better search. Turning to algorithms, we give a polynomial time solution for instances of CSS where G does not constrain the choice of subtrees and \u03b3 favors nodes with at most k links. We call this extension Promise-Pruning and construct a class of R-trees where its application reduces the search space exponentially."}, {"paper_id": "807019", "adju_relevance": 0, "title": "Semi-supervised learning using randomized mincuts", "background_label": "In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data.", "abstract": "In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data."}, {"paper_id": "10752264", "adju_relevance": 0, "title": "Improving Grammaticality in Statistical Sentence Generation: Introducing a Dependency Spanning Tree Algorithm with an Argument Satisfaction Model", "background_label": "Abstract-like text summarisation requires a means of producing novel summary sentences. In order to improve the grammaticality of the generated sentence, we model a global (sentence) level syntactic structure.", "method_label": "We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We treat the allocation of modifiers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory.", "result_label": "Using BLEU to measure performance on a string regeneration task, we found an improvement, illustrating the benefit of the spanning tree approach armed with an argument satisfaction model.", "abstract": "Abstract-like text summarisation requires a means of producing novel summary sentences. Abstract-like text summarisation requires a means of producing novel summary sentences. In order to improve the grammaticality of the generated sentence, we model a global (sentence) level syntactic structure. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We treat the allocation of modifiers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory. Using BLEU to measure performance on a string regeneration task, we found an improvement, illustrating the benefit of the spanning tree approach armed with an argument satisfaction model."}, {"paper_id": "16517586", "adju_relevance": 0, "title": "Efficient and Progressive Group Steiner Tree Search", "background_label": "The Group Steiner Tree (GST) problem is a fundamental problem in database area that has been successfully applied to keyword search in relational databases and team search in social networks. The state-of-the-art algorithm for the GST problem is a parameterized dynamic programming (DP) algorithm, which finds the optimal tree in O(3kn+2k(n log n + m)) time, where k is the number of given groups, m and n are the number of the edges and nodes of the graph respectively. The major limitations of the parameterized DP algorithm are twofold: (i) it is intractable even for very small values of k (e.g., k=8) in large graphs due to its exponential complexity, and (ii) it cannot generate a solution until the algorithm has completed its entire execution.", "abstract": "The Group Steiner Tree (GST) problem is a fundamental problem in database area that has been successfully applied to keyword search in relational databases and team search in social networks. The Group Steiner Tree (GST) problem is a fundamental problem in database area that has been successfully applied to keyword search in relational databases and team search in social networks. The state-of-the-art algorithm for the GST problem is a parameterized dynamic programming (DP) algorithm, which finds the optimal tree in O(3kn+2k(n log n + m)) time, where k is the number of given groups, m and n are the number of the edges and nodes of the graph respectively. The Group Steiner Tree (GST) problem is a fundamental problem in database area that has been successfully applied to keyword search in relational databases and team search in social networks. The state-of-the-art algorithm for the GST problem is a parameterized dynamic programming (DP) algorithm, which finds the optimal tree in O(3kn+2k(n log n + m)) time, where k is the number of given groups, m and n are the number of the edges and nodes of the graph respectively. The major limitations of the parameterized DP algorithm are twofold: (i) it is intractable even for very small values of k (e.g., k=8) in large graphs due to its exponential complexity, and (ii) it cannot generate a solution until the algorithm has completed its entire execution."}, {"paper_id": "13079931", "adju_relevance": 0, "title": "Cache-Oblivious Peeling of Random Hypergraphs", "background_label": "The computation of a peeling order in a randomly generated hypergraph is the most time-consuming step in a number of constructions, such as perfect hashing schemes, random $r$-SAT solvers, error-correcting codes, and approximate set encodings. While there exists a straightforward linear time algorithm, its poor I/O performance makes it impractical for hypergraphs whose size exceeds the available internal memory.", "method_label": "We show how to reduce the computation of a peeling order to a small number of sequential scans and sorts, and analyze its I/O complexity in the cache-oblivious model. The resulting algorithm requires $O(\\mathrm{sort}(n))$ I/Os and $O(n \\log n)$ time to peel a random hypergraph with $n$ edges.", "result_label": "We experimentally evaluate the performance of our implementation of this algorithm in a real-world scenario by using the construction of minimal perfect hash functions (MPHF) as our test case: our algorithm builds a MPHF of $7.6$ billion keys in less than $21$ hours on a single machine. The resulting data structure is both more space-efficient and faster than that obtained with the current state-of-the-art MPHF construction for large-scale key sets.", "abstract": "The computation of a peeling order in a randomly generated hypergraph is the most time-consuming step in a number of constructions, such as perfect hashing schemes, random $r$-SAT solvers, error-correcting codes, and approximate set encodings. The computation of a peeling order in a randomly generated hypergraph is the most time-consuming step in a number of constructions, such as perfect hashing schemes, random $r$-SAT solvers, error-correcting codes, and approximate set encodings. While there exists a straightforward linear time algorithm, its poor I/O performance makes it impractical for hypergraphs whose size exceeds the available internal memory. We show how to reduce the computation of a peeling order to a small number of sequential scans and sorts, and analyze its I/O complexity in the cache-oblivious model. We show how to reduce the computation of a peeling order to a small number of sequential scans and sorts, and analyze its I/O complexity in the cache-oblivious model. The resulting algorithm requires $O(\\mathrm{sort}(n))$ I/Os and $O(n \\log n)$ time to peel a random hypergraph with $n$ edges. We experimentally evaluate the performance of our implementation of this algorithm in a real-world scenario by using the construction of minimal perfect hash functions (MPHF) as our test case: our algorithm builds a MPHF of $7.6$ billion keys in less than $21$ hours on a single machine. We experimentally evaluate the performance of our implementation of this algorithm in a real-world scenario by using the construction of minimal perfect hash functions (MPHF) as our test case: our algorithm builds a MPHF of $7.6$ billion keys in less than $21$ hours on a single machine. The resulting data structure is both more space-efficient and faster than that obtained with the current state-of-the-art MPHF construction for large-scale key sets."}, {"paper_id": "52836908", "adju_relevance": 0, "title": "Distributed verification of minimum spanning trees", "background_label": "The problem of verifying a Minimum Spanning Tree (MST) was introduced by Tarjan in a sequential setting. Given a graph and a tree that spans it, the algorithm is required to check whether this tree is an MST.", "abstract": "The problem of verifying a Minimum Spanning Tree (MST) was introduced by Tarjan in a sequential setting. The problem of verifying a Minimum Spanning Tree (MST) was introduced by Tarjan in a sequential setting. Given a graph and a tree that spans it, the algorithm is required to check whether this tree is an MST."}, {"paper_id": "855901", "adju_relevance": 0, "title": "Fast Self-Stabilizing Minimum Spanning Tree Construction", "background_label": "We present a novel self-stabilizing algorithm for minimum spanning tree (MST) construction. The space complexity of our solution is $O(\\log^2n)$ bits and it converges in $O(n^2)$ rounds.", "method_label": "Thus, this algorithm improves the convergence time of all previously known self-stabilizing asynchronous MST algorithms by a multiplicative factor $\\Theta(n)$, to the price of increasing the best known space complexity by a factor $O(\\log n)$.", "result_label": "The main ingredient used in our algorithm is the design, for the first time in self-stabilizing settings, of a labeling scheme for computing the nearest common ancestor with only $O(\\log^2n)$ bits.", "abstract": "We present a novel self-stabilizing algorithm for minimum spanning tree (MST) construction. We present a novel self-stabilizing algorithm for minimum spanning tree (MST) construction. The space complexity of our solution is $O(\\log^2n)$ bits and it converges in $O(n^2)$ rounds. Thus, this algorithm improves the convergence time of all previously known self-stabilizing asynchronous MST algorithms by a multiplicative factor $\\Theta(n)$, to the price of increasing the best known space complexity by a factor $O(\\log n)$. The main ingredient used in our algorithm is the design, for the first time in self-stabilizing settings, of a labeling scheme for computing the nearest common ancestor with only $O(\\log^2n)$ bits."}, {"paper_id": "639189", "adju_relevance": 0, "title": "Tight Bounds for Distributed Minimum-Weight Spanning Tree Verification", "background_label": "This paper introduces the notion of distributed verification without preprocessing. It focuses on the Minimum-weight Spanning Tree (MST) verification problem and establishes tight upper and lower bounds for the time and message complexities of this problem.", "method_label": "Specifically, we provide an MST verification algorithm that achieves simultaneously O(m) messages and O($\\sqrt$ n+D) time, where m is the number of edges in the given graph G, n is the number of nodes, and D is G's diameter. On the other hand, we show that any MST verification algorithm must send {\\Omega}(m) messages and incur {\\Omega}($\\sqrt$ n + D) time in worst case.", "result_label": "Our upper bound result appears to indicate that the verification of an MST may be easier than its construction, since for MST construction, both lower bounds of {\\Omega}(m) messages and {\\Omega}($\\sqrt$ n+D time hold, but at the moment there is no known distributed algorithm that constructs an MST and achieves simultaneously O(m) messages and O($\\sqrt$ n + D) time. Specifically, the best known time-optimal algorithm (using O($\\sqrt$ n + D) time) requires O(m + n 3/2) messages, and the best known message-optimal algorithm (using O(m) messages) requires O(n) time. On the other hand, our lower bound results indicate that the verification of an MST is not significantly easier than its construction.", "abstract": "This paper introduces the notion of distributed verification without preprocessing. This paper introduces the notion of distributed verification without preprocessing. It focuses on the Minimum-weight Spanning Tree (MST) verification problem and establishes tight upper and lower bounds for the time and message complexities of this problem. Specifically, we provide an MST verification algorithm that achieves simultaneously O(m) messages and O($\\sqrt$ n+D) time, where m is the number of edges in the given graph G, n is the number of nodes, and D is G's diameter. Specifically, we provide an MST verification algorithm that achieves simultaneously O(m) messages and O($\\sqrt$ n+D) time, where m is the number of edges in the given graph G, n is the number of nodes, and D is G's diameter. On the other hand, we show that any MST verification algorithm must send {\\Omega}(m) messages and incur {\\Omega}($\\sqrt$ n + D) time in worst case. Our upper bound result appears to indicate that the verification of an MST may be easier than its construction, since for MST construction, both lower bounds of {\\Omega}(m) messages and {\\Omega}($\\sqrt$ n+D time hold, but at the moment there is no known distributed algorithm that constructs an MST and achieves simultaneously O(m) messages and O($\\sqrt$ n + D) time. Our upper bound result appears to indicate that the verification of an MST may be easier than its construction, since for MST construction, both lower bounds of {\\Omega}(m) messages and {\\Omega}($\\sqrt$ n+D time hold, but at the moment there is no known distributed algorithm that constructs an MST and achieves simultaneously O(m) messages and O($\\sqrt$ n + D) time. Specifically, the best known time-optimal algorithm (using O($\\sqrt$ n + D) time) requires O(m + n 3/2) messages, and the best known message-optimal algorithm (using O(m) messages) requires O(n) time. Our upper bound result appears to indicate that the verification of an MST may be easier than its construction, since for MST construction, both lower bounds of {\\Omega}(m) messages and {\\Omega}($\\sqrt$ n+D time hold, but at the moment there is no known distributed algorithm that constructs an MST and achieves simultaneously O(m) messages and O($\\sqrt$ n + D) time. Specifically, the best known time-optimal algorithm (using O($\\sqrt$ n + D) time) requires O(m + n 3/2) messages, and the best known message-optimal algorithm (using O(m) messages) requires O(n) time. On the other hand, our lower bound results indicate that the verification of an MST is not significantly easier than its construction."}, {"paper_id": "9064440", "adju_relevance": 0, "title": "A series of approximation algorithms for the acyclic directed steiner tree problem", "background_label": "Given an acyclic directed network, a subsetS of nodes (terminals), and a rootr, theacyclic directed Steiner tree problem requires a minimum-cost subnetwork which contains paths fromr to each terminal. It is known that unlessNP\u2286DTIME[n polylogn ] no polynomial-time algorithm can guarantee better than (lnk)/4-approximation, wherek is the number of terminals.", "method_label": "In this paper we give anO(k \u03b5)-approximation algorithm for any \u03b5>0.", "result_label": "This result improves the previously knownk-approximation.", "abstract": "Given an acyclic directed network, a subsetS of nodes (terminals), and a rootr, theacyclic directed Steiner tree problem requires a minimum-cost subnetwork which contains paths fromr to each terminal. Given an acyclic directed network, a subsetS of nodes (terminals), and a rootr, theacyclic directed Steiner tree problem requires a minimum-cost subnetwork which contains paths fromr to each terminal. It is known that unlessNP\u2286DTIME[n polylogn ] no polynomial-time algorithm can guarantee better than (lnk)/4-approximation, wherek is the number of terminals. In this paper we give anO(k \u03b5)-approximation algorithm for any \u03b5>0. This result improves the previously knownk-approximation."}, {"paper_id": "14722146", "adju_relevance": 0, "title": "Efficient Construction of Probabilistic Tree Embeddings", "background_label": "In this paper we describe an algorithm that embeds a graph metric $(V,d_G)$ on an undirected weighted graph $G=(V,E)$ into a distribution of tree metrics $(T,D_T)$ such that for every pair $u,v\\in V$, $d_G(u,v)\\leq d_T(u,v)$ and ${\\bf{E}}_{T}[d_T(u,v)]\\leq O(\\log n)\\cdot d_G(u,v)$. Such embeddings have proved highly useful in designing fast approximation algorithms, as many hard problems on graphs are easy to solve on tree instances.", "abstract": "In this paper we describe an algorithm that embeds a graph metric $(V,d_G)$ on an undirected weighted graph $G=(V,E)$ into a distribution of tree metrics $(T,D_T)$ such that for every pair $u,v\\in V$, $d_G(u,v)\\leq d_T(u,v)$ and ${\\bf{E}}_{T}[d_T(u,v)]\\leq O(\\log n)\\cdot d_G(u,v)$. In this paper we describe an algorithm that embeds a graph metric $(V,d_G)$ on an undirected weighted graph $G=(V,E)$ into a distribution of tree metrics $(T,D_T)$ such that for every pair $u,v\\in V$, $d_G(u,v)\\leq d_T(u,v)$ and ${\\bf{E}}_{T}[d_T(u,v)]\\leq O(\\log n)\\cdot d_G(u,v)$. Such embeddings have proved highly useful in designing fast approximation algorithms, as many hard problems on graphs are easy to solve on tree instances."}, {"paper_id": "52273116", "adju_relevance": 0, "title": "Efficient Graph Cut Optimization for Full CRFs with Quantized Edges", "background_label": "Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Previous work develops efficient approximate optimization based on mean field inference, which is a local optimization method and can be far from the optimum. Our quantized edge CRF is an approximation to the Gaussian edge CRF, and gets closer to it as superpixel size decreases. Being an approximation, our model offers an intuition about the regularization properties of the Guassian edge Full-CRF.", "method_label": "We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. To quantize edge weights, we partition the image into superpixels and assume that the weight of an edge between any two pixels depends only on the superpixels these pixels belong to. For efficient inference, we first consider the two-label case and develop an approximate method based on transforming the original problem into a smaller domain. Then we handle multi-label CRF by showing how to implement expansion moves.", "result_label": "In both binary and multi-label cases, our solutions have significantly lower energy compared to that of mean field inference. We also show the effectiveness of our approach on semantic segmentation task.", "abstract": "Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Previous work develops efficient approximate optimization based on mean field inference, which is a local optimization method and can be far from the optimum. We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. To quantize edge weights, we partition the image into superpixels and assume that the weight of an edge between any two pixels depends only on the superpixels these pixels belong to. Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Previous work develops efficient approximate optimization based on mean field inference, which is a local optimization method and can be far from the optimum. Our quantized edge CRF is an approximation to the Gaussian edge CRF, and gets closer to it as superpixel size decreases. Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Previous work develops efficient approximate optimization based on mean field inference, which is a local optimization method and can be far from the optimum. Our quantized edge CRF is an approximation to the Gaussian edge CRF, and gets closer to it as superpixel size decreases. Being an approximation, our model offers an intuition about the regularization properties of the Guassian edge Full-CRF. We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. To quantize edge weights, we partition the image into superpixels and assume that the weight of an edge between any two pixels depends only on the superpixels these pixels belong to. For efficient inference, we first consider the two-label case and develop an approximate method based on transforming the original problem into a smaller domain. We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. To quantize edge weights, we partition the image into superpixels and assume that the weight of an edge between any two pixels depends only on the superpixels these pixels belong to. For efficient inference, we first consider the two-label case and develop an approximate method based on transforming the original problem into a smaller domain. Then we handle multi-label CRF by showing how to implement expansion moves. In both binary and multi-label cases, our solutions have significantly lower energy compared to that of mean field inference. In both binary and multi-label cases, our solutions have significantly lower energy compared to that of mean field inference. We also show the effectiveness of our approach on semantic segmentation task."}, {"paper_id": "5879819", "adju_relevance": 0, "title": "Stochastic Minimum Spanning Trees and Related Problems", "background_label": "AbstractWe investigate the computational complexity of minimum spanning trees and maximum flows in a simple model of stochastic networks, where each node or edge of an undirected master graph can fail with an independent and arbitrary probability. We show that computing the expected length of the MST or the value of the max-flow is #P -Hard, but that for the MST it can be approximated within O(log n) factor for metric graphs.", "method_label": "The hardness proof for the MST applies even to Euclidean graphs in 3 dimensions. We also show that the tail bounds for the MST cannot be approximated in general to any multiplicative factor unless P = N P . This stochastic MST problem was mentioned but left unanswered by Bertsimas, Jaillet and Odoni [Operations Research, 1990] in their work on a priori optimization. More generally, we also consider the complexity of linear programming under probabilistic constraints, and show it to be #P -Hard. If the linear program has a constant number of variables, then it can be solved exactly in polynomial time.", "result_label": "For general dimensions, we give a randomized algorithm for approximating the probability of LP feasibility.", "abstract": "AbstractWe investigate the computational complexity of minimum spanning trees and maximum flows in a simple model of stochastic networks, where each node or edge of an undirected master graph can fail with an independent and arbitrary probability. AbstractWe investigate the computational complexity of minimum spanning trees and maximum flows in a simple model of stochastic networks, where each node or edge of an undirected master graph can fail with an independent and arbitrary probability. We show that computing the expected length of the MST or the value of the max-flow is #P -Hard, but that for the MST it can be approximated within O(log n) factor for metric graphs. The hardness proof for the MST applies even to Euclidean graphs in 3 dimensions. The hardness proof for the MST applies even to Euclidean graphs in 3 dimensions. We also show that the tail bounds for the MST cannot be approximated in general to any multiplicative factor unless P = N P . The hardness proof for the MST applies even to Euclidean graphs in 3 dimensions. We also show that the tail bounds for the MST cannot be approximated in general to any multiplicative factor unless P = N P . This stochastic MST problem was mentioned but left unanswered by Bertsimas, Jaillet and Odoni [Operations Research, 1990] in their work on a priori optimization. The hardness proof for the MST applies even to Euclidean graphs in 3 dimensions. We also show that the tail bounds for the MST cannot be approximated in general to any multiplicative factor unless P = N P . This stochastic MST problem was mentioned but left unanswered by Bertsimas, Jaillet and Odoni [Operations Research, 1990] in their work on a priori optimization. More generally, we also consider the complexity of linear programming under probabilistic constraints, and show it to be #P -Hard. The hardness proof for the MST applies even to Euclidean graphs in 3 dimensions. We also show that the tail bounds for the MST cannot be approximated in general to any multiplicative factor unless P = N P . This stochastic MST problem was mentioned but left unanswered by Bertsimas, Jaillet and Odoni [Operations Research, 1990] in their work on a priori optimization. More generally, we also consider the complexity of linear programming under probabilistic constraints, and show it to be #P -Hard. If the linear program has a constant number of variables, then it can be solved exactly in polynomial time. For general dimensions, we give a randomized algorithm for approximating the probability of LP feasibility."}, {"paper_id": "97584", "adju_relevance": 0, "title": "Simple and optimal output-sensitive construction of contour trees using monotone paths", "background_label": "AbstractContour trees are used when high-dimensional data are preprocessed for efficient extraction of isocontours for the purpose of visualization. So far, efficient algorithms for contour trees are based on processing the data in sorted order.", "method_label": "We present a new algorithm that avoids sorting of the whole dataset, but sorts only a subset of socalled component-critical points. They form only a small fraction of the vertices in the dataset, for typical data that arise in practice. The algorithm is simple, achieves the optimal output-sensitive bound in running time, and works in any dimension.", "result_label": "Our experiments show that the algorithm compares favorably with the previous best algorithm.", "abstract": "AbstractContour trees are used when high-dimensional data are preprocessed for efficient extraction of isocontours for the purpose of visualization. AbstractContour trees are used when high-dimensional data are preprocessed for efficient extraction of isocontours for the purpose of visualization. So far, efficient algorithms for contour trees are based on processing the data in sorted order. We present a new algorithm that avoids sorting of the whole dataset, but sorts only a subset of socalled component-critical points. We present a new algorithm that avoids sorting of the whole dataset, but sorts only a subset of socalled component-critical points. They form only a small fraction of the vertices in the dataset, for typical data that arise in practice. We present a new algorithm that avoids sorting of the whole dataset, but sorts only a subset of socalled component-critical points. They form only a small fraction of the vertices in the dataset, for typical data that arise in practice. The algorithm is simple, achieves the optimal output-sensitive bound in running time, and works in any dimension. Our experiments show that the algorithm compares favorably with the previous best algorithm."}, {"paper_id": "820669", "adju_relevance": 0, "title": "Incremental Topological Ordering and Strong Component Maintenance", "background_label": "We present an on-line algorithm for maintaining a topological order of a directed acyclic graph as arcs are added, and detecting a cycle when one is created. Our algorithm takes O(m^{1/2}) amortized time per arc, where m is the total number of arcs.", "method_label": "For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. Instead, the deterministic version of our algorithm uses (approximate) median-finding. The randomized version of our algorithm avoids this complication, making it very simple. We extend our topological ordering algorithm to give the first detailed algorithm for maintaining the strong components of a directed graph, and a topological order of these components, as arcs are added.", "result_label": "This extension also has an amortized time bound of O(m^{1/2}) per arc.", "abstract": "We present an on-line algorithm for maintaining a topological order of a directed acyclic graph as arcs are added, and detecting a cycle when one is created. We present an on-line algorithm for maintaining a topological order of a directed acyclic graph as arcs are added, and detecting a cycle when one is created. Our algorithm takes O(m^{1/2}) amortized time per arc, where m is the total number of arcs. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. Instead, the deterministic version of our algorithm uses (approximate) median-finding. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. Instead, the deterministic version of our algorithm uses (approximate) median-finding. The randomized version of our algorithm avoids this complication, making it very simple. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. Instead, the deterministic version of our algorithm uses (approximate) median-finding. The randomized version of our algorithm avoids this complication, making it very simple. We extend our topological ordering algorithm to give the first detailed algorithm for maintaining the strong components of a directed graph, and a topological order of these components, as arcs are added. This extension also has an amortized time bound of O(m^{1/2}) per arc."}, {"paper_id": "164989", "adju_relevance": 0, "title": "A golden ratio parameterized algorithm for cluster editing", "background_label": "The Cluster Editing problem asks to transform a graph by at most k edge modifications into a disjoint union of cliques. The problem is NP-complete, but several parameterized algorithms are known.", "method_label": "We present a novel search tree algorithm for the problem, which improves running time from O * (1.76In detail, we can show that we can always branch with branching vector (2, 1) or better, resulting in the golden ratio as the base of the search tree size. Our algorithm uses a well-known transformation to the integer-weighted counterpart of the problem. To achieve our result, we combine three techniques: First, we show that zero-edges in the graph enforce structural features that allow us to branch more efficiently. Second, by repeatedly branching we can isolate vertices, releasing costs. Finally, we use a known characterization of graphs with few conflicts.", "abstract": " The Cluster Editing problem asks to transform a graph by at most k edge modifications into a disjoint union of cliques. The Cluster Editing problem asks to transform a graph by at most k edge modifications into a disjoint union of cliques. The problem is NP-complete, but several parameterized algorithms are known. We present a novel search tree algorithm for the problem, which improves running time from O * (1.76In detail, we can show that we can always branch with branching vector (2, 1) or better, resulting in the golden ratio as the base of the search tree size. We present a novel search tree algorithm for the problem, which improves running time from O * (1.76In detail, we can show that we can always branch with branching vector (2, 1) or better, resulting in the golden ratio as the base of the search tree size. Our algorithm uses a well-known transformation to the integer-weighted counterpart of the problem. We present a novel search tree algorithm for the problem, which improves running time from O * (1.76In detail, we can show that we can always branch with branching vector (2, 1) or better, resulting in the golden ratio as the base of the search tree size. Our algorithm uses a well-known transformation to the integer-weighted counterpart of the problem. To achieve our result, we combine three techniques: First, we show that zero-edges in the graph enforce structural features that allow us to branch more efficiently. We present a novel search tree algorithm for the problem, which improves running time from O * (1.76In detail, we can show that we can always branch with branching vector (2, 1) or better, resulting in the golden ratio as the base of the search tree size. Our algorithm uses a well-known transformation to the integer-weighted counterpart of the problem. To achieve our result, we combine three techniques: First, we show that zero-edges in the graph enforce structural features that allow us to branch more efficiently. Second, by repeatedly branching we can isolate vertices, releasing costs. We present a novel search tree algorithm for the problem, which improves running time from O * (1.76In detail, we can show that we can always branch with branching vector (2, 1) or better, resulting in the golden ratio as the base of the search tree size. Our algorithm uses a well-known transformation to the integer-weighted counterpart of the problem. To achieve our result, we combine three techniques: First, we show that zero-edges in the graph enforce structural features that allow us to branch more efficiently. Second, by repeatedly branching we can isolate vertices, releasing costs. Finally, we use a known characterization of graphs with few conflicts."}, {"paper_id": "5897173", "adju_relevance": 0, "title": "Bilexical Grammars And Their Cubic-Time Parsing Algorithms", "background_label": "This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic selectional influences on each other. Such \u2018bilexicalism\u2019 has been a theme of much current work in parsing.", "method_label": "The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and a slight modification yields link grammars. Its scoring approach is compatible with a wide variety of probability models.", "abstract": "This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic selectional influences on each other. This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic selectional influences on each other. Such \u2018bilexicalism\u2019 has been a theme of much current work in parsing. The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and a slight modification yields link grammars. The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and a slight modification yields link grammars. Its scoring approach is compatible with a wide variety of probability models."}, {"paper_id": "53282531", "adju_relevance": 0, "title": "PruneJuice: Pruning Trillion-edge Graphs to a Precise Pattern-Matching Solution", "background_label": "Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching.", "abstract": "Pattern matching is a powerful graph analysis tool. Pattern matching is a powerful graph analysis tool. Unfortunately, existing solutions have limited scalability, support only a limited set of search patterns, and/or focus on only a subset of the real-world problems associated with pattern matching."}, {"paper_id": "6171878", "adju_relevance": 0, "title": "Ricci-Ollivier Curvature of the Rooted Phylogenetic Subtree-Prune-Regraft Graph", "background_label": "Statistical phylogenetic inference methods use tree rearrangement operations to perform either hill-climbing local search or Markov chain Monte Carlo across tree topologies. The canonical class of such moves are the subtree-prune-regraft (SPR) moves that remove a subtree and reattach it somewhere else via the cut edge of the subtree. Phylogenetic trees and such moves naturally form the vertices and edges of a graph, such that tree search algorithms perform a (potentially stochastic) traversal of this SPR graph. Despite the centrality of such graphs in phylogenetic inference, rather little is known about their large-scale properties.", "method_label": "In this paper we learn about the rooted-tree version of the graph, known as the rSPR graph, by calculating the Ricci-Ollivier curvature for pairs of vertices in the rSPR graph with respect to two simple random walks on the rSPR graph. By proving theorems and direct calculation with novel algorithms, we find a remarkable diversity of different curvatures on the rSPR graph for pairs of vertices separated by the same distance.", "result_label": "We confirm using simulation that degree and curvature have the expected impact on mean access time distributions, demonstrating relevance of these curvature results to stochastic tree search. This indicates significant structure of the rSPR graph beyond that which was previously understood in terms of pairwise distances and vertex degrees; a greater understanding of curvature could ultimately lead to improved strategies for tree search.", "abstract": "Statistical phylogenetic inference methods use tree rearrangement operations to perform either hill-climbing local search or Markov chain Monte Carlo across tree topologies. Statistical phylogenetic inference methods use tree rearrangement operations to perform either hill-climbing local search or Markov chain Monte Carlo across tree topologies. The canonical class of such moves are the subtree-prune-regraft (SPR) moves that remove a subtree and reattach it somewhere else via the cut edge of the subtree. Statistical phylogenetic inference methods use tree rearrangement operations to perform either hill-climbing local search or Markov chain Monte Carlo across tree topologies. The canonical class of such moves are the subtree-prune-regraft (SPR) moves that remove a subtree and reattach it somewhere else via the cut edge of the subtree. Phylogenetic trees and such moves naturally form the vertices and edges of a graph, such that tree search algorithms perform a (potentially stochastic) traversal of this SPR graph. Statistical phylogenetic inference methods use tree rearrangement operations to perform either hill-climbing local search or Markov chain Monte Carlo across tree topologies. The canonical class of such moves are the subtree-prune-regraft (SPR) moves that remove a subtree and reattach it somewhere else via the cut edge of the subtree. Phylogenetic trees and such moves naturally form the vertices and edges of a graph, such that tree search algorithms perform a (potentially stochastic) traversal of this SPR graph. Despite the centrality of such graphs in phylogenetic inference, rather little is known about their large-scale properties. In this paper we learn about the rooted-tree version of the graph, known as the rSPR graph, by calculating the Ricci-Ollivier curvature for pairs of vertices in the rSPR graph with respect to two simple random walks on the rSPR graph. In this paper we learn about the rooted-tree version of the graph, known as the rSPR graph, by calculating the Ricci-Ollivier curvature for pairs of vertices in the rSPR graph with respect to two simple random walks on the rSPR graph. By proving theorems and direct calculation with novel algorithms, we find a remarkable diversity of different curvatures on the rSPR graph for pairs of vertices separated by the same distance. We confirm using simulation that degree and curvature have the expected impact on mean access time distributions, demonstrating relevance of these curvature results to stochastic tree search. We confirm using simulation that degree and curvature have the expected impact on mean access time distributions, demonstrating relevance of these curvature results to stochastic tree search. This indicates significant structure of the rSPR graph beyond that which was previously understood in terms of pairwise distances and vertex degrees; a greater understanding of curvature could ultimately lead to improved strategies for tree search."}, {"paper_id": "2775144", "adju_relevance": 0, "title": "Simple parallel biconnectivity algorithms for multicore platforms", "background_label": "We present two new algorithms for finding the biconnected components of a large undirected sparse graph.", "method_label": "The first algorithm is based on identifying articulation points and labeling edges using multiple connectivity queries, and the second approach uses the color propagation technique to decompose the graph. Both methods use a breadth-first spanning tree and some auxiliary information computed during Breadth-First Search (BFS). These methods are simpler than the Tarjan-Vishkin PRAM algorithm for biconnectivity and do not require Euler tour computation or any auxiliary graph construction. We identify steps in these algorithms that can be parallelized in a shared-memory environment and develop tuned OpenMP implementations. Using a collection of large-scale real-world graph instances, we show that these methods outperform the state-of-the-art Cong-Bader biconnected components implementation, which is based on the Tarjan-Vishkin algorithm.", "result_label": "We achieve up to 7.1\u00d7 and 4.2\u00d7 parallel speedup over the serial Hopcroft-Tarjan and parallel Cong-Bader algorithms, respectively, on a 16-core Intel Sandy Bridge system. For some graph instances, due to the fast BFS-based preprocessing step, the single-threaded implementation of our first algorithm is faster than the serial Hopcroft-Tarjan algorithm.", "abstract": "We present two new algorithms for finding the biconnected components of a large undirected sparse graph. The first algorithm is based on identifying articulation points and labeling edges using multiple connectivity queries, and the second approach uses the color propagation technique to decompose the graph. The first algorithm is based on identifying articulation points and labeling edges using multiple connectivity queries, and the second approach uses the color propagation technique to decompose the graph. Both methods use a breadth-first spanning tree and some auxiliary information computed during Breadth-First Search (BFS). The first algorithm is based on identifying articulation points and labeling edges using multiple connectivity queries, and the second approach uses the color propagation technique to decompose the graph. Both methods use a breadth-first spanning tree and some auxiliary information computed during Breadth-First Search (BFS). These methods are simpler than the Tarjan-Vishkin PRAM algorithm for biconnectivity and do not require Euler tour computation or any auxiliary graph construction. The first algorithm is based on identifying articulation points and labeling edges using multiple connectivity queries, and the second approach uses the color propagation technique to decompose the graph. Both methods use a breadth-first spanning tree and some auxiliary information computed during Breadth-First Search (BFS). These methods are simpler than the Tarjan-Vishkin PRAM algorithm for biconnectivity and do not require Euler tour computation or any auxiliary graph construction. We identify steps in these algorithms that can be parallelized in a shared-memory environment and develop tuned OpenMP implementations. The first algorithm is based on identifying articulation points and labeling edges using multiple connectivity queries, and the second approach uses the color propagation technique to decompose the graph. Both methods use a breadth-first spanning tree and some auxiliary information computed during Breadth-First Search (BFS). These methods are simpler than the Tarjan-Vishkin PRAM algorithm for biconnectivity and do not require Euler tour computation or any auxiliary graph construction. We identify steps in these algorithms that can be parallelized in a shared-memory environment and develop tuned OpenMP implementations. Using a collection of large-scale real-world graph instances, we show that these methods outperform the state-of-the-art Cong-Bader biconnected components implementation, which is based on the Tarjan-Vishkin algorithm. We achieve up to 7.1\u00d7 and 4.2\u00d7 parallel speedup over the serial Hopcroft-Tarjan and parallel Cong-Bader algorithms, respectively, on a 16-core Intel Sandy Bridge system. We achieve up to 7.1\u00d7 and 4.2\u00d7 parallel speedup over the serial Hopcroft-Tarjan and parallel Cong-Bader algorithms, respectively, on a 16-core Intel Sandy Bridge system. For some graph instances, due to the fast BFS-based preprocessing step, the single-threaded implementation of our first algorithm is faster than the serial Hopcroft-Tarjan algorithm."}, {"paper_id": "14874053", "adju_relevance": 0, "title": "The generalized minimum spanning tree problem: a parameterized complexity analysis of bi-level optimisation", "background_label": "Bi-level optimisation problems have gained increasing interest in the field of combinatorial optimisation in recent years.", "abstract": "Bi-level optimisation problems have gained increasing interest in the field of combinatorial optimisation in recent years."}, {"paper_id": "9889030", "adju_relevance": 0, "title": "Fully Sequential and Distributed Dynamic Algorithms for Minimum Spanning Trees", "background_label": "In this paper, we present a fully-dynamic distributed algorithm for maintaining a minimum spanning tree on general graphs with positive real edge weights.", "abstract": "In this paper, we present a fully-dynamic distributed algorithm for maintaining a minimum spanning tree on general graphs with positive real edge weights."}, {"paper_id": "184487753", "adju_relevance": 0, "title": "Effective Representation for Easy-First Dependency Parsing", "background_label": "Easy-first parsing relies on subtree re-ranking to build the complete parse tree. Whereas the intermediate state of parsing processing is represented by various subtrees, whose internal structural information is the key lead for later parsing action decisions, we explore a better representation for such subtrees.", "method_label": "In detail, this work introduces a bottom-up subtree encoding method based on the child-sum tree-LSTM. Starting from an easy-first dependency parser without other handcraft features, we show that the effective subtree encoder does promote the parsing process, and can make a greedy search easy-first parser achieve promising results on benchmark treebanks compared to state-of-the-art baselines.", "result_label": "Furthermore, with the help of the current pre-training language model, we further improve the state-of-the-art results of the easy-first approach.", "abstract": "Easy-first parsing relies on subtree re-ranking to build the complete parse tree. Easy-first parsing relies on subtree re-ranking to build the complete parse tree. Whereas the intermediate state of parsing processing is represented by various subtrees, whose internal structural information is the key lead for later parsing action decisions, we explore a better representation for such subtrees. In detail, this work introduces a bottom-up subtree encoding method based on the child-sum tree-LSTM. In detail, this work introduces a bottom-up subtree encoding method based on the child-sum tree-LSTM. Starting from an easy-first dependency parser without other handcraft features, we show that the effective subtree encoder does promote the parsing process, and can make a greedy search easy-first parser achieve promising results on benchmark treebanks compared to state-of-the-art baselines. Furthermore, with the help of the current pre-training language model, we further improve the state-of-the-art results of the easy-first approach."}, {"paper_id": "3085391", "adju_relevance": 0, "title": "The node-depth encoding: analysis and application to the bounded-diameter minimum spanning tree problem", "background_label": "The node-depth encoding has elements from direct and indirect encoding for trees which encodes trees by storing the depth of nodes in a list. Node-depth encoding applies specific search operators that is a typical characteristic for direct encodings. An investigation into the bias of the initialization process and the mutation operators of the node-depth encoding shows that the initialization process has a bias to solutions with small depths and diameters, and a bias towards stars.", "method_label": "This investigation, also, shows that the mutation operators are unbiased. The performance of node-depth encoding is investigated for the bounded-diameter minimum spanning tree problem.", "result_label": "The results are presented for Euclidean instances presented in the literature. In contrast with the expectation, the evolutionary algorithm using the biased initialization operator does not allow evolutionary algorithms to find better solutions compared to an unbiased initialization. In comparison to other evolutionary algorithms for the bounded-diameter minimum spanning tree evolutionary algorithms using the node-depth encoding have a good performance.", "abstract": "The node-depth encoding has elements from direct and indirect encoding for trees which encodes trees by storing the depth of nodes in a list. The node-depth encoding has elements from direct and indirect encoding for trees which encodes trees by storing the depth of nodes in a list. Node-depth encoding applies specific search operators that is a typical characteristic for direct encodings. The node-depth encoding has elements from direct and indirect encoding for trees which encodes trees by storing the depth of nodes in a list. Node-depth encoding applies specific search operators that is a typical characteristic for direct encodings. An investigation into the bias of the initialization process and the mutation operators of the node-depth encoding shows that the initialization process has a bias to solutions with small depths and diameters, and a bias towards stars. This investigation, also, shows that the mutation operators are unbiased. This investigation, also, shows that the mutation operators are unbiased. The performance of node-depth encoding is investigated for the bounded-diameter minimum spanning tree problem. The results are presented for Euclidean instances presented in the literature. The results are presented for Euclidean instances presented in the literature. In contrast with the expectation, the evolutionary algorithm using the biased initialization operator does not allow evolutionary algorithms to find better solutions compared to an unbiased initialization. The results are presented for Euclidean instances presented in the literature. In contrast with the expectation, the evolutionary algorithm using the biased initialization operator does not allow evolutionary algorithms to find better solutions compared to an unbiased initialization. In comparison to other evolutionary algorithms for the bounded-diameter minimum spanning tree evolutionary algorithms using the node-depth encoding have a good performance."}, {"paper_id": "7032958", "adju_relevance": 0, "title": "Engineering an External Memory Minimum Spanning Tree Algorithm", "background_label": "AbstractWe develop an external memory algorithm for computing minimum spanning trees. The algorithm is considerably simpler than previously known external memory algorithms for this problem and needs a factor of at least four less I/Os for realistic inputs.", "result_label": "Our implementation indicates that this algorithm processes graphs only limited by the disk capacity of most current machines in time no more than a factor 2-5 of a good internal algorithm with sufficient memory space.", "abstract": "AbstractWe develop an external memory algorithm for computing minimum spanning trees. AbstractWe develop an external memory algorithm for computing minimum spanning trees. The algorithm is considerably simpler than previously known external memory algorithms for this problem and needs a factor of at least four less I/Os for realistic inputs. Our implementation indicates that this algorithm processes graphs only limited by the disk capacity of most current machines in time no more than a factor 2-5 of a good internal algorithm with sufficient memory space."}, {"paper_id": "16543146", "adju_relevance": 0, "title": "The I/O Complexity of Sorting and Related Problems (Extended Abstract)", "background_label": "We show how to compute single-source shortest paths in undirected graphs with non-negative edge lengths in 0( \u221anm/B log n + MST (n, m)) I/Os, where n is the number of vertices, m is the number of edges, B is the disk block size, and MST (n,m) is the I/O-cost of computing a minimum spanning tree.", "method_label": "For sparse graphs, the new algorithm performs O((n/\u221aB) log n) I/Os.", "result_label": "This result removes our previous algorithm's dependence on the edge lengths in the graph.", "abstract": "We show how to compute single-source shortest paths in undirected graphs with non-negative edge lengths in 0( \u221anm/B log n + MST (n, m)) I/Os, where n is the number of vertices, m is the number of edges, B is the disk block size, and MST (n,m) is the I/O-cost of computing a minimum spanning tree. For sparse graphs, the new algorithm performs O((n/\u221aB) log n) I/Os. This result removes our previous algorithm's dependence on the edge lengths in the graph."}, {"paper_id": "49556516", "adju_relevance": 0, "title": "Fusing First-order Knowledge Compilation and the Lifted Junction Tree Algorithm", "background_label": "Standard approaches for inference in probabilistic formalisms with first-order constructs include lifted variable elimination (LVE) for single queries as well as first-order knowledge compilation (FOKC) based on weighted model counting.", "method_label": "To handle multiple queries efficiently, the lifted junction tree algorithm (LJT) uses a first-order cluster representation of a model and LVE as a subroutine in its computations. For certain inputs, the implementations of LVE and, as a result, LJT ground parts of a model where FOKC has a lifted run.", "abstract": "Standard approaches for inference in probabilistic formalisms with first-order constructs include lifted variable elimination (LVE) for single queries as well as first-order knowledge compilation (FOKC) based on weighted model counting. To handle multiple queries efficiently, the lifted junction tree algorithm (LJT) uses a first-order cluster representation of a model and LVE as a subroutine in its computations. To handle multiple queries efficiently, the lifted junction tree algorithm (LJT) uses a first-order cluster representation of a model and LVE as a subroutine in its computations. For certain inputs, the implementations of LVE and, as a result, LJT ground parts of a model where FOKC has a lifted run."}, {"paper_id": "2319797", "adju_relevance": 0, "title": "Untangling Tanglegrams: Comparing Trees by Their Drawings", "background_label": "A tanglegram is a pair of trees on the same set of leaves with matching leaves in the two trees joined by an edge. Tanglegrams are widely used in biology\u2014to compare evolutionary histories of host and parasite species and to analyze genes of species in the same geographical area.", "method_label": "We consider optimization problems in tanglegram drawings. We show a linear time algorithm to decide if a tanglegram admits a planar embedding by a reduction to the planar graph drawing problem.", "result_label": "This problem was also studied by Fernau et al.", "abstract": "A tanglegram is a pair of trees on the same set of leaves with matching leaves in the two trees joined by an edge. A tanglegram is a pair of trees on the same set of leaves with matching leaves in the two trees joined by an edge. Tanglegrams are widely used in biology\u2014to compare evolutionary histories of host and parasite species and to analyze genes of species in the same geographical area. We consider optimization problems in tanglegram drawings. We consider optimization problems in tanglegram drawings. We show a linear time algorithm to decide if a tanglegram admits a planar embedding by a reduction to the planar graph drawing problem. This problem was also studied by Fernau et al."}, {"paper_id": "11717298", "adju_relevance": 0, "title": "Optimal hierarchical decompositions for congestion minimization in networks", "background_label": "Hierarchical graph decompositions play an important role in the design of approximation and online algorithms for graph problems. This is mainly due to the fact that the results concerning the approximation of metric spaces by tree metrics (e.g. [10,11,14,16]) depend on hierarchical graph decompositions. We call such decompositions cut-based decompositions. It has been shown that they also can be used to design approximation and online algorithms for a wide variety of different problems, but at the current state of the art the performance guarantee goes down by an O(log2n log log n)-factor when making the transition from tree networks to general graphs.", "method_label": "In this line of work a probability distribution over tree graphs is constructed from a given input graph, in such a way that the tree distances closely resemble the distances in the original graph. This allows it, to solve many problems with a distance-based cost function on trees, and then transfer the tree solution to general undirected graphs with only a logarithmic loss in the performance guarantee.", "result_label": "The results about oblivious routing [30,22] in general undirected graphs are based on hierarchical decompositions of a different type in the sense that they are aiming to approximate the bottlenecks in the network (instead of the point-to-point distances).", "abstract": "Hierarchical graph decompositions play an important role in the design of approximation and online algorithms for graph problems. Hierarchical graph decompositions play an important role in the design of approximation and online algorithms for graph problems. This is mainly due to the fact that the results concerning the approximation of metric spaces by tree metrics (e.g. Hierarchical graph decompositions play an important role in the design of approximation and online algorithms for graph problems. This is mainly due to the fact that the results concerning the approximation of metric spaces by tree metrics (e.g. [10,11,14,16]) depend on hierarchical graph decompositions. In this line of work a probability distribution over tree graphs is constructed from a given input graph, in such a way that the tree distances closely resemble the distances in the original graph. In this line of work a probability distribution over tree graphs is constructed from a given input graph, in such a way that the tree distances closely resemble the distances in the original graph. This allows it, to solve many problems with a distance-based cost function on trees, and then transfer the tree solution to general undirected graphs with only a logarithmic loss in the performance guarantee. The results about oblivious routing [30,22] in general undirected graphs are based on hierarchical decompositions of a different type in the sense that they are aiming to approximate the bottlenecks in the network (instead of the point-to-point distances). Hierarchical graph decompositions play an important role in the design of approximation and online algorithms for graph problems. This is mainly due to the fact that the results concerning the approximation of metric spaces by tree metrics (e.g. [10,11,14,16]) depend on hierarchical graph decompositions. We call such decompositions cut-based decompositions. Hierarchical graph decompositions play an important role in the design of approximation and online algorithms for graph problems. This is mainly due to the fact that the results concerning the approximation of metric spaces by tree metrics (e.g. [10,11,14,16]) depend on hierarchical graph decompositions. We call such decompositions cut-based decompositions. It has been shown that they also can be used to design approximation and online algorithms for a wide variety of different problems, but at the current state of the art the performance guarantee goes down by an O(log2n log log n)-factor when making the transition from tree networks to general graphs."}, {"paper_id": "5944121", "adju_relevance": 0, "title": "Minimum Spanning Tree Based Classification Model for Massive Data with MapReduce Implementation", "background_label": "Rapid growth of data has provided us with more information, yet challenges the tradition techniques to extract the useful knowledge.", "abstract": "Rapid growth of data has provided us with more information, yet challenges the tradition techniques to extract the useful knowledge."}, {"paper_id": "11719997", "adju_relevance": 0, "title": "3D Randomized Connection Network with Graph-based Label Inference", "method_label": "The convolutional LSTM and 3D convolution are employed as network units to capture the long-term and short-term 3D properties respectively. To assemble these two kinds of spatial-temporal information and refine the deep learning outcomes, we further introduce an efficient graph-based node selection and label inference method.", "result_label": "Experiments have been carried out on two publicly available databases and results demonstrate that the proposed method can obtain competitive performances as compared with other state-of-the-art methods.", "abstract": " The convolutional LSTM and 3D convolution are employed as network units to capture the long-term and short-term 3D properties respectively. The convolutional LSTM and 3D convolution are employed as network units to capture the long-term and short-term 3D properties respectively. To assemble these two kinds of spatial-temporal information and refine the deep learning outcomes, we further introduce an efficient graph-based node selection and label inference method. Experiments have been carried out on two publicly available databases and results demonstrate that the proposed method can obtain competitive performances as compared with other state-of-the-art methods."}, {"paper_id": "53878655", "adju_relevance": 0, "title": "Decremental Single-Source Shortest Paths on Undirected Graphs in Near-Linear Total Update Time", "background_label": "In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the <i>undirected unweighted</i> case. In this case, the classic <i>O</i>(<i>mn</i>) total update time of Even and Shiloach [16] has been the fastest known algorithm for three decades. At the cost of a (1+\u03b5)-approximation factor, the running time was recently improved to <i>n</i><sup>2+<i>o</i>(1)</sup> by Bernstein and Roditty [9]. The expected running time bound of our algorithm holds against an oblivious adversary. In contrast to the previous results, which rely on maintaining a sparse emulator, our algorithm relies on maintaining a so-called <i>sparse</i> (<i>h</i>, <i>\u03b5</i>)-<i>hop set</i> introduced by Cohen [12] in the PRAM literature.", "method_label": "In this article, we bring the running time down to near-linear: We give a (1+\u03b5)-approximation algorithm with <i>m</i><sup>1+<i>o</i>(1)</sup> expected total update time, thus obtaining <i>near-linear time</i>. Moreover, we obtain <i>m</i><sup>1+<i>o</i>(1)</sup> log <i>W</i> time for the weighted case, where the edge weights are integers from 1 to <i>W</i>. An (<i>h</i>, \u03b5)-hop set of a graph <i>G</i>=(<i>V</i>, <i>E</i>) is a set <i>F</i> of weighted edges such that the distance between any pair of nodes in <i>G</i> can be (1+\u03b5)-approximated by their <i>h</i>-hop distance (given by a path containing at most <i>h</i> edges) on <i>G</i><sup>\u2032</sup>=(<i>V</i>, <i>E</i> \u222a <i>F</i>).", "result_label": "The only prior work on weighted graphs in <i>o</i>(<i>mn</i>) time is the <i>mn</i><sup>0.9 + <i>o</i>(1)</sup>-time algorithm by Henzinger et al. [18, 19], which works for directed graphs with quasi-polynomial edge weights. Our algorithm can maintain an (<i>n</i><sup><i>o</i>(1)</sup>, \u03b5)-hop set of near-linear size in near-linear time under edge deletions. It is the first of its kind to the best of our knowledge.", "abstract": "In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the <i>undirected unweighted</i> case. In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the <i>undirected unweighted</i> case. In this case, the classic <i>O</i>(<i>mn</i>) total update time of Even and Shiloach [16] has been the fastest known algorithm for three decades. In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the <i>undirected unweighted</i> case. In this case, the classic <i>O</i>(<i>mn</i>) total update time of Even and Shiloach [16] has been the fastest known algorithm for three decades. At the cost of a (1+\u03b5)-approximation factor, the running time was recently improved to <i>n</i><sup>2+<i>o</i>(1)</sup> by Bernstein and Roditty [9]. In this article, we bring the running time down to near-linear: We give a (1+\u03b5)-approximation algorithm with <i>m</i><sup>1+<i>o</i>(1)</sup> expected total update time, thus obtaining <i>near-linear time</i>. In this article, we bring the running time down to near-linear: We give a (1+\u03b5)-approximation algorithm with <i>m</i><sup>1+<i>o</i>(1)</sup> expected total update time, thus obtaining <i>near-linear time</i>. Moreover, we obtain <i>m</i><sup>1+<i>o</i>(1)</sup> log <i>W</i> time for the weighted case, where the edge weights are integers from 1 to <i>W</i>. The only prior work on weighted graphs in <i>o</i>(<i>mn</i>) time is the <i>mn</i><sup>0.9 + <i>o</i>(1)</sup>-time algorithm by Henzinger et al. The only prior work on weighted graphs in <i>o</i>(<i>mn</i>) time is the <i>mn</i><sup>0.9 + <i>o</i>(1)</sup>-time algorithm by Henzinger et al. [18, 19], which works for directed graphs with quasi-polynomial edge weights. In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the <i>undirected unweighted</i> case. In this case, the classic <i>O</i>(<i>mn</i>) total update time of Even and Shiloach [16] has been the fastest known algorithm for three decades. At the cost of a (1+\u03b5)-approximation factor, the running time was recently improved to <i>n</i><sup>2+<i>o</i>(1)</sup> by Bernstein and Roditty [9]. The expected running time bound of our algorithm holds against an oblivious adversary. In the decremental single-source shortest paths (SSSP) problem, we want to maintain the distances between a given source node <i>s</i> and every other node in an <i>n</i>-node <i>m</i>-edge graph <i>G</i> undergoing edge deletions. While its static counterpart can be solved in near-linear time, this decremental problem is much more challenging even in the <i>undirected unweighted</i> case. In this case, the classic <i>O</i>(<i>mn</i>) total update time of Even and Shiloach [16] has been the fastest known algorithm for three decades. At the cost of a (1+\u03b5)-approximation factor, the running time was recently improved to <i>n</i><sup>2+<i>o</i>(1)</sup> by Bernstein and Roditty [9]. The expected running time bound of our algorithm holds against an oblivious adversary. In contrast to the previous results, which rely on maintaining a sparse emulator, our algorithm relies on maintaining a so-called <i>sparse</i> (<i>h</i>, <i>\u03b5</i>)-<i>hop set</i> introduced by Cohen [12] in the PRAM literature. In this article, we bring the running time down to near-linear: We give a (1+\u03b5)-approximation algorithm with <i>m</i><sup>1+<i>o</i>(1)</sup> expected total update time, thus obtaining <i>near-linear time</i>. Moreover, we obtain <i>m</i><sup>1+<i>o</i>(1)</sup> log <i>W</i> time for the weighted case, where the edge weights are integers from 1 to <i>W</i>. An (<i>h</i>, \u03b5)-hop set of a graph <i>G</i>=(<i>V</i>, <i>E</i>) is a set <i>F</i> of weighted edges such that the distance between any pair of nodes in <i>G</i> can be (1+\u03b5)-approximated by their <i>h</i>-hop distance (given by a path containing at most <i>h</i> edges) on <i>G</i><sup>\u2032</sup>=(<i>V</i>, <i>E</i> \u222a <i>F</i>). The only prior work on weighted graphs in <i>o</i>(<i>mn</i>) time is the <i>mn</i><sup>0.9 + <i>o</i>(1)</sup>-time algorithm by Henzinger et al. [18, 19], which works for directed graphs with quasi-polynomial edge weights. Our algorithm can maintain an (<i>n</i><sup><i>o</i>(1)</sup>, \u03b5)-hop set of near-linear size in near-linear time under edge deletions. The only prior work on weighted graphs in <i>o</i>(<i>mn</i>) time is the <i>mn</i><sup>0.9 + <i>o</i>(1)</sup>-time algorithm by Henzinger et al. [18, 19], which works for directed graphs with quasi-polynomial edge weights. Our algorithm can maintain an (<i>n</i><sup><i>o</i>(1)</sup>, \u03b5)-hop set of near-linear size in near-linear time under edge deletions. It is the first of its kind to the best of our knowledge."}, {"paper_id": "6025739", "adju_relevance": 0, "title": "Landmark Guided Probabilistic Roadmap Queries", "background_label": "A landmark based heuristic is investigated for reducing query phase run-time of the probabilistic roadmap (\\PRM) motion planning method.", "method_label": "The heuristic is generated by storing minimum spanning trees from a small number of vertices within the \\PRM graph and using these trees to approximate the cost of a shortest path between any two vertices of the graph. The intermediate step of preprocessing the graph increases the time and memory requirements of the classical motion planning technique in exchange for speeding up individual queries making the method advantageous in multi-query applications.", "result_label": "This paper investigates these trade-offs on \\PRM graphs constructed in randomized environments as well as a practical manipulator simulation.We conclude that the method is preferable to Dijkstra's algorithm or the ${\\rm A}^*$ algorithm with conventional heuristics in multi-query applications.", "abstract": "A landmark based heuristic is investigated for reducing query phase run-time of the probabilistic roadmap (\\PRM) motion planning method. The heuristic is generated by storing minimum spanning trees from a small number of vertices within the \\PRM graph and using these trees to approximate the cost of a shortest path between any two vertices of the graph. The heuristic is generated by storing minimum spanning trees from a small number of vertices within the \\PRM graph and using these trees to approximate the cost of a shortest path between any two vertices of the graph. The intermediate step of preprocessing the graph increases the time and memory requirements of the classical motion planning technique in exchange for speeding up individual queries making the method advantageous in multi-query applications. This paper investigates these trade-offs on \\PRM graphs constructed in randomized environments as well as a practical manipulator simulation.We conclude that the method is preferable to Dijkstra's algorithm or the ${\\rm A}^*$ algorithm with conventional heuristics in multi-query applications."}, {"paper_id": "505758", "adju_relevance": 0, "title": "Random Spanning Trees and the Prediction of Weighted Graphs", "background_label": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph.", "method_label": "The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space.", "result_label": "Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.", "abstract": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice."}, {"paper_id": "5756730", "adju_relevance": 0, "title": "Computing a Clique Tree with the Algorithm Maximal Label Search", "background_label": "Abstract:The algorithm MLS (Maximal Label Search) is a graph search algorithm that generalizes the algorithms Maximum Cardinality Search (MCS), Lexicographic Breadth-First Search (LexBFS), Lexicographic Depth-First Search (LexDFS) and Maximal Neighborhood Search (MNS). On a chordal graph, MLS computes a PEO (perfect elimination ordering) of the graph.", "method_label": "We show how the algorithm MLS can be modified to compute a PMO (perfect moplex ordering), as well as a clique tree and the minimal separators of a chordal graph. We give a necessary and sufficient condition on the labeling structure of MLS for the beginning of a new clique in the clique tree to be detected by a condition on labels. MLS is also used to compute a clique tree of the complement graph, and new cliques in the complement graph can be detected by a condition on labels for any labeling structure. We provide a linear time algorithm computing a PMO and the corresponding generators of the maximal cliques and minimal separators of the complement graph.", "result_label": "On a non-chordal graph, the algorithm MLSM, a graph search algorithm computing an MEO and a minimal triangulation of the graph, is used to compute an atom tree of the clique minimal separator decomposition of any graph.", "abstract": "Abstract:The algorithm MLS (Maximal Label Search) is a graph search algorithm that generalizes the algorithms Maximum Cardinality Search (MCS), Lexicographic Breadth-First Search (LexBFS), Lexicographic Depth-First Search (LexDFS) and Maximal Neighborhood Search (MNS). Abstract:The algorithm MLS (Maximal Label Search) is a graph search algorithm that generalizes the algorithms Maximum Cardinality Search (MCS), Lexicographic Breadth-First Search (LexBFS), Lexicographic Depth-First Search (LexDFS) and Maximal Neighborhood Search (MNS). On a chordal graph, MLS computes a PEO (perfect elimination ordering) of the graph. We show how the algorithm MLS can be modified to compute a PMO (perfect moplex ordering), as well as a clique tree and the minimal separators of a chordal graph. We show how the algorithm MLS can be modified to compute a PMO (perfect moplex ordering), as well as a clique tree and the minimal separators of a chordal graph. We give a necessary and sufficient condition on the labeling structure of MLS for the beginning of a new clique in the clique tree to be detected by a condition on labels. We show how the algorithm MLS can be modified to compute a PMO (perfect moplex ordering), as well as a clique tree and the minimal separators of a chordal graph. We give a necessary and sufficient condition on the labeling structure of MLS for the beginning of a new clique in the clique tree to be detected by a condition on labels. MLS is also used to compute a clique tree of the complement graph, and new cliques in the complement graph can be detected by a condition on labels for any labeling structure. We show how the algorithm MLS can be modified to compute a PMO (perfect moplex ordering), as well as a clique tree and the minimal separators of a chordal graph. We give a necessary and sufficient condition on the labeling structure of MLS for the beginning of a new clique in the clique tree to be detected by a condition on labels. MLS is also used to compute a clique tree of the complement graph, and new cliques in the complement graph can be detected by a condition on labels for any labeling structure. We provide a linear time algorithm computing a PMO and the corresponding generators of the maximal cliques and minimal separators of the complement graph. On a non-chordal graph, the algorithm MLSM, a graph search algorithm computing an MEO and a minimal triangulation of the graph, is used to compute an atom tree of the clique minimal separator decomposition of any graph."}, {"paper_id": "850921", "adju_relevance": 0, "title": "Efficient Tree-based Approximation for Entailment Graph Learning", "background_label": "AbstractLearning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years.", "abstract": "AbstractLearning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years."}, {"paper_id": "21826124", "adju_relevance": 0, "title": "Fast Algorithms for Constructing Minimal Spanning Trees in Coordinate Spaces", "background_label": "Algorithms are presented that construct the shortest connecting network, or minimal spanning tree (MST), of N points embedded in k-dimensional coordinate space.", "method_label": "These algorithms take advantage of the geometry of such spaces to substantially reduce the computation from that required to construct MST's of more general graphs. An algorithm is also presented that constructs a spanning tree that is very nearly minimal with computation proportional to N log N for all k.", "abstract": "Algorithms are presented that construct the shortest connecting network, or minimal spanning tree (MST), of N points embedded in k-dimensional coordinate space. These algorithms take advantage of the geometry of such spaces to substantially reduce the computation from that required to construct MST's of more general graphs. These algorithms take advantage of the geometry of such spaces to substantially reduce the computation from that required to construct MST's of more general graphs. An algorithm is also presented that constructs a spanning tree that is very nearly minimal with computation proportional to N log N for all k."}, {"paper_id": "6420473", "adju_relevance": 0, "title": "Fixed-Parameter Algorithms for DAG Partitioning", "background_label": "Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an arc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs with total weight at most $k$ such that each resulting weakly-connected component contains exactly one sink---a vertex without outgoing arcs. DAG Partitioning is NP-hard. We use our obtained optimal solutions to evaluate the solution quality of Leskovec et al. 's heuristic. We show that Leskovec et al.", "method_label": "We show an algorithm to solve DAG Partitioning in $O(2^k \\cdot (n+m))$ time, that is, in linear time for fixed $k$. We complement it with linear-time executable data reduction rules. 's heuristic works optimally on trees and generalize this result by showing that DAG Partitioning is solvable in $2^{O(w^2)}\\cdot n$ time if a width-$w$ tree decomposition of the input graph is given. Thus, we improve an algorithm and answer an open question of Alamdari and Mehrabian [WAW 2012].", "result_label": "Our experiments show that, in combination, they can optimally solve DAG Partitioning on simulated citation networks within five minutes for $k\\leq190$ and $m$ being $10^7$ and larger. We complement our algorithms by lower bounds on the running time of exact algorithms and on the effectivity of data reduction.", "abstract": "Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an arc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs with total weight at most $k$ such that each resulting weakly-connected component contains exactly one sink---a vertex without outgoing arcs. Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an arc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs with total weight at most $k$ such that each resulting weakly-connected component contains exactly one sink---a vertex without outgoing arcs. DAG Partitioning is NP-hard. We show an algorithm to solve DAG Partitioning in $O(2^k \\cdot (n+m))$ time, that is, in linear time for fixed $k$. We show an algorithm to solve DAG Partitioning in $O(2^k \\cdot (n+m))$ time, that is, in linear time for fixed $k$. We complement it with linear-time executable data reduction rules. Our experiments show that, in combination, they can optimally solve DAG Partitioning on simulated citation networks within five minutes for $k\\leq190$ and $m$ being $10^7$ and larger. Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an arc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs with total weight at most $k$ such that each resulting weakly-connected component contains exactly one sink---a vertex without outgoing arcs. DAG Partitioning is NP-hard. We use our obtained optimal solutions to evaluate the solution quality of Leskovec et al. Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an arc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs with total weight at most $k$ such that each resulting weakly-connected component contains exactly one sink---a vertex without outgoing arcs. DAG Partitioning is NP-hard. We use our obtained optimal solutions to evaluate the solution quality of Leskovec et al. 's heuristic. Finding the origin of short phrases propagating through the web has been formalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an arc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs with total weight at most $k$ such that each resulting weakly-connected component contains exactly one sink---a vertex without outgoing arcs. DAG Partitioning is NP-hard. We use our obtained optimal solutions to evaluate the solution quality of Leskovec et al. 's heuristic. We show that Leskovec et al. We show an algorithm to solve DAG Partitioning in $O(2^k \\cdot (n+m))$ time, that is, in linear time for fixed $k$. We complement it with linear-time executable data reduction rules. 's heuristic works optimally on trees and generalize this result by showing that DAG Partitioning is solvable in $2^{O(w^2)}\\cdot n$ time if a width-$w$ tree decomposition of the input graph is given. We show an algorithm to solve DAG Partitioning in $O(2^k \\cdot (n+m))$ time, that is, in linear time for fixed $k$. We complement it with linear-time executable data reduction rules. 's heuristic works optimally on trees and generalize this result by showing that DAG Partitioning is solvable in $2^{O(w^2)}\\cdot n$ time if a width-$w$ tree decomposition of the input graph is given. Thus, we improve an algorithm and answer an open question of Alamdari and Mehrabian [WAW 2012]. Our experiments show that, in combination, they can optimally solve DAG Partitioning on simulated citation networks within five minutes for $k\\leq190$ and $m$ being $10^7$ and larger. We complement our algorithms by lower bounds on the running time of exact algorithms and on the effectivity of data reduction."}, {"paper_id": "477251", "adju_relevance": 0, "title": "A Polynomial-Time Algorithm for Computing the Maximum Common Connected Edge Subgraph of Outerplanar Graphs of Bounded Degree", "background_label": "Abstract:The maximum common connected edge subgraph problem is to find a connected graph with the maximum number of edges that is isomorphic to a subgraph of each of the two input graphs, where it has applications in pattern recognition and chemistry.", "method_label": "This paper presents a dynamic programming algorithm for the problem when the two input graphs are outerplanar graphs of a bounded vertex degree, where it is known that the problem is NP-hard, even for outerplanar graphs of an unbounded degree.", "result_label": "Although the algorithm repeatedly modifies input graphs, it is shown that the number of relevant subproblems is polynomially bounded, and thus, the algorithm works in polynomial time.", "abstract": "Abstract:The maximum common connected edge subgraph problem is to find a connected graph with the maximum number of edges that is isomorphic to a subgraph of each of the two input graphs, where it has applications in pattern recognition and chemistry. This paper presents a dynamic programming algorithm for the problem when the two input graphs are outerplanar graphs of a bounded vertex degree, where it is known that the problem is NP-hard, even for outerplanar graphs of an unbounded degree. Although the algorithm repeatedly modifies input graphs, it is shown that the number of relevant subproblems is polynomially bounded, and thus, the algorithm works in polynomial time."}, {"paper_id": "8727935", "adju_relevance": 0, "title": "Approximate MRF Inference Using Bounded Treewidth Subgraphs", "background_label": "Graph cut algorithms [9] , commonly used in computer vision, solve a first-order MRF over binary variables. The state of the art for this NP-hard problem is QPBO [1, 2] , which finds the values for a subset of the variables in the global minimum. While QPBO is very effective overall there are still many difficult problems where it can only label a small subset of the variables.", "method_label": "We propose a new approach that, instead of optimizing the original graphical model, instead optimizes a tractable sub-model, defined as an energy function that uses a subset of the pairwise interactions of the original, but which for which exact inference can be done efficiently. Our Bounded Treewidth Subgraph (k-BTS) algorithm greedily computes a large weight treewidth-k subgraph of the signed graph, then solves the energy minimization problem for this subgraph by dynamic programming. The edges omitted by our greedy method provide a per-instance lower bound.", "result_label": "We demonstrate promising experimental results for binary deconvolution, a challenging problem used to benchmark QPBO [2]: our algorithm performs an order of magnitude better than QPBO or its common variants [4] , both in terms of energy and accuracy, and the visual quality of our output is strikingly better as well. We also obtain a significant improvement in energy and accuracy on a stereo benchmark with 2nd order priors [5] , although the improvement in visual quality is more modest. Our method's running time is comparable to QPBO.", "abstract": " Graph cut algorithms [9] , commonly used in computer vision, solve a first-order MRF over binary variables. Graph cut algorithms [9] , commonly used in computer vision, solve a first-order MRF over binary variables. The state of the art for this NP-hard problem is QPBO [1, 2] , which finds the values for a subset of the variables in the global minimum. Graph cut algorithms [9] , commonly used in computer vision, solve a first-order MRF over binary variables. The state of the art for this NP-hard problem is QPBO [1, 2] , which finds the values for a subset of the variables in the global minimum. While QPBO is very effective overall there are still many difficult problems where it can only label a small subset of the variables. We propose a new approach that, instead of optimizing the original graphical model, instead optimizes a tractable sub-model, defined as an energy function that uses a subset of the pairwise interactions of the original, but which for which exact inference can be done efficiently. We propose a new approach that, instead of optimizing the original graphical model, instead optimizes a tractable sub-model, defined as an energy function that uses a subset of the pairwise interactions of the original, but which for which exact inference can be done efficiently. Our Bounded Treewidth Subgraph (k-BTS) algorithm greedily computes a large weight treewidth-k subgraph of the signed graph, then solves the energy minimization problem for this subgraph by dynamic programming. We propose a new approach that, instead of optimizing the original graphical model, instead optimizes a tractable sub-model, defined as an energy function that uses a subset of the pairwise interactions of the original, but which for which exact inference can be done efficiently. Our Bounded Treewidth Subgraph (k-BTS) algorithm greedily computes a large weight treewidth-k subgraph of the signed graph, then solves the energy minimization problem for this subgraph by dynamic programming. The edges omitted by our greedy method provide a per-instance lower bound. We demonstrate promising experimental results for binary deconvolution, a challenging problem used to benchmark QPBO [2]: our algorithm performs an order of magnitude better than QPBO or its common variants [4] , both in terms of energy and accuracy, and the visual quality of our output is strikingly better as well. We demonstrate promising experimental results for binary deconvolution, a challenging problem used to benchmark QPBO [2]: our algorithm performs an order of magnitude better than QPBO or its common variants [4] , both in terms of energy and accuracy, and the visual quality of our output is strikingly better as well. We also obtain a significant improvement in energy and accuracy on a stereo benchmark with 2nd order priors [5] , although the improvement in visual quality is more modest. We demonstrate promising experimental results for binary deconvolution, a challenging problem used to benchmark QPBO [2]: our algorithm performs an order of magnitude better than QPBO or its common variants [4] , both in terms of energy and accuracy, and the visual quality of our output is strikingly better as well. We also obtain a significant improvement in energy and accuracy on a stereo benchmark with 2nd order priors [5] , although the improvement in visual quality is more modest. Our method's running time is comparable to QPBO."}, {"paper_id": "7624306", "adju_relevance": 0, "title": "Randomized minimum spanning tree algorithms using exponentially fewer random bits", "background_label": "For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) In some cases we are able to reduce the number of random bits from linear to nearly constant, without affecting the expected running time. Most of our results are obtained by adjusting or reorganizing existing randomized algorithms to work well with a pairwise or O(1)-wise independent sampler. The prominent exception, and the main focus of this article, is a linear-time randomized minimum spanning tree algorithm that is not derived from the well-known Karger-Klein-Tarjan algorithm.", "method_label": "One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. In this article we develop some general methods for reducing exponentially the consumption of random bits in comparison-based algorithms. In many ways it resembles more closely the deterministic minimum spanning tree algorithms based on soft heaps. Further, using our algorithm as a guide, we present a unified view of the existing \u201cnongreedy\u201d minimum spanning tree algorithms.", "result_label": "Concepts from the Karger-Klein-Tarjan algorithm, such as F-lightness, MST verification, and sampled graphs, are related to the concepts of edge corruption, subgraph contractibility, and soft heaps, which are the basis of the deterministic MST algorithms of Chazelle and Pettie-Ramachandran.", "abstract": "For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. In this article we develop some general methods for reducing exponentially the consumption of random bits in comparison-based algorithms. For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) In some cases we are able to reduce the number of random bits from linear to nearly constant, without affecting the expected running time. For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) In some cases we are able to reduce the number of random bits from linear to nearly constant, without affecting the expected running time. Most of our results are obtained by adjusting or reorganizing existing randomized algorithms to work well with a pairwise or O(1)-wise independent sampler. For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) In some cases we are able to reduce the number of random bits from linear to nearly constant, without affecting the expected running time. Most of our results are obtained by adjusting or reorganizing existing randomized algorithms to work well with a pairwise or O(1)-wise independent sampler. The prominent exception, and the main focus of this article, is a linear-time randomized minimum spanning tree algorithm that is not derived from the well-known Karger-Klein-Tarjan algorithm. One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. In this article we develop some general methods for reducing exponentially the consumption of random bits in comparison-based algorithms. In many ways it resembles more closely the deterministic minimum spanning tree algorithms based on soft heaps. One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. In this article we develop some general methods for reducing exponentially the consumption of random bits in comparison-based algorithms. In many ways it resembles more closely the deterministic minimum spanning tree algorithms based on soft heaps. Further, using our algorithm as a guide, we present a unified view of the existing \u201cnongreedy\u201d minimum spanning tree algorithms. Concepts from the Karger-Klein-Tarjan algorithm, such as F-lightness, MST verification, and sampled graphs, are related to the concepts of edge corruption, subgraph contractibility, and soft heaps, which are the basis of the deterministic MST algorithms of Chazelle and Pettie-Ramachandran."}, {"paper_id": "2833982", "adju_relevance": 0, "title": "Fully Dynamic Algorithm for Top-$k$ Densest Subgraphs", "background_label": "Given a large graph, the densest-subgraph problem asks to find a subgraph with maximum average degree. When considering the top-$k$ version of this problem, a na\\\"ive solution is to iteratively find the densest subgraph and remove it in each iteration. However, such a solution is impractical due to high processing cost. The problem is further complicated when dealing with dynamic graphs, since adding or removing an edge requires re-running the algorithm.", "method_label": "In this paper, we study the top-$k$ densest-subgraph problem in the sliding-window model and propose an efficient fully-dynamic algorithm.", "abstract": "Given a large graph, the densest-subgraph problem asks to find a subgraph with maximum average degree. Given a large graph, the densest-subgraph problem asks to find a subgraph with maximum average degree. When considering the top-$k$ version of this problem, a na\\\"ive solution is to iteratively find the densest subgraph and remove it in each iteration. Given a large graph, the densest-subgraph problem asks to find a subgraph with maximum average degree. When considering the top-$k$ version of this problem, a na\\\"ive solution is to iteratively find the densest subgraph and remove it in each iteration. However, such a solution is impractical due to high processing cost. Given a large graph, the densest-subgraph problem asks to find a subgraph with maximum average degree. When considering the top-$k$ version of this problem, a na\\\"ive solution is to iteratively find the densest subgraph and remove it in each iteration. However, such a solution is impractical due to high processing cost. The problem is further complicated when dealing with dynamic graphs, since adding or removing an edge requires re-running the algorithm. In this paper, we study the top-$k$ densest-subgraph problem in the sliding-window model and propose an efficient fully-dynamic algorithm."}, {"paper_id": "41494472", "adju_relevance": 0, "title": "On minimum spanning tree-like metric spaces", "abstract": ""}, {"paper_id": "7909460", "adju_relevance": 0, "title": "Energy Efficient Minimum Spanning Tree in OpenFlow Networks", "background_label": "Software Defined Networking and OpenFlow are enabling a new range of applications and services for tomorrow networks. At the same time, reducing the energy impact of datacentres and carrier networks will be an important topic in next years.", "abstract": "Software Defined Networking and OpenFlow are enabling a new range of applications and services for tomorrow networks. Software Defined Networking and OpenFlow are enabling a new range of applications and services for tomorrow networks. At the same time, reducing the energy impact of datacentres and carrier networks will be an important topic in next years."}, {"paper_id": "13074505", "adju_relevance": 0, "title": "Constructing the Maximum Consensus Tree from Rooted Triples", "background_label": "We investigated the problem of constructing the maximum consensus tree from rooted triples. We showed the NP-hardness of the problem and developed exact and heuristic algorithms.", "method_label": "The exact algorithm is based on the dynamic programming strategy and runs in O((m + n2)3n) time and O(2n) space. The heuristic algorithms run in polynomial time and their performances are tested and shown by comparing with the optimal solutions. We also implemented the two heuristic algorithms proposed by Gasieniec et al.", "result_label": "In the tests, the worst and average relative error ratios are 1.200 and 1.072 respectively. The experimental result shows that our heuristic algorithm is better than theirs in most of the tests.", "abstract": "We investigated the problem of constructing the maximum consensus tree from rooted triples. We investigated the problem of constructing the maximum consensus tree from rooted triples. We showed the NP-hardness of the problem and developed exact and heuristic algorithms. The exact algorithm is based on the dynamic programming strategy and runs in O((m + n2)3n) time and O(2n) space. The exact algorithm is based on the dynamic programming strategy and runs in O((m + n2)3n) time and O(2n) space. The heuristic algorithms run in polynomial time and their performances are tested and shown by comparing with the optimal solutions. In the tests, the worst and average relative error ratios are 1.200 and 1.072 respectively. The exact algorithm is based on the dynamic programming strategy and runs in O((m + n2)3n) time and O(2n) space. The heuristic algorithms run in polynomial time and their performances are tested and shown by comparing with the optimal solutions. We also implemented the two heuristic algorithms proposed by Gasieniec et al. In the tests, the worst and average relative error ratios are 1.200 and 1.072 respectively. The experimental result shows that our heuristic algorithm is better than theirs in most of the tests."}, {"paper_id": "202676751", "adju_relevance": 0, "title": "Low Diameter Graph Decompositions by Approximate Distance Computation", "background_label": "In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. There is a large body of literature on low diameter graph decomposition with small edge cutting probabilities, with all existing techniques heavily building on single source shortest paths (SSSP) computations. Unfortunately, in many theoretical models for large-scale computations, the SSSP task constitutes a complexity bottleneck. Therefore, it is desirable to replace exact SSSP computations with approximate ones. The current paper overcomes this obstacle by developing a technique termed blurry ball growing. By combining this technique with a clever algorithmic idea of Miller et al.", "result_label": "However this imposes a fundamental challenge since the existing constructions of such decompositions inherently rely on the subtractive form of the triangle inequality. Our embeddings have the additional useful property that the tree can be mapped back to the original graph such that each edge is\"used\"only O(log n) times, which is of interest for capacitated problems and simulating Congest algorithms on the tree into which the graph is embedded.", "method_label": "(SPAA 13), we obtain a construction of low diameter decompositions with small edge cutting probabilities which replaces exact SSSP computations by (a small number of) approximate ones. The utility of our approach is showcased by deriving efficient algorithms that work in the Congest, PRAM, and semi-streaming models of computation. As an application, we obtain metric tree embedding algorithms in the vein of Bartal (FOCS 96) whose computational complexities in these models are optimal up to polylogarithmic factors.", "abstract": "In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. There is a large body of literature on low diameter graph decomposition with small edge cutting probabilities, with all existing techniques heavily building on single source shortest paths (SSSP) computations. In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. There is a large body of literature on low diameter graph decomposition with small edge cutting probabilities, with all existing techniques heavily building on single source shortest paths (SSSP) computations. Unfortunately, in many theoretical models for large-scale computations, the SSSP task constitutes a complexity bottleneck. In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. There is a large body of literature on low diameter graph decomposition with small edge cutting probabilities, with all existing techniques heavily building on single source shortest paths (SSSP) computations. Unfortunately, in many theoretical models for large-scale computations, the SSSP task constitutes a complexity bottleneck. Therefore, it is desirable to replace exact SSSP computations with approximate ones. However this imposes a fundamental challenge since the existing constructions of such decompositions inherently rely on the subtractive form of the triangle inequality. In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. There is a large body of literature on low diameter graph decomposition with small edge cutting probabilities, with all existing techniques heavily building on single source shortest paths (SSSP) computations. Unfortunately, in many theoretical models for large-scale computations, the SSSP task constitutes a complexity bottleneck. Therefore, it is desirable to replace exact SSSP computations with approximate ones. The current paper overcomes this obstacle by developing a technique termed blurry ball growing. In many models for large-scale computation, decomposition of the problem is key to efficient algorithms. For distance-related graph problems, it is often crucial that such a decomposition results in clusters of small diameter, while the probability that an edge is cut by the decomposition scales linearly with the length of the edge. There is a large body of literature on low diameter graph decomposition with small edge cutting probabilities, with all existing techniques heavily building on single source shortest paths (SSSP) computations. Unfortunately, in many theoretical models for large-scale computations, the SSSP task constitutes a complexity bottleneck. Therefore, it is desirable to replace exact SSSP computations with approximate ones. The current paper overcomes this obstacle by developing a technique termed blurry ball growing. By combining this technique with a clever algorithmic idea of Miller et al. (SPAA 13), we obtain a construction of low diameter decompositions with small edge cutting probabilities which replaces exact SSSP computations by (a small number of) approximate ones. (SPAA 13), we obtain a construction of low diameter decompositions with small edge cutting probabilities which replaces exact SSSP computations by (a small number of) approximate ones. The utility of our approach is showcased by deriving efficient algorithms that work in the Congest, PRAM, and semi-streaming models of computation. (SPAA 13), we obtain a construction of low diameter decompositions with small edge cutting probabilities which replaces exact SSSP computations by (a small number of) approximate ones. The utility of our approach is showcased by deriving efficient algorithms that work in the Congest, PRAM, and semi-streaming models of computation. As an application, we obtain metric tree embedding algorithms in the vein of Bartal (FOCS 96) whose computational complexities in these models are optimal up to polylogarithmic factors. However this imposes a fundamental challenge since the existing constructions of such decompositions inherently rely on the subtractive form of the triangle inequality. Our embeddings have the additional useful property that the tree can be mapped back to the original graph such that each edge is\"used\"only O(log n) times, which is of interest for capacitated problems and simulating Congest algorithms on the tree into which the graph is embedded."}, {"paper_id": "567820", "adju_relevance": 0, "title": "Parsing With Soft And Hard Constraints On Dependency Length", "background_label": "AbstractIn lexicalized phrase-structure or dependency parses, a word's modifiers tend to fall near it in the string.", "abstract": "AbstractIn lexicalized phrase-structure or dependency parses, a word's modifiers tend to fall near it in the string."}, {"paper_id": "10970317", "adju_relevance": 0, "title": "Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems", "background_label": "Ordered labeled trees are trees in which the left-to-right order among siblings is. significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another.", "abstract": " Ordered labeled trees are trees in which the left-to-right order among siblings is. Ordered labeled trees are trees in which the left-to-right order among siblings is. significant. Ordered labeled trees are trees in which the left-to-right order among siblings is. significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another."}, {"paper_id": "4397051", "adju_relevance": 0, "title": "A Distributed Algorithm for Finding All Best Swap Edges of a Minimum-Diameter Spanning Tree", "background_label": "Communication in networks suffers if a link fails. When the links are edges of a tree that has been chosen from an underlying graph of all possible links, a broken link even disconnects the network. Most often, the link is restored rapidly. A good policy to deal with this sort of transient link failures is swap rerouting, where the temporarily broken link is replaced by a single swap link from the underlying graph. A rapid replacement of a broken link by a swap link is only possible if all swap links have been precomputed.", "abstract": "Communication in networks suffers if a link fails. Communication in networks suffers if a link fails. When the links are edges of a tree that has been chosen from an underlying graph of all possible links, a broken link even disconnects the network. Communication in networks suffers if a link fails. When the links are edges of a tree that has been chosen from an underlying graph of all possible links, a broken link even disconnects the network. Most often, the link is restored rapidly. Communication in networks suffers if a link fails. When the links are edges of a tree that has been chosen from an underlying graph of all possible links, a broken link even disconnects the network. Most often, the link is restored rapidly. A good policy to deal with this sort of transient link failures is swap rerouting, where the temporarily broken link is replaced by a single swap link from the underlying graph. Communication in networks suffers if a link fails. When the links are edges of a tree that has been chosen from an underlying graph of all possible links, a broken link even disconnects the network. Most often, the link is restored rapidly. A good policy to deal with this sort of transient link failures is swap rerouting, where the temporarily broken link is replaced by a single swap link from the underlying graph. A rapid replacement of a broken link by a swap link is only possible if all swap links have been precomputed."}, {"paper_id": "59619293", "adju_relevance": 0, "title": "Multi-class Bayes error estimation with a global minimal spanning tree", "background_label": "Henze-Penrose (HP) divergence has been used in many information theory, statistics and machine learning contexts, including the estimation of two-class Bayes classification error. Previous work has shown HP divergence can be directly estimated using the Friedman-Rafsky (FR) multivariate run test statistic.", "method_label": "For the multi-class classification problem, HP divergence can also be used to bound the Bayes error by estimating the sum of pairwise Bayes errors between classes. In situations in which the dataset and number of classes are large, this approach is infeasible. In this paper, we present a new generalized measure that allows us to estimate the Bayes error rate without the need to compute pairwise estimates.", "result_label": "We compare our new approach with the pairwise HP bound and the bound proposed by Lin [1], and show that our upper bound on Bayes error is tighter, while also having lower computational complexity.", "abstract": "Henze-Penrose (HP) divergence has been used in many information theory, statistics and machine learning contexts, including the estimation of two-class Bayes classification error. Henze-Penrose (HP) divergence has been used in many information theory, statistics and machine learning contexts, including the estimation of two-class Bayes classification error. Previous work has shown HP divergence can be directly estimated using the Friedman-Rafsky (FR) multivariate run test statistic. For the multi-class classification problem, HP divergence can also be used to bound the Bayes error by estimating the sum of pairwise Bayes errors between classes. For the multi-class classification problem, HP divergence can also be used to bound the Bayes error by estimating the sum of pairwise Bayes errors between classes. In situations in which the dataset and number of classes are large, this approach is infeasible. For the multi-class classification problem, HP divergence can also be used to bound the Bayes error by estimating the sum of pairwise Bayes errors between classes. In situations in which the dataset and number of classes are large, this approach is infeasible. In this paper, we present a new generalized measure that allows us to estimate the Bayes error rate without the need to compute pairwise estimates. We compare our new approach with the pairwise HP bound and the bound proposed by Lin [1], and show that our upper bound on Bayes error is tighter, while also having lower computational complexity."}, {"paper_id": "7858319", "adju_relevance": 0, "title": "Computing Maximum Cardinality Matchings in Parallel on Bipartite Graphs via Tree-Grafting", "background_label": "It is difficult to obtain high performance when computing matchings on parallel processors because matching algorithms explicitly or implicitly search for paths in the graph, and when these paths become long, there is little concurrency. In spite of this limitation, we present a new algorithm and its shared-memory parallelization that achieves good performance and scalability in computing maximum cardinality matchings in bipartite graphs. We also employ the recent direction-optimizing BFS algorithm as a subroutine to discover augmenting paths faster. Our algorithm compares favorably with the current best algorithms in terms of the number of edges traversed, the average augmenting path length, and the number of iterations.", "method_label": "Our algorithm searches for augmenting paths via specialized breadthfirst searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Algorithms that employ multiple-source searches cannot discard a search tree once no augmenting path is discovered from the tree, unlike algorithms that rely on single-source searches. We describe a novel tree-grafting method that eliminates most of the redundant edge traversals resulting from this property of multiple-source searches. We provide a proof of correctness for our algorithm. Our NUMA-aware implementation is scalable to 80 threads of an Intel multiprocessor and to 240 threads on an Intel Knights Corner coprocessor.", "result_label": "On average, our parallel algorithm runs an order of magnitude faster than the fastest algorithms available. The performance improvement is more significant on graphs with small matching number.", "abstract": "It is difficult to obtain high performance when computing matchings on parallel processors because matching algorithms explicitly or implicitly search for paths in the graph, and when these paths become long, there is little concurrency. It is difficult to obtain high performance when computing matchings on parallel processors because matching algorithms explicitly or implicitly search for paths in the graph, and when these paths become long, there is little concurrency. In spite of this limitation, we present a new algorithm and its shared-memory parallelization that achieves good performance and scalability in computing maximum cardinality matchings in bipartite graphs. Our algorithm searches for augmenting paths via specialized breadthfirst searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Our algorithm searches for augmenting paths via specialized breadthfirst searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Algorithms that employ multiple-source searches cannot discard a search tree once no augmenting path is discovered from the tree, unlike algorithms that rely on single-source searches. Our algorithm searches for augmenting paths via specialized breadthfirst searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Algorithms that employ multiple-source searches cannot discard a search tree once no augmenting path is discovered from the tree, unlike algorithms that rely on single-source searches. We describe a novel tree-grafting method that eliminates most of the redundant edge traversals resulting from this property of multiple-source searches. It is difficult to obtain high performance when computing matchings on parallel processors because matching algorithms explicitly or implicitly search for paths in the graph, and when these paths become long, there is little concurrency. In spite of this limitation, we present a new algorithm and its shared-memory parallelization that achieves good performance and scalability in computing maximum cardinality matchings in bipartite graphs. We also employ the recent direction-optimizing BFS algorithm as a subroutine to discover augmenting paths faster. It is difficult to obtain high performance when computing matchings on parallel processors because matching algorithms explicitly or implicitly search for paths in the graph, and when these paths become long, there is little concurrency. In spite of this limitation, we present a new algorithm and its shared-memory parallelization that achieves good performance and scalability in computing maximum cardinality matchings in bipartite graphs. We also employ the recent direction-optimizing BFS algorithm as a subroutine to discover augmenting paths faster. Our algorithm compares favorably with the current best algorithms in terms of the number of edges traversed, the average augmenting path length, and the number of iterations. Our algorithm searches for augmenting paths via specialized breadthfirst searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Algorithms that employ multiple-source searches cannot discard a search tree once no augmenting path is discovered from the tree, unlike algorithms that rely on single-source searches. We describe a novel tree-grafting method that eliminates most of the redundant edge traversals resulting from this property of multiple-source searches. We provide a proof of correctness for our algorithm. Our algorithm searches for augmenting paths via specialized breadthfirst searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Algorithms that employ multiple-source searches cannot discard a search tree once no augmenting path is discovered from the tree, unlike algorithms that rely on single-source searches. We describe a novel tree-grafting method that eliminates most of the redundant edge traversals resulting from this property of multiple-source searches. We provide a proof of correctness for our algorithm. Our NUMA-aware implementation is scalable to 80 threads of an Intel multiprocessor and to 240 threads on an Intel Knights Corner coprocessor. On average, our parallel algorithm runs an order of magnitude faster than the fastest algorithms available. On average, our parallel algorithm runs an order of magnitude faster than the fastest algorithms available. The performance improvement is more significant on graphs with small matching number."}, {"paper_id": "7944869", "adju_relevance": 0, "title": "pplacer: linear time maximum-likelihood and Bayesian phylogenetic placement of sequences onto a fixed reference tree", "background_label": "Likelihood-based phylogenetic inference is generally considered to be the most reliable classification method for unknown sequences. However, traditional likelihood-based phylogenetic methods cannot be applied to large volumes of short reads from next-generation sequencing due to computational complexity issues and lack of phylogenetic signal. \"Phylogenetic placement,\"where a reference tree is fixed and the unknown query sequences are placed onto the tree via a reference alignment, is a way to bring the inferential power of likelihood-based approaches to large data sets.", "abstract": "Likelihood-based phylogenetic inference is generally considered to be the most reliable classification method for unknown sequences. Likelihood-based phylogenetic inference is generally considered to be the most reliable classification method for unknown sequences. However, traditional likelihood-based phylogenetic methods cannot be applied to large volumes of short reads from next-generation sequencing due to computational complexity issues and lack of phylogenetic signal. Likelihood-based phylogenetic inference is generally considered to be the most reliable classification method for unknown sequences. However, traditional likelihood-based phylogenetic methods cannot be applied to large volumes of short reads from next-generation sequencing due to computational complexity issues and lack of phylogenetic signal. \"Phylogenetic placement,\"where a reference tree is fixed and the unknown query sequences are placed onto the tree via a reference alignment, is a way to bring the inferential power of likelihood-based approaches to large data sets."}, {"paper_id": "15524603", "adju_relevance": 0, "title": "DoA Estimation Via Manifold Separation for Arbitrary Array Structures", "background_label": "In this paper, we consider the manifold separation technique (MST), which stems from the wavefield modeling formalism developed for array processing. In particular, we study the effect of noisy calibration measurements on subspace-based DoA algorithms using MST. Expressions describing the error in the DoA estimates due to calibration noise and truncation are derived.", "method_label": "MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. This allows fast direction-of-arrival (DoA) algorithms designed for linear arrays to be used on arrays with arbitrary configuration. In real-world applications, the calibration measurements used to determine the sampling matrix are corrupted by noise. This allows predicting the performance of MST-based algorithms in real-world applications. The analysis is verified by simulations. We established a link between the optimal number of selected modes and the statistics of calibration noise.", "result_label": "This impairs the performance of MST-based algorithms. We analyze the modeling error when MST is used for 1-D (azimuth) DoA estimation.", "abstract": "In this paper, we consider the manifold separation technique (MST), which stems from the wavefield modeling formalism developed for array processing. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. This allows fast direction-of-arrival (DoA) algorithms designed for linear arrays to be used on arrays with arbitrary configuration. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. This allows fast direction-of-arrival (DoA) algorithms designed for linear arrays to be used on arrays with arbitrary configuration. In real-world applications, the calibration measurements used to determine the sampling matrix are corrupted by noise. This impairs the performance of MST-based algorithms. In this paper, we consider the manifold separation technique (MST), which stems from the wavefield modeling formalism developed for array processing. In particular, we study the effect of noisy calibration measurements on subspace-based DoA algorithms using MST. In this paper, we consider the manifold separation technique (MST), which stems from the wavefield modeling formalism developed for array processing. In particular, we study the effect of noisy calibration measurements on subspace-based DoA algorithms using MST. Expressions describing the error in the DoA estimates due to calibration noise and truncation are derived. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. This allows fast direction-of-arrival (DoA) algorithms designed for linear arrays to be used on arrays with arbitrary configuration. In real-world applications, the calibration measurements used to determine the sampling matrix are corrupted by noise. This allows predicting the performance of MST-based algorithms in real-world applications. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. This allows fast direction-of-arrival (DoA) algorithms designed for linear arrays to be used on arrays with arbitrary configuration. In real-world applications, the calibration measurements used to determine the sampling matrix are corrupted by noise. This allows predicting the performance of MST-based algorithms in real-world applications. The analysis is verified by simulations. MST is a method for modeling the steering vector of antenna arrays of practical interest with arbitrary 2-D or 3-D geometry. It is the product of a sampling matrix (dependent on the antenna array only) and a Vandermonde structured coefficients vector depending on the wavefield only. This allows fast direction-of-arrival (DoA) algorithms designed for linear arrays to be used on arrays with arbitrary configuration. In real-world applications, the calibration measurements used to determine the sampling matrix are corrupted by noise. This allows predicting the performance of MST-based algorithms in real-world applications. The analysis is verified by simulations. We established a link between the optimal number of selected modes and the statistics of calibration noise. This impairs the performance of MST-based algorithms. We analyze the modeling error when MST is used for 1-D (azimuth) DoA estimation."}, {"paper_id": "13870809", "adju_relevance": 0, "title": "A Temporal Tree Decomposition for Generating Temporal Graphs", "background_label": "Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. Recent work at the intersection of formal language theory and graph theory has found that a Hyperedge Replacement Grammar (HRG) can be extracted from a tree decomposition of any graph. This HRG can be used to generate new graphs that share properties that are similar to the original graph. Because the extracted HRG is directly dependent on the shape and contents of the of tree decomposition, it is unlikely that informative graph-processes are actually being captured with the extraction algorithm.", "method_label": "To address this problem, the current work presents a new extraction algorithm called temporal HRG (tHRG) that learns HRG production rules from a temporal tree decomposition of the graph.", "result_label": "We observe problems with the assumptions that are made in a temporal HRG model. In experiments on large real world networks, we show and provide reasoning as to why tHRG does not perform as well as HRG and other graph generators.", "abstract": "Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. Recent work at the intersection of formal language theory and graph theory has found that a Hyperedge Replacement Grammar (HRG) can be extracted from a tree decomposition of any graph. Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. Recent work at the intersection of formal language theory and graph theory has found that a Hyperedge Replacement Grammar (HRG) can be extracted from a tree decomposition of any graph. This HRG can be used to generate new graphs that share properties that are similar to the original graph. Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. Recent work at the intersection of formal language theory and graph theory has found that a Hyperedge Replacement Grammar (HRG) can be extracted from a tree decomposition of any graph. This HRG can be used to generate new graphs that share properties that are similar to the original graph. Because the extracted HRG is directly dependent on the shape and contents of the of tree decomposition, it is unlikely that informative graph-processes are actually being captured with the extraction algorithm. To address this problem, the current work presents a new extraction algorithm called temporal HRG (tHRG) that learns HRG production rules from a temporal tree decomposition of the graph. We observe problems with the assumptions that are made in a temporal HRG model. We observe problems with the assumptions that are made in a temporal HRG model. In experiments on large real world networks, we show and provide reasoning as to why tHRG does not perform as well as HRG and other graph generators."}, {"paper_id": "8556072", "adju_relevance": 0, "title": "Controlled Generation of Hard and Easy Bayesian Networks: Impact on Maximal Clique Tree in Tree Clustering", "background_label": "AbstractThis article presents and analyzes algorithms that systematically generate random Bayesian networks of varying difficulty levels, with respect to inference using tree clustering. The results are relevant to research on efficient Bayesian network inference, such as computing a most probable explanation or belief updating, since they allow controlled experimentation to determine the impact of improvements to inference algorithms.", "method_label": "The results are also relevant to research on machine learning of Bayesian networks, since they support controlled generation of a large number of data sets at a given difficulty level. Our generation algorithms, called BPART and MPART, support controlled but random construction of bipartite and multipartite Bayesian networks. The Bayesian network parameters that we vary are the total number of nodes, degree of connectivity, the ratio of the number of non-root nodes to the number of root nodes, regularity of the underlying graph, and characteristics of the conditional probability tables. The main dependent parameter is the size of the maximal clique as generated by tree clustering.", "result_label": "This article presents extensive empirical analysis using the H tree clustering approach as well as theoretical analysis related to the random generation of Bayesian networks using BPART and MPART.", "abstract": "AbstractThis article presents and analyzes algorithms that systematically generate random Bayesian networks of varying difficulty levels, with respect to inference using tree clustering. AbstractThis article presents and analyzes algorithms that systematically generate random Bayesian networks of varying difficulty levels, with respect to inference using tree clustering. The results are relevant to research on efficient Bayesian network inference, such as computing a most probable explanation or belief updating, since they allow controlled experimentation to determine the impact of improvements to inference algorithms. The results are also relevant to research on machine learning of Bayesian networks, since they support controlled generation of a large number of data sets at a given difficulty level. The results are also relevant to research on machine learning of Bayesian networks, since they support controlled generation of a large number of data sets at a given difficulty level. Our generation algorithms, called BPART and MPART, support controlled but random construction of bipartite and multipartite Bayesian networks. The results are also relevant to research on machine learning of Bayesian networks, since they support controlled generation of a large number of data sets at a given difficulty level. Our generation algorithms, called BPART and MPART, support controlled but random construction of bipartite and multipartite Bayesian networks. The Bayesian network parameters that we vary are the total number of nodes, degree of connectivity, the ratio of the number of non-root nodes to the number of root nodes, regularity of the underlying graph, and characteristics of the conditional probability tables. The results are also relevant to research on machine learning of Bayesian networks, since they support controlled generation of a large number of data sets at a given difficulty level. Our generation algorithms, called BPART and MPART, support controlled but random construction of bipartite and multipartite Bayesian networks. The Bayesian network parameters that we vary are the total number of nodes, degree of connectivity, the ratio of the number of non-root nodes to the number of root nodes, regularity of the underlying graph, and characteristics of the conditional probability tables. The main dependent parameter is the size of the maximal clique as generated by tree clustering. This article presents extensive empirical analysis using the H tree clustering approach as well as theoretical analysis related to the random generation of Bayesian networks using BPART and MPART."}, {"paper_id": "12616009", "adju_relevance": 0, "title": "Minimum spanning tree adaptive image filtering", "method_label": "We present in this work neighborhood filters defined on the minimal spanning tree (MST) of an image (according to a local dissimilarity measure between adjacent pixels). The designed filters take advantage of the property of the MST to detect and follow the local features of an image. This approach leads to neighborhood filters where the structuring elements adapt their shape to the minimal spanning tree structure and therefore to the local image features.", "result_label": "We demonstrate the quality of this method on natural and synthetic images.", "abstract": " We present in this work neighborhood filters defined on the minimal spanning tree (MST) of an image (according to a local dissimilarity measure between adjacent pixels). We present in this work neighborhood filters defined on the minimal spanning tree (MST) of an image (according to a local dissimilarity measure between adjacent pixels). The designed filters take advantage of the property of the MST to detect and follow the local features of an image. We present in this work neighborhood filters defined on the minimal spanning tree (MST) of an image (according to a local dissimilarity measure between adjacent pixels). The designed filters take advantage of the property of the MST to detect and follow the local features of an image. This approach leads to neighborhood filters where the structuring elements adapt their shape to the minimal spanning tree structure and therefore to the local image features. We demonstrate the quality of this method on natural and synthetic images."}, {"paper_id": "17427872", "adju_relevance": 0, "title": "Path-tree: An efficient reachability indexing scheme for large directed graphs", "background_label": "Reachability query is one of the fundamental queries in graph database. The main idea behind answering reachability queries is to assign vertices with certain labels such that the reachability between any two vertices can be determined by the labeling information. Though several approaches have been proposed for building these reachability labels, it remains open issues on how to handle increasingly large number of vertices in real-world graphs, and how to find the best tradeoff among the labeling size, the query answering time, and the construction time.", "method_label": "In this article, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We show path-tree can be generalized to chain-tree which theoretically can has smaller labeling cost. On top of path-tree and chain-tree index, we also introduce a new compression scheme which groups vertices with similar labels together to further reduce the labeling size. In addition, we also propose an efficient incremental update algorithm for dynamic index maintenance.", "result_label": "Finally, we demonstrate both analytically and empirically the effectiveness and efficiency of our new approaches.", "abstract": "Reachability query is one of the fundamental queries in graph database. Reachability query is one of the fundamental queries in graph database. The main idea behind answering reachability queries is to assign vertices with certain labels such that the reachability between any two vertices can be determined by the labeling information. Reachability query is one of the fundamental queries in graph database. The main idea behind answering reachability queries is to assign vertices with certain labels such that the reachability between any two vertices can be determined by the labeling information. Though several approaches have been proposed for building these reachability labels, it remains open issues on how to handle increasingly large number of vertices in real-world graphs, and how to find the best tradeoff among the labeling size, the query answering time, and the construction time. In this article, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. In this article, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. In this article, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We show path-tree can be generalized to chain-tree which theoretically can has smaller labeling cost. In this article, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We show path-tree can be generalized to chain-tree which theoretically can has smaller labeling cost. On top of path-tree and chain-tree index, we also introduce a new compression scheme which groups vertices with similar labels together to further reduce the labeling size. In this article, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We show path-tree can be generalized to chain-tree which theoretically can has smaller labeling cost. On top of path-tree and chain-tree index, we also introduce a new compression scheme which groups vertices with similar labels together to further reduce the labeling size. In addition, we also propose an efficient incremental update algorithm for dynamic index maintenance. Finally, we demonstrate both analytically and empirically the effectiveness and efficiency of our new approaches."}, {"paper_id": "15449653", "adju_relevance": 0, "title": "A polynomial-time maximum common subgraph algorithm for outerplanar graphs and its application to chemoinformatics", "background_label": "Metrics for structured data have received an increasing interest in the machine learning community. Graphs provide a natural representation for structured data, but a lot of operations on graphs are computationally intractable.", "abstract": "Metrics for structured data have received an increasing interest in the machine learning community. Metrics for structured data have received an increasing interest in the machine learning community. Graphs provide a natural representation for structured data, but a lot of operations on graphs are computationally intractable."}, {"paper_id": "16826827", "adju_relevance": 0, "title": "An efficient algorithm for generalized minimum spanning tree problem", "background_label": "The Generalized Minimum Spanning Tree problem (GMST) has attracted much attention during the last few years. Since it is in-tractable, many heuristic algorithms have been proposed to solve large GMST instances.", "abstract": "The Generalized Minimum Spanning Tree problem (GMST) has attracted much attention during the last few years. The Generalized Minimum Spanning Tree problem (GMST) has attracted much attention during the last few years. Since it is in-tractable, many heuristic algorithms have been proposed to solve large GMST instances."}, {"paper_id": "1088103", "adju_relevance": 0, "title": "Parallel Computation of the Topology of Level Sets", "background_label": "This paper introduces two efficient algorithms that compute the Contour Tree of a three-dimensional scalar field F and its augmented version with the Betti numbers of each isosurface. The Contour Tree is a fundamental data structure in scientific visualization that is used to pre-process the domain mesh to allow optimal computation of isosurfaces with minimal overhead storage.", "abstract": "This paper introduces two efficient algorithms that compute the Contour Tree of a three-dimensional scalar field F and its augmented version with the Betti numbers of each isosurface. This paper introduces two efficient algorithms that compute the Contour Tree of a three-dimensional scalar field F and its augmented version with the Betti numbers of each isosurface. The Contour Tree is a fundamental data structure in scientific visualization that is used to pre-process the domain mesh to allow optimal computation of isosurfaces with minimal overhead storage."}, {"paper_id": "55994574", "adju_relevance": 0, "title": "Expediting MRSH-v2 Approximate Matching with Hierarchical Bloom Filter Trees", "background_label": "Perhaps the most common task encountered by digital forensic investigators consists of searching through a seized device for pertinent data. Frequently, an investigator will be in possession of a collection of \u201cknown-illegal\u201d files (e.g. a collection of child pornographic images) and will seek to find whether copies of these are stored on the seized drive.", "method_label": "Traditional hash matching techniques can efficiently find files that precisely match.", "result_label": "However, these will fail in the case of merged files, embedded files, partial files, or if a file has been changed in any way.", "abstract": "Perhaps the most common task encountered by digital forensic investigators consists of searching through a seized device for pertinent data. Perhaps the most common task encountered by digital forensic investigators consists of searching through a seized device for pertinent data. Frequently, an investigator will be in possession of a collection of \u201cknown-illegal\u201d files (e.g. Perhaps the most common task encountered by digital forensic investigators consists of searching through a seized device for pertinent data. Frequently, an investigator will be in possession of a collection of \u201cknown-illegal\u201d files (e.g. a collection of child pornographic images) and will seek to find whether copies of these are stored on the seized drive. Traditional hash matching techniques can efficiently find files that precisely match. However, these will fail in the case of merged files, embedded files, partial files, or if a file has been changed in any way."}, {"paper_id": "25896384", "adju_relevance": 0, "title": "Minimum spanning tree release under differential privacy constraints", "background_label": "We investigate the problem of nodes clustering under privacy constraints when representing a dataset as a graph.", "abstract": "We investigate the problem of nodes clustering under privacy constraints when representing a dataset as a graph."}, {"paper_id": "10069776", "adju_relevance": 0, "title": "Automated Generation of Search Tree Algorithms for Hard Graph Modification Problems", "background_label": "We present a framework for an automated generation of exact search tree algorithms for NP-hard problems.", "abstract": "We present a framework for an automated generation of exact search tree algorithms for NP-hard problems."}, {"paper_id": "26355928", "adju_relevance": 0, "title": "Graph edit distance as a quadratic program", "background_label": "The graph edit distance (GED) measures the amount of distortion needed to transform a graph into another graph. Such a distance, developed in the context of error-tolerant graph matching, is one of the most flexible tool used in structural pattern recognition. However, the computation of the exact GED is NP-complete. Hence several suboptimal solutions, such as the ones based on bipartite assignments with edition, have been proposed.", "abstract": "The graph edit distance (GED) measures the amount of distortion needed to transform a graph into another graph. The graph edit distance (GED) measures the amount of distortion needed to transform a graph into another graph. Such a distance, developed in the context of error-tolerant graph matching, is one of the most flexible tool used in structural pattern recognition. The graph edit distance (GED) measures the amount of distortion needed to transform a graph into another graph. Such a distance, developed in the context of error-tolerant graph matching, is one of the most flexible tool used in structural pattern recognition. However, the computation of the exact GED is NP-complete. The graph edit distance (GED) measures the amount of distortion needed to transform a graph into another graph. Such a distance, developed in the context of error-tolerant graph matching, is one of the most flexible tool used in structural pattern recognition. However, the computation of the exact GED is NP-complete. Hence several suboptimal solutions, such as the ones based on bipartite assignments with edition, have been proposed."}, {"paper_id": "832583", "adju_relevance": 0, "title": "A randomized linear-time algorithm to find minimum spanning trees", "background_label": "We present a randomized linear-time algorithm to find a minimum spanning tree in a connected graph with edge weights.", "method_label": "The algorithm uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons.", "abstract": "We present a randomized linear-time algorithm to find a minimum spanning tree in a connected graph with edge weights. The algorithm uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. The algorithm uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons."}, {"paper_id": "14183947", "adju_relevance": 0, "title": "Finding Optimal Bayesian Network Given a Super-Structure", "background_label": "Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase.", "method_label": "Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(\u03b3m n ), where \u03b3m < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree \u02dc m and sparse structures allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even finds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; significance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy.", "result_label": "For incomplete super-structures, a greedily post-processed version (COS+) still enables to significantly outperform other heuristic searches.", "abstract": "Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(\u03b3m n ), where \u03b3m < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree \u02dc m and sparse structures allow larger graphs to be calculated. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(\u03b3m n ), where \u03b3m < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree \u02dc m and sparse structures allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even finds more accurate results when given a sound super-structure. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(\u03b3m n ), where \u03b3m < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree \u02dc m and sparse structures allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even finds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; significance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to significantly outperform other heuristic searches."}, {"paper_id": "16318436", "adju_relevance": 0, "title": "Dependency grammar and dependency parsing", "background_label": "Despite a long and venerable tradition in descriptive linguistics, dependency grammar has until recently played a fairly marginal role both in theoretical linguistics and in natural language processing. The increasing interest in dependency-based representations in natural language parsing in recent years appears to be motivated both by the potential usefulness of bilexical relations in disambiguation and by the gains in efficiency that result from the more constrained parsing problem for these representations.", "abstract": "Despite a long and venerable tradition in descriptive linguistics, dependency grammar has until recently played a fairly marginal role both in theoretical linguistics and in natural language processing. Despite a long and venerable tradition in descriptive linguistics, dependency grammar has until recently played a fairly marginal role both in theoretical linguistics and in natural language processing. The increasing interest in dependency-based representations in natural language parsing in recent years appears to be motivated both by the potential usefulness of bilexical relations in disambiguation and by the gains in efficiency that result from the more constrained parsing problem for these representations."}, {"paper_id": "1585700", "adju_relevance": 0, "title": "The CoNLL 2007 Shared Task on Dependency Parsing", "background_label": "AbstractThe Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track.", "method_label": "In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages.", "result_label": "In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.", "abstract": "AbstractThe Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. AbstractThe Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results."}, {"paper_id": "15369124", "adju_relevance": 0, "title": "Connectivity Preserving Iterative Compaction and Finding 2 Disjoint Rooted Paths in Linear Time", "background_label": "In this paper we show how to combine two algorithmic techniques to obtain linear time algorithms for various optimization problems on graphs, and present a subroutine which will be useful in doing so. The first technique is iterative shrinking.", "method_label": "In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In an iterative compaction algorithm, we insist that the graphs decrease by a constant fraction of the entire graph. Another approach to solving optimization problems is to exploit the structural properties implied by the connectivity of the input graph. This approach can be used on graphs which are not highly connected by decomposing an input graph into its highly connected pieces, solving subproblems on these specially structured pieces and then combining their solutions. We combine these two techniques by developing compaction algorithms which when applied to the highly connected pieces preserve their connectivity properties. The structural properties this connectivity implies can be helpful both in finding further compactions in later iterations and when we are manipulating solutions in the second phase of an iterative compaction algorithm.", "result_label": "To illustrate how this compaction algorithm can be used as a subroutine, we present a linear time algorithm that given four vertices $\\{s_1,s_2,t_1,t_2\\}$ of a graph $G$, either finds a pair of disjoint paths $P_1$ and $P_2$ of $G$ such that $P_i$ has endpoints $s_i$ and $t_i$, or returns a planar embedding of an auxiliary graph which shows that no such pair exists.", "abstract": "In this paper we show how to combine two algorithmic techniques to obtain linear time algorithms for various optimization problems on graphs, and present a subroutine which will be useful in doing so. In this paper we show how to combine two algorithmic techniques to obtain linear time algorithms for various optimization problems on graphs, and present a subroutine which will be useful in doing so. The first technique is iterative shrinking. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In an iterative compaction algorithm, we insist that the graphs decrease by a constant fraction of the entire graph. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In an iterative compaction algorithm, we insist that the graphs decrease by a constant fraction of the entire graph. Another approach to solving optimization problems is to exploit the structural properties implied by the connectivity of the input graph. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In an iterative compaction algorithm, we insist that the graphs decrease by a constant fraction of the entire graph. Another approach to solving optimization problems is to exploit the structural properties implied by the connectivity of the input graph. This approach can be used on graphs which are not highly connected by decomposing an input graph into its highly connected pieces, solving subproblems on these specially structured pieces and then combining their solutions. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In an iterative compaction algorithm, we insist that the graphs decrease by a constant fraction of the entire graph. Another approach to solving optimization problems is to exploit the structural properties implied by the connectivity of the input graph. This approach can be used on graphs which are not highly connected by decomposing an input graph into its highly connected pieces, solving subproblems on these specially structured pieces and then combining their solutions. We combine these two techniques by developing compaction algorithms which when applied to the highly connected pieces preserve their connectivity properties. In the first phase of an iterative shrinking algorithm, we construct a sequence of graphs of decreasing size $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some shrinking algorithm. In the second phase we work through the sequence in reverse, repeatedly constructing a solution for a graph from the solution for its successor. In an iterative compaction algorithm, we insist that the graphs decrease by a constant fraction of the entire graph. Another approach to solving optimization problems is to exploit the structural properties implied by the connectivity of the input graph. This approach can be used on graphs which are not highly connected by decomposing an input graph into its highly connected pieces, solving subproblems on these specially structured pieces and then combining their solutions. We combine these two techniques by developing compaction algorithms which when applied to the highly connected pieces preserve their connectivity properties. The structural properties this connectivity implies can be helpful both in finding further compactions in later iterations and when we are manipulating solutions in the second phase of an iterative compaction algorithm. To illustrate how this compaction algorithm can be used as a subroutine, we present a linear time algorithm that given four vertices $\\{s_1,s_2,t_1,t_2\\}$ of a graph $G$, either finds a pair of disjoint paths $P_1$ and $P_2$ of $G$ such that $P_i$ has endpoints $s_i$ and $t_i$, or returns a planar embedding of an auxiliary graph which shows that no such pair exists."}, {"paper_id": "8048965", "adju_relevance": 0, "title": "On the impact of causal independence", "background_label": "Reasoning in Bayesian networks is exponential in a graph parameter called induced-width (also known as tree-width and max-clique size).", "abstract": "Reasoning in Bayesian networks is exponential in a graph parameter called induced-width (also known as tree-width and max-clique size)."}, {"paper_id": "18288773", "adju_relevance": 0, "title": "Learning prediction suffix trees with Winnow", "background_label": "Prediction suffix trees (PSTs) are a popular tool for modeling sequences and have been successfully applied in many domains such as compression and language modeling.", "abstract": "Prediction suffix trees (PSTs) are a popular tool for modeling sequences and have been successfully applied in many domains such as compression and language modeling."}, {"paper_id": "18902751", "adju_relevance": 0, "title": "Minimum Spanning Trees and Single Linkage Cluster Analysis", "background_label": "Minimum spanning trees (MST) and single linkage cluster analysis (SLCA) are explained and it is shown that all the information required for the SLCA of a set of points is contained in their MST. Known algorithms for finding the MST are discussed.", "method_label": "They are efficient even when there are very many points; this makes a SLCA practicable when other methods of cluster analysis are not. The relevant computing procedures are published in the Algorithm section of the same issue of Applied Statistics.", "result_label": "The use of the MST in the interpretation of vector diagrams arising in multivariate analysis is illustrated by an example.", "abstract": "Minimum spanning trees (MST) and single linkage cluster analysis (SLCA) are explained and it is shown that all the information required for the SLCA of a set of points is contained in their MST. Minimum spanning trees (MST) and single linkage cluster analysis (SLCA) are explained and it is shown that all the information required for the SLCA of a set of points is contained in their MST. Known algorithms for finding the MST are discussed. They are efficient even when there are very many points; this makes a SLCA practicable when other methods of cluster analysis are not. They are efficient even when there are very many points; this makes a SLCA practicable when other methods of cluster analysis are not. The relevant computing procedures are published in the Algorithm section of the same issue of Applied Statistics. The use of the MST in the interpretation of vector diagrams arising in multivariate analysis is illustrated by an example."}]