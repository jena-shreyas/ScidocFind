[{"paper_id": "1936997", "title": "\"Not not bad\"is not\"bad\": A distributional account of negation", "background_label": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing.", "abstract": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing."}, {"paper_id": "3882934", "adju_relevance": 3, "title": "*SEM 2012 Shared Task: Resolving the Scope and Focus of Negation", "background_label": "AbstractThe Joint Conference on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation.", "abstract": "AbstractThe Joint Conference on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. AbstractThe Joint Conference on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation."}, {"paper_id": "9299839", "adju_relevance": 3, "title": "Representing and Resolving Negation for Sentiment Analysis", "background_label": "Proper treatment of negation is an important characteristic of methods for sentiment analysis. However, while there is a growing body of research on the automatic resolution of negation, it is not yet clear as to how negation is best represented for different applications.", "method_label": "To begin to address this issue, we review representation alternatives and present a state-of-the-art system for negation resolution that is interoperable across these schemes.", "result_label": "By employing different configurations of this system as a component in a test bed for lexically-based sentiment classification, we demonstrate that the choice of representation can have a significant impact on downstream processing.", "abstract": "Proper treatment of negation is an important characteristic of methods for sentiment analysis. Proper treatment of negation is an important characteristic of methods for sentiment analysis. However, while there is a growing body of research on the automatic resolution of negation, it is not yet clear as to how negation is best represented for different applications. To begin to address this issue, we review representation alternatives and present a state-of-the-art system for negation resolution that is interoperable across these schemes. By employing different configurations of this system as a component in a test bed for lexically-based sentiment classification, we demonstrate that the choice of representation can have a significant impact on downstream processing."}, {"paper_id": "14230035", "adju_relevance": 2, "title": "Word vectors and quantum logic: Experiments with negation and disjunction", "background_label": "A calculus which combined the flexible geometric structure o f vector models with the crisp efficiency of Boolean logic would be extrem ely beneficial for modelling natural language.", "abstract": "A calculus which combined the flexible geometric structure o f vector models with the crisp efficiency of Boolean logic would be extrem ely beneficial for modelling natural language."}, {"paper_id": "3909257", "adju_relevance": 2, "title": "Type-preserving CPS translation of \u03a3 and \u03a0 types is not not possible", "background_label": "Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. By preserving dependent types through each compiler pass, we could preserve source-level specifications and correctness proofs into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In 2002, Barthe and Uustalu showed that type-preserving CPS is not possible for languages such as Coq. In essence, the problem is that the standard typed CPS translation by double-negation, in which computations are assigned types of the form (A \u2192 \u22a5) \u2192 \u22a5, disrupts the term/type equivalence that is used during type checking in a dependently typed language. In this paper, we prove that type-preserving CPS translation for dependently typed languages is not not possible.", "method_label": "Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. We develop both call-by-name and call-by-value CPS translations from the Calculus of Constructions with both \u03a0 and \u03a3 types (CC) to a dependently typed target language, and prove type preservation and compiler correctness of each translation. Our target language is CC extended with an additional equivalence rule and an additional typing rule, which we prove consistent by giving a model in the extensional Calculus of Constructions. Our key observation is that we can use a CPS translation that employs answer-type polymorphism, where CPS-translated computations have type \u2200 \u03b1. (A \u2192 \u03b1) \u2192 \u03b1. This type justifies, by a free theorem, the new equality rule in our target language and allows us to recover the term/type equivalences that CPS translation disrupts.", "result_label": "In 2016, Morrisett noticed a similar problem with the standard call-by-value CPS translation for dependent functions (\u03a0 types). Finally, we conjecture that our translation extends to dependent case analysis on sums, despite the impossibility result, and provide a proof sketch.", "abstract": "Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. By preserving dependent types through each compiler pass, we could preserve source-level specifications and correctness proofs into the generated target-language programs. Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. By preserving dependent types through each compiler pass, we could preserve source-level specifications and correctness proofs into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. By preserving dependent types through each compiler pass, we could preserve source-level specifications and correctness proofs into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In 2002, Barthe and Uustalu showed that type-preserving CPS is not possible for languages such as Coq. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. In 2016, Morrisett noticed a similar problem with the standard call-by-value CPS translation for dependent functions (\u03a0 types). Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. By preserving dependent types through each compiler pass, we could preserve source-level specifications and correctness proofs into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In 2002, Barthe and Uustalu showed that type-preserving CPS is not possible for languages such as Coq. In essence, the problem is that the standard typed CPS translation by double-negation, in which computations are assigned types of the form (A \u2192 \u22a5) \u2192 \u22a5, disrupts the term/type equivalence that is used during type checking in a dependently typed language. Dependently typed languages such as Coq are used to specify and prove functional correctness of source programs, but what we ultimately need are guarantees about correctness of compiled code. By preserving dependent types through each compiler pass, we could preserve source-level specifications and correctness proofs into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In 2002, Barthe and Uustalu showed that type-preserving CPS is not possible for languages such as Coq. In essence, the problem is that the standard typed CPS translation by double-negation, in which computations are assigned types of the form (A \u2192 \u22a5) \u2192 \u22a5, disrupts the term/type equivalence that is used during type checking in a dependently typed language. In this paper, we prove that type-preserving CPS translation for dependently typed languages is not not possible. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. We develop both call-by-name and call-by-value CPS translations from the Calculus of Constructions with both \u03a0 and \u03a3 types (CC) to a dependently typed target language, and prove type preservation and compiler correctness of each translation. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. We develop both call-by-name and call-by-value CPS translations from the Calculus of Constructions with both \u03a0 and \u03a3 types (CC) to a dependently typed target language, and prove type preservation and compiler correctness of each translation. Our target language is CC extended with an additional equivalence rule and an additional typing rule, which we prove consistent by giving a model in the extensional Calculus of Constructions. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. We develop both call-by-name and call-by-value CPS translations from the Calculus of Constructions with both \u03a0 and \u03a3 types (CC) to a dependently typed target language, and prove type preservation and compiler correctness of each translation. Our target language is CC extended with an additional equivalence rule and an additional typing rule, which we prove consistent by giving a model in the extensional Calculus of Constructions. Our key observation is that we can use a CPS translation that employs answer-type polymorphism, where CPS-translated computations have type \u2200 \u03b1. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. We develop both call-by-name and call-by-value CPS translations from the Calculus of Constructions with both \u03a0 and \u03a3 types (CC) to a dependently typed target language, and prove type preservation and compiler correctness of each translation. Our target language is CC extended with an additional equivalence rule and an additional typing rule, which we prove consistent by giving a model in the extensional Calculus of Constructions. Our key observation is that we can use a CPS translation that employs answer-type polymorphism, where CPS-translated computations have type \u2200 \u03b1. (A \u2192 \u03b1) \u2192 \u03b1. Specifically, they showed that for strong dependent pairs (\u03a3 types), the standard typed call-by-name CPS is not type preserving. They further proved that for dependent case analysis on sums, a class of typed CPS translations\u2014including the standard translation\u2014is not possible. We develop both call-by-name and call-by-value CPS translations from the Calculus of Constructions with both \u03a0 and \u03a3 types (CC) to a dependently typed target language, and prove type preservation and compiler correctness of each translation. Our target language is CC extended with an additional equivalence rule and an additional typing rule, which we prove consistent by giving a model in the extensional Calculus of Constructions. Our key observation is that we can use a CPS translation that employs answer-type polymorphism, where CPS-translated computations have type \u2200 \u03b1. (A \u2192 \u03b1) \u2192 \u03b1. This type justifies, by a free theorem, the new equality rule in our target language and allows us to recover the term/type equivalences that CPS translation disrupts. In 2016, Morrisett noticed a similar problem with the standard call-by-value CPS translation for dependent functions (\u03a0 types). Finally, we conjecture that our translation extends to dependent case analysis on sums, despite the impossibility result, and provide a proof sketch."}, {"paper_id": "10021081", "adju_relevance": 2, "title": "On Strong and Default Negation in Logic Program Updates (Extended Version)", "background_label": "Existing semantics for answer-set program updates fall into two categories: either they consider only strong negation in heads of rules, or they primarily rely on default negation in heads of rules and optionally provide support for strong negation by means of a syntactic transformation.", "abstract": "Existing semantics for answer-set program updates fall into two categories: either they consider only strong negation in heads of rules, or they primarily rely on default negation in heads of rules and optionally provide support for strong negation by means of a syntactic transformation."}, {"paper_id": "423511", "adju_relevance": 2, "title": "Logic knowledge bases with two default rules", "background_label": "Logic knowledge based systems (LKBS) containing at most one form of default negation and explicit (or \u201cclassical\u201d) negation have been studied in the literature. In this paper we describe a class of LKBS containing multiple forms of default negation in addition to explicit negation. We define a semantics for these systems in terms of the well\u2010founded semantics defined by Van Gelder et al.", "method_label": "(1988) and the stable semantics introduced by Gelfond and Lifschitz (1988) and later extended to the 3\u2010valued case by Przymusinski (1991). We investigate properties of the new combined semantics and calculate the computational complexity of three main reasoning tasks for this semantics, namely existence of models, skeptical and credulous reasoning. An effective procedure to construct the collection of models characterizing the semantics of such a system is given. Applications to knowledge representation and knowledge base merging are presented.", "abstract": "Logic knowledge based systems (LKBS) containing at most one form of default negation and explicit (or \u201cclassical\u201d) negation have been studied in the literature. Logic knowledge based systems (LKBS) containing at most one form of default negation and explicit (or \u201cclassical\u201d) negation have been studied in the literature. In this paper we describe a class of LKBS containing multiple forms of default negation in addition to explicit negation. Logic knowledge based systems (LKBS) containing at most one form of default negation and explicit (or \u201cclassical\u201d) negation have been studied in the literature. In this paper we describe a class of LKBS containing multiple forms of default negation in addition to explicit negation. We define a semantics for these systems in terms of the well\u2010founded semantics defined by Van Gelder et al. (1988) and the stable semantics introduced by Gelfond and Lifschitz (1988) and later extended to the 3\u2010valued case by Przymusinski (1991). (1988) and the stable semantics introduced by Gelfond and Lifschitz (1988) and later extended to the 3\u2010valued case by Przymusinski (1991). We investigate properties of the new combined semantics and calculate the computational complexity of three main reasoning tasks for this semantics, namely existence of models, skeptical and credulous reasoning. (1988) and the stable semantics introduced by Gelfond and Lifschitz (1988) and later extended to the 3\u2010valued case by Przymusinski (1991). We investigate properties of the new combined semantics and calculate the computational complexity of three main reasoning tasks for this semantics, namely existence of models, skeptical and credulous reasoning. An effective procedure to construct the collection of models characterizing the semantics of such a system is given. (1988) and the stable semantics introduced by Gelfond and Lifschitz (1988) and later extended to the 3\u2010valued case by Przymusinski (1991). We investigate properties of the new combined semantics and calculate the computational complexity of three main reasoning tasks for this semantics, namely existence of models, skeptical and credulous reasoning. An effective procedure to construct the collection of models characterizing the semantics of such a system is given. Applications to knowledge representation and knowledge base merging are presented."}, {"paper_id": "1065884", "adju_relevance": 1, "title": "Why Not?", "background_label": "As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. Yet most database access is via limited interfaces that deprive end users of the ability to alter their query in any way to garner better understanding of the dataset and result set. Users are unable to question why a particular data item is Not in the result set of a given query.", "method_label": "In this work, we develop a model for answers to WHY NOT?", "abstract": "As humans, we have expectations for the results of any action, e.g. As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. Yet most database access is via limited interfaces that deprive end users of the ability to alter their query in any way to garner better understanding of the dataset and result set. As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. Yet most database access is via limited interfaces that deprive end users of the ability to alter their query in any way to garner better understanding of the dataset and result set. Users are unable to question why a particular data item is Not in the result set of a given query. In this work, we develop a model for answers to WHY NOT?"}, {"paper_id": "251940", "adju_relevance": 1, "title": "Semantic Composition via Probabilistic Model Theory", "background_label": "Semantic composition remains an open problem for vector space models of semantics.", "abstract": "Semantic composition remains an open problem for vector space models of semantics."}, {"paper_id": "294175", "adju_relevance": 1, "title": "On Compositional Semantics", "background_label": "We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous.", "method_label": "We then show that when one requires compositional semantics to be \"systematic\" (that is the meaning function cannot be arbitrary, but must belong to some class), one can easily distinguish between compositional and non-compositional semantics. We also present an example of a simple grammar for which there is no \"systematic\" compositional semantics. This implies that it is possible to distinguish \"good\" and \"bad\" grammars on the basis of whether they can have compositional semantics.", "result_label": "As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic comparisons of different systems of grammars and NLU programs.", "abstract": "We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous. We then show that when one requires compositional semantics to be \"systematic\" (that is the meaning function cannot be arbitrary, but must belong to some class), one can easily distinguish between compositional and non-compositional semantics. We then show that when one requires compositional semantics to be \"systematic\" (that is the meaning function cannot be arbitrary, but must belong to some class), one can easily distinguish between compositional and non-compositional semantics. We also present an example of a simple grammar for which there is no \"systematic\" compositional semantics. We then show that when one requires compositional semantics to be \"systematic\" (that is the meaning function cannot be arbitrary, but must belong to some class), one can easily distinguish between compositional and non-compositional semantics. We also present an example of a simple grammar for which there is no \"systematic\" compositional semantics. This implies that it is possible to distinguish \"good\" and \"bad\" grammars on the basis of whether they can have compositional semantics. As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic comparisons of different systems of grammars and NLU programs."}, {"paper_id": "3080804", "adju_relevance": 1, "title": "Bialgebraic Semantics for Logic Programming", "background_label": "Bialgebrae provide an abstract framework encompassing the semantics of different kinds of computational models.", "abstract": "Bialgebrae provide an abstract framework encompassing the semantics of different kinds of computational models."}, {"paper_id": "5471801", "adju_relevance": 1, "title": "Entailment, Intensionality And Text Understanding", "background_label": "We argue that the detection of entailment and contradiction relations between texts is a minimal metric for the evaluation of text understanding systems. Intensionality, which is widespread in natural language, raises a number of detection issues that cannot be brushed aside.", "method_label": "We describe a contexted clausal representation, derived from approaches in formal semantics, that permits an extended range of intensional entailments and contradictions to be tractably detected.", "abstract": "We argue that the detection of entailment and contradiction relations between texts is a minimal metric for the evaluation of text understanding systems. We argue that the detection of entailment and contradiction relations between texts is a minimal metric for the evaluation of text understanding systems. Intensionality, which is widespread in natural language, raises a number of detection issues that cannot be brushed aside. We describe a contexted clausal representation, derived from approaches in formal semantics, that permits an extended range of intensional entailments and contradictions to be tractably detected."}, {"paper_id": "15616495", "adju_relevance": 1, "title": "Estimating Linear Models for Compositional Distributional Semantics", "background_label": "AbstractIn distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored.", "abstract": "AbstractIn distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. AbstractIn distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored."}, {"paper_id": "17981782", "adju_relevance": 1, "title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "background_label": "AbstractModelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing.", "abstract": "AbstractModelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing."}, {"paper_id": "455112", "adju_relevance": 1, "title": "Domain and Function: A Dual-Space Model of Semantic Relations and Compositions", "background_label": "Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings).", "method_label": "In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. The function of kennel is similar to the function of house (the function of shelters).", "result_label": "By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.", "abstract": "Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. Carpenter and wood share the same domain, the domain of carpentry. Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics."}, {"paper_id": "20687969", "adju_relevance": 1, "title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics", "background_label": "This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text. This thesis shows how this approach can be theoretically extended and practically implemented to produce concrete compositional distributional models of natural language semantics. It furthermore demonstrates that such models can perform on par with, or better than, other competing approaches in the field of natural language processing.", "method_label": "This thesis focuses on a particular approach to this compositionality problem, namely using the categorical framework developed by Coecke, Sadrzadeh, and Clark, which combines syntactic analysis formalisms with distributional semantic representations of meaning to produce syntactically motivated composition operations.", "abstract": "This thesis is about the problem of compositionality in distributional semantics. This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text. This thesis focuses on a particular approach to this compositionality problem, namely using the categorical framework developed by Coecke, Sadrzadeh, and Clark, which combines syntactic analysis formalisms with distributional semantic representations of meaning to produce syntactically motivated composition operations. This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text. This thesis shows how this approach can be theoretically extended and practically implemented to produce concrete compositional distributional models of natural language semantics. This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text. This thesis shows how this approach can be theoretically extended and practically implemented to produce concrete compositional distributional models of natural language semantics. It furthermore demonstrates that such models can perform on par with, or better than, other competing approaches in the field of natural language processing."}, {"paper_id": "4954256", "adju_relevance": 1, "title": "Distributional Semantics in Use", "background_label": "AbstractIn this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. However, they need to be extended in order to account for language use in a discourse or dialogue context.", "result_label": "We discuss phenomena that the new generation of distributional semantic models should capture, and propose concrete tasks on which they could be tested.", "abstract": "AbstractIn this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. AbstractIn this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. AbstractIn this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. However, they need to be extended in order to account for language use in a discourse or dialogue context. We discuss phenomena that the new generation of distributional semantic models should capture, and propose concrete tasks on which they could be tested."}, {"paper_id": "18631267", "adju_relevance": 1, "title": "From Logical to Distributional Models", "background_label": "The paper relates two variants of semantic models for natural language, logical functional models and compositional distributional vector space models, by transferring the logic and reasoning from the logical to the distributional models.", "method_label": "The geometrical operations of quantum logic are reformulated as algebraic operations on vectors. A map from functional models to vector space models makes it possible to compare the meaning of sentences word by word.", "abstract": "The paper relates two variants of semantic models for natural language, logical functional models and compositional distributional vector space models, by transferring the logic and reasoning from the logical to the distributional models. The geometrical operations of quantum logic are reformulated as algebraic operations on vectors. The geometrical operations of quantum logic are reformulated as algebraic operations on vectors. A map from functional models to vector space models makes it possible to compare the meaning of sentences word by word."}, {"paper_id": "36331005", "adju_relevance": 1, "title": "Reified Input/Output logic: Combining Input/Output logic and Reification to represent norms coming from existing legislation", "background_label": "The latter is a wide-coverage logic for Natural Language Semantics able to handle a fairly large set of linguistic phenomena into a simple logical formalism. The result is a new framework that we will call 'reified Input/Output logic'.", "abstract": " The latter is a wide-coverage logic for Natural Language Semantics able to handle a fairly large set of linguistic phenomena into a simple logical formalism. The latter is a wide-coverage logic for Natural Language Semantics able to handle a fairly large set of linguistic phenomena into a simple logical formalism. The result is a new framework that we will call 'reified Input/Output logic'."}, {"paper_id": "15636957", "adju_relevance": 1, "title": "Negation in the Head of CP-logic Rules", "background_label": "CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. In other words, these logics lack the epistemic component of ASP, in which interpretations represent the beliefs or knowledge of a rational agent. Consequently, neither CP-logic nor FO(ID) have the need for two kinds of negations: there is only one negation, and its meaning is that of objective falsehood. Nevertheless, the formal semantics of this objective negation is mathematically more similar to ASP's negation-as-failure than to its classical negation. The reason is that both CP-logic and FO(ID) have a constructive semantics in which all atoms start out as false, and may only become true as the result of a rule application.", "abstract": "CP-logic is a probabilistic extension of the logic FO(ID). CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. In other words, these logics lack the epistemic component of ASP, in which interpretations represent the beliefs or knowledge of a rational agent. CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. In other words, these logics lack the epistemic component of ASP, in which interpretations represent the beliefs or knowledge of a rational agent. Consequently, neither CP-logic nor FO(ID) have the need for two kinds of negations: there is only one negation, and its meaning is that of objective falsehood. CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. In other words, these logics lack the epistemic component of ASP, in which interpretations represent the beliefs or knowledge of a rational agent. Consequently, neither CP-logic nor FO(ID) have the need for two kinds of negations: there is only one negation, and its meaning is that of objective falsehood. Nevertheless, the formal semantics of this objective negation is mathematically more similar to ASP's negation-as-failure than to its classical negation. CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both of these logics adhere to a Tarskian informal semantics, in which interpretations represent objective states-of-affairs. In other words, these logics lack the epistemic component of ASP, in which interpretations represent the beliefs or knowledge of a rational agent. Consequently, neither CP-logic nor FO(ID) have the need for two kinds of negations: there is only one negation, and its meaning is that of objective falsehood. Nevertheless, the formal semantics of this objective negation is mathematically more similar to ASP's negation-as-failure than to its classical negation. The reason is that both CP-logic and FO(ID) have a constructive semantics in which all atoms start out as false, and may only become true as the result of a rule application."}, {"paper_id": "2816192", "adju_relevance": 1, "title": "Combined Distributional and Logical Semantics", "background_label": "We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words.", "method_label": "We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. Different senses of a word are disambiguated based on their induced types.", "result_label": "We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite.", "abstract": "We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. Different senses of a word are disambiguated based on their induced types. We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite."}, {"paper_id": "1588782", "adju_relevance": 1, "title": "A Structured Vector Space Model for Word Meaning in Context", "background_label": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account.", "method_label": "We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context.", "result_label": "In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."}, {"paper_id": "26901423", "adju_relevance": 1, "title": "Composition in distributional models of semantics.", "background_label": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words.", "abstract": "Vector-based models of word meaning have become increasingly popular in cognitive science. Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words."}, {"paper_id": "2158386", "adju_relevance": 1, "title": "Montague Meets Markov: Deep Semantics with Probabilistic Logical Form", "background_label": "AbstractWe combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs).", "method_label": "We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives.", "result_label": "We also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance.", "abstract": "AbstractWe combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance."}, {"paper_id": "326903", "adju_relevance": 1, "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "background_label": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al.", "method_label": "(arXiv:1003.4394v1 [cs.CL]) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.", "result_label": "Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.", "abstract": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (arXiv:1003.4394v1 [cs.CL]) using data from the BNC and evaluate it. (arXiv:1003.4394v1 [cs.CL]) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. (arXiv:1003.4394v1 [cs.CL]) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model."}, {"paper_id": "23273296", "adju_relevance": 1, "title": "Distributed Representations for Compositional Semantics", "background_label": "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level.", "abstract": "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level."}, {"paper_id": "370914", "adju_relevance": 1, "title": "Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors", "background_label": "The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature.", "abstract": "The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature."}, {"paper_id": "3264852", "adju_relevance": 1, "title": "Reasoning with Higher-Order Abstract Syntax in a Logical Framework", "background_label": "Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction.", "method_label": "In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation.", "abstract": "Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation."}, {"paper_id": "14298291", "adju_relevance": 1, "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "background_label": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.", "abstract": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning."}, {"paper_id": "198953482", "adju_relevance": 1, "title": "Revisiting Explicit Negation in Answer Set Programming", "background_label": "A common feature in Answer Set Programming is the use of a second negation, stronger than default negation and sometimes called explicit, strong or classical negation. This explicit negation is normally used in front of atoms, rather than allowing its use as a regular operator. In this paper we consider the arbitrary combination of explicit negation with nested expressions, as those defined by Lifschitz, Tang and Turner.", "method_label": "We extend the concept of reduct for this new syntax and then prove that it can be captured by an extension of Equilibrium Logic with this second negation. We study some properties of this variant and compare to the already known combination of Equilibrium Logic with Nelson's strong negation.", "result_label": "Under consideration for acceptance in TPLP.", "abstract": "A common feature in Answer Set Programming is the use of a second negation, stronger than default negation and sometimes called explicit, strong or classical negation. A common feature in Answer Set Programming is the use of a second negation, stronger than default negation and sometimes called explicit, strong or classical negation. This explicit negation is normally used in front of atoms, rather than allowing its use as a regular operator. A common feature in Answer Set Programming is the use of a second negation, stronger than default negation and sometimes called explicit, strong or classical negation. This explicit negation is normally used in front of atoms, rather than allowing its use as a regular operator. In this paper we consider the arbitrary combination of explicit negation with nested expressions, as those defined by Lifschitz, Tang and Turner. We extend the concept of reduct for this new syntax and then prove that it can be captured by an extension of Equilibrium Logic with this second negation. We extend the concept of reduct for this new syntax and then prove that it can be captured by an extension of Equilibrium Logic with this second negation. We study some properties of this variant and compare to the already known combination of Equilibrium Logic with Nelson's strong negation. Under consideration for acceptance in TPLP."}, {"paper_id": "9338281", "adju_relevance": 1, "title": "Challenges for Distributional Compositional Semantics", "background_label": "This paper summarises the current state-of-the art in the study of compositionality in distributional semantics, and major challenges for this area.", "method_label": "We single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory. Once suitable theories have been developed, algorithms will be needed to apply the theory to tasks.", "result_label": "Evaluation is a major problem; we single out application to recognising textual entailment and machine translation for this purpose.", "abstract": "This paper summarises the current state-of-the art in the study of compositionality in distributional semantics, and major challenges for this area. We single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory. We single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory. Once suitable theories have been developed, algorithms will be needed to apply the theory to tasks. Evaluation is a major problem; we single out application to recognising textual entailment and machine translation for this purpose."}, {"paper_id": "9727714", "adju_relevance": 1, "title": "A relatedness benchmark to test the role of determiners in compositional distributional semantics", "background_label": "AbstractDistributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words.", "abstract": "AbstractDistributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words."}, {"paper_id": "1922970", "adju_relevance": 1, "title": "What is in a text, what isn't, and what this has to do with lexical semantics", "background_label": "AbstractThis paper queries which aspects of lexical semantics can reasonably be expected to be modelled by corpus-based theories such as distributional semantics or techniques such as ontology extraction. We argue that a full lexical semantics theory must take into account the extensional potential of words.", "result_label": "We investigate to which extent corpora provide the necessary data to model this information and suggest that it may be partly learnable from text-based distributions, partly inferred from annotated data, using the insight that a concept's features are extensionally interdependent.", "abstract": "AbstractThis paper queries which aspects of lexical semantics can reasonably be expected to be modelled by corpus-based theories such as distributional semantics or techniques such as ontology extraction. AbstractThis paper queries which aspects of lexical semantics can reasonably be expected to be modelled by corpus-based theories such as distributional semantics or techniques such as ontology extraction. We argue that a full lexical semantics theory must take into account the extensional potential of words. We investigate to which extent corpora provide the necessary data to model this information and suggest that it may be partly learnable from text-based distributions, partly inferred from annotated data, using the insight that a concept's features are extensionally interdependent."}, {"paper_id": "2920392", "adju_relevance": 1, "title": "Integrating Logical Representations with Probabilistic Information using Markov Logic", "background_label": "AbstractFirst-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning.", "abstract": "AbstractFirst-order logic provides a powerful and flexible mechanism for representing natural language semantics. AbstractFirst-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning."}, {"paper_id": "806709", "adju_relevance": 1, "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "background_label": "AbstractSingle-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language.", "abstract": "AbstractSingle-word vector space models have been very successful at learning lexical information. AbstractSingle-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language."}, {"paper_id": "7202818", "adju_relevance": 1, "title": "Semantics, Modelling, and the Problem of Representation of Meaning -- a Brief Survey of Recent Literature", "background_label": "Over the past 50 years many have debated what representation should be used to capture the meaning of natural language utterances. Recently new needs of such representations have been raised in research.", "abstract": "Over the past 50 years many have debated what representation should be used to capture the meaning of natural language utterances. Over the past 50 years many have debated what representation should be used to capture the meaning of natural language utterances. Recently new needs of such representations have been raised in research."}, {"paper_id": "11691908", "adju_relevance": 1, "title": "A Unified Sentence Space for Categorical Distributional-Compositional Semantics: Theory and Experiments", "background_label": "ABSTRACTThis short paper summarizes a faithful implementation of the categorical framework of Coecke et al.", "abstract": "ABSTRACTThis short paper summarizes a faithful implementation of the categorical framework of Coecke et al."}, {"paper_id": "12474263", "adju_relevance": 1, "title": "Bringing Machine Learning and Compositional Semantics Together", "background_label": "Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases.", "abstract": "Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases."}, {"paper_id": "36117198", "adju_relevance": 0, "title": "DeepMind_Commentary", "background_label": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy.", "abstract": "We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential. However, we favor an approach that centers on one additional ingredient: autonomy."}, {"paper_id": "29792562", "adju_relevance": 0, "title": "Inconsistency-Tolerant Query Answering: Rationality Properties and Computational Complexity Analysis", "background_label": "Abstract. Generalising the state of the art, an inconsistency-tolerant semantics can be seen as a couple composed of a modifier operator and an inference strategy.", "abstract": "Abstract. Abstract. Generalising the state of the art, an inconsistency-tolerant semantics can be seen as a couple composed of a modifier operator and an inference strategy."}, {"paper_id": "10181469", "adju_relevance": 0, "title": "\"Sometime\" is sometimes \"not never\": on the temporal logic of programs", "background_label": "Pnueli [15] has recently introduced the idea of using temporal logic [18] as the logical basis for proving correctness properties of concurrent programs. This has permitted an elegant unifying formulation of previous proof methods.", "abstract": "Pnueli [15] has recently introduced the idea of using temporal logic [18] as the logical basis for proving correctness properties of concurrent programs. Pnueli [15] has recently introduced the idea of using temporal logic [18] as the logical basis for proving correctness properties of concurrent programs. This has permitted an elegant unifying formulation of previous proof methods."}, {"paper_id": "16595363", "adju_relevance": 0, "title": "A Gaussian input is not too bad", "background_label": "We consider the problem of choosing a robust input for communicating over an input constrained additive-noise channel where the noise distribution is arbitrary. We show that the mutual information rate achievable using a white Gaussian input never incurs a loss of more than half a bit per sample with respect to the power constrained capacity.", "method_label": "For comparison, for the family of colored Gaussian noise channels a white Gaussian input loses at most log(e)/2e/spl ap/0.265 bit per sample with respect to the optimum water-pouring solution. For general input constraints, we derive a formula for choosing the best input in the min-max capacity loss (bound) sense.", "result_label": "The bound on the capacity loss is tight for pulse position modulation (PPM) in the presence of a bursty jammer.", "abstract": "We consider the problem of choosing a robust input for communicating over an input constrained additive-noise channel where the noise distribution is arbitrary. We consider the problem of choosing a robust input for communicating over an input constrained additive-noise channel where the noise distribution is arbitrary. We show that the mutual information rate achievable using a white Gaussian input never incurs a loss of more than half a bit per sample with respect to the power constrained capacity. For comparison, for the family of colored Gaussian noise channels a white Gaussian input loses at most log(e)/2e/spl ap/0.265 bit per sample with respect to the optimum water-pouring solution. For comparison, for the family of colored Gaussian noise channels a white Gaussian input loses at most log(e)/2e/spl ap/0.265 bit per sample with respect to the optimum water-pouring solution. For general input constraints, we derive a formula for choosing the best input in the min-max capacity loss (bound) sense. The bound on the capacity loss is tight for pulse position modulation (PPM) in the presence of a bursty jammer."}, {"paper_id": "25845573", "adju_relevance": 0, "title": "Wide-Coverage Semantic Analysis with Boxer", "background_label": "Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).", "method_label": "Used together with the CC (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren't; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases.", "result_label": "Boxer is distributed with the C&C tools and freely available for research purposes.", "abstract": "Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the CC (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren't; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C&C tools and freely available for research purposes."}, {"paper_id": "7603943", "adju_relevance": 0, "title": "To share or not to share: that is not the question.", "background_label": "There is an increasing awareness of the power of integrating multiple sources of data to accelerate biomedical discoveries. Some even argue that it is unethical not to share data that could be used for the public good. However, the challenges involved in sharing clinical and biomedical data are seldom discussed.", "method_label": "I briefly review some of these challenges and provide an overview of how they are being addressed by the scientific community.", "abstract": "There is an increasing awareness of the power of integrating multiple sources of data to accelerate biomedical discoveries. There is an increasing awareness of the power of integrating multiple sources of data to accelerate biomedical discoveries. Some even argue that it is unethical not to share data that could be used for the public good. There is an increasing awareness of the power of integrating multiple sources of data to accelerate biomedical discoveries. Some even argue that it is unethical not to share data that could be used for the public good. However, the challenges involved in sharing clinical and biomedical data are seldom discussed. I briefly review some of these challenges and provide an overview of how they are being addressed by the scientific community."}, {"paper_id": "989810", "adju_relevance": 0, "title": "From compositional to systematic semantics", "background_label": "We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous.", "method_label": "We then show that when compositional semantics is required to be\"systematic\"(that is, the meaning function cannot be arbitrary, but must belong to some class), it is possible to distinguish between compositional and non-compositional semantics.", "result_label": "As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic formal comparisons of different systems of grammars.", "abstract": "We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous. We then show that when compositional semantics is required to be\"systematic\"(that is, the meaning function cannot be arbitrary, but must belong to some class), it is possible to distinguish between compositional and non-compositional semantics. As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic formal comparisons of different systems of grammars."}, {"paper_id": "116918892", "adju_relevance": 0, "title": "Bicat is not triequivalent to Gray", "background_label": "Bicat is the tricategory of bicategories, homomorphisms, pseudonatural transformations, and modifications. Gray is the subtricategory of 2-categories, 2-functors, pseudonatural transformations, and modifications.", "result_label": "We show that these two tricategories are not triequivalent.", "abstract": "Bicat is the tricategory of bicategories, homomorphisms, pseudonatural transformations, and modifications. Bicat is the tricategory of bicategories, homomorphisms, pseudonatural transformations, and modifications. Gray is the subtricategory of 2-categories, 2-functors, pseudonatural transformations, and modifications. We show that these two tricategories are not triequivalent."}, {"paper_id": "1144461", "adju_relevance": 0, "title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "background_label": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research.", "method_label": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched.", "abstract": "How do people know as much as they do with as little information as they get? How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."}, {"paper_id": "15703563", "adju_relevance": 0, "title": "Disciplined Heterogeneous Modeling", "background_label": "Complex systems demand diversity in the modeling mechanisms. One way to deal with a diversity of requirements is to create flexible modeling frameworks that can be adapted to cover the field of interest. The downside of this approach is a weakening of the semantics of the modeling frameworks that compromises interoperability, understandability, and analyzability of the models.", "abstract": "Complex systems demand diversity in the modeling mechanisms. Complex systems demand diversity in the modeling mechanisms. One way to deal with a diversity of requirements is to create flexible modeling frameworks that can be adapted to cover the field of interest. Complex systems demand diversity in the modeling mechanisms. One way to deal with a diversity of requirements is to create flexible modeling frameworks that can be adapted to cover the field of interest. The downside of this approach is a weakening of the semantics of the modeling frameworks that compromises interoperability, understandability, and analyzability of the models."}, {"paper_id": "21699063", "adju_relevance": 0, "title": "Choice is Not True or False: The Domain of Rhetorical Argumentation", "background_label": "Leading contemporary argumentation theories such as those of Ralph Johnson, van Eemeren and Houtlosser, and Tindale, in their attempt to address rhetoric, tend to define rhetorical argumentation with reference to (a) the rhetorical arguer\u2019s goal (to persuade effectively), and (b) the means he employs to do so. However, a central strand in the rhetorical tradition itself, led by Aristotle, and arguably the dominant view, sees rhetorical argumentation as defined with reference to the domain of issues discussed. On that view, the domain of rhetorical argumentation is centered on choice of action in the civic sphere, and the distinctive nature of issues in this domain is considered crucial.", "method_label": "Hence, argumentation theories such as those discussed, insofar as they do not see rhetoric as defined by its distinctive domain, apply an understanding of rhetoric that is historically inadequate.", "result_label": "It is further suggested that theories adopting this understanding of rhetoric risk ignoring important distinctive features of argumentation about action.", "abstract": "Leading contemporary argumentation theories such as those of Ralph Johnson, van Eemeren and Houtlosser, and Tindale, in their attempt to address rhetoric, tend to define rhetorical argumentation with reference to (a) the rhetorical arguer\u2019s goal (to persuade effectively), and (b) the means he employs to do so. Leading contemporary argumentation theories such as those of Ralph Johnson, van Eemeren and Houtlosser, and Tindale, in their attempt to address rhetoric, tend to define rhetorical argumentation with reference to (a) the rhetorical arguer\u2019s goal (to persuade effectively), and (b) the means he employs to do so. However, a central strand in the rhetorical tradition itself, led by Aristotle, and arguably the dominant view, sees rhetorical argumentation as defined with reference to the domain of issues discussed. Leading contemporary argumentation theories such as those of Ralph Johnson, van Eemeren and Houtlosser, and Tindale, in their attempt to address rhetoric, tend to define rhetorical argumentation with reference to (a) the rhetorical arguer\u2019s goal (to persuade effectively), and (b) the means he employs to do so. However, a central strand in the rhetorical tradition itself, led by Aristotle, and arguably the dominant view, sees rhetorical argumentation as defined with reference to the domain of issues discussed. On that view, the domain of rhetorical argumentation is centered on choice of action in the civic sphere, and the distinctive nature of issues in this domain is considered crucial. Hence, argumentation theories such as those discussed, insofar as they do not see rhetoric as defined by its distinctive domain, apply an understanding of rhetoric that is historically inadequate. It is further suggested that theories adopting this understanding of rhetoric risk ignoring important distinctive features of argumentation about action."}, {"paper_id": "31981096", "adju_relevance": 0, "title": "Probabilistic Semantics and Program Analysis", "background_label": "Abstract.", "method_label": "The aims of these lecture notes are two-fold: (i) we investigate the relation between the operational semantics of probabilistic programming languages and Discrete Time Markov Chains (DTMCs), and (ii) we present a framework for probabilistic program analysis which is inspired by the classical Abstract Interpretation framework by Cousot & Cousot and which we introduced as Probabilistic Abstract Interpretation (PAI) in [1] . The link between programming languages and DTMCs is the construction of a so-called Linear Operator semantics (LOS) in a syntax-directed or compositional way. The main element in this construction is the use of tensor product to combine information about different aspects of a program.", "result_label": "Although this inevitably results in a combinatorial explosion of the size of the semantics of program, the PAI approach allows us to keep some control and to obtain reasonably sized abstract models.", "abstract": "Abstract. The aims of these lecture notes are two-fold: (i) we investigate the relation between the operational semantics of probabilistic programming languages and Discrete Time Markov Chains (DTMCs), and (ii) we present a framework for probabilistic program analysis which is inspired by the classical Abstract Interpretation framework by Cousot & Cousot and which we introduced as Probabilistic Abstract Interpretation (PAI) in [1] . The aims of these lecture notes are two-fold: (i) we investigate the relation between the operational semantics of probabilistic programming languages and Discrete Time Markov Chains (DTMCs), and (ii) we present a framework for probabilistic program analysis which is inspired by the classical Abstract Interpretation framework by Cousot & Cousot and which we introduced as Probabilistic Abstract Interpretation (PAI) in [1] . The link between programming languages and DTMCs is the construction of a so-called Linear Operator semantics (LOS) in a syntax-directed or compositional way. The aims of these lecture notes are two-fold: (i) we investigate the relation between the operational semantics of probabilistic programming languages and Discrete Time Markov Chains (DTMCs), and (ii) we present a framework for probabilistic program analysis which is inspired by the classical Abstract Interpretation framework by Cousot & Cousot and which we introduced as Probabilistic Abstract Interpretation (PAI) in [1] . The link between programming languages and DTMCs is the construction of a so-called Linear Operator semantics (LOS) in a syntax-directed or compositional way. The main element in this construction is the use of tensor product to combine information about different aspects of a program. Although this inevitably results in a combinatorial explosion of the size of the semantics of program, the PAI approach allows us to keep some control and to obtain reasonably sized abstract models."}, {"paper_id": "14964272", "adju_relevance": 0, "title": "Clutching Is Not (Necessarily) the Enemy", "background_label": "Clutching is usually assumed to be triggered by a lack of physical space and detrimental to pointing performance.", "method_label": "We conduct a controlled experiment using a laptop trackpad where the effect of clutching on pointing performance is dissociated from the effects of control-to-display transfer functions. Participants performed a series of target acquisition tasks using typical cursor acceleration functions with and without clutching.", "result_label": "All pointing tasks were feasible without clutching, but clutch-less movements were harder to perform, caused more errors, required more preparation time, and were not faster than clutch-enabled movements.", "abstract": "Clutching is usually assumed to be triggered by a lack of physical space and detrimental to pointing performance. We conduct a controlled experiment using a laptop trackpad where the effect of clutching on pointing performance is dissociated from the effects of control-to-display transfer functions. We conduct a controlled experiment using a laptop trackpad where the effect of clutching on pointing performance is dissociated from the effects of control-to-display transfer functions. Participants performed a series of target acquisition tasks using typical cursor acceleration functions with and without clutching. All pointing tasks were feasible without clutching, but clutch-less movements were harder to perform, caused more errors, required more preparation time, and were not faster than clutch-enabled movements."}, {"paper_id": "61274663", "adju_relevance": 0, "title": "Software is Not Fragile", "background_label": "Trying all simple changes (first order mutations) to executed C, C++ and CUDA source code shows software engineering artefacts are more robust than is often assumed. Of those that compile, up to 89 % run without error. Indeed a few mutants are improvements. Program fitness landscapes are smoother.", "result_label": "Analysis of these programs, a parallel nVidia GPGPU kernel, all CUDA samples and the GNU C library shows many lines of code and integer values are repeated and may follow Zipf\u2019s law.", "abstract": "Trying all simple changes (first order mutations) to executed C, C++ and CUDA source code shows software engineering artefacts are more robust than is often assumed. Trying all simple changes (first order mutations) to executed C, C++ and CUDA source code shows software engineering artefacts are more robust than is often assumed. Of those that compile, up to 89 % run without error. Trying all simple changes (first order mutations) to executed C, C++ and CUDA source code shows software engineering artefacts are more robust than is often assumed. Of those that compile, up to 89 % run without error. Indeed a few mutants are improvements. Trying all simple changes (first order mutations) to executed C, C++ and CUDA source code shows software engineering artefacts are more robust than is often assumed. Of those that compile, up to 89 % run without error. Indeed a few mutants are improvements. Program fitness landscapes are smoother. Analysis of these programs, a parallel nVidia GPGPU kernel, all CUDA samples and the GNU C library shows many lines of code and integer values are repeated and may follow Zipf\u2019s law."}, {"paper_id": "3107589", "adju_relevance": 0, "title": "A Corpus-based Toy Model for DisCoCat", "background_label": "The categorical compositional distributional (DisCoCat) model of meaning rigorously connects distributional semantics and pregroup grammars, and has found a variety of applications in computational linguistics. From a more abstract standpoint, the DisCoCat paradigm predicates the construction of a mapping from syntax to categorical semantics.", "abstract": "The categorical compositional distributional (DisCoCat) model of meaning rigorously connects distributional semantics and pregroup grammars, and has found a variety of applications in computational linguistics. The categorical compositional distributional (DisCoCat) model of meaning rigorously connects distributional semantics and pregroup grammars, and has found a variety of applications in computational linguistics. From a more abstract standpoint, the DisCoCat paradigm predicates the construction of a mapping from syntax to categorical semantics."}, {"paper_id": "1877316", "adju_relevance": 0, "title": "An Algebraic View on the Semantics of model Composition", "background_label": "Due to the increased complexity of software development projects more and more systems are described by models. The sheer size makes it impractical to describe these systems by a single model. Instead many models are developed that provide several complementary views on the system to be developed. This however leads to a need for compositional models.", "abstract": "Due to the increased complexity of software development projects more and more systems are described by models. Due to the increased complexity of software development projects more and more systems are described by models. The sheer size makes it impractical to describe these systems by a single model. Due to the increased complexity of software development projects more and more systems are described by models. The sheer size makes it impractical to describe these systems by a single model. Instead many models are developed that provide several complementary views on the system to be developed. Due to the increased complexity of software development projects more and more systems are described by models. The sheer size makes it impractical to describe these systems by a single model. Instead many models are developed that provide several complementary views on the system to be developed. This however leads to a need for compositional models."}, {"paper_id": "73572517", "adju_relevance": 0, "title": "Mental representations as simulated affordances: not intrinsic, not so much functional, but intentionally-driven", "abstract": ""}, {"paper_id": "57558611", "adju_relevance": 0, "title": "Unrest Assured: Why Unipolarity Is Not Peaceful", "background_label": "The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. Second, if the unipole remains engaged in the world, those minor powers that decide not to accommodate it will be unable to find a great power sponsor. Placed in this situation of extreme self-help, they will try to revise the status quo in their favor, a dynamic that is likely to trigger conflict with the unipole. Therefore, neither the structure of a unipolar world nor U.S. strategic choices clearly benefit the overall prospects for peace. For the world as a whole, unipolarity makes conflict likely. For the unipole, it presents a difficult choice between disengagement and frequent conflict.", "result_label": "First, the unipole may disengage from a particular region, thus removing constraints on regional conflicts. In neither case will the unipole be able to easily convert its power into favorable outcomes peacefully.", "abstract": "The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. First, the unipole may disengage from a particular region, thus removing constraints on regional conflicts. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. Second, if the unipole remains engaged in the world, those minor powers that decide not to accommodate it will be unable to find a great power sponsor. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. Second, if the unipole remains engaged in the world, those minor powers that decide not to accommodate it will be unable to find a great power sponsor. Placed in this situation of extreme self-help, they will try to revise the status quo in their favor, a dynamic that is likely to trigger conflict with the unipole. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. Second, if the unipole remains engaged in the world, those minor powers that decide not to accommodate it will be unable to find a great power sponsor. Placed in this situation of extreme self-help, they will try to revise the status quo in their favor, a dynamic that is likely to trigger conflict with the unipole. Therefore, neither the structure of a unipolar world nor U.S. strategic choices clearly benefit the overall prospects for peace. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. Second, if the unipole remains engaged in the world, those minor powers that decide not to accommodate it will be unable to find a great power sponsor. Placed in this situation of extreme self-help, they will try to revise the status quo in their favor, a dynamic that is likely to trigger conflict with the unipole. Therefore, neither the structure of a unipolar world nor U.S. strategic choices clearly benefit the overall prospects for peace. For the world as a whole, unipolarity makes conflict likely. The United States has been at war for thirteen of the twenty-two years since the Cold War ended and the world became unipolar. Still, the consensual view among international relations theorists is that unipolarity is peaceful. They base this view on two assumptions: first, the unipole will guarantee the global status quo and, second, no state will balance against it. Both assumptions are problematic. Second, if the unipole remains engaged in the world, those minor powers that decide not to accommodate it will be unable to find a great power sponsor. Placed in this situation of extreme self-help, they will try to revise the status quo in their favor, a dynamic that is likely to trigger conflict with the unipole. Therefore, neither the structure of a unipolar world nor U.S. strategic choices clearly benefit the overall prospects for peace. For the world as a whole, unipolarity makes conflict likely. For the unipole, it presents a difficult choice between disengagement and frequent conflict. First, the unipole may disengage from a particular region, thus removing constraints on regional conflicts. In neither case will the unipole be able to easily convert its power into favorable outcomes peacefully."}, {"paper_id": "7788178", "adju_relevance": 0, "title": "Cutting Recursive Autoencoder Trees", "background_label": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense.", "method_label": "We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text.", "result_label": "We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment.", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment."}, {"paper_id": "22870263", "adju_relevance": 0, "title": "Seeing is not believing", "background_label": "Altering digital imagery is now ubiquitous. People have come to expect it in the fashion and entertainment world, where airbrushing blemishes and wrinkles away is routine. And anyone surfing the Web is routinely subjected to crude photographic mashups like the Palin hoax, whose creators clearly aren't interested in realism but in whatever titillation or outrage they can generate. Even as experts continue to develop techniques for exposing photographic frauds, new techniques for creating better and harder-to-detect fakes are also evolving. As in the battle against spam and computer viruses, it seems inevitable that the arms race between the forger and the forensic analyst will continue to escalate, with no clear victor. Improved image forensics will never be able to eradicate or prevent digital tampering, but these techniques can make it more time-consuming and difficult for forgers to ply their trade.", "result_label": "Tomorrow's technology will almost certainly enable digital manipulations that today seem unimaginable, and the science of digital forensics will have to work hard to keep pace. It is my hope that these new techniques, along with a greater awareness of the technological possibilities and sensible updates in policy and law, will help the media, the courts, and the public contend with the exciting but often baffling events of our digital age.", "abstract": "Altering digital imagery is now ubiquitous. Altering digital imagery is now ubiquitous. People have come to expect it in the fashion and entertainment world, where airbrushing blemishes and wrinkles away is routine. Altering digital imagery is now ubiquitous. People have come to expect it in the fashion and entertainment world, where airbrushing blemishes and wrinkles away is routine. And anyone surfing the Web is routinely subjected to crude photographic mashups like the Palin hoax, whose creators clearly aren't interested in realism but in whatever titillation or outrage they can generate. Altering digital imagery is now ubiquitous. People have come to expect it in the fashion and entertainment world, where airbrushing blemishes and wrinkles away is routine. And anyone surfing the Web is routinely subjected to crude photographic mashups like the Palin hoax, whose creators clearly aren't interested in realism but in whatever titillation or outrage they can generate. Even as experts continue to develop techniques for exposing photographic frauds, new techniques for creating better and harder-to-detect fakes are also evolving. Altering digital imagery is now ubiquitous. People have come to expect it in the fashion and entertainment world, where airbrushing blemishes and wrinkles away is routine. And anyone surfing the Web is routinely subjected to crude photographic mashups like the Palin hoax, whose creators clearly aren't interested in realism but in whatever titillation or outrage they can generate. Even as experts continue to develop techniques for exposing photographic frauds, new techniques for creating better and harder-to-detect fakes are also evolving. As in the battle against spam and computer viruses, it seems inevitable that the arms race between the forger and the forensic analyst will continue to escalate, with no clear victor. Altering digital imagery is now ubiquitous. People have come to expect it in the fashion and entertainment world, where airbrushing blemishes and wrinkles away is routine. And anyone surfing the Web is routinely subjected to crude photographic mashups like the Palin hoax, whose creators clearly aren't interested in realism but in whatever titillation or outrage they can generate. Even as experts continue to develop techniques for exposing photographic frauds, new techniques for creating better and harder-to-detect fakes are also evolving. As in the battle against spam and computer viruses, it seems inevitable that the arms race between the forger and the forensic analyst will continue to escalate, with no clear victor. Improved image forensics will never be able to eradicate or prevent digital tampering, but these techniques can make it more time-consuming and difficult for forgers to ply their trade. Tomorrow's technology will almost certainly enable digital manipulations that today seem unimaginable, and the science of digital forensics will have to work hard to keep pace. Tomorrow's technology will almost certainly enable digital manipulations that today seem unimaginable, and the science of digital forensics will have to work hard to keep pace. It is my hope that these new techniques, along with a greater awareness of the technological possibilities and sensible updates in policy and law, will help the media, the courts, and the public contend with the exciting but often baffling events of our digital age."}, {"paper_id": "116974286", "adju_relevance": 0, "title": "Relevance Logic and Entailment", "background_label": "Note carefully that the title of this piece is not \u2018A Survey of Relevance Logic\u2019. Such a project would be impossible given the development of the field and even the space limitations of this Handbook. For example Anderson and Belnap\u2019s [1975] book Entailment: The Logic of Relevance and Necessity, volume 1 runs over 500 pages, and is their summary of just \u2018half\u2019 of the work done by them and their co-workers up to about the early 70s.1", "abstract": "Note carefully that the title of this piece is not \u2018A Survey of Relevance Logic\u2019. Note carefully that the title of this piece is not \u2018A Survey of Relevance Logic\u2019. Such a project would be impossible given the development of the field and even the space limitations of this Handbook. Note carefully that the title of this piece is not \u2018A Survey of Relevance Logic\u2019. Such a project would be impossible given the development of the field and even the space limitations of this Handbook. For example Anderson and Belnap\u2019s [1975] book Entailment: The Logic of Relevance and Necessity, volume 1 runs over 500 pages, and is their summary of just \u2018half\u2019 of the work done by them and their co-workers up to about the early 70s.1"}, {"paper_id": "15969603", "adju_relevance": 0, "title": "Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can't See What I Mean", "background_label": "Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning. Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts. However, such concepts are comparatively rare in everyday language.", "abstract": "Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning. Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning. Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts. Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning. Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts. However, such concepts are comparatively rare in everyday language."}, {"paper_id": "195798821", "adju_relevance": 0, "title": "On Open-Universe Causal Reasoning", "background_label": "We extend two kinds of causal models, structural equation models and simulation models, to infinite variable spaces.", "method_label": "This enables a semantics for conditionals founded on a calculus of intervention, and axiomatization of causal reasoning for rich, expressive generative models---including those in which a causal representation exists only implicitly---in an open-universe setting. Further, we show that under suitable restrictions the two kinds of models are equivalent, perhaps surprisingly as their axiomatizations differ substantially in the general case.", "result_label": "We give a series of complete axiomatizations in which the open-universe nature of the setting is seen to be essential.", "abstract": "We extend two kinds of causal models, structural equation models and simulation models, to infinite variable spaces. This enables a semantics for conditionals founded on a calculus of intervention, and axiomatization of causal reasoning for rich, expressive generative models---including those in which a causal representation exists only implicitly---in an open-universe setting. This enables a semantics for conditionals founded on a calculus of intervention, and axiomatization of causal reasoning for rich, expressive generative models---including those in which a causal representation exists only implicitly---in an open-universe setting. Further, we show that under suitable restrictions the two kinds of models are equivalent, perhaps surprisingly as their axiomatizations differ substantially in the general case. We give a series of complete axiomatizations in which the open-universe nature of the setting is seen to be essential."}, {"paper_id": "199552244", "adju_relevance": 0, "title": "Attention is not not Explanation", "background_label": "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019).", "method_label": "We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models.", "result_label": "We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.", "abstract": "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."}, {"paper_id": "18597583", "adju_relevance": 0, "title": "Vector-based Models of Semantic Composition", "abstract": ""}, {"paper_id": "13623031", "adju_relevance": 0, "title": "Not all laughs are alike: voiced but not unvoiced laughter readily elicits positive affect.", "background_label": "We tested whether listeners are differentially responsive to the presence or absence of voicing, a salient, distinguishing acoustic feature, in laughter. Each of 128 participants rated 50 voiced and 20 unvoiced laughs twice according to one of five different rating strategies.", "result_label": "Results were highly consistent regardless of whether participants rated their own emotional responses, likely responses of other people, or one of three perceived attributes concerning the laughers, thus indicating that participants were experiencing similarly differentiated affective responses in all these cases. Specifically, voiced, songlike laughs were significantly more likely to elicit positive responses than were variants such as unvoiced grunts, pants, and snortlike sounds. Participants were also highly consistent in their relative dislike of these other sounds, especially those produced by females. Based on these results, we argue that laughers use the acoustic features of their vocalizations to shape listener affect.", "abstract": "We tested whether listeners are differentially responsive to the presence or absence of voicing, a salient, distinguishing acoustic feature, in laughter. We tested whether listeners are differentially responsive to the presence or absence of voicing, a salient, distinguishing acoustic feature, in laughter. Each of 128 participants rated 50 voiced and 20 unvoiced laughs twice according to one of five different rating strategies. Results were highly consistent regardless of whether participants rated their own emotional responses, likely responses of other people, or one of three perceived attributes concerning the laughers, thus indicating that participants were experiencing similarly differentiated affective responses in all these cases. Results were highly consistent regardless of whether participants rated their own emotional responses, likely responses of other people, or one of three perceived attributes concerning the laughers, thus indicating that participants were experiencing similarly differentiated affective responses in all these cases. Specifically, voiced, songlike laughs were significantly more likely to elicit positive responses than were variants such as unvoiced grunts, pants, and snortlike sounds. Results were highly consistent regardless of whether participants rated their own emotional responses, likely responses of other people, or one of three perceived attributes concerning the laughers, thus indicating that participants were experiencing similarly differentiated affective responses in all these cases. Specifically, voiced, songlike laughs were significantly more likely to elicit positive responses than were variants such as unvoiced grunts, pants, and snortlike sounds. Participants were also highly consistent in their relative dislike of these other sounds, especially those produced by females. Results were highly consistent regardless of whether participants rated their own emotional responses, likely responses of other people, or one of three perceived attributes concerning the laughers, thus indicating that participants were experiencing similarly differentiated affective responses in all these cases. Specifically, voiced, songlike laughs were significantly more likely to elicit positive responses than were variants such as unvoiced grunts, pants, and snortlike sounds. Participants were also highly consistent in their relative dislike of these other sounds, especially those produced by females. Based on these results, we argue that laughers use the acoustic features of their vocalizations to shape listener affect."}, {"paper_id": "11567084", "adju_relevance": 0, "title": "A Comparison of Vector-based Representations for Semantic Composition", "background_label": "We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora.", "method_label": "We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection.", "result_label": "The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.", "abstract": " We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method."}, {"paper_id": "8360910", "adju_relevance": 0, "title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space", "background_label": "AbstractWe propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.", "method_label": "Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter.", "result_label": "We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.", "abstract": "AbstractWe propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task."}, {"paper_id": "3416500", "adju_relevance": 0, "title": "Unhappy Developers: Bad for Themselves, Bad for Process, and Bad for Software Product", "background_label": "Recent research in software engineering supports the\"happy-productive\"thesis, and the desire of flourishing happiness among programmers is often expressed by industry practitioners. Recent literature has suggested that a cost-effective way to foster happiness and productivity among workers could be to limit unhappiness of developers due to its negative impact. However, possible negative effects of unhappiness are still largely unknown in the software development context.", "abstract": "Recent research in software engineering supports the\"happy-productive\"thesis, and the desire of flourishing happiness among programmers is often expressed by industry practitioners. Recent research in software engineering supports the\"happy-productive\"thesis, and the desire of flourishing happiness among programmers is often expressed by industry practitioners. Recent literature has suggested that a cost-effective way to foster happiness and productivity among workers could be to limit unhappiness of developers due to its negative impact. Recent research in software engineering supports the\"happy-productive\"thesis, and the desire of flourishing happiness among programmers is often expressed by industry practitioners. Recent literature has suggested that a cost-effective way to foster happiness and productivity among workers could be to limit unhappiness of developers due to its negative impact. However, possible negative effects of unhappiness are still largely unknown in the software development context."}, {"paper_id": "166753895", "adju_relevance": 0, "title": "A City is Not a Tree", "background_label": "A set is a collection of elements which for some reason we think of as belonging together. Since, as designers, we are concerned with the physical living city and its physical backbone, we must naturally restrict ourselves to considering sets which are collections of material elements such as people, blades of grass, cars, molecules, houses, gardens, water pipes, the water molecules in them etc.", "abstract": " A set is a collection of elements which for some reason we think of as belonging together. A set is a collection of elements which for some reason we think of as belonging together. Since, as designers, we are concerned with the physical living city and its physical backbone, we must naturally restrict ourselves to considering sets which are collections of material elements such as people, blades of grass, cars, molecules, houses, gardens, water pipes, the water molecules in them etc."}, {"paper_id": "13783080", "adju_relevance": 0, "title": "Allostery: absence of a change in shape does not imply that allostery is not at play.", "background_label": "Allostery is essential for controlled catalysis, signal transmission, receptor trafficking, turning genes on and off, and apoptosis. It governs the organism's response to environmental and metabolic cues, dictating transient partner interactions in the cellular network. Textbooks taught us that allostery is a change of shape at one site on the protein surface brought about by ligand binding to another. For several years, it has been broadly accepted that the change of shape is not induced; rather, it is observed simply because a larger protein population presents it.", "result_label": "Current data indicate that while side chains can reorient and rewire, allostery may not even involve a change of (backbone) shape. Assuming that the enthalpy change does not reverse the free-energy change due to the change in entropy, entropy is mainly responsible for binding.", "abstract": "Allostery is essential for controlled catalysis, signal transmission, receptor trafficking, turning genes on and off, and apoptosis. Allostery is essential for controlled catalysis, signal transmission, receptor trafficking, turning genes on and off, and apoptosis. It governs the organism's response to environmental and metabolic cues, dictating transient partner interactions in the cellular network. Allostery is essential for controlled catalysis, signal transmission, receptor trafficking, turning genes on and off, and apoptosis. It governs the organism's response to environmental and metabolic cues, dictating transient partner interactions in the cellular network. Textbooks taught us that allostery is a change of shape at one site on the protein surface brought about by ligand binding to another. Allostery is essential for controlled catalysis, signal transmission, receptor trafficking, turning genes on and off, and apoptosis. It governs the organism's response to environmental and metabolic cues, dictating transient partner interactions in the cellular network. Textbooks taught us that allostery is a change of shape at one site on the protein surface brought about by ligand binding to another. For several years, it has been broadly accepted that the change of shape is not induced; rather, it is observed simply because a larger protein population presents it. Current data indicate that while side chains can reorient and rewire, allostery may not even involve a change of (backbone) shape. Current data indicate that while side chains can reorient and rewire, allostery may not even involve a change of (backbone) shape. Assuming that the enthalpy change does not reverse the free-energy change due to the change in entropy, entropy is mainly responsible for binding."}, {"paper_id": "14214642", "adju_relevance": 0, "title": "The Hidden Markov Topic Model: A Probabilistic Model of Semantic Representation", "background_label": "In this paper, we describe a model that learns semantic representations from the distributional statistics of language. This model, however, goes beyond the common bag-of-words paradigm, and infers semantic representations by taking into account the inherent sequential nature of linguistic data.", "method_label": "The model we describe, which we refer to as a Hidden Markov Topics model, is a natural extension of the current state of the art in Bayesian bag-of-words models, that is, the Topics model of Griffiths, Steyvers, and Tenenbaum (2007), preserving its strengths while extending its scope to incorporate more fine-grained linguistic information.", "abstract": "In this paper, we describe a model that learns semantic representations from the distributional statistics of language. In this paper, we describe a model that learns semantic representations from the distributional statistics of language. This model, however, goes beyond the common bag-of-words paradigm, and infers semantic representations by taking into account the inherent sequential nature of linguistic data. The model we describe, which we refer to as a Hidden Markov Topics model, is a natural extension of the current state of the art in Bayesian bag-of-words models, that is, the Topics model of Griffiths, Steyvers, and Tenenbaum (2007), preserving its strengths while extending its scope to incorporate more fine-grained linguistic information."}, {"paper_id": "15706947", "adju_relevance": 0, "title": "The bliss (not the problem) of motor abundance (not redundancy)", "background_label": "Motor control is an area of natural science exploring how the nervous system interacts with other body parts and the environment to produce purposeful, coordinated actions. A central problem of motor control\u2014the problem of motor redundancy\u2014was formulated by Nikolai Bernstein as the problem of elimination of redundant degrees-of-freedom. Traditionally, this problem has been addressed using optimization methods based on a variety of cost functions.", "abstract": "Motor control is an area of natural science exploring how the nervous system interacts with other body parts and the environment to produce purposeful, coordinated actions. Motor control is an area of natural science exploring how the nervous system interacts with other body parts and the environment to produce purposeful, coordinated actions. A central problem of motor control\u2014the problem of motor redundancy\u2014was formulated by Nikolai Bernstein as the problem of elimination of redundant degrees-of-freedom. Motor control is an area of natural science exploring how the nervous system interacts with other body parts and the environment to produce purposeful, coordinated actions. A central problem of motor control\u2014the problem of motor redundancy\u2014was formulated by Nikolai Bernstein as the problem of elimination of redundant degrees-of-freedom. Traditionally, this problem has been addressed using optimization methods based on a variety of cost functions."}, {"paper_id": "52945237", "adju_relevance": 0, "title": "Towards Verifying Semantic Roles Co-occurrence", "background_label": "Semantic role theory considers roles as a small universal set of unanalyzed entities. It means that formally there are no restrictions on role combinations. We argue that the semantic roles co-occur in verb representations. It means that there are hidden restrictions on role combinations.", "method_label": "To demonstrate that a practical and evidence-based approach has been built on in-depth analysis of the largest verb database VerbNet.", "result_label": "The consequences of this approach are considered.", "abstract": "Semantic role theory considers roles as a small universal set of unanalyzed entities. Semantic role theory considers roles as a small universal set of unanalyzed entities. It means that formally there are no restrictions on role combinations. Semantic role theory considers roles as a small universal set of unanalyzed entities. It means that formally there are no restrictions on role combinations. We argue that the semantic roles co-occur in verb representations. Semantic role theory considers roles as a small universal set of unanalyzed entities. It means that formally there are no restrictions on role combinations. We argue that the semantic roles co-occur in verb representations. It means that there are hidden restrictions on role combinations. To demonstrate that a practical and evidence-based approach has been built on in-depth analysis of the largest verb database VerbNet. The consequences of this approach are considered."}, {"paper_id": "2775184", "adju_relevance": 0, "title": "Security-Informed Safety: If It's Not Secure, It's Not Safe", "background_label": "Traditionally, safety and security have been treated as separate disciplines, but this position is increasingly becoming untenable and stakeholders are beginning to argue that if it's not secure, it's not safe.", "abstract": " Traditionally, safety and security have been treated as separate disciplines, but this position is increasingly becoming untenable and stakeholders are beginning to argue that if it's not secure, it's not safe."}, {"paper_id": "46279910", "adju_relevance": 0, "title": "Age of acquisition, not word frequency, affects object naming, not object recognition.", "background_label": "Word frequency is widely believed to affect object naming speed, despite several studies in which it has been reported that frequency effects may be redundant upon age of acquisition. We report, first, a reanalysis of data from the study by Oldfield and Wingfield (1965), which is standardly cited as evidence for a word frequency effect in object naming; then we report two new experiments.", "method_label": "The reanalysis of Oldfield and Wingfield shows that age of acquisition is the major determinant of naming speed, and that frequency plays no independent role when its correlation with other variables is taken into account. In Experiment 1, age of acquisition and phoneme length proved to be the primary determinants of object naming speed. Frequency, prototypicality, and imageability had no independent effect. In Experiment 2, subjects classified objects into two semantic categories (natural or man-made).", "result_label": "Prototypicality and semantic category were the only variables to have a significant effect on reaction time, with no effect of age of acquisition, frequency, imageability, or word length. We conclude that age of acquisition, not word frequency, affects the retrieval and/or execution of object names, not the process of object recognition. The locus of this effect is discussed, along with the possibility that words learned in early childhood may be more resistant to the effects of brain injury in at least some adult aphasics than words learned somewhat later.", "abstract": "Word frequency is widely believed to affect object naming speed, despite several studies in which it has been reported that frequency effects may be redundant upon age of acquisition. Word frequency is widely believed to affect object naming speed, despite several studies in which it has been reported that frequency effects may be redundant upon age of acquisition. We report, first, a reanalysis of data from the study by Oldfield and Wingfield (1965), which is standardly cited as evidence for a word frequency effect in object naming; then we report two new experiments. The reanalysis of Oldfield and Wingfield shows that age of acquisition is the major determinant of naming speed, and that frequency plays no independent role when its correlation with other variables is taken into account. The reanalysis of Oldfield and Wingfield shows that age of acquisition is the major determinant of naming speed, and that frequency plays no independent role when its correlation with other variables is taken into account. In Experiment 1, age of acquisition and phoneme length proved to be the primary determinants of object naming speed. The reanalysis of Oldfield and Wingfield shows that age of acquisition is the major determinant of naming speed, and that frequency plays no independent role when its correlation with other variables is taken into account. In Experiment 1, age of acquisition and phoneme length proved to be the primary determinants of object naming speed. Frequency, prototypicality, and imageability had no independent effect. The reanalysis of Oldfield and Wingfield shows that age of acquisition is the major determinant of naming speed, and that frequency plays no independent role when its correlation with other variables is taken into account. In Experiment 1, age of acquisition and phoneme length proved to be the primary determinants of object naming speed. Frequency, prototypicality, and imageability had no independent effect. In Experiment 2, subjects classified objects into two semantic categories (natural or man-made). Prototypicality and semantic category were the only variables to have a significant effect on reaction time, with no effect of age of acquisition, frequency, imageability, or word length. Prototypicality and semantic category were the only variables to have a significant effect on reaction time, with no effect of age of acquisition, frequency, imageability, or word length. We conclude that age of acquisition, not word frequency, affects the retrieval and/or execution of object names, not the process of object recognition. Prototypicality and semantic category were the only variables to have a significant effect on reaction time, with no effect of age of acquisition, frequency, imageability, or word length. We conclude that age of acquisition, not word frequency, affects the retrieval and/or execution of object names, not the process of object recognition. The locus of this effect is discussed, along with the possibility that words learned in early childhood may be more resistant to the effects of brain injury in at least some adult aphasics than words learned somewhat later."}, {"paper_id": "3806881", "adju_relevance": 0, "title": "A Generalised Quantifier Theory of Natural Language in Categorical Compositional Distributional Semantics with Bialgebras", "background_label": "Categorical compositional distributional semantics is a model of natural language; it combines the statistical vector space models of words with the compositional models of grammar. We formalise in this model the generalised quantifier theory of natural language, due to Barwise and Cooper.", "method_label": "The underlying setting is a compact closed category with bialgebras. We start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it, then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps. We prove the equivalence of the relational instantiation to the truth theoretic semantics of generalised quantifiers.", "result_label": "The vector space instantiation formalises the statistical usages of words and enables us to, for the first time, reason about quantified phrases and sentences compositionally in distributional semantics.", "abstract": "Categorical compositional distributional semantics is a model of natural language; it combines the statistical vector space models of words with the compositional models of grammar. Categorical compositional distributional semantics is a model of natural language; it combines the statistical vector space models of words with the compositional models of grammar. We formalise in this model the generalised quantifier theory of natural language, due to Barwise and Cooper. The underlying setting is a compact closed category with bialgebras. The underlying setting is a compact closed category with bialgebras. We start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it, then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps. The underlying setting is a compact closed category with bialgebras. We start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it, then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps. We prove the equivalence of the relational instantiation to the truth theoretic semantics of generalised quantifiers. The vector space instantiation formalises the statistical usages of words and enables us to, for the first time, reason about quantified phrases and sentences compositionally in distributional semantics."}, {"paper_id": "1374085", "adju_relevance": 0, "title": "Parameterized Aspect Calculus: A Core Calculus for the Direct Study of Aspect-Oriented Languages", "background_label": "Formal study of aspect-oriented languages is difficult because current theoretical models provide a range of features that is too limited and rely on encodings using lower-level abstractions, which involve a cumbersome level of indirection. We present a calculus, based on Abadi and Cardelli\u2019s object calculus, that explicitly models a base language and a variety of point cut description languages.", "method_label": "This explicit modeling makes clear the aspect-oriented features of the calculus by removing the indirection of some existing models.", "result_label": "We demonstrate the generality of our calculus by presenting models for AspectJ\u2019s open classes and advice, and HyperJ\u2019s compositions, and sketching a model for DemeterJ\u2019s adaptive methods.", "abstract": "Formal study of aspect-oriented languages is difficult because current theoretical models provide a range of features that is too limited and rely on encodings using lower-level abstractions, which involve a cumbersome level of indirection. Formal study of aspect-oriented languages is difficult because current theoretical models provide a range of features that is too limited and rely on encodings using lower-level abstractions, which involve a cumbersome level of indirection. We present a calculus, based on Abadi and Cardelli\u2019s object calculus, that explicitly models a base language and a variety of point cut description languages. This explicit modeling makes clear the aspect-oriented features of the calculus by removing the indirection of some existing models. We demonstrate the generality of our calculus by presenting models for AspectJ\u2019s open classes and advice, and HyperJ\u2019s compositions, and sketching a model for DemeterJ\u2019s adaptive methods."}, {"paper_id": "52928664", "adju_relevance": 0, "title": "Abstracting Probabilistic Relational Models", "background_label": "AbstractAbstraction is a powerful idea widely used in science, to model, reason and explain the behavior of systems in a more tractable search space, by omitting irrelevant details.", "abstract": "AbstractAbstraction is a powerful idea widely used in science, to model, reason and explain the behavior of systems in a more tractable search space, by omitting irrelevant details."}, {"paper_id": "82456167", "adju_relevance": 0, "title": "Janeway's Immunobiology", "background_label": "Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11.", "method_label": "The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7.", "result_label": "The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.", "abstract": "Part I An Introduction to Immunobiology and Innate Immunity 1. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. The Generation of Lymphocyte Antigen Receptors 5. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. The Generation of Lymphocyte Antigen Receptors 5. Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6. Signaling Through Immune System Receptors 7. The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Part I An Introduction to Immunobiology and Innate Immunity 1. Basic Concepts in Immunology 2. Innate Immunity Part II The Recognition of Antigen 3. Antigen Recognition by B-cell and T-cell Receptors 4. T Cell-Mediated Immunity 9. The Humoral Immune Response 10. Dynamics of Adaptive Immunity 11."}, {"paper_id": "5849086", "adju_relevance": 0, "title": "Test-Score Semantics For Natural Languages", "background_label": "Test-score semantics is based on the premise that almost everything that relates to natural languages is a matter of degree. Viewed from this perspective, any semantic entity in a natural language, e.g., a predicate, predicate-modifier, proposition, quantifier, command, question, etc. may be represented as a system of elastic constraints on a collection of objects or derived objects in a universe of discourse.", "result_label": "In this sense, test-score semantics may be viewed as a generalization of truth-conditional, possible-world and model-theoretic semantics, but its expressive power is substantially greater.", "abstract": "Test-score semantics is based on the premise that almost everything that relates to natural languages is a matter of degree. Test-score semantics is based on the premise that almost everything that relates to natural languages is a matter of degree. Viewed from this perspective, any semantic entity in a natural language, e.g., a predicate, predicate-modifier, proposition, quantifier, command, question, etc. Test-score semantics is based on the premise that almost everything that relates to natural languages is a matter of degree. Viewed from this perspective, any semantic entity in a natural language, e.g., a predicate, predicate-modifier, proposition, quantifier, command, question, etc. may be represented as a system of elastic constraints on a collection of objects or derived objects in a universe of discourse. In this sense, test-score semantics may be viewed as a generalization of truth-conditional, possible-world and model-theoretic semantics, but its expressive power is substantially greater."}, {"paper_id": "15637264", "adju_relevance": 0, "title": "Why IDLs are not ideal", "background_label": "The dominant approach to addressing heterogeneity, interoperability and legacy software components at present is based on the use of interface description languages (IDLs) such as the OMG/CORBA IDL. We believe that this approach has serious drawbacks.", "abstract": "The dominant approach to addressing heterogeneity, interoperability and legacy software components at present is based on the use of interface description languages (IDLs) such as the OMG/CORBA IDL. The dominant approach to addressing heterogeneity, interoperability and legacy software components at present is based on the use of interface description languages (IDLs) such as the OMG/CORBA IDL. We believe that this approach has serious drawbacks."}, {"paper_id": "119425731", "adju_relevance": 0, "title": "Unzerlegbare Darstellungen I", "background_label": "LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e).", "method_label": "We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.", "abstract": "LetK be the structure got by forgetting the composition law of morphisms in a given category. LetK be the structure got by forgetting the composition law of morphisms in a given category. A linear representation ofK is given by a map V associating with any morphism \u03d5: a\u2192e ofK a linear vector space map V(\u03d5): V(a)\u2192V(e). We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations."}, {"paper_id": "17991397", "adju_relevance": 0, "title": "Well-Behaved Model Transformations with Model Subtyping", "background_label": "In model-driven engineering, models abstract the relevant features of software artefacts and model transformations act on them automating complex tasks of the development process. It is, thus, crucially important to provide pragmatic, reliable methods to verify that model transformations guarantee the correctness of generated models in order to ensure the quality of the final end product.", "method_label": "In this paper, we build on an object-oriented algebraic encoding of metamodels and models as defined in the standard Meta-Object Facility and in tools, such as the Eclipse Modeling Framework, to specify a domain-specific language for representing the action part of model transformations. We introduce the big-step operational structural semantics of this language and its type system, which includes a notion of polymorphic model subtyping, showing that well-typed model transformations are well behaved.", "result_label": "That is, that metamodel-conformant model transformations never go wrong.", "abstract": "In model-driven engineering, models abstract the relevant features of software artefacts and model transformations act on them automating complex tasks of the development process. In model-driven engineering, models abstract the relevant features of software artefacts and model transformations act on them automating complex tasks of the development process. It is, thus, crucially important to provide pragmatic, reliable methods to verify that model transformations guarantee the correctness of generated models in order to ensure the quality of the final end product. In this paper, we build on an object-oriented algebraic encoding of metamodels and models as defined in the standard Meta-Object Facility and in tools, such as the Eclipse Modeling Framework, to specify a domain-specific language for representing the action part of model transformations. In this paper, we build on an object-oriented algebraic encoding of metamodels and models as defined in the standard Meta-Object Facility and in tools, such as the Eclipse Modeling Framework, to specify a domain-specific language for representing the action part of model transformations. We introduce the big-step operational structural semantics of this language and its type system, which includes a notion of polymorphic model subtyping, showing that well-typed model transformations are well behaved. That is, that metamodel-conformant model transformations never go wrong."}, {"paper_id": "2161880", "adju_relevance": 0, "title": "Small Is Not Always Beautiful", "background_label": "Peer-to-peer content distribution systems have been enjoying great popularity, and are now gaining momentum as a means of disseminating video streams over the Internet. In many of these protocols, including the popular BitTorrent, content is split into mostly fixed-size pieces, allowing a client to download data from many peers simultaneously. This makes piece size potentially critical for performance. However, previous research efforts have largely overlooked this parameter, opting to focus on others instead.", "method_label": "This paper presents the results of real experiments with varying piece sizes on a controlled BitTorrent testbed.", "result_label": "We demonstrate that this parameter is indeed critical, as it determines the degree of parallelism in the system, and we investigate optimal piece sizes for distributing small and large content. We also pinpoint a related design trade-off, and explain how BitTorrent's choice of dividing pieces into subpieces attempts to address it.", "abstract": "Peer-to-peer content distribution systems have been enjoying great popularity, and are now gaining momentum as a means of disseminating video streams over the Internet. Peer-to-peer content distribution systems have been enjoying great popularity, and are now gaining momentum as a means of disseminating video streams over the Internet. In many of these protocols, including the popular BitTorrent, content is split into mostly fixed-size pieces, allowing a client to download data from many peers simultaneously. Peer-to-peer content distribution systems have been enjoying great popularity, and are now gaining momentum as a means of disseminating video streams over the Internet. In many of these protocols, including the popular BitTorrent, content is split into mostly fixed-size pieces, allowing a client to download data from many peers simultaneously. This makes piece size potentially critical for performance. Peer-to-peer content distribution systems have been enjoying great popularity, and are now gaining momentum as a means of disseminating video streams over the Internet. In many of these protocols, including the popular BitTorrent, content is split into mostly fixed-size pieces, allowing a client to download data from many peers simultaneously. This makes piece size potentially critical for performance. However, previous research efforts have largely overlooked this parameter, opting to focus on others instead. This paper presents the results of real experiments with varying piece sizes on a controlled BitTorrent testbed. We demonstrate that this parameter is indeed critical, as it determines the degree of parallelism in the system, and we investigate optimal piece sizes for distributing small and large content. We demonstrate that this parameter is indeed critical, as it determines the degree of parallelism in the system, and we investigate optimal piece sizes for distributing small and large content. We also pinpoint a related design trade-off, and explain how BitTorrent's choice of dividing pieces into subpieces attempts to address it."}, {"paper_id": "7768929", "adju_relevance": 0, "title": "Role Semantics for Better Models of Implicit Discourse Relations", "background_label": "Predicting the structure of a discourse is challenging because relations between discourse segments are often implicit and thus hard to distinguish computationally. I extend previous work to classify implicit discourse relations by introducing a novel set of features on the level of semantic roles.", "result_label": "My results demonstrate that such features are helpful, yielding results competitive with other feature-rich approaches on the PDTB. My main contribution is an analysis of improvements that can be traced back to role-based features, providing insights into why and when role semantics is helpful.", "abstract": "Predicting the structure of a discourse is challenging because relations between discourse segments are often implicit and thus hard to distinguish computationally. Predicting the structure of a discourse is challenging because relations between discourse segments are often implicit and thus hard to distinguish computationally. I extend previous work to classify implicit discourse relations by introducing a novel set of features on the level of semantic roles. My results demonstrate that such features are helpful, yielding results competitive with other feature-rich approaches on the PDTB. My results demonstrate that such features are helpful, yielding results competitive with other feature-rich approaches on the PDTB. My main contribution is an analysis of improvements that can be traced back to role-based features, providing insights into why and when role semantics is helpful."}, {"paper_id": "19304772", "adju_relevance": 0, "title": "The prosodic word is not universal, but emergent", "background_label": "In Prosodic Phonology, domains for the application of phonological patterns are commonly modeled as a Prosodic Hierarchy. The theory predicts, among other things, that (i) prosodic domains cluster on a single universal set of domains ('Clustering'), and (ii) no level of prosodic structure is skipped in the building of prosodic structure unless this is required by independently motivated higher ranking principles or constraints ('Strict Succession'). In this paper, we demonstrate that if, as is standardly done, evidence is limited to lexically general phonological processes, some languages systematically violate the Strict Succession Prediction, evidencing no prosodic word domain, and some languages systematically violate the Clustering Prediction, evidencing more than one domain between the phonological phrase and the foot.", "method_label": "We substantiate these claims by in-depth studies of phonological rule domains in Vietnamese (Austroasiatic) and Limbu (Sino-Tibetan). As an alternative to the Prosodic Hierarchy framework, we advocate a heuristic for cross-linguistic comparison in which prosodic domains are conceived of as language-particular, intrinsic and highly specific properties of individual phonological rules or constraints. This allows us to explore empirically the actual degree of variation to be encountered across prosodic systems.", "result_label": "It turns out that the 'word' has no privileged or universal status in phonology, but only emerges through frequent reference of sound patterns to a given construction type in a given language.", "abstract": "In Prosodic Phonology, domains for the application of phonological patterns are commonly modeled as a Prosodic Hierarchy. In Prosodic Phonology, domains for the application of phonological patterns are commonly modeled as a Prosodic Hierarchy. The theory predicts, among other things, that (i) prosodic domains cluster on a single universal set of domains ('Clustering'), and (ii) no level of prosodic structure is skipped in the building of prosodic structure unless this is required by independently motivated higher ranking principles or constraints ('Strict Succession'). In Prosodic Phonology, domains for the application of phonological patterns are commonly modeled as a Prosodic Hierarchy. The theory predicts, among other things, that (i) prosodic domains cluster on a single universal set of domains ('Clustering'), and (ii) no level of prosodic structure is skipped in the building of prosodic structure unless this is required by independently motivated higher ranking principles or constraints ('Strict Succession'). In this paper, we demonstrate that if, as is standardly done, evidence is limited to lexically general phonological processes, some languages systematically violate the Strict Succession Prediction, evidencing no prosodic word domain, and some languages systematically violate the Clustering Prediction, evidencing more than one domain between the phonological phrase and the foot. We substantiate these claims by in-depth studies of phonological rule domains in Vietnamese (Austroasiatic) and Limbu (Sino-Tibetan). We substantiate these claims by in-depth studies of phonological rule domains in Vietnamese (Austroasiatic) and Limbu (Sino-Tibetan). As an alternative to the Prosodic Hierarchy framework, we advocate a heuristic for cross-linguistic comparison in which prosodic domains are conceived of as language-particular, intrinsic and highly specific properties of individual phonological rules or constraints. We substantiate these claims by in-depth studies of phonological rule domains in Vietnamese (Austroasiatic) and Limbu (Sino-Tibetan). As an alternative to the Prosodic Hierarchy framework, we advocate a heuristic for cross-linguistic comparison in which prosodic domains are conceived of as language-particular, intrinsic and highly specific properties of individual phonological rules or constraints. This allows us to explore empirically the actual degree of variation to be encountered across prosodic systems. It turns out that the 'word' has no privileged or universal status in phonology, but only emerges through frequent reference of sound patterns to a given construction type in a given language."}, {"paper_id": "14771077", "adju_relevance": 0, "title": "Archaeal ancestors of eukaryotes: not so elusive any more", "background_label": "The origin of eukaryotes is one of the hardest problems in evolutionary biology and sometimes raises the ominous specter of irreducible complexity. Reconstruction of the gene repertoire of the last eukaryotic common ancestor (LECA) has revealed a highly complex organism with a variety of advanced features but no detectable evolutionary intermediates to explain their origin. Recently, however, genome analysis of diverse archaea led to the discovery of apparent ancestral versions of several signature eukaryotic systems, such as the actin cytoskeleton and the ubiquitin network, that are scattered among archaea.", "method_label": "These findings inspired the hypothesis that the archaeal ancestor of eukaryotes was an unusually complex form with an elaborate intracellular organization. The latest striking discovery made by deep metagenomic sequencing vindicates this hypothesis by showing that in phylogenetic trees eukaryotes fall within a newly identified archaeal group, the Lokiarchaeota, which combine several eukaryotic signatures previously identified in different archaea.", "result_label": "The discovery of complex archaea that are the closest living relatives of eukaryotes is most compatible with the symbiogenetic scenario for eukaryogenesis.", "abstract": "The origin of eukaryotes is one of the hardest problems in evolutionary biology and sometimes raises the ominous specter of irreducible complexity. The origin of eukaryotes is one of the hardest problems in evolutionary biology and sometimes raises the ominous specter of irreducible complexity. Reconstruction of the gene repertoire of the last eukaryotic common ancestor (LECA) has revealed a highly complex organism with a variety of advanced features but no detectable evolutionary intermediates to explain their origin. The origin of eukaryotes is one of the hardest problems in evolutionary biology and sometimes raises the ominous specter of irreducible complexity. Reconstruction of the gene repertoire of the last eukaryotic common ancestor (LECA) has revealed a highly complex organism with a variety of advanced features but no detectable evolutionary intermediates to explain their origin. Recently, however, genome analysis of diverse archaea led to the discovery of apparent ancestral versions of several signature eukaryotic systems, such as the actin cytoskeleton and the ubiquitin network, that are scattered among archaea. These findings inspired the hypothesis that the archaeal ancestor of eukaryotes was an unusually complex form with an elaborate intracellular organization. These findings inspired the hypothesis that the archaeal ancestor of eukaryotes was an unusually complex form with an elaborate intracellular organization. The latest striking discovery made by deep metagenomic sequencing vindicates this hypothesis by showing that in phylogenetic trees eukaryotes fall within a newly identified archaeal group, the Lokiarchaeota, which combine several eukaryotic signatures previously identified in different archaea. The discovery of complex archaea that are the closest living relatives of eukaryotes is most compatible with the symbiogenetic scenario for eukaryogenesis."}, {"paper_id": "5941289", "adju_relevance": 0, "title": "Complex Probabilistic Modeling with Recursive Relational Bayesian Networks", "background_label": "A number of representation systems have been proposed that extend the purely propositional Bayesian network paradigm with representation tools for some types of first-order probabilistic dependencies. Examples of such systems are dynamic Bayesian networks and systems for knowledge based model construction. We can identify the representation of probabilistic relational models as a common well-defined semantic core of such systems. Recursive relational Bayesian networks (RRBNs) are a framework for the representation of probabilistic relational models.", "abstract": "A number of representation systems have been proposed that extend the purely propositional Bayesian network paradigm with representation tools for some types of first-order probabilistic dependencies. A number of representation systems have been proposed that extend the purely propositional Bayesian network paradigm with representation tools for some types of first-order probabilistic dependencies. Examples of such systems are dynamic Bayesian networks and systems for knowledge based model construction. A number of representation systems have been proposed that extend the purely propositional Bayesian network paradigm with representation tools for some types of first-order probabilistic dependencies. Examples of such systems are dynamic Bayesian networks and systems for knowledge based model construction. We can identify the representation of probabilistic relational models as a common well-defined semantic core of such systems. A number of representation systems have been proposed that extend the purely propositional Bayesian network paradigm with representation tools for some types of first-order probabilistic dependencies. Examples of such systems are dynamic Bayesian networks and systems for knowledge based model construction. We can identify the representation of probabilistic relational models as a common well-defined semantic core of such systems. Recursive relational Bayesian networks (RRBNs) are a framework for the representation of probabilistic relational models."}, {"paper_id": "18982392", "adju_relevance": 0, "title": "Why I am not a co-citationist", "background_label": "Some of us make modest use of citation analysis in our work, 1 but remain radically skeptical of the claims of those who devote more prime time and energy to the elaboration of such methods. Why do we not accept the faith? Why can we not do the proper Kuhnian thing and let the \u201cparadigmatic achievements\u201d of the new quantitative methods define the field for us\u2014posing our fundamental problems, laying down agreed techniques, prefiguring acceptable answers, and unrolling a \u201cprogressive research program \u201c?", "abstract": "Some of us make modest use of citation analysis in our work, 1 but remain radically skeptical of the claims of those who devote more prime time and energy to the elaboration of such methods. Some of us make modest use of citation analysis in our work, 1 but remain radically skeptical of the claims of those who devote more prime time and energy to the elaboration of such methods. Why do we not accept the faith? Some of us make modest use of citation analysis in our work, 1 but remain radically skeptical of the claims of those who devote more prime time and energy to the elaboration of such methods. Why do we not accept the faith? Why can we not do the proper Kuhnian thing and let the \u201cparadigmatic achievements\u201d of the new quantitative methods define the field for us\u2014posing our fundamental problems, laying down agreed techniques, prefiguring acceptable answers, and unrolling a \u201cprogressive research program \u201c?"}, {"paper_id": "44049223", "adju_relevance": 0, "title": "Regular $g$-measures are not always Gibbsian", "background_label": "Regular $g$-measures are discrete-time processes determined by conditional expectations with respect to the past. One-dimensional Gibbs measures, on the other hand, are fields determined by simultaneous conditioning on past and future. For the Markovian and exponentially continuous cases both theories are known to be equivalent. Its equivalence for more general cases was an open problem.", "method_label": "We present a simple example settling this issue in a negative way: there exist $g$-measures that are continuous and non-null but are not Gibbsian.", "result_label": "Our example belongs, in fact, to a well-studied family of processes with rather nice attributes: It is a chain with variable-length memory, characterized by the absence of phase coexistence and the existence of a visible renewal scheme.", "abstract": "Regular $g$-measures are discrete-time processes determined by conditional expectations with respect to the past. Regular $g$-measures are discrete-time processes determined by conditional expectations with respect to the past. One-dimensional Gibbs measures, on the other hand, are fields determined by simultaneous conditioning on past and future. Regular $g$-measures are discrete-time processes determined by conditional expectations with respect to the past. One-dimensional Gibbs measures, on the other hand, are fields determined by simultaneous conditioning on past and future. For the Markovian and exponentially continuous cases both theories are known to be equivalent. Regular $g$-measures are discrete-time processes determined by conditional expectations with respect to the past. One-dimensional Gibbs measures, on the other hand, are fields determined by simultaneous conditioning on past and future. For the Markovian and exponentially continuous cases both theories are known to be equivalent. Its equivalence for more general cases was an open problem. We present a simple example settling this issue in a negative way: there exist $g$-measures that are continuous and non-null but are not Gibbsian. Our example belongs, in fact, to a well-studied family of processes with rather nice attributes: It is a chain with variable-length memory, characterized by the absence of phase coexistence and the existence of a visible renewal scheme."}, {"paper_id": "24823034", "adju_relevance": 0, "title": "Invertebrate immune systems--not homogeneous, not simple, not well understood.", "background_label": "The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. Although recent work understandably has emphasized the commonalities of innate defenses, there is also ample evidence, as from completed genome studies, to suggest that even members of the same invertebrate order have taken significantly different approaches to internal defense. These data suggest that novel immune capabilities will be found among the different phyla. Many invertebrates have intimate associations with symbionts that may play more of a role in internal defense than generally appreciated. Some invertebrates that are either long lived or have colonial body plans may diversify components of their defense systems via somatic mutation. Somatic diversification following pathogen exposure, as seen in plants, has been investigated little in invertebrates.", "result_label": "Recent molecular studies of sponges, cnidarians, shrimp, mollusks, sea urchins, tunicates, and lancelets have found surprisingly diversified immune molecules, and a model is presented that supports the adaptive value of diversified non-self recognition molecules in invertebrates. Interactions between invertebrates and viruses also remain poorly understood. As we are in the midst of alarming losses of coral reefs, increased pathogen challenge to invertebrate aquaculture, and rampant invertebrate-transmitted parasites of humans and domestic animals, we need a better understanding of invertebrate immunology.", "abstract": "The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. Although recent work understandably has emphasized the commonalities of innate defenses, there is also ample evidence, as from completed genome studies, to suggest that even members of the same invertebrate order have taken significantly different approaches to internal defense. The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. Although recent work understandably has emphasized the commonalities of innate defenses, there is also ample evidence, as from completed genome studies, to suggest that even members of the same invertebrate order have taken significantly different approaches to internal defense. These data suggest that novel immune capabilities will be found among the different phyla. The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. Although recent work understandably has emphasized the commonalities of innate defenses, there is also ample evidence, as from completed genome studies, to suggest that even members of the same invertebrate order have taken significantly different approaches to internal defense. These data suggest that novel immune capabilities will be found among the different phyla. Many invertebrates have intimate associations with symbionts that may play more of a role in internal defense than generally appreciated. The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. Although recent work understandably has emphasized the commonalities of innate defenses, there is also ample evidence, as from completed genome studies, to suggest that even members of the same invertebrate order have taken significantly different approaches to internal defense. These data suggest that novel immune capabilities will be found among the different phyla. Many invertebrates have intimate associations with symbionts that may play more of a role in internal defense than generally appreciated. Some invertebrates that are either long lived or have colonial body plans may diversify components of their defense systems via somatic mutation. The approximate 30 extant invertebrate phyla have diversified along separate evolutionary trajectories for hundreds of millions of years. Although recent work understandably has emphasized the commonalities of innate defenses, there is also ample evidence, as from completed genome studies, to suggest that even members of the same invertebrate order have taken significantly different approaches to internal defense. These data suggest that novel immune capabilities will be found among the different phyla. Many invertebrates have intimate associations with symbionts that may play more of a role in internal defense than generally appreciated. Some invertebrates that are either long lived or have colonial body plans may diversify components of their defense systems via somatic mutation. Somatic diversification following pathogen exposure, as seen in plants, has been investigated little in invertebrates. Recent molecular studies of sponges, cnidarians, shrimp, mollusks, sea urchins, tunicates, and lancelets have found surprisingly diversified immune molecules, and a model is presented that supports the adaptive value of diversified non-self recognition molecules in invertebrates. Recent molecular studies of sponges, cnidarians, shrimp, mollusks, sea urchins, tunicates, and lancelets have found surprisingly diversified immune molecules, and a model is presented that supports the adaptive value of diversified non-self recognition molecules in invertebrates. Interactions between invertebrates and viruses also remain poorly understood. Recent molecular studies of sponges, cnidarians, shrimp, mollusks, sea urchins, tunicates, and lancelets have found surprisingly diversified immune molecules, and a model is presented that supports the adaptive value of diversified non-self recognition molecules in invertebrates. Interactions between invertebrates and viruses also remain poorly understood. As we are in the midst of alarming losses of coral reefs, increased pathogen challenge to invertebrate aquaculture, and rampant invertebrate-transmitted parasites of humans and domestic animals, we need a better understanding of invertebrate immunology."}, {"paper_id": "17362755", "adju_relevance": 0, "title": "When a Red Herring in Not a Red Herring: Using Compositional Methods to Detect Non-Compositional Phrases", "background_label": "Non-compositional phrases such as `red herring' and weakly compositional phrases such as `spelling bee' are an integral part of natural language (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods.", "method_label": "We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.", "abstract": "Non-compositional phrases such as `red herring' and weakly compositional phrases such as `spelling bee' are an integral part of natural language (Sag, 2002). Non-compositional phrases such as `red herring' and weakly compositional phrases such as `spelling bee' are an integral part of natural language (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Non-compositional phrases such as `red herring' and weakly compositional phrases such as `spelling bee' are an integral part of natural language (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings."}, {"paper_id": "12698795", "adju_relevance": 0, "title": "Markov logic networks", "background_label": "A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause).", "method_label": "Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques.", "result_label": "Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.", "abstract": " A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach."}, {"paper_id": "13364281", "adju_relevance": 0, "title": "How we BLESSed distributional semantic evaluation", "background_label": "AbstractWe introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples.", "abstract": "AbstractWe introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. AbstractWe introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples."}, {"paper_id": "202565468", "adju_relevance": 0, "title": "Which of My Transient Type Checks Are Not (Almost) Free?", "background_label": "One form of type checking used in gradually typed language is transient type checking: whenever an object 'flows' through code with a type annotation, the object is dynamically checked to ensure it has the methods required by the annotation. Just-in-time compilation and optimisation in virtual machines can eliminate much of the overhead of run-time transient type checks. Unfortunately this optimisation is not uniform: some type checks will significantly decrease, or even increase, a program's performance.", "method_label": "In this paper, we refine the so called\"Takikawa\"protocol, and use it to identify which type annotations have the greatest effects on performance. In particular, we show how graphing the performance of such benchmarks when varying which type annotations are present in the source code can be used to discern potential patterns in performance. We demonstrate our approach by testing the Moth virtual machine: for many of the benchmarks where Moth's transient type checking impacts performance, we have been able to identify one or two specific type annotations that are the likely cause.", "result_label": "Without these type annotations, the performance impact of transient type checking becomes negligible. Using our technique programmers can optimise programs by removing expensive type checks, and VM engineers can identify new opportunities for compiler optimisation.", "abstract": "One form of type checking used in gradually typed language is transient type checking: whenever an object 'flows' through code with a type annotation, the object is dynamically checked to ensure it has the methods required by the annotation. One form of type checking used in gradually typed language is transient type checking: whenever an object 'flows' through code with a type annotation, the object is dynamically checked to ensure it has the methods required by the annotation. Just-in-time compilation and optimisation in virtual machines can eliminate much of the overhead of run-time transient type checks. One form of type checking used in gradually typed language is transient type checking: whenever an object 'flows' through code with a type annotation, the object is dynamically checked to ensure it has the methods required by the annotation. Just-in-time compilation and optimisation in virtual machines can eliminate much of the overhead of run-time transient type checks. Unfortunately this optimisation is not uniform: some type checks will significantly decrease, or even increase, a program's performance. In this paper, we refine the so called\"Takikawa\"protocol, and use it to identify which type annotations have the greatest effects on performance. In this paper, we refine the so called\"Takikawa\"protocol, and use it to identify which type annotations have the greatest effects on performance. In particular, we show how graphing the performance of such benchmarks when varying which type annotations are present in the source code can be used to discern potential patterns in performance. In this paper, we refine the so called\"Takikawa\"protocol, and use it to identify which type annotations have the greatest effects on performance. In particular, we show how graphing the performance of such benchmarks when varying which type annotations are present in the source code can be used to discern potential patterns in performance. We demonstrate our approach by testing the Moth virtual machine: for many of the benchmarks where Moth's transient type checking impacts performance, we have been able to identify one or two specific type annotations that are the likely cause. Without these type annotations, the performance impact of transient type checking becomes negligible. Without these type annotations, the performance impact of transient type checking becomes negligible. Using our technique programmers can optimise programs by removing expensive type checks, and VM engineers can identify new opportunities for compiler optimisation."}, {"paper_id": "14418928", "adju_relevance": 0, "title": "Why not, WINE?: towards answering why-not questions in social image search", "background_label": "Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. However, as humans, such why-not questions are natural when expected images are missing in the query results returned by a TagIR system. Clearly, it would be very helpful to users if they could pose follow-up why-not questions to seek clarifications on missing images in query results.", "abstract": "Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. However, as humans, such why-not questions are natural when expected images are missing in the query results returned by a TagIR system. Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. However, as humans, such why-not questions are natural when expected images are missing in the query results returned by a TagIR system. Clearly, it would be very helpful to users if they could pose follow-up why-not questions to seek clarifications on missing images in query results."}, {"paper_id": "86395766", "adju_relevance": 0, "title": "The Negation of Basic Probability Assignment", "background_label": "In many cases, we obtain information using various methods in order to make better decisions. The everything in nature and society has its negative, the negation of negation has significant meaning. Considering the problem from two aspects, we can get more accurate information. However, in most cases, the information of negation is ignored. However, existing negation method mainly apply to probability distribution. How to get the negation of basic probability assignment (BPA) in Dempster-Shafer (D-S) theory is still an open issue.", "result_label": "Hence, the negation provides a new view to obtain information. Finally, practical application is used to be discussed the application of proposed method.", "method_label": "The paper proposed the new negation method of BPA. Besides, some numerical examples are given to this approach for better understanding. Moreover, in order to demonstrate the efficiency of the proposed method, the paper compared the changes of uncertainty between original and negation by using some uncertain measurement methods.", "abstract": "In many cases, we obtain information using various methods in order to make better decisions. In many cases, we obtain information using various methods in order to make better decisions. The everything in nature and society has its negative, the negation of negation has significant meaning. In many cases, we obtain information using various methods in order to make better decisions. The everything in nature and society has its negative, the negation of negation has significant meaning. Considering the problem from two aspects, we can get more accurate information. In many cases, we obtain information using various methods in order to make better decisions. The everything in nature and society has its negative, the negation of negation has significant meaning. Considering the problem from two aspects, we can get more accurate information. However, in most cases, the information of negation is ignored. Hence, the negation provides a new view to obtain information. In many cases, we obtain information using various methods in order to make better decisions. The everything in nature and society has its negative, the negation of negation has significant meaning. Considering the problem from two aspects, we can get more accurate information. However, in most cases, the information of negation is ignored. However, existing negation method mainly apply to probability distribution. In many cases, we obtain information using various methods in order to make better decisions. The everything in nature and society has its negative, the negation of negation has significant meaning. Considering the problem from two aspects, we can get more accurate information. However, in most cases, the information of negation is ignored. However, existing negation method mainly apply to probability distribution. How to get the negation of basic probability assignment (BPA) in Dempster-Shafer (D-S) theory is still an open issue. The paper proposed the new negation method of BPA. The paper proposed the new negation method of BPA. Besides, some numerical examples are given to this approach for better understanding. The paper proposed the new negation method of BPA. Besides, some numerical examples are given to this approach for better understanding. Moreover, in order to demonstrate the efficiency of the proposed method, the paper compared the changes of uncertainty between original and negation by using some uncertain measurement methods. Hence, the negation provides a new view to obtain information. Finally, practical application is used to be discussed the application of proposed method."}, {"paper_id": "4756608", "adju_relevance": 0, "title": "Selective correlations; not voodoo.", "background_label": "The problem of \"voodoo\" correlations-exceptionally high observed correlations in selected regions of the brain-is well recognized in neuroimaging. It arises when quantities of interest are estimated from the same data that was used to select them as interesting. In statistical terminology, the problem of inference following selection from the same data is that of selective inference. They do so adaptively, in that they coincide with the standard CIs when far away from the selection point. We complement existing analytic proofs with a simulation, showing that the proposed intervals control the FCR in realistic social neuroscience problems.", "method_label": "Motivated by the unwelcome side-effects of splitting the data- the recommended remedy-we adapt the recent developments in selective inference in order to construct confidence intervals (CIs) with good reproducibility prospects, even if selection and estimation are done with the same data. These intervals control the expected proportion of non-covered correlations in the selected voxels-the False Coverage Rate (FCR). We also suggest a \"confidence calibration plot\", to allow the intervals to be reported in a clear and interpretable way. Applying the proposed methodology on a loss-aversion study, we demonstrate that with the sample size and selection type employed, selection bias is considerable. Finally, selective intervals are compared to the currently recommended data-splitting approach.", "result_label": "They extend further toward zero than standard intervals, thus attenuating the impression made by highly biased observed correlations. We discover that our approach has more power and typically more informative, as no data is discarded. Computation of the intervals is implemented in an accompanying software package.", "abstract": "The problem of \"voodoo\" correlations-exceptionally high observed correlations in selected regions of the brain-is well recognized in neuroimaging. The problem of \"voodoo\" correlations-exceptionally high observed correlations in selected regions of the brain-is well recognized in neuroimaging. It arises when quantities of interest are estimated from the same data that was used to select them as interesting. The problem of \"voodoo\" correlations-exceptionally high observed correlations in selected regions of the brain-is well recognized in neuroimaging. It arises when quantities of interest are estimated from the same data that was used to select them as interesting. In statistical terminology, the problem of inference following selection from the same data is that of selective inference. Motivated by the unwelcome side-effects of splitting the data- the recommended remedy-we adapt the recent developments in selective inference in order to construct confidence intervals (CIs) with good reproducibility prospects, even if selection and estimation are done with the same data. Motivated by the unwelcome side-effects of splitting the data- the recommended remedy-we adapt the recent developments in selective inference in order to construct confidence intervals (CIs) with good reproducibility prospects, even if selection and estimation are done with the same data. These intervals control the expected proportion of non-covered correlations in the selected voxels-the False Coverage Rate (FCR). They extend further toward zero than standard intervals, thus attenuating the impression made by highly biased observed correlations. The problem of \"voodoo\" correlations-exceptionally high observed correlations in selected regions of the brain-is well recognized in neuroimaging. It arises when quantities of interest are estimated from the same data that was used to select them as interesting. In statistical terminology, the problem of inference following selection from the same data is that of selective inference. They do so adaptively, in that they coincide with the standard CIs when far away from the selection point. The problem of \"voodoo\" correlations-exceptionally high observed correlations in selected regions of the brain-is well recognized in neuroimaging. It arises when quantities of interest are estimated from the same data that was used to select them as interesting. In statistical terminology, the problem of inference following selection from the same data is that of selective inference. They do so adaptively, in that they coincide with the standard CIs when far away from the selection point. We complement existing analytic proofs with a simulation, showing that the proposed intervals control the FCR in realistic social neuroscience problems. Motivated by the unwelcome side-effects of splitting the data- the recommended remedy-we adapt the recent developments in selective inference in order to construct confidence intervals (CIs) with good reproducibility prospects, even if selection and estimation are done with the same data. These intervals control the expected proportion of non-covered correlations in the selected voxels-the False Coverage Rate (FCR). We also suggest a \"confidence calibration plot\", to allow the intervals to be reported in a clear and interpretable way. Motivated by the unwelcome side-effects of splitting the data- the recommended remedy-we adapt the recent developments in selective inference in order to construct confidence intervals (CIs) with good reproducibility prospects, even if selection and estimation are done with the same data. These intervals control the expected proportion of non-covered correlations in the selected voxels-the False Coverage Rate (FCR). We also suggest a \"confidence calibration plot\", to allow the intervals to be reported in a clear and interpretable way. Applying the proposed methodology on a loss-aversion study, we demonstrate that with the sample size and selection type employed, selection bias is considerable. Motivated by the unwelcome side-effects of splitting the data- the recommended remedy-we adapt the recent developments in selective inference in order to construct confidence intervals (CIs) with good reproducibility prospects, even if selection and estimation are done with the same data. These intervals control the expected proportion of non-covered correlations in the selected voxels-the False Coverage Rate (FCR). We also suggest a \"confidence calibration plot\", to allow the intervals to be reported in a clear and interpretable way. Applying the proposed methodology on a loss-aversion study, we demonstrate that with the sample size and selection type employed, selection bias is considerable. Finally, selective intervals are compared to the currently recommended data-splitting approach. They extend further toward zero than standard intervals, thus attenuating the impression made by highly biased observed correlations. We discover that our approach has more power and typically more informative, as no data is discarded. They extend further toward zero than standard intervals, thus attenuating the impression made by highly biased observed correlations. We discover that our approach has more power and typically more informative, as no data is discarded. Computation of the intervals is implemented in an accompanying software package."}, {"paper_id": "16758049", "adju_relevance": 0, "title": "A Label Semantics Approach to Linguistic Hedges", "background_label": "We introduce a model for the linguistic hedges `very' and `quite' within the label semantics framework, and combined with the prototype and conceptual spaces theories of concepts.", "method_label": "The proposed model emerges naturally from the representational framework we use and as such, has a clear semantic grounding.", "result_label": "We give generalisations of these hedge models and show that they can be composed with themselves and with other functions, going on to examine their behaviour in the limit of composition.", "abstract": "We introduce a model for the linguistic hedges `very' and `quite' within the label semantics framework, and combined with the prototype and conceptual spaces theories of concepts. The proposed model emerges naturally from the representational framework we use and as such, has a clear semantic grounding. We give generalisations of these hedge models and show that they can be composed with themselves and with other functions, going on to examine their behaviour in the limit of composition."}, {"paper_id": "118636771", "adju_relevance": 0, "title": "Locality of not-so-weak coloring", "background_label": "Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Recently, Chang et al. However, it is currently poorly understood where this jump takes place when one looks at defective colorings. To study this question, we define $k$-partial $c$-coloring as follows: nodes are labeled with numbers between $1$ and $c$, and every node is incident to at least $k$ properly colored edges. It is known that $1$-partial $2$-coloring (a.k.a. weak $2$-coloring) is easy for any $d \\ge 1$.", "method_label": "(2016) showed that in bounded-degree graphs, every LCL problem belongs to one of the following classes: -\"Easy\": solvable in $O(\\log^* n)$ rounds with both deterministic and randomized distributed algorithms. -\"Hard\": requires at least $\\Omega(\\log n)$ rounds with deterministic and $\\Omega(\\log \\log n)$ rounds with randomized distributed algorithms. Hence for any parameterized LCL problem, when we move from local problems towards global problems, there is some point at which complexity suddenly jumps from easy to hard. As our main result, we show that $k$-partial $2$-coloring becomes hard as soon as $k \\ge 2$, no matter how large a $d$ we have.", "result_label": "For example, for vertex coloring in $d$-regular graphs it is now known that this jump is at precisely $d$ colors: coloring with $d+1$ colors is easy, while coloring with $d$ colors is hard. We also show that this is fundamentally different from $k$-partial $3$-coloring: no matter which $k \\ge 3$ we choose, the problem is always hard for $d = k$ but it becomes easy when $d \\gg k$. The same was known previously for partial $c$-coloring with $c \\ge 4$, but the case of $c<4$ was open.", "abstract": "Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Recently, Chang et al. (2016) showed that in bounded-degree graphs, every LCL problem belongs to one of the following classes: -\"Easy\": solvable in $O(\\log^* n)$ rounds with both deterministic and randomized distributed algorithms. (2016) showed that in bounded-degree graphs, every LCL problem belongs to one of the following classes: -\"Easy\": solvable in $O(\\log^* n)$ rounds with both deterministic and randomized distributed algorithms. -\"Hard\": requires at least $\\Omega(\\log n)$ rounds with deterministic and $\\Omega(\\log \\log n)$ rounds with randomized distributed algorithms. (2016) showed that in bounded-degree graphs, every LCL problem belongs to one of the following classes: -\"Easy\": solvable in $O(\\log^* n)$ rounds with both deterministic and randomized distributed algorithms. -\"Hard\": requires at least $\\Omega(\\log n)$ rounds with deterministic and $\\Omega(\\log \\log n)$ rounds with randomized distributed algorithms. Hence for any parameterized LCL problem, when we move from local problems towards global problems, there is some point at which complexity suddenly jumps from easy to hard. For example, for vertex coloring in $d$-regular graphs it is now known that this jump is at precisely $d$ colors: coloring with $d+1$ colors is easy, while coloring with $d$ colors is hard. Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Recently, Chang et al. However, it is currently poorly understood where this jump takes place when one looks at defective colorings. Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Recently, Chang et al. However, it is currently poorly understood where this jump takes place when one looks at defective colorings. To study this question, we define $k$-partial $c$-coloring as follows: nodes are labeled with numbers between $1$ and $c$, and every node is incident to at least $k$ properly colored edges. Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Recently, Chang et al. However, it is currently poorly understood where this jump takes place when one looks at defective colorings. To study this question, we define $k$-partial $c$-coloring as follows: nodes are labeled with numbers between $1$ and $c$, and every node is incident to at least $k$ properly colored edges. It is known that $1$-partial $2$-coloring (a.k.a. Many graph problems are locally checkable: a solution is globally feasible if it looks valid in all constant-radius neighborhoods. This idea is formalized in the concept of locally checkable labelings (LCLs), introduced by Naor and Stockmeyer (1995). Recently, Chang et al. However, it is currently poorly understood where this jump takes place when one looks at defective colorings. To study this question, we define $k$-partial $c$-coloring as follows: nodes are labeled with numbers between $1$ and $c$, and every node is incident to at least $k$ properly colored edges. It is known that $1$-partial $2$-coloring (a.k.a. weak $2$-coloring) is easy for any $d \\ge 1$. (2016) showed that in bounded-degree graphs, every LCL problem belongs to one of the following classes: -\"Easy\": solvable in $O(\\log^* n)$ rounds with both deterministic and randomized distributed algorithms. -\"Hard\": requires at least $\\Omega(\\log n)$ rounds with deterministic and $\\Omega(\\log \\log n)$ rounds with randomized distributed algorithms. Hence for any parameterized LCL problem, when we move from local problems towards global problems, there is some point at which complexity suddenly jumps from easy to hard. As our main result, we show that $k$-partial $2$-coloring becomes hard as soon as $k \\ge 2$, no matter how large a $d$ we have. For example, for vertex coloring in $d$-regular graphs it is now known that this jump is at precisely $d$ colors: coloring with $d+1$ colors is easy, while coloring with $d$ colors is hard. We also show that this is fundamentally different from $k$-partial $3$-coloring: no matter which $k \\ge 3$ we choose, the problem is always hard for $d = k$ but it becomes easy when $d \\gg k$. For example, for vertex coloring in $d$-regular graphs it is now known that this jump is at precisely $d$ colors: coloring with $d+1$ colors is easy, while coloring with $d$ colors is hard. We also show that this is fundamentally different from $k$-partial $3$-coloring: no matter which $k \\ge 3$ we choose, the problem is always hard for $d = k$ but it becomes easy when $d \\gg k$. The same was known previously for partial $c$-coloring with $c \\ge 4$, but the case of $c<4$ was open."}, {"paper_id": "118988729", "adju_relevance": 0, "title": "A Microphotonic Astrocomb", "background_label": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.", "method_label": "Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.", "result_label": "As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.", "abstract": "One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers. It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources. A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer. As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research."}, {"paper_id": "5917203", "adju_relevance": 0, "title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "background_label": "We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, for which we rely on the algebra of Pregroups, introduced by Lambek.", "abstract": "We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, for which we rely on the algebra of Pregroups, introduced by Lambek."}, {"paper_id": "195866186", "adju_relevance": 0, "title": "Abstraction mechanisms in discrete-event inductive modeling", "background_label": "The power of abstraction lies in its ability to deal with \"lack\" of knowledge. In this regard, success in modeling and simulation rests on discovering useful abstractions that can support objectives of modeling. In our treatment, we refer to \"data abstraction\" as opposed to \"structure simplification\" since we consider a system's behavior rather than its structure. A system's behavior can be represented as time varying input/output segments.", "method_label": "Given the behavior of a causal, time-invariant system, we define some basic abstraction mechanisms to support inductive modeling. The basis for these abstraction mechanisms are a set of general assumptions which allow consistent abstraction of IO segments. Then, given these assumptions and non-monotonic reasoning paradigm, capable of handling them, we try to tackle the fundamental problem of insufficient knowledge in the realm of inductive modeling.", "result_label": "In this way, by making useful abstractions, we can predict a system's unobserved behavior according to a well-defined framework of discrete-event inductive modeling.", "abstract": "The power of abstraction lies in its ability to deal with \"lack\" of knowledge. The power of abstraction lies in its ability to deal with \"lack\" of knowledge. In this regard, success in modeling and simulation rests on discovering useful abstractions that can support objectives of modeling. The power of abstraction lies in its ability to deal with \"lack\" of knowledge. In this regard, success in modeling and simulation rests on discovering useful abstractions that can support objectives of modeling. In our treatment, we refer to \"data abstraction\" as opposed to \"structure simplification\" since we consider a system's behavior rather than its structure. The power of abstraction lies in its ability to deal with \"lack\" of knowledge. In this regard, success in modeling and simulation rests on discovering useful abstractions that can support objectives of modeling. In our treatment, we refer to \"data abstraction\" as opposed to \"structure simplification\" since we consider a system's behavior rather than its structure. A system's behavior can be represented as time varying input/output segments. Given the behavior of a causal, time-invariant system, we define some basic abstraction mechanisms to support inductive modeling. Given the behavior of a causal, time-invariant system, we define some basic abstraction mechanisms to support inductive modeling. The basis for these abstraction mechanisms are a set of general assumptions which allow consistent abstraction of IO segments. Given the behavior of a causal, time-invariant system, we define some basic abstraction mechanisms to support inductive modeling. The basis for these abstraction mechanisms are a set of general assumptions which allow consistent abstraction of IO segments. Then, given these assumptions and non-monotonic reasoning paradigm, capable of handling them, we try to tackle the fundamental problem of insufficient knowledge in the realm of inductive modeling. In this way, by making useful abstractions, we can predict a system's unobserved behavior according to a well-defined framework of discrete-event inductive modeling."}, {"paper_id": "2752001", "adju_relevance": 0, "title": "On the Properties of Metamodeling in OWL", "background_label": "A common practice in conceptual modeling is to separate the intensional from the extensional model. Although very intuitive, this approach is inadequate for many complex domains, where the borderline between the two models is not clear-cut. Therefore, OWL-Full, the most expressive of the Semantic Web ontology languages, allows combining the intensional and the extensional model by a feature we refer to as metamodeling.", "method_label": "In this paper, we show that the semantics of metamodeling adopted in OWL-Full leads to undecidability of basic inference problems, due to free mixing of logical and metalogical symbols. Based on this result, we propose two alternative semantics for metamodeling: the contextual and the HiLog semantics. We show that SHOIQ- a description logic underlying OWL-DL- extended with metamodeling under either semantics is decidable.", "result_label": "Finally, we show how the latter semantics can be used in practice to axiomatize the logical interaction between concepts and metaconcepts.", "abstract": "A common practice in conceptual modeling is to separate the intensional from the extensional model. A common practice in conceptual modeling is to separate the intensional from the extensional model. Although very intuitive, this approach is inadequate for many complex domains, where the borderline between the two models is not clear-cut. A common practice in conceptual modeling is to separate the intensional from the extensional model. Although very intuitive, this approach is inadequate for many complex domains, where the borderline between the two models is not clear-cut. Therefore, OWL-Full, the most expressive of the Semantic Web ontology languages, allows combining the intensional and the extensional model by a feature we refer to as metamodeling. In this paper, we show that the semantics of metamodeling adopted in OWL-Full leads to undecidability of basic inference problems, due to free mixing of logical and metalogical symbols. In this paper, we show that the semantics of metamodeling adopted in OWL-Full leads to undecidability of basic inference problems, due to free mixing of logical and metalogical symbols. Based on this result, we propose two alternative semantics for metamodeling: the contextual and the HiLog semantics. In this paper, we show that the semantics of metamodeling adopted in OWL-Full leads to undecidability of basic inference problems, due to free mixing of logical and metalogical symbols. Based on this result, we propose two alternative semantics for metamodeling: the contextual and the HiLog semantics. We show that SHOIQ- a description logic underlying OWL-DL- extended with metamodeling under either semantics is decidable. Finally, we show how the latter semantics can be used in practice to axiomatize the logical interaction between concepts and metaconcepts."}, {"paper_id": "118916966", "adju_relevance": 0, "title": "Paraconsistent Vagueness: Why Not?", "background_label": "The idea that the phenomenon of vagueness might be modelled by a paraconsistent logic has been little discussed in contemporary work on vagueness, just as the idea that paraconsistent logics might be fruitfully applied to the phenomenon of vagueness has been little discussed in contemporary work on paraconsistency. This is prima facie surprising given that the earliest formalisations of paraconsistent logics presented in Jaskowski and Hallden were presented as logics of vagueness. One possible explanation for this is that, despite initial advocacy by pioneers of paraconsistency, the prospects for a paraconsistent account of vagueness are so poor as to warrant little further consideration.", "method_label": "In this paper we look at the reasons that might be offered in defence of this negative claim.", "result_label": "As we shall show, they are far from compelling. Paraconsistent accounts of vagueness deserve further attention.", "abstract": "The idea that the phenomenon of vagueness might be modelled by a paraconsistent logic has been little discussed in contemporary work on vagueness, just as the idea that paraconsistent logics might be fruitfully applied to the phenomenon of vagueness has been little discussed in contemporary work on paraconsistency. The idea that the phenomenon of vagueness might be modelled by a paraconsistent logic has been little discussed in contemporary work on vagueness, just as the idea that paraconsistent logics might be fruitfully applied to the phenomenon of vagueness has been little discussed in contemporary work on paraconsistency. This is prima facie surprising given that the earliest formalisations of paraconsistent logics presented in Jaskowski and Hallden were presented as logics of vagueness. The idea that the phenomenon of vagueness might be modelled by a paraconsistent logic has been little discussed in contemporary work on vagueness, just as the idea that paraconsistent logics might be fruitfully applied to the phenomenon of vagueness has been little discussed in contemporary work on paraconsistency. This is prima facie surprising given that the earliest formalisations of paraconsistent logics presented in Jaskowski and Hallden were presented as logics of vagueness. One possible explanation for this is that, despite initial advocacy by pioneers of paraconsistency, the prospects for a paraconsistent account of vagueness are so poor as to warrant little further consideration. In this paper we look at the reasons that might be offered in defence of this negative claim. As we shall show, they are far from compelling. As we shall show, they are far from compelling. Paraconsistent accounts of vagueness deserve further attention."}]