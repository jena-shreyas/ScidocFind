{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-06T19:43:24.902740Z","iopub.execute_input":"2023-03-06T19:43:24.903579Z","iopub.status.idle":"2023-03-06T19:43:24.930248Z","shell.execute_reply.started":"2023-03-06T19:43:24.903530Z","shell.execute_reply":"2023-03-06T19:43:24.929150Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/parsed-annotations/parsed_annotations/11844559-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/6173686-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/53082542-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/11629674-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/10695055-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/5052952-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/8781666-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/2090262-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1587-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/52194540-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/7898033-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/80628431-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/5270848-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1791179-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/929877-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/189897839-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/6431039-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/8781666-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/3264891-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/5764728-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/11310392-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/51977123-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/10014168-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/2865563-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1936997-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/52194540-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/2468783-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/2468783-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1587-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/13949438-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1936997-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1198964-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1306065-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/10052042-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/80628431-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/2360770-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/53080736-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/5052952-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/174799296-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/189897839-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1791179-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/53080736-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/174799296-result-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/10010426-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/1198964-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/929877-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/11844559-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/102353905-method-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/6431039-background-adju.json\n/kaggle/input/parsed-annotations/parsed_annotations/3264891-background-adju.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport math\nimport json\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-03-07T17:58:17.123609Z","iopub.execute_input":"2023-03-07T17:58:17.124391Z","iopub.status.idle":"2023-03-07T17:58:22.227216Z","shell.execute_reply.started":"2023-03-07T17:58:17.124361Z","shell.execute_reply":"2023-03-07T17:58:22.225855Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import *\n\ntokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\nmodel = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', output_attentions=False, output_hidden_states=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T17:58:22.229372Z","iopub.execute_input":"2023-03-07T17:58:22.229913Z","iopub.status.idle":"2023-03-07T18:00:30.375249Z","shell.execute_reply.started":"2023-03-07T17:58:22.229874Z","shell.execute_reply":"2023-03-07T18:00:30.374071Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/generation_utils.py:27: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/generation_tf_utils.py:27: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/generation_flax_utils.py:27: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n  FutureWarning,\nLoading custom CUDA kernels...\nCould not locate the tokenizer configuration file, will try to use the model config instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68694e86c2f64bcdb00d565ba26aacd9"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 31090\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c273b023de2e4bad8dd0b0eac380a5a0"}},"metadata":{}},{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/vocab.txt\nloading file tokenizer.json from cache at None\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 31090\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 31090\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 31090\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fccb34816c504b61be3a56c820bd2f9b"}},"metadata":{}},{"name":"stderr","text":"loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/pytorch_model.bin\nSome weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_rankings(ip_dir, file, max_length):\n    input_dir = ip_dir\n    filename = file\n    query_facet = filename.split('-')[1]\n    rankings = dict()\n    max_length = max_length\n\n    with open(input_dir + filename, 'r') as f:\n        annot = json.load(f)\n        query_dict = annot[0]\n        candidate_dicts = annot[1:]\n        candidate_relevances = [dict_['adju_relevance'] for dict_ in candidate_dicts]\n        \n        query_text = query_dict[query_facet + '_label']\n        candi_texts = list()\n        candi_encodings = list()\n        candi_ratings = list()\n        for candi_dict in candidate_dicts:\n            try:\n                candi_texts.append(candi_dict[query_facet+'_label'])\n            except:\n                candi_texts.append('')\n\n        query_encoding = tokenizer(query_text, padding='max_length', return_tensors='pt')\n        query_dense_output = model(**query_encoding)['pooler_output'].flatten()\n#         print(query_dense_output.size())\n        csim = nn.CosineSimilarity(dim=0)\n\n        for text in candi_texts:\n            encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n            candi_output = model(**encoding)['pooler_output'].flatten()\n#             print(candi_output.size())\n            candi_encodings.append(candi_output)\n            rating = csim(query_dense_output, candi_output)\n            candi_ratings.append(rating)\n            print(rating)\n\n        sorted_indices = torch.argsort(torch.tensor(candi_ratings), descending=True)\n        rankings = {i: int(docid) for i, docid in enumerate(sorted_indices)}\n        candi_relevances = [candidate_relevances[i] for i in sorted_indices]\n        return rankings, candi_relevances\n#         except:\n#             print(\"Facet : {}, Dict : {}\".format(query_facet, query_dict.keys()))\n#             return None","metadata":{"execution":{"iopub.status.busy":"2023-03-07T18:47:40.345722Z","iopub.execute_input":"2023-03-07T18:47:40.346482Z","iopub.status.idle":"2023-03-07T18:47:40.359175Z","shell.execute_reply.started":"2023-03-07T18:47:40.346443Z","shell.execute_reply":"2023-03-07T18:47:40.358437Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"input_dir = '/kaggle/input/parsed-annotations/parsed_annotations/'\nmax_length = 400\n\nall_files  = os.listdir(input_dir)\nfilenames = [file for file in all_files if os.path.isfile(os.path.join(input_dir, file))]\nresults = dict()\nfor i, filename in enumerate(filenames):\n    paper_id = filename.split('-')[0]\n    if i > 0:\n        break\n    try:\n        rankings, relevances = compute_rankings(input_dir, filename, max_length)\n        results[paper_id] = {'rankings':rankings, 'relevances':relevances}\n    except:\n        results[paper_id] = None","metadata":{"execution":{"iopub.status.busy":"2023-03-07T18:51:17.538062Z","iopub.execute_input":"2023-03-07T18:51:17.538803Z","iopub.status.idle":"2023-03-07T18:51:31.479351Z","shell.execute_reply.started":"2023-03-07T18:51:17.538768Z","shell.execute_reply":"2023-03-07T18:51:31.478177Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"tensor(0.6459, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.5658, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.9161, grad_fn=<SumBackward1>)\ntensor(0.3507, grad_fn=<SumBackward1>)\ntensor(0.2248, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.3946, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.8215, grad_fn=<SumBackward1>)\ntensor(0.6289, grad_fn=<SumBackward1>)\ntensor(0.4543, grad_fn=<SumBackward1>)\ntensor(0.3009, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.2440, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6956, grad_fn=<SumBackward1>)\ntensor(0.8753, grad_fn=<SumBackward1>)\ntensor(0.8893, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.9258, grad_fn=<SumBackward1>)\ntensor(0.8713, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.4863, grad_fn=<SumBackward1>)\ntensor(0.7868, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.3232, grad_fn=<SumBackward1>)\ntensor(0.9192, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6273, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.8033, grad_fn=<SumBackward1>)\ntensor(0.6295, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.4601, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.1286, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.3547, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6132, grad_fn=<SumBackward1>)\ntensor(0.9344, grad_fn=<SumBackward1>)\ntensor(0.2992, grad_fn=<SumBackward1>)\ntensor(0.9227, grad_fn=<SumBackward1>)\ntensor(0.3613, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.3462, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.9288, grad_fn=<SumBackward1>)\ntensor(0.6947, grad_fn=<SumBackward1>)\ntensor(0.8152, grad_fn=<SumBackward1>)\ntensor(0.6446, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.4857, grad_fn=<SumBackward1>)\ntensor(0.9403, grad_fn=<SumBackward1>)\ntensor(0.9418, grad_fn=<SumBackward1>)\ntensor(0.9541, grad_fn=<SumBackward1>)\ntensor(0.3963, grad_fn=<SumBackward1>)\ntensor(0.9212, grad_fn=<SumBackward1>)\ntensor(0.7810, grad_fn=<SumBackward1>)\ntensor(0.2661, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6938, grad_fn=<SumBackward1>)\ntensor(0.4918, grad_fn=<SumBackward1>)\ntensor(0.2713, grad_fn=<SumBackward1>)\ntensor(0.5400, grad_fn=<SumBackward1>)\ntensor(0.6584, grad_fn=<SumBackward1>)\ntensor(0.9107, grad_fn=<SumBackward1>)\ntensor(0.9094, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.8722, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.3122, grad_fn=<SumBackward1>)\ntensor(0.7183, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6714, grad_fn=<SumBackward1>)\ntensor(0.3717, grad_fn=<SumBackward1>)\ntensor(0.4051, grad_fn=<SumBackward1>)\ntensor(0.6684, grad_fn=<SumBackward1>)\ntensor(0.8488, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.2543, grad_fn=<SumBackward1>)\ntensor(0.3077, grad_fn=<SumBackward1>)\ntensor(0.2356, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6816, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.4638, grad_fn=<SumBackward1>)\ntensor(0.6610, grad_fn=<SumBackward1>)\ntensor(0.2501, grad_fn=<SumBackward1>)\ntensor(0.1148, grad_fn=<SumBackward1>)\ntensor(0.6847, grad_fn=<SumBackward1>)\ntensor(0.9328, grad_fn=<SumBackward1>)\ntensor(0.9308, grad_fn=<SumBackward1>)\ntensor(0.3043, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.9029, grad_fn=<SumBackward1>)\ntensor(0.4865, grad_fn=<SumBackward1>)\ntensor(0.6674, grad_fn=<SumBackward1>)\ntensor(0.8291, grad_fn=<SumBackward1>)\ntensor(0.6939, grad_fn=<SumBackward1>)\ntensor(0.7235, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.3035, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\ntensor(0.6514, grad_fn=<SumBackward1>)\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('ranking_relevances.json', 'w') as f:\n    json.dump(results, f)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T18:51:42.995583Z","iopub.execute_input":"2023-03-07T18:51:42.996011Z","iopub.status.idle":"2023-03-07T18:51:43.006736Z","shell.execute_reply.started":"2023-03-07T18:51:42.995963Z","shell.execute_reply":"2023-03-07T18:51:43.005561Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}